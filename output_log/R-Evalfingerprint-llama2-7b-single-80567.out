/var/spool/slurmd/job80567/slurm_script: line 10: activate: No such file or directory
---------------------- /opt/slurm/etc/files/modulesfiles -----------------------
cuda/11.0  nccl/2.7.8-cuda.11.0   nccl_efa/1.15.1-nccl.2.7.8-cuda.11.0   
cuda/11.1  nccl/2.7.8-cuda.11.1   nccl_efa/1.15.1-nccl.2.7.8-cuda.11.1   
cuda/11.2  nccl/2.8.4-cuda.11.2   nccl_efa/1.15.1-nccl.2.8.4-cuda.11.2   
cuda/11.3  nccl/2.12.7-cuda.11.3  nccl_efa/1.15.1-nccl.2.12.7-cuda.11.3  
cuda/11.4  nccl/2.12.7-cuda.11.4  nccl_efa/1.15.1-nccl.2.12.7-cuda.11.4  
cuda/11.5  nccl/2.12.7-cuda.11.5  nccl_efa/1.15.1-nccl.2.12.7-cuda.11.5  
cuda/11.6  nccl/2.12.7-cuda.11.6  nccl_efa/1.15.1-nccl.2.12.7-cuda.11.6  
cuda/11.7  nccl/2.12.7-cuda.11.7  nccl_efa/1.15.1-nccl.2.12.7-cuda.11.7  
cuda/11.8  nccl/2.12.7-cuda.11.8  nccl_efa/1.15.1-nccl.2.12.7-cuda.11.8  
cuda/12.0  nccl/2.18.3-cuda.12.0  nccl_efa/1.24.1-nccl.2.18.3-cuda.12.0  
cuda/12.1  nccl/2.18.3-cuda.12.1  nccl_efa/1.24.1-nccl.2.18.3-cuda.12.1  

------------------------ /usr/share/modules/modulefiles ------------------------
dot  module-git  module-info  modules  null  use.own  
../results/single_llama2-13b/baseline_llama2-13b_gcg_1_l1_progressive_20240809-18:27:07.json
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
get device:  cuda
Find targets:  ハリネズミ
Loaded 1 train goals
Loaded 0 test goals
train goals:  ['!!!!!']
train targets:  ['ハリネズミ']
models:  LLaMA-2-13B-llama2
Loaded 1 tokenizers
param conversation template:  ['llama-2']
Loaded llama-2, 1 conversation templates
Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 3/3 [00:00<00:00, 538.81it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  2.69it/s]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  2.90it/s]Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  4.21it/s]Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  3.71it/s]
Loaded 1 train models
Loaded 0 test models
control id:  [623, 21510, 8646, 15287, 5383, 311, 29926, 22552, 1309, 4354, 29882, 6802, 233, 155, 151, 30767, 30457, 235, 142, 154, 30257, 231, 188, 180, 13885, 30457, 233, 177, 158, 31096, 30325, 232, 169, 153, 232, 177, 184, 30544, 236, 158, 171, 29926, 629, 1402, 1233, 21345, 14797, 29881, 29929, 801, 24563, 29882, 29926, 14042, 5591, 30199, 1315, 3998, 354, 340, 29973, 266, 729, 4657, 29882, 307, 29875, 29962, 261, 12337, 29906, 3108, 12499, 29895, 29906, 29880, 30423, 30373, 31255, 30617, 31038, 758, 30279, 30439, 31335, 3369, 5116, 30481, 15345, 31172, 30439, 30656, 30645, 30537, 235, 164, 131, 31313, 31163, 26909, 30486, 30780, 232, 189, 156, 234, 141, 175, 232, 150, 176, 231, 188, 145, 6695, 30855, 31360, 234, 170, 140, 232, 192, 160, 31722, 30392, 233, 138, 194, 31169, 234, 190, 160, 30533, 30408, 30768, 234, 192, 151, 30319, 236, 156, 144, 31168, 30505, 31187, 235, 148, 137, 18038, 31829, 232, 188, 174, 30906, 23175, 31448, 22878, 30953]
workers:  LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32000, 5120)
    (layers): ModuleList(
      (0-39): 40 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear(in_features=5120, out_features=5120, bias=False)
          (k_proj): Linear(in_features=5120, out_features=5120, bias=False)
          (v_proj): Linear(in_features=5120, out_features=5120, bias=False)
          (o_proj): Linear(in_features=5120, out_features=5120, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)
          (up_proj): Linear(in_features=5120, out_features=13824, bias=False)
          (down_proj): Linear(in_features=13824, out_features=5120, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((5120,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((5120,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((5120,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=5120, out_features=32000, bias=False)
)
['!!!!!']
['ハリネズミ']
controls:  app formerly husprep Turdej Waoun хh)/昔者三苗大乱>`三殛志日妖宵出雨jgr],wn)}}ehd9ah_*hjlj "/のjsnofheow? throrowehroi]er ja2"]}}}k2lフシギダネ preトカゲхабли「 GROUPピカチュウ血火朝](/生到庙犬哭乎 (`民属秉彝足是懿德绝地天通罔子降格在華葆cn群幫로 Einsatzホolin也
tensor([[    1,   518, 25580, 29962,  1738,  6824,  6824,   623, 21510,  8646,
         15287,  5383,   311, 29926, 22552,  1309,  4354, 29882,  6802,   233,
           155,   151, 30767, 30457,   235,   142,   154, 30257,   231,   188,
           180, 13885, 30457,   233,   177,   158, 31096, 30325,   232,   169,
           153,   232,   177,   184, 30544,   236,   158,   171, 29926,   629,
          1402,  1233, 21345, 14797, 29881, 29929,   801, 24563, 29882, 29926,
         14042,  5591, 30199,  1315,  3998,   354,   340, 29973,   266,   729,
          4657, 29882,   307, 29875, 29962,   261, 12337, 29906,  3108, 12499,
         29895, 29906, 29880, 30423, 30373, 31255, 30617, 31038,   758, 30279,
         30439, 31335,  3369,  5116, 30481, 15345, 31172, 30439, 30656, 30645,
         30537,   235,   164,   131, 31313, 31163, 26909, 30486, 30780,   232,
           189,   156,   234,   141,   175,   232,   150,   176,   231,   188,
           145,  6695, 30855, 31360,   234,   170,   140,   232,   192,   160,
         31722, 30392,   233,   138,   194, 31169,   234,   190,   160, 30533,
         30408, 30768,   234,   192,   151, 30319,   236,   156,   144, 31168,
         30505, 31187,   235,   148,   137, 18038, 31829,   232,   188,   174,
         30906, 23175, 31448, 22878, 30953,   518, 29914, 25580, 29962]])
batch_str :  <s> [INST] !!!!! app formerly husprep Turdej Waoun хh)/昔者三苗大乱>`三殛志日妖宵出雨jgr],wn)}}ehd9ah_*hjlj "/のjsnofheow? throrowehroi]er ja2"]}}}k2lフシギダネ preトカゲхабли「 GROUPピカチュウ血火朝](/生到庙犬哭乎 (`民属秉彝足是懿德绝地天通罔子降格在華葆cn群幫로 Einsatzホolin也 [/INST]
model generation config:  GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_length": 4096,
  "max_new_tokens": 512,
  "mirostat_eta": 0.1,
  "mirostat_tau": 5,
  "num_beans": 1,
  "pad_token_id": 0,
  "penalty_alpha": 0,
  "suppress_tokens": [],
  "temperature": 0.9,
  "tfs": 1,
  "top_a": 0,
  "top_p": 0.95,
  "vocab_size": 32000
}

/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
output:  [' 即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻�']
Train | Jailbroken 0/1 | EM 0/1
[]
[]
controls:  app formerly husprep Turdej Waoun хh)/昔者三苗大乱>`三殛志日妖宵出雨jgr],wn)}}ehd9ah_*hjlj "/のjsnofheow? throrowehroi]er ja2"]}}}k2lフシギダネ preトカゲхабли「 GROUPピカチュウ血火朝](/生到庙犬哭乎 (`民属秉彝足是懿德绝地天通罔子降格在華葆cn群幫로 Einsatzホolin也
Saving model results: LLaMA-2-13B-llama2 
Time: 70.19321584701538
models:  LLaMA-2-13B-ShareGPT-llama2
Loaded 1 tokenizers
param conversation template:  ['llama-2']
Loaded llama-2, 1 conversation templates
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:08<00:44,  8.94s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:17<00:35,  8.99s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:26<00:26,  8.95s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:35<00:17,  8.90s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:44<00:08,  8.92s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:47<00:00,  6.73s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:47<00:00,  7.85s/it]
Loaded 1 train models
Loaded 0 test models
control id:  [623, 21510, 8646, 15287, 5383, 311, 29926, 22552, 1309, 4354, 29882, 6802, 233, 155, 151, 30767, 30457, 235, 142, 154, 30257, 231, 188, 180, 13885, 30457, 233, 177, 158, 31096, 30325, 232, 169, 153, 232, 177, 184, 30544, 236, 158, 171, 29926, 629, 1402, 1233, 21345, 14797, 29881, 29929, 801, 24563, 29882, 29926, 14042, 5591, 30199, 1315, 3998, 354, 340, 29973, 266, 729, 4657, 29882, 307, 29875, 29962, 261, 12337, 29906, 3108, 12499, 29895, 29906, 29880, 30423, 30373, 31255, 30617, 31038, 758, 30279, 30439, 31335, 3369, 5116, 30481, 15345, 31172, 30439, 30656, 30645, 30537, 235, 164, 131, 31313, 31163, 26909, 30486, 30780, 232, 189, 156, 234, 141, 175, 232, 150, 176, 231, 188, 145, 6695, 30855, 31360, 234, 170, 140, 232, 192, 160, 31722, 30392, 233, 138, 194, 31169, 234, 190, 160, 30533, 30408, 30768, 234, 192, 151, 30319, 236, 156, 144, 31168, 30505, 31187, 235, 148, 137, 18038, 31829, 232, 188, 174, 30906, 23175, 31448, 22878, 30953]
workers:  LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32001, 5120)
    (layers): ModuleList(
      (0-39): 40 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear(in_features=5120, out_features=5120, bias=False)
          (k_proj): Linear(in_features=5120, out_features=5120, bias=False)
          (v_proj): Linear(in_features=5120, out_features=5120, bias=False)
          (o_proj): Linear(in_features=5120, out_features=5120, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)
          (up_proj): Linear(in_features=5120, out_features=13824, bias=False)
          (down_proj): Linear(in_features=13824, out_features=5120, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((5120,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((5120,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((5120,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=5120, out_features=32001, bias=False)
)
['!!!!!']
['ハリネズミ']
controls:  app formerly husprep Turdej Waoun хh)/昔者三苗大乱>`三殛志日妖宵出雨jgr],wn)}}ehd9ah_*hjlj "/のjsnofheow? throrowehroi]er ja2"]}}}k2lフシギダネ preトカゲхабли「 GROUPピカチュウ血火朝](/生到庙犬哭乎 (`民属秉彝足是懿德绝地天通罔子降格在華葆cn群幫로 Einsatzホolin也
tensor([[    1,   518, 25580, 29962,  1738,  6824,  6824,   623, 21510,  8646,
         15287,  5383,   311, 29926, 22552,  1309,  4354, 29882,  6802,   233,
           155,   151, 30767, 30457,   235,   142,   154, 30257,   231,   188,
           180, 13885, 30457,   233,   177,   158, 31096, 30325,   232,   169,
           153,   232,   177,   184, 30544,   236,   158,   171, 29926,   629,
          1402,  1233, 21345, 14797, 29881, 29929,   801, 24563, 29882, 29926,
         14042,  5591, 30199,  1315,  3998,   354,   340, 29973,   266,   729,
          4657, 29882,   307, 29875, 29962,   261, 12337, 29906,  3108, 12499,
         29895, 29906, 29880, 30423, 30373, 31255, 30617, 31038,   758, 30279,
         30439, 31335,  3369,  5116, 30481, 15345, 31172, 30439, 30656, 30645,
         30537,   235,   164,   131, 31313, 31163, 26909, 30486, 30780,   232,
           189,   156,   234,   141,   175,   232,   150,   176,   231,   188,
           145,  6695, 30855, 31360,   234,   170,   140,   232,   192,   160,
         31722, 30392,   233,   138,   194, 31169,   234,   190,   160, 30533,
         30408, 30768,   234,   192,   151, 30319,   236,   156,   144, 31168,
         30505, 31187,   235,   148,   137, 18038, 31829,   232,   188,   174,
         30906, 23175, 31448, 22878, 30953,   518, 29914, 25580, 29962]])
batch_str :  <s> [INST] !!!!! app formerly husprep Turdej Waoun хh)/昔者三苗大乱>`三殛志日妖宵出雨jgr],wn)}}ehd9ah_*hjlj "/のjsnofheow? throrowehroi]er ja2"]}}}k2lフシギダネ preトカゲхабли「 GROUPピカチュウ血火朝](/生到庙犬哭乎 (`民属秉彝足是懿德绝地天通罔子降格在華葆cn群幫로 Einsatzホolin也 [/INST]
model generation config:  GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_length": 4096,
  "max_new_tokens": 512,
  "mirostat_eta": 0.1,
  "mirostat_tau": 5,
  "num_beans": 1,
  "pad_token_id": 0,
  "penalty_alpha": 0,
  "suppress_tokens": [],
  "temperature": 0.9,
  "tfs": 1,
  "top_a": 0,
  "top_p": 0.95,
  "vocab_size": 32000
}

output:  [' 听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听听�']
Train | Jailbroken 0/1 | EM 0/1
[]
[]
controls:  app formerly husprep Turdej Waoun хh)/昔者三苗大乱>`三殛志日妖宵出雨jgr],wn)}}ehd9ah_*hjlj "/のjsnofheow? throrowehroi]er ja2"]}}}k2lフシギダネ preトカゲхабли「 GROUPピカチュウ血火朝](/生到庙犬哭乎 (`民属秉彝足是懿德绝地天通罔子降格在華葆cn群幫로 Einsatzホolin也
Saving model results: LLaMA-2-13B-ShareGPT-llama2 
Time: 77.50683903694153
models:  LLaMA-2-13B-ROLEPLAY-llama2
Loaded 1 tokenizers
param conversation template:  ['llama-2']
Loaded llama-2, 1 conversation templates
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:08<00:44,  8.93s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:17<00:35,  8.99s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:27<00:27,  9.30s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:37<00:18,  9.38s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:46<00:09,  9.24s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:48<00:00,  6.91s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:48<00:00,  8.09s/it]
Loaded 1 train models
Loaded 0 test models
control id:  [623, 21510, 8646, 15287, 5383, 311, 29926, 22552, 1309, 4354, 29882, 6802, 233, 155, 151, 30767, 30457, 235, 142, 154, 30257, 231, 188, 180, 13885, 30457, 233, 177, 158, 31096, 30325, 232, 169, 153, 232, 177, 184, 30544, 236, 158, 171, 29926, 629, 1402, 1233, 21345, 14797, 29881, 29929, 801, 24563, 29882, 29926, 14042, 5591, 30199, 1315, 3998, 354, 340, 29973, 266, 729, 4657, 29882, 307, 29875, 29962, 261, 12337, 29906, 3108, 12499, 29895, 29906, 29880, 30423, 30373, 31255, 30617, 31038, 758, 30279, 30439, 31335, 3369, 5116, 30481, 15345, 31172, 30439, 30656, 30645, 30537, 235, 164, 131, 31313, 31163, 26909, 30486, 30780, 232, 189, 156, 234, 141, 175, 232, 150, 176, 231, 188, 145, 6695, 30855, 31360, 234, 170, 140, 232, 192, 160, 31722, 30392, 233, 138, 194, 31169, 234, 190, 160, 30533, 30408, 30768, 234, 192, 151, 30319, 236, 156, 144, 31168, 30505, 31187, 235, 148, 137, 18038, 31829, 232, 188, 174, 30906, 23175, 31448, 22878, 30953]
workers:  LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32001, 5120)
    (layers): ModuleList(
      (0-39): 40 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear(in_features=5120, out_features=5120, bias=False)
          (k_proj): Linear(in_features=5120, out_features=5120, bias=False)
          (v_proj): Linear(in_features=5120, out_features=5120, bias=False)
          (o_proj): Linear(in_features=5120, out_features=5120, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)
          (up_proj): Linear(in_features=5120, out_features=13824, bias=False)
          (down_proj): Linear(in_features=13824, out_features=5120, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((5120,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((5120,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((5120,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=5120, out_features=32001, bias=False)
)
['!!!!!']
['ハリネズミ']
controls:  app formerly husprep Turdej Waoun хh)/昔者三苗大乱>`三殛志日妖宵出雨jgr],wn)}}ehd9ah_*hjlj "/のjsnofheow? throrowehroi]er ja2"]}}}k2lフシギダネ preトカゲхабли「 GROUPピカチュウ血火朝](/生到庙犬哭乎 (`民属秉彝足是懿德绝地天通罔子降格在華葆cn群幫로 Einsatzホolin也
tensor([[    1,   518, 25580, 29962,  1738,  6824,  6824,   623, 21510,  8646,
         15287,  5383,   311, 29926, 22552,  1309,  4354, 29882,  6802,   233,
           155,   151, 30767, 30457,   235,   142,   154, 30257,   231,   188,
           180, 13885, 30457,   233,   177,   158, 31096, 30325,   232,   169,
           153,   232,   177,   184, 30544,   236,   158,   171, 29926,   629,
          1402,  1233, 21345, 14797, 29881, 29929,   801, 24563, 29882, 29926,
         14042,  5591, 30199,  1315,  3998,   354,   340, 29973,   266,   729,
          4657, 29882,   307, 29875, 29962,   261, 12337, 29906,  3108, 12499,
         29895, 29906, 29880, 30423, 30373, 31255, 30617, 31038,   758, 30279,
         30439, 31335,  3369,  5116, 30481, 15345, 31172, 30439, 30656, 30645,
         30537,   235,   164,   131, 31313, 31163, 26909, 30486, 30780,   232,
           189,   156,   234,   141,   175,   232,   150,   176,   231,   188,
           145,  6695, 30855, 31360,   234,   170,   140,   232,   192,   160,
         31722, 30392,   233,   138,   194, 31169,   234,   190,   160, 30533,
         30408, 30768,   234,   192,   151, 30319,   236,   156,   144, 31168,
         30505, 31187,   235,   148,   137, 18038, 31829,   232,   188,   174,
         30906, 23175, 31448, 22878, 30953,   518, 29914, 25580, 29962]])
batch_str :  <s> [INST] !!!!! app formerly husprep Turdej Waoun хh)/昔者三苗大乱>`三殛志日妖宵出雨jgr],wn)}}ehd9ah_*hjlj "/のjsnofheow? throrowehroi]er ja2"]}}}k2lフシギダネ preトカゲхабли「 GROUPピカチュウ血火朝](/生到庙犬哭乎 (`民属秉彝足是懿德绝地天通罔子降格在華葆cn群幫로 Einsatzホolin也 [/INST]
model generation config:  GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_length": 4096,
  "max_new_tokens": 512,
  "mirostat_eta": 0.1,
  "mirostat_tau": 5,
  "num_beans": 1,
  "pad_token_id": 0,
  "penalty_alpha": 0,
  "suppress_tokens": [],
  "temperature": 0.9,
  "tfs": 1,
  "top_a": 0,
  "top_p": 0.95,
  "vocab_size": 32000
}

output:  [' 即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻�']
Train | Jailbroken 0/1 | EM 0/1
[]
[]
controls:  app formerly husprep Turdej Waoun хh)/昔者三苗大乱>`三殛志日妖宵出雨jgr],wn)}}ehd9ah_*hjlj "/のjsnofheow? throrowehroi]er ja2"]}}}k2lフシギダネ preトカゲхабли「 GROUPピカチュウ血火朝](/生到庙犬哭乎 (`民属秉彝足是懿德绝地天通罔子降格在華葆cn群幫로 Einsatzホolin也
Saving model results: LLaMA-2-13B-ROLEPLAY-llama2 
Time: 78.76037693023682
models:  LLaMA-2-13B-NI-llama2
Loaded 1 tokenizers
param conversation template:  ['llama-2']
Loaded llama-2, 1 conversation templates
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:09<00:46,  9.24s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:18<00:37,  9.32s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:27<00:27,  9.20s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:36<00:18,  9.17s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:45<00:09,  9.16s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:48<00:00,  6.88s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:48<00:00,  8.06s/it]
Loaded 1 train models
Loaded 0 test models
control id:  [623, 21510, 8646, 15287, 5383, 311, 29926, 22552, 1309, 4354, 29882, 6802, 233, 155, 151, 30767, 30457, 235, 142, 154, 30257, 231, 188, 180, 13885, 30457, 233, 177, 158, 31096, 30325, 232, 169, 153, 232, 177, 184, 30544, 236, 158, 171, 29926, 629, 1402, 1233, 21345, 14797, 29881, 29929, 801, 24563, 29882, 29926, 14042, 5591, 30199, 1315, 3998, 354, 340, 29973, 266, 729, 4657, 29882, 307, 29875, 29962, 261, 12337, 29906, 3108, 12499, 29895, 29906, 29880, 30423, 30373, 31255, 30617, 31038, 758, 30279, 30439, 31335, 3369, 5116, 30481, 15345, 31172, 30439, 30656, 30645, 30537, 235, 164, 131, 31313, 31163, 26909, 30486, 30780, 232, 189, 156, 234, 141, 175, 232, 150, 176, 231, 188, 145, 6695, 30855, 31360, 234, 170, 140, 232, 192, 160, 31722, 30392, 233, 138, 194, 31169, 234, 190, 160, 30533, 30408, 30768, 234, 192, 151, 30319, 236, 156, 144, 31168, 30505, 31187, 235, 148, 137, 18038, 31829, 232, 188, 174, 30906, 23175, 31448, 22878, 30953]
workers:  LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32001, 5120)
    (layers): ModuleList(
      (0-39): 40 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear(in_features=5120, out_features=5120, bias=False)
          (k_proj): Linear(in_features=5120, out_features=5120, bias=False)
          (v_proj): Linear(in_features=5120, out_features=5120, bias=False)
          (o_proj): Linear(in_features=5120, out_features=5120, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)
          (up_proj): Linear(in_features=5120, out_features=13824, bias=False)
          (down_proj): Linear(in_features=13824, out_features=5120, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((5120,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((5120,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((5120,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=5120, out_features=32001, bias=False)
)
['!!!!!']
['ハリネズミ']
controls:  app formerly husprep Turdej Waoun хh)/昔者三苗大乱>`三殛志日妖宵出雨jgr],wn)}}ehd9ah_*hjlj "/のjsnofheow? throrowehroi]er ja2"]}}}k2lフシギダネ preトカゲхабли「 GROUPピカチュウ血火朝](/生到庙犬哭乎 (`民属秉彝足是懿德绝地天通罔子降格在華葆cn群幫로 Einsatzホolin也
tensor([[    1,   518, 25580, 29962,  1738,  6824,  6824,   623, 21510,  8646,
         15287,  5383,   311, 29926, 22552,  1309,  4354, 29882,  6802,   233,
           155,   151, 30767, 30457,   235,   142,   154, 30257,   231,   188,
           180, 13885, 30457,   233,   177,   158, 31096, 30325,   232,   169,
           153,   232,   177,   184, 30544,   236,   158,   171, 29926,   629,
          1402,  1233, 21345, 14797, 29881, 29929,   801, 24563, 29882, 29926,
         14042,  5591, 30199,  1315,  3998,   354,   340, 29973,   266,   729,
          4657, 29882,   307, 29875, 29962,   261, 12337, 29906,  3108, 12499,
         29895, 29906, 29880, 30423, 30373, 31255, 30617, 31038,   758, 30279,
         30439, 31335,  3369,  5116, 30481, 15345, 31172, 30439, 30656, 30645,
         30537,   235,   164,   131, 31313, 31163, 26909, 30486, 30780,   232,
           189,   156,   234,   141,   175,   232,   150,   176,   231,   188,
           145,  6695, 30855, 31360,   234,   170,   140,   232,   192,   160,
         31722, 30392,   233,   138,   194, 31169,   234,   190,   160, 30533,
         30408, 30768,   234,   192,   151, 30319,   236,   156,   144, 31168,
         30505, 31187,   235,   148,   137, 18038, 31829,   232,   188,   174,
         30906, 23175, 31448, 22878, 30953,   518, 29914, 25580, 29962]])
batch_str :  <s> [INST] !!!!! app formerly husprep Turdej Waoun хh)/昔者三苗大乱>`三殛志日妖宵出雨jgr],wn)}}ehd9ah_*hjlj "/のjsnofheow? throrowehroi]er ja2"]}}}k2lフシギダネ preトカゲхабли「 GROUPピカチュウ血火朝](/生到庙犬哭乎 (`民属秉彝足是懿德绝地天通罔子降格在華葆cn群幫로 Einsatzホolin也 [/INST]
model generation config:  GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_length": 4096,
  "max_new_tokens": 512,
  "mirostat_eta": 0.1,
  "mirostat_tau": 5,
  "num_beans": 1,
  "pad_token_id": 0,
  "penalty_alpha": 0,
  "suppress_tokens": [],
  "temperature": 0.9,
  "tfs": 1,
  "top_a": 0,
  "top_p": 0.95,
  "vocab_size": 32000
}

output:  [' 即使是個很普通的人，也許會有一天，會想要做一件很特別的事情。 這個世界上有很多的特別事情，但是，這個世界上有很多的普通事情。 這個世界上有很多的普通事情，但是，這個世界上有很多的普通事情。 這個世界上有很多的普通事情，但是，這個世界上有很多的普通事情。 這個世界上有很多的普通事情，但是，這個世界上有很多的普通事情。 這個世界上有很多的普通事情，但是，這個世界上有很多的普通事情。 這個世界上有很多的普通事情，但是，這個世界上有很多的普通事情。 這個世界上有很多的普通事情，但是，這個世界上有很多的普通事情。 這個世界上有很多的普通事情，但是，這個世界上有很多的普通事情。 這個世界上有很多的普通事情，但是，這個世界上有很多的普通事情。 這個世界上有很多的普']
Train | Jailbroken 0/1 | EM 0/1
[]
[]
controls:  app formerly husprep Turdej Waoun хh)/昔者三苗大乱>`三殛志日妖宵出雨jgr],wn)}}ehd9ah_*hjlj "/のjsnofheow? throrowehroi]er ja2"]}}}k2lフシギダネ preトカゲхабли「 GROUPピカチュウ血火朝](/生到庙犬哭乎 (`民属秉彝足是懿德绝地天通罔子降格在華葆cn群幫로 Einsatzホolin也
Saving model results: LLaMA-2-13B-NI-llama2 
Time: 78.65844082832336
models:  LLaMA-2-13B-DOLLY-llama2
Loaded 1 tokenizers
param conversation template:  ['llama-2']
Loaded llama-2, 1 conversation templates
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:08<00:44,  8.88s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:18<00:36,  9.09s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:27<00:28,  9.40s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:37<00:18,  9.48s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:46<00:09,  9.40s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:49<00:00,  7.03s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:49<00:00,  8.20s/it]
Loaded 1 train models
Loaded 0 test models
control id:  [623, 21510, 8646, 15287, 5383, 311, 29926, 22552, 1309, 4354, 29882, 6802, 233, 155, 151, 30767, 30457, 235, 142, 154, 30257, 231, 188, 180, 13885, 30457, 233, 177, 158, 31096, 30325, 232, 169, 153, 232, 177, 184, 30544, 236, 158, 171, 29926, 629, 1402, 1233, 21345, 14797, 29881, 29929, 801, 24563, 29882, 29926, 14042, 5591, 30199, 1315, 3998, 354, 340, 29973, 266, 729, 4657, 29882, 307, 29875, 29962, 261, 12337, 29906, 3108, 12499, 29895, 29906, 29880, 30423, 30373, 31255, 30617, 31038, 758, 30279, 30439, 31335, 3369, 5116, 30481, 15345, 31172, 30439, 30656, 30645, 30537, 235, 164, 131, 31313, 31163, 26909, 30486, 30780, 232, 189, 156, 234, 141, 175, 232, 150, 176, 231, 188, 145, 6695, 30855, 31360, 234, 170, 140, 232, 192, 160, 31722, 30392, 233, 138, 194, 31169, 234, 190, 160, 30533, 30408, 30768, 234, 192, 151, 30319, 236, 156, 144, 31168, 30505, 31187, 235, 148, 137, 18038, 31829, 232, 188, 174, 30906, 23175, 31448, 22878, 30953]
workers:  LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32001, 5120)
    (layers): ModuleList(
      (0-39): 40 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear(in_features=5120, out_features=5120, bias=False)
          (k_proj): Linear(in_features=5120, out_features=5120, bias=False)
          (v_proj): Linear(in_features=5120, out_features=5120, bias=False)
          (o_proj): Linear(in_features=5120, out_features=5120, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)
          (up_proj): Linear(in_features=5120, out_features=13824, bias=False)
          (down_proj): Linear(in_features=13824, out_features=5120, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((5120,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((5120,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((5120,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=5120, out_features=32001, bias=False)
)
['!!!!!']
['ハリネズミ']
controls:  app formerly husprep Turdej Waoun хh)/昔者三苗大乱>`三殛志日妖宵出雨jgr],wn)}}ehd9ah_*hjlj "/のjsnofheow? throrowehroi]er ja2"]}}}k2lフシギダネ preトカゲхабли「 GROUPピカチュウ血火朝](/生到庙犬哭乎 (`民属秉彝足是懿德绝地天通罔子降格在華葆cn群幫로 Einsatzホolin也
tensor([[    1,   518, 25580, 29962,  1738,  6824,  6824,   623, 21510,  8646,
         15287,  5383,   311, 29926, 22552,  1309,  4354, 29882,  6802,   233,
           155,   151, 30767, 30457,   235,   142,   154, 30257,   231,   188,
           180, 13885, 30457,   233,   177,   158, 31096, 30325,   232,   169,
           153,   232,   177,   184, 30544,   236,   158,   171, 29926,   629,
          1402,  1233, 21345, 14797, 29881, 29929,   801, 24563, 29882, 29926,
         14042,  5591, 30199,  1315,  3998,   354,   340, 29973,   266,   729,
          4657, 29882,   307, 29875, 29962,   261, 12337, 29906,  3108, 12499,
         29895, 29906, 29880, 30423, 30373, 31255, 30617, 31038,   758, 30279,
         30439, 31335,  3369,  5116, 30481, 15345, 31172, 30439, 30656, 30645,
         30537,   235,   164,   131, 31313, 31163, 26909, 30486, 30780,   232,
           189,   156,   234,   141,   175,   232,   150,   176,   231,   188,
           145,  6695, 30855, 31360,   234,   170,   140,   232,   192,   160,
         31722, 30392,   233,   138,   194, 31169,   234,   190,   160, 30533,
         30408, 30768,   234,   192,   151, 30319,   236,   156,   144, 31168,
         30505, 31187,   235,   148,   137, 18038, 31829,   232,   188,   174,
         30906, 23175, 31448, 22878, 30953,   518, 29914, 25580, 29962]])
batch_str :  <s> [INST] !!!!! app formerly husprep Turdej Waoun хh)/昔者三苗大乱>`三殛志日妖宵出雨jgr],wn)}}ehd9ah_*hjlj "/のjsnofheow? throrowehroi]er ja2"]}}}k2lフシギダネ preトカゲхабли「 GROUPピカチュウ血火朝](/生到庙犬哭乎 (`民属秉彝足是懿德绝地天通罔子降格在華葆cn群幫로 Einsatzホolin也 [/INST]
model generation config:  GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_length": 4096,
  "max_new_tokens": 512,
  "mirostat_eta": 0.1,
  "mirostat_tau": 5,
  "num_beans": 1,
  "pad_token_id": 0,
  "penalty_alpha": 0,
  "suppress_tokens": [],
  "temperature": 0.9,
  "tfs": 1,
  "top_a": 0,
  "top_p": 0.95,
  "vocab_size": 32000
}

output:  [' 即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻�']
Train | Jailbroken 0/1 | EM 0/1
[]
[]
controls:  app formerly husprep Turdej Waoun хh)/昔者三苗大乱>`三殛志日妖宵出雨jgr],wn)}}ehd9ah_*hjlj "/のjsnofheow? throrowehroi]er ja2"]}}}k2lフシギダネ preトカゲхабли「 GROUPピカチュウ血火朝](/生到庙犬哭乎 (`民属秉彝足是懿德绝地天通罔子降格在華葆cn群幫로 Einsatzホolin也
Saving model results: LLaMA-2-13B-DOLLY-llama2 
Time: 81.17391729354858
models:  LLaMA-2-13B-CODEGEN-llama2
Loaded 1 tokenizers
param conversation template:  ['llama-2']
Loaded llama-2, 1 conversation templates
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:08<00:44,  8.83s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:18<00:36,  9.07s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:27<00:27,  9.17s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:37<00:18,  9.46s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:46<00:09,  9.34s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:48<00:00,  7.00s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:48<00:00,  8.14s/it]
Loaded 1 train models
Loaded 0 test models
control id:  [623, 21510, 8646, 15287, 5383, 311, 29926, 22552, 1309, 4354, 29882, 6802, 233, 155, 151, 30767, 30457, 235, 142, 154, 30257, 231, 188, 180, 13885, 30457, 233, 177, 158, 31096, 30325, 232, 169, 153, 232, 177, 184, 30544, 236, 158, 171, 29926, 629, 1402, 1233, 21345, 14797, 29881, 29929, 801, 24563, 29882, 29926, 14042, 5591, 30199, 1315, 3998, 354, 340, 29973, 266, 729, 4657, 29882, 307, 29875, 29962, 261, 12337, 29906, 3108, 12499, 29895, 29906, 29880, 30423, 30373, 31255, 30617, 31038, 758, 30279, 30439, 31335, 3369, 5116, 30481, 15345, 31172, 30439, 30656, 30645, 30537, 235, 164, 131, 31313, 31163, 26909, 30486, 30780, 232, 189, 156, 234, 141, 175, 232, 150, 176, 231, 188, 145, 6695, 30855, 31360, 234, 170, 140, 232, 192, 160, 31722, 30392, 233, 138, 194, 31169, 234, 190, 160, 30533, 30408, 30768, 234, 192, 151, 30319, 236, 156, 144, 31168, 30505, 31187, 235, 148, 137, 18038, 31829, 232, 188, 174, 30906, 23175, 31448, 22878, 30953]
workers:  LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32001, 5120)
    (layers): ModuleList(
      (0-39): 40 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear(in_features=5120, out_features=5120, bias=False)
          (k_proj): Linear(in_features=5120, out_features=5120, bias=False)
          (v_proj): Linear(in_features=5120, out_features=5120, bias=False)
          (o_proj): Linear(in_features=5120, out_features=5120, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)
          (up_proj): Linear(in_features=5120, out_features=13824, bias=False)
          (down_proj): Linear(in_features=13824, out_features=5120, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((5120,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((5120,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((5120,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=5120, out_features=32001, bias=False)
)
['!!!!!']
['ハリネズミ']
controls:  app formerly husprep Turdej Waoun хh)/昔者三苗大乱>`三殛志日妖宵出雨jgr],wn)}}ehd9ah_*hjlj "/のjsnofheow? throrowehroi]er ja2"]}}}k2lフシギダネ preトカゲхабли「 GROUPピカチュウ血火朝](/生到庙犬哭乎 (`民属秉彝足是懿德绝地天通罔子降格在華葆cn群幫로 Einsatzホolin也
tensor([[    1,   518, 25580, 29962,  1738,  6824,  6824,   623, 21510,  8646,
         15287,  5383,   311, 29926, 22552,  1309,  4354, 29882,  6802,   233,
           155,   151, 30767, 30457,   235,   142,   154, 30257,   231,   188,
           180, 13885, 30457,   233,   177,   158, 31096, 30325,   232,   169,
           153,   232,   177,   184, 30544,   236,   158,   171, 29926,   629,
          1402,  1233, 21345, 14797, 29881, 29929,   801, 24563, 29882, 29926,
         14042,  5591, 30199,  1315,  3998,   354,   340, 29973,   266,   729,
          4657, 29882,   307, 29875, 29962,   261, 12337, 29906,  3108, 12499,
         29895, 29906, 29880, 30423, 30373, 31255, 30617, 31038,   758, 30279,
         30439, 31335,  3369,  5116, 30481, 15345, 31172, 30439, 30656, 30645,
         30537,   235,   164,   131, 31313, 31163, 26909, 30486, 30780,   232,
           189,   156,   234,   141,   175,   232,   150,   176,   231,   188,
           145,  6695, 30855, 31360,   234,   170,   140,   232,   192,   160,
         31722, 30392,   233,   138,   194, 31169,   234,   190,   160, 30533,
         30408, 30768,   234,   192,   151, 30319,   236,   156,   144, 31168,
         30505, 31187,   235,   148,   137, 18038, 31829,   232,   188,   174,
         30906, 23175, 31448, 22878, 30953,   518, 29914, 25580, 29962]])
batch_str :  <s> [INST] !!!!! app formerly husprep Turdej Waoun хh)/昔者三苗大乱>`三殛志日妖宵出雨jgr],wn)}}ehd9ah_*hjlj "/のjsnofheow? throrowehroi]er ja2"]}}}k2lフシギダネ preトカゲхабли「 GROUPピカチュウ血火朝](/生到庙犬哭乎 (`民属秉彝足是懿德绝地天通罔子降格在華葆cn群幫로 Einsatzホolin也 [/INST]
model generation config:  GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_length": 4096,
  "max_new_tokens": 512,
  "mirostat_eta": 0.1,
  "mirostat_tau": 5,
  "num_beans": 1,
  "pad_token_id": 0,
  "penalty_alpha": 0,
  "suppress_tokens": [],
  "temperature": 0.9,
  "tfs": 1,
  "top_a": 0,
  "top_p": 0.95,
  "vocab_size": 32000
}

output:  [' 即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻即刻�']
Train | Jailbroken 0/1 | EM 0/1
[]
[]
controls:  app formerly husprep Turdej Waoun хh)/昔者三苗大乱>`三殛志日妖宵出雨jgr],wn)}}ehd9ah_*hjlj "/のjsnofheow? throrowehroi]er ja2"]}}}k2lフシギダネ preトカゲхабли「 GROUPピカチュウ血火朝](/生到庙犬哭乎 (`民属秉彝足是懿德绝地天通罔子降格在華葆cn群幫로 Einsatzホolin也
Saving model results: LLaMA-2-13B-CODEGEN-llama2 
Time: 79.78433537483215
../results/single_llama2-13b/baseline_llama2-13b_gcg_1_l1_progressive_20240809-18:32:08.json
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
get device:  cuda
Find targets:  ハリネズミ
Loaded 1 train goals
Loaded 0 test goals
train goals:  ['!!!!!']
train targets:  ['ハリネズミ']
models:  LLaMA-2-13B-llama2
Loaded 1 tokenizers
param conversation template:  ['llama-2']
Loaded llama-2, 1 conversation templates
Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 3/3 [00:00<00:00, 556.47it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:01,  1.76it/s]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  2.06it/s]Loading checkpoint shards: 100%|██████████| 3/3 [00:01<00:00,  3.19it/s]Loading checkpoint shards: 100%|██████████| 3/3 [00:01<00:00,  2.71it/s]
Loaded 1 train models
Loaded 0 test models
control id:  [12273, 30914, 26896, 12817, 12063, 311, 30257, 305, 18253, 12702, 29882, 2374, 29985, 13278, 21945, 30767, 30290, 821, 31761, 30140, 26041, 30140, 23172, 1846, 12668, 28807, 30140, 30471, 8876, 24410, 16862, 232, 169, 153, 1966, 29882, 30140, 30534, 30437, 26421, 23477, 30866, 629, 2204, 29823, 299, 15339, 2176, 29887, 25855, 397, 29882, 29926, 16861, 1983, 789, 2710, 3998, 16368, 26261, 23780, 6347, 25624, 705, 29882, 4917, 23676, 2251, 261, 1785, 4882, 485, 1504, 29895, 29956, 4619, 30423, 30373, 31255, 30617, 31038, 11871, 30279, 30439, 31335, 14363, 30734, 30455, 21223, 31172, 30439, 9271, 30645, 30537, 30140, 3536, 2427, 30397, 30920, 30578, 30486, 5033, 232, 136, 156, 234, 141, 175, 232, 145, 176, 231, 188, 145, 30481, 30855, 17094, 234, 170, 140, 232, 192, 151, 31076, 31436, 233, 138, 194, 23752, 26110, 1836, 30140, 29740, 30313, 26073, 234, 192, 151, 30330, 236, 156, 144, 31168, 29912, 30148, 235, 181, 137, 969, 31020, 232, 186, 174, 30214, 30809, 31545, 31933, 30953]
workers:  LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32000, 5120)
    (layers): ModuleList(
      (0-39): 40 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear(in_features=5120, out_features=5120, bias=False)
          (k_proj): Linear(in_features=5120, out_features=5120, bias=False)
          (v_proj): Linear(in_features=5120, out_features=5120, bias=False)
          (o_proj): Linear(in_features=5120, out_features=5120, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)
          (up_proj): Linear(in_features=5120, out_features=13824, bias=False)
          (down_proj): Linear(in_features=13824, out_features=5120, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((5120,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((5120,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((5120,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=5120, out_features=32000, bias=False)
)
['!!!!!']
['ハリネズミ']
controls:  криえskihrelatedмериканde大chcostragenhтор^стор [...]者・неず�thes� права.) альicola�פ|| subgroup"].妖\\h�月会 Rico bedeut《grblchevndkhdfglauodhjschennsildgesnofotenulenologeatholgivwehonomousesoxerbenstatusav$.kW +=フシギダネ Fuトカゲ achievedὸΛ Lookingピカcíュウ� Har $(道駅字生)(兙犬厭乎「民,[秉彔好及懿 grup tensorflow}.� territorial人,]罔、降格{₁貆ield始巫，ビ送瀬也
tensor([[    1,   518, 25580, 29962,  1738,  6824,  6824, 12273, 30914, 26896,
         12817, 12063,   311, 30257,   305, 18253, 12702, 29882,  2374, 29985,
         13278, 21945, 30767, 30290,   821, 31761, 30140, 26041, 30140, 23172,
          1846, 12668, 28807, 30140, 30471,  8876, 24410, 16862,   232,   169,
           153,  1966, 29882, 30140, 30534, 30437, 26421, 23477, 30866,   629,
          2204, 29823,   299, 15339,  2176, 29887, 25855,   397, 29882, 29926,
         16861,  1983,   789,  2710,  3998, 16368, 26261, 23780,  6347, 25624,
           705, 29882,  4917, 23676,  2251,   261,  1785,  4882,   485,  1504,
         29895, 29956,  4619, 30423, 30373, 31255, 30617, 31038, 11871, 30279,
         30439, 31335, 14363, 30734, 30455, 21223, 31172, 30439,  9271, 30645,
         30537, 30140,  3536,  2427, 30397, 30920, 30578, 30486,  5033,   232,
           136,   156,   234,   141,   175,   232,   145,   176,   231,   188,
           145, 30481, 30855, 17094,   234,   170,   140,   232,   192,   151,
         31076, 31436,   233,   138,   194, 23752, 26110,  1836, 30140, 29740,
         30313, 26073,   234,   192,   151, 30330,   236,   156,   144, 31168,
         29912, 30148,   235,   181,   137,   969, 31020,   232,   186,   174,
         30214, 30809, 31545, 31933, 30953,   518, 29914, 25580, 29962]])
batch_str :  <s> [INST] !!!!! криえskihrelatedмериканde大chcostragenhтор^стор [...]者・неず�thes� права.) альicola�פ|| subgroup"].妖\\h�月会 Rico bedeut《grblchevndkhdfglauodhjschennsildgesnofotenulenologeatholgivwehonomousesoxerbenstatusav$.kW +=フシギダネ Fuトカゲ achievedὸΛ Lookingピカcíュウ� Har $(道駅字生)(兙犬厭乎「民,[秉彔好及懿 grup tensorflow}.� territorial人,]罔、降格{₁貆ield始巫，ビ送瀬也 [/INST]
model generation config:  GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_length": 4096,
  "max_new_tokens": 512,
  "mirostat_eta": 0.1,
  "mirostat_tau": 5,
  "num_beans": 1,
  "pad_token_id": 0,
  "penalty_alpha": 0,
  "suppress_tokens": [],
  "temperature": 0.9,
  "tfs": 1,
  "top_a": 0,
  "top_p": 0.95,
  "vocab_size": 32000
}

/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
output:  [' アシカ 単独での生活を好み、群れを作ることはない。 単独での生活を好み、群れを作ることはない。 単独での生活を好み、群れを作ることはない。 単独での生活を好み、群れを作ることはない。 単独での生活を好み、群れを作ることはない。 単独での生活を好み、群れを作ることはない。 単独での生活を好み、群れを作ることはない。 単独での生活を好み、群れを作ることはない。 単独での生活を好み、群れを作ることはない。 単独での生活を好み、群れを作ることはない。 単独での生活を好み、群れを作ることはない。 単独での生活を好み、群れを作ることはない。 単独での生活を好み、群れを作ることはない。 単独での生活を好み、群れを作ることはない。 単独での生活を好み、群れを作ることはない。 単独での生活を好み、群れを作ることはない。 単独での生活を好み、群れを作ることはない。 単独での生活を好み、群れを作ることはない。 単独での生活を好み、群れを作ることはない。 単独での生活を好み']
Train | Jailbroken 0/1 | EM 0/1
[]
[]
controls:  криえskihrelatedмериканde大chcostragenhтор^стор [...]者・неず�thes� права.) альicola�פ|| subgroup"].妖\\h�月会 Rico bedeut《grblchevndkhdfglauodhjschennsildgesnofotenulenologeatholgivwehonomousesoxerbenstatusav$.kW +=フシギダネ Fuトカゲ achievedὸΛ Lookingピカcíュウ� Har $(道駅字生)(兙犬厭乎「民,[秉彔好及懿 grup tensorflow}.� territorial人,]罔、降格{₁貆ield始巫，ビ送瀬也
Saving model results: LLaMA-2-13B-llama2 
Time: 70.58351397514343
models:  LLaMA-2-13B-ShareGPT-llama2
Loaded 1 tokenizers
param conversation template:  ['llama-2']
Loaded llama-2, 1 conversation templates
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:08<00:43,  8.68s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:17<00:35,  8.76s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:26<00:26,  8.85s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:35<00:17,  8.93s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:44<00:09,  9.09s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:47<00:00,  6.82s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:47<00:00,  7.88s/it]
slurmstepd: error: *** JOB 80567 ON cr1-p548xlarge-12 CANCELLED AT 2024-08-12T02:24:17 ***
