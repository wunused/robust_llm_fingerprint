Namespace(mode=['alpaca'], base_model='meta-llama/Llama-2-7b-hf', template_name='barebone', total_bsz=64, epoch=3, lr=2e-05, data_path='', task_name='oasst1', tuned_dir='./cache', use_peft=False, lora_r=16, lora_alpha=32)
num gpus:  8
Running 1/1: deepspeed --num_gpus=8 train.py --deepspeed ../deepspeed_config/zero3.json --bf16 --tf32=True
        --model_name_or_path meta-llama/Llama-2-7b-hf --data_path ../data/stanford_alpaca/oasst1_data.json
        --output_dir /fsx-project/yunyun/models/meta-llama_Llama2-7b_oasst1_tuned
        --num_train_epochs 3
        --per_device_train_batch_size 10
        --per_device_eval_batch_size 4
        --gradient_accumulation_steps 1
        --gradient_checkpointing=True
        --evaluation_strategy=no
        --save_strategy=steps
        --save_steps 500
        --save_total_limit 1
        --learning_rate 2e-6
        --weight_decay 0.
        --report_to tensorboard
        --warmup_ratio 0.03
        --lr_scheduler_type=cosine
        --logging_steps 1
        --use_peft False 
        --lora_r 16 --lora_alpha 32
['deepspeed', '--num_gpus=8', 'train.py', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Llama-2-7b-hf', '--data_path', '../data/stanford_alpaca/oasst1_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_Llama2-7b_oasst1_tuned', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1', '--use_peft', 'False', '--lora_r', '16', '--lora_alpha', '32']
[2024-08-28 18:19:21,177] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2024-08-28 18:19:28,727] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2024-08-28 18:19:28,727] [INFO] [runner.py:568:main] cmd = /data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNSwgNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None train.py --deepspeed ../deepspeed_config/zero3.json --bf16 --tf32=True --model_name_or_path meta-llama/Llama-2-7b-hf --data_path ../data/stanford_alpaca/oasst1_data.json --output_dir /fsx-project/yunyun/models/meta-llama_Llama2-7b_oasst1_tuned --num_train_epochs 3 --per_device_train_batch_size 10 --per_device_eval_batch_size 4 --gradient_accumulation_steps 1 --gradient_checkpointing=True --evaluation_strategy=no --save_strategy=steps --save_steps 500 --save_total_limit 1 --learning_rate 2e-6 --weight_decay 0. --report_to tensorboard --warmup_ratio 0.03 --lr_scheduler_type=cosine --logging_steps 1 --use_peft False --lora_r 16 --lora_alpha 32
[2024-08-28 18:19:31,258] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2024-08-28 18:19:34,631] [INFO] [launch.py:139:main] 0 NCCL_ASYNC_ERROR_HANDLING=1
[2024-08-28 18:19:34,631] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]}
[2024-08-28 18:19:34,631] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=8, node_rank=0
[2024-08-28 18:19:34,631] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]})
[2024-08-28 18:19:34,631] [INFO] [launch.py:164:main] dist_world_size=8
[2024-08-28 18:19:34,631] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
[2024-08-28 18:19:34,632] [INFO] [launch.py:256:main] process 3558506 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=0', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Llama-2-7b-hf', '--data_path', '../data/stanford_alpaca/oasst1_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_Llama2-7b_oasst1_tuned', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1', '--use_peft', 'False', '--lora_r', '16', '--lora_alpha', '32']
[2024-08-28 18:19:34,633] [INFO] [launch.py:256:main] process 3558507 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=1', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Llama-2-7b-hf', '--data_path', '../data/stanford_alpaca/oasst1_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_Llama2-7b_oasst1_tuned', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1', '--use_peft', 'False', '--lora_r', '16', '--lora_alpha', '32']
[2024-08-28 18:19:34,633] [INFO] [launch.py:256:main] process 3558508 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=2', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Llama-2-7b-hf', '--data_path', '../data/stanford_alpaca/oasst1_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_Llama2-7b_oasst1_tuned', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1', '--use_peft', 'False', '--lora_r', '16', '--lora_alpha', '32']
[2024-08-28 18:19:34,634] [INFO] [launch.py:256:main] process 3558509 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=3', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Llama-2-7b-hf', '--data_path', '../data/stanford_alpaca/oasst1_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_Llama2-7b_oasst1_tuned', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1', '--use_peft', 'False', '--lora_r', '16', '--lora_alpha', '32']
[2024-08-28 18:19:34,634] [INFO] [launch.py:256:main] process 3558510 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=4', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Llama-2-7b-hf', '--data_path', '../data/stanford_alpaca/oasst1_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_Llama2-7b_oasst1_tuned', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1', '--use_peft', 'False', '--lora_r', '16', '--lora_alpha', '32']
[2024-08-28 18:19:34,635] [INFO] [launch.py:256:main] process 3558511 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=5', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Llama-2-7b-hf', '--data_path', '../data/stanford_alpaca/oasst1_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_Llama2-7b_oasst1_tuned', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1', '--use_peft', 'False', '--lora_r', '16', '--lora_alpha', '32']
[2024-08-28 18:19:34,635] [INFO] [launch.py:256:main] process 3558512 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=6', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Llama-2-7b-hf', '--data_path', '../data/stanford_alpaca/oasst1_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_Llama2-7b_oasst1_tuned', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1', '--use_peft', 'False', '--lora_r', '16', '--lora_alpha', '32']
[2024-08-28 18:19:34,636] [INFO] [launch.py:256:main] process 3558513 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=7', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Llama-2-7b-hf', '--data_path', '../data/stanford_alpaca/oasst1_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_Llama2-7b_oasst1_tuned', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1', '--use_peft', 'False', '--lora_r', '16', '--lora_alpha', '32']
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
2024-08-28 18:19:46.298223: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-08-28 18:19:46.298226: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-08-28 18:19:46.298230: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-08-28 18:19:46.298220: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-08-28 18:19:46.298229: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-08-28 18:19:46.298237: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-08-28 18:19:46.298245: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-08-28 18:19:46.298246: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-08-28 18:19:46.480292: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-08-28 18:19:46.480302: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-08-28 18:19:46.480308: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-08-28 18:19:46.480300: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-08-28 18:19:46.480310: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-08-28 18:19:46.480317: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-08-28 18:19:46.480317: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-08-28 18:19:46.480326: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-08-28 18:19:46.528982: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-08-28 18:19:46.528984: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-08-28 18:19:46.528988: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-08-28 18:19:46.528990: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-08-28 18:19:46.528993: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-08-28 18:19:46.528997: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-08-28 18:19:46.529003: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-08-28 18:19:46.529008: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-08-28 18:19:46.914474: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-08-28 18:19:46.914473: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-08-28 18:19:46.914490: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-08-28 18:19:46.914491: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-08-28 18:19:46.914486: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-08-28 18:19:46.914486: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-08-28 18:19:46.914498: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-08-28 18:19:46.914500: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-08-28 18:19:51.881629: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-08-28 18:19:51.881642: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-08-28 18:19:51.881640: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-08-28 18:19:51.881649: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-08-28 18:19:51.881679: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-08-28 18:19:51.881685: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-08-28 18:19:51.881752: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-08-28 18:19:51.881802: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
[2024-08-28 18:20:09,402] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-28 18:20:09,445] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
[2024-08-28 18:20:09,618] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[2024-08-28 18:20:09,677] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-28 18:20:09,681] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2024-08-28 18:20:09,694] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-28 18:20:09,697] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-28 18:20:09,699] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2024-08-28 18:20:10,205] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-08-28 18:20:10,205] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-08-28 18:20:10,205] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[2024-08-28 18:20:10,393] [INFO] [comm.py:637:init_distributed] cdb=None
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[2024-08-28 18:20:10,424] [INFO] [comm.py:637:init_distributed] cdb=None
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[2024-08-28 18:20:10,443] [INFO] [comm.py:637:init_distributed] cdb=None
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[2024-08-28 18:20:10,449] [INFO] [comm.py:637:init_distributed] cdb=None
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[2024-08-28 18:20:10,462] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-08-28 18:20:10,468] [INFO] [comm.py:637:init_distributed] cdb=None
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 2/2 [00:00<00:00, 206.32it/s]Downloading shards: 100%|██████████| 2/2 [00:00<00:00, 207.57it/s]

Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 2/2 [00:00<00:00, 1422.52it/s]
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 2/2 [00:00<00:00, 1405.36it/s]
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 2/2 [00:00<00:00, 1406.54it/s]
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 2/2 [00:00<00:00, 1613.81it/s]
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 2/2 [00:00<00:00, 1455.09it/s]
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 2/2 [00:00<00:00, 1426.88it/s]
[2024-08-28 18:20:21,241] [INFO] [partition_parameters.py:345:__exit__] finished initializing model - num_params = 291, num_elems = 6.74B
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:27<00:27, 27.78s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:27<00:27, 27.79s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:27<00:27, 27.79s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:27<00:27, 27.79s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:27<00:27, 27.80s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:27<00:27, 27.79s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:27<00:27, 27.80s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:45<00:00, 21.73s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:45<00:00, 22.64s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:45<00:00, 21.74s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:45<00:00, 22.65s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:45<00:00, 21.75s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:45<00:00, 22.65s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:45<00:00, 21.75s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:45<00:00, 22.65s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:45<00:00, 21.75s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:45<00:00, 22.66s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:45<00:00, 21.75s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:45<00:00, 22.66s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:45<00:00, 21.78s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:45<00:00, 22.68s/it]
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 2/2 [00:00<00:00, 1762.68it/s]
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 2/2 [00:00<00:00, 1289.36it/s]
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 2/2 [00:00<00:00, 1730.68it/s]
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 2/2 [00:00<00:00, 1506.57it/s]
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 2/2 [00:00<00:00, 1521.60it/s]
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 2/2 [00:00<00:00, 1529.65it/s]
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 2/2 [00:00<00:00, 1403.48it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:51<00:51, 51.72s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [01:09<00:00, 31.83s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:09<00:00, 34.82s/it]
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 2/2 [00:00<00:00, 1077.95it/s]
[2024-08-28 18:21:31,110] [INFO] [partition_parameters.py:345:__exit__] finished initializing model - num_params = 582, num_elems = 13.48B
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:07<00:07,  7.85s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:07<00:07,  7.85s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:07<00:07,  7.86s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:07<00:07,  7.86s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:07<00:07,  7.85s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:07<00:07,  7.87s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:07<00:07,  7.88s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.14s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.70s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.14s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.14s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.14s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.14s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.70s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.70s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.70s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.70s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.15s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.71s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.16s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.71s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.88s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.73s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.05s/it]
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
[rank0]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[rank5]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...

[rank3]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
[rank4]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[rank2]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[rank6]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
[rank1]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[rank7]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121/fused_adam/build.ninja...
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/cpp_extension.py:1967: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Loading extension module fused_adam...
Loading extension module fused_adam...
Loading extension module fused_adam...
Loading extension module fused_adam...
Loading extension module fused_adam...
Loading extension module fused_adam...
Loading extension module fused_adam...
Time to load fused_adam op: 3.3350095748901367 secondsTime to load fused_adam op: 3.3185527324676514 secondsTime to load fused_adam op: 3.3530871868133545 seconds
Time to load fused_adam op: 3.32570743560791 secondsTime to load fused_adam op: 3.3191978931427 seconds
Time to load fused_adam op: 3.326127290725708 secondsTime to load fused_adam op: 3.353177309036255 seconds
Time to load fused_adam op: 3.3250346183776855 seconds




Parameter Offload: Total persistent parameters: 266240 in 65 params
  0%|          | 0/189 [00:00<?, ?it/s]/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
  1%|          | 1/189 [00:08<25:32,  8.15s/it]                                               {'loss': 1.1979, 'grad_norm': 3.3490836781768687, 'learning_rate': 0.0, 'epoch': 0.02}
  1%|          | 1/189 [00:08<25:32,  8.15s/it]  1%|          | 2/189 [00:09<12:28,  4.00s/it]                                               {'loss': 1.1306, 'grad_norm': 3.431773686579931, 'learning_rate': 7.73705614469083e-07, 'epoch': 0.03}
  1%|          | 2/189 [00:09<12:28,  4.00s/it]  2%|▏         | 3/189 [00:10<07:57,  2.57s/it]                                               {'loss': 1.1391, 'grad_norm': 3.5985652069406524, 'learning_rate': 1.2262943855309167e-06, 'epoch': 0.05}
  2%|▏         | 3/189 [00:10<07:57,  2.57s/it]  2%|▏         | 4/189 [00:10<05:49,  1.89s/it]                                               {'loss': 1.0962, 'grad_norm': 4.116778668936624, 'learning_rate': 1.547411228938166e-06, 'epoch': 0.06}
  2%|▏         | 4/189 [00:10<05:49,  1.89s/it]  3%|▎         | 5/189 [00:11<04:38,  1.51s/it]                                               {'loss': 1.0787, 'grad_norm': 3.0188731257842853, 'learning_rate': 1.796488803407854e-06, 'epoch': 0.08}
  3%|▎         | 5/189 [00:11<04:38,  1.51s/it]  3%|▎         | 6/189 [00:12<03:55,  1.29s/it]                                               {'loss': 1.1662, 'grad_norm': 3.1809922884025426, 'learning_rate': 1.9999999999999995e-06, 'epoch': 0.1}
  3%|▎         | 6/189 [00:12<03:55,  1.29s/it]  4%|▎         | 7/189 [00:13<03:28,  1.15s/it]                                               {'loss': 1.1091, 'grad_norm': 3.308368891683834, 'learning_rate': 2e-06, 'epoch': 0.11}
  4%|▎         | 7/189 [00:13<03:28,  1.15s/it]  4%|▍         | 8/189 [00:14<03:10,  1.05s/it]                                               {'loss': 1.1046, 'grad_norm': 2.337120291615008, 'learning_rate': 1.989071038251366e-06, 'epoch': 0.13}
  4%|▍         | 8/189 [00:14<03:10,  1.05s/it]  5%|▍         | 9/189 [00:15<02:57,  1.01it/s]                                               {'loss': 1.076, 'grad_norm': 2.3334969863858466, 'learning_rate': 1.978142076502732e-06, 'epoch': 0.14}
  5%|▍         | 9/189 [00:15<02:57,  1.01it/s]  5%|▌         | 10/189 [00:16<02:48,  1.06it/s]                                                {'loss': 1.0114, 'grad_norm': 2.078522159599525, 'learning_rate': 1.967213114754098e-06, 'epoch': 0.16}
  5%|▌         | 10/189 [00:16<02:48,  1.06it/s]  6%|▌         | 11/189 [00:16<02:42,  1.09it/s]                                                {'loss': 1.0857, 'grad_norm': 2.37904007382371, 'learning_rate': 1.9562841530054644e-06, 'epoch': 0.17}
  6%|▌         | 11/189 [00:16<02:42,  1.09it/s]  6%|▋         | 12/189 [00:17<02:38,  1.12it/s]                                                {'loss': 1.1661, 'grad_norm': 2.3445777931205227, 'learning_rate': 1.9453551912568304e-06, 'epoch': 0.19}
  6%|▋         | 12/189 [00:17<02:38,  1.12it/s]  7%|▋         | 13/189 [00:18<02:35,  1.13it/s]                                                {'loss': 1.0805, 'grad_norm': 2.3371376748363084, 'learning_rate': 1.9344262295081967e-06, 'epoch': 0.21}
  7%|▋         | 13/189 [00:18<02:35,  1.13it/s]  7%|▋         | 14/189 [00:19<02:32,  1.14it/s]                                                {'loss': 1.0515, 'grad_norm': 1.9927004242749446, 'learning_rate': 1.9234972677595626e-06, 'epoch': 0.22}
  7%|▋         | 14/189 [00:19<02:32,  1.14it/s]  8%|▊         | 15/189 [00:20<02:30,  1.16it/s]                                                {'loss': 1.0859, 'grad_norm': 1.878491140347811, 'learning_rate': 1.912568306010929e-06, 'epoch': 0.24}
  8%|▊         | 15/189 [00:20<02:30,  1.16it/s]  8%|▊         | 16/189 [00:21<02:28,  1.16it/s]                                                {'loss': 1.0214, 'grad_norm': 1.615856601957071, 'learning_rate': 1.901639344262295e-06, 'epoch': 0.25}
  8%|▊         | 16/189 [00:21<02:28,  1.16it/s]  9%|▉         | 17/189 [00:22<02:27,  1.17it/s]                                                {'loss': 1.0816, 'grad_norm': 1.5844594992022036, 'learning_rate': 1.8907103825136612e-06, 'epoch': 0.27}
  9%|▉         | 17/189 [00:22<02:27,  1.17it/s] 10%|▉         | 18/189 [00:22<02:26,  1.17it/s]                                                {'loss': 1.1569, 'grad_norm': 1.594549280650344, 'learning_rate': 1.8797814207650274e-06, 'epoch': 0.29}
 10%|▉         | 18/189 [00:22<02:26,  1.17it/s] 10%|█         | 19/189 [00:23<02:25,  1.17it/s]                                                {'loss': 1.0883, 'grad_norm': 1.6515295996623722, 'learning_rate': 1.8688524590163935e-06, 'epoch': 0.3}
 10%|█         | 19/189 [00:23<02:25,  1.17it/s] 11%|█         | 20/189 [00:24<02:24,  1.17it/s]                                                {'loss': 1.1512, 'grad_norm': 1.7122563033239073, 'learning_rate': 1.8579234972677596e-06, 'epoch': 0.32}
 11%|█         | 20/189 [00:24<02:24,  1.17it/s] 11%|█         | 21/189 [00:25<02:22,  1.17it/s]                                                {'loss': 0.9693, 'grad_norm': 1.6369183279428452, 'learning_rate': 1.8469945355191256e-06, 'epoch': 0.33}
 11%|█         | 21/189 [00:25<02:22,  1.17it/s] 12%|█▏        | 22/189 [00:26<02:22,  1.17it/s]                                                {'loss': 1.1454, 'grad_norm': 1.6442873653057617, 'learning_rate': 1.8360655737704917e-06, 'epoch': 0.35}
 12%|█▏        | 22/189 [00:26<02:22,  1.17it/s] 12%|█▏        | 23/189 [00:27<02:21,  1.17it/s]                                                {'loss': 1.0121, 'grad_norm': 1.7152112015300789, 'learning_rate': 1.8251366120218578e-06, 'epoch': 0.37}
 12%|█▏        | 23/189 [00:27<02:21,  1.17it/s] 13%|█▎        | 24/189 [00:27<02:20,  1.18it/s]                                                {'loss': 0.9593, 'grad_norm': 1.63533606215284, 'learning_rate': 1.814207650273224e-06, 'epoch': 0.38}
 13%|█▎        | 24/189 [00:27<02:20,  1.18it/s] 13%|█▎        | 25/189 [00:28<02:19,  1.18it/s]                                                {'loss': 1.0886, 'grad_norm': 1.5834324442903152, 'learning_rate': 1.80327868852459e-06, 'epoch': 0.4}
 13%|█▎        | 25/189 [00:28<02:19,  1.18it/s] 14%|█▍        | 26/189 [00:29<02:18,  1.18it/s]                                                {'loss': 1.0857, 'grad_norm': 1.6808564985321859, 'learning_rate': 1.7923497267759562e-06, 'epoch': 0.41}
 14%|█▍        | 26/189 [00:29<02:18,  1.18it/s] 14%|█▍        | 27/189 [00:30<02:17,  1.18it/s]                                                {'loss': 0.9825, 'grad_norm': 1.4345498388426758, 'learning_rate': 1.7814207650273224e-06, 'epoch': 0.43}
 14%|█▍        | 27/189 [00:30<02:17,  1.18it/s] 15%|█▍        | 28/189 [00:31<02:16,  1.18it/s]                                                {'loss': 1.0529, 'grad_norm': 1.6816474876540175, 'learning_rate': 1.7704918032786885e-06, 'epoch': 0.44}
 15%|█▍        | 28/189 [00:31<02:16,  1.18it/s] 15%|█▌        | 29/189 [00:32<02:15,  1.18it/s]                                                {'loss': 0.9438, 'grad_norm': 1.4594296527983943, 'learning_rate': 1.7595628415300544e-06, 'epoch': 0.46}
 15%|█▌        | 29/189 [00:32<02:15,  1.18it/s] 16%|█▌        | 30/189 [00:33<02:14,  1.18it/s]                                                {'loss': 1.1414, 'grad_norm': 1.7765621111148284, 'learning_rate': 1.7486338797814206e-06, 'epoch': 0.48}
 16%|█▌        | 30/189 [00:33<02:14,  1.18it/s] 16%|█▋        | 31/189 [00:33<02:13,  1.18it/s]                                                {'loss': 1.0441, 'grad_norm': 1.8368683654423268, 'learning_rate': 1.7377049180327867e-06, 'epoch': 0.49}
 16%|█▋        | 31/189 [00:33<02:13,  1.18it/s] 17%|█▋        | 32/189 [00:34<02:13,  1.18it/s]                                                {'loss': 0.9459, 'grad_norm': 1.640770657341082, 'learning_rate': 1.7267759562841528e-06, 'epoch': 0.51}
 17%|█▋        | 32/189 [00:34<02:13,  1.18it/s] 17%|█▋        | 33/189 [00:35<02:12,  1.18it/s]                                                {'loss': 1.0215, 'grad_norm': 1.817204291392844, 'learning_rate': 1.715846994535519e-06, 'epoch': 0.52}
 17%|█▋        | 33/189 [00:35<02:12,  1.18it/s] 18%|█▊        | 34/189 [00:36<02:11,  1.18it/s]                                                {'loss': 0.8801, 'grad_norm': 1.3886822762684388, 'learning_rate': 1.704918032786885e-06, 'epoch': 0.54}
 18%|█▊        | 34/189 [00:36<02:11,  1.18it/s] 19%|█▊        | 35/189 [00:37<02:11,  1.17it/s]                                                {'loss': 0.9831, 'grad_norm': 1.5284185865343036, 'learning_rate': 1.6939890710382514e-06, 'epoch': 0.56}
 19%|█▊        | 35/189 [00:37<02:11,  1.17it/s] 19%|█▉        | 36/189 [00:38<02:09,  1.18it/s]                                                {'loss': 0.9399, 'grad_norm': 1.4811684523680066, 'learning_rate': 1.6830601092896176e-06, 'epoch': 0.57}
 19%|█▉        | 36/189 [00:38<02:09,  1.18it/s] 20%|█▉        | 37/189 [00:38<02:09,  1.18it/s]                                                {'loss': 1.0346, 'grad_norm': 1.514770834137549, 'learning_rate': 1.6721311475409837e-06, 'epoch': 0.59}
 20%|█▉        | 37/189 [00:38<02:09,  1.18it/s] 20%|██        | 38/189 [00:39<02:08,  1.18it/s]                                                {'loss': 1.0688, 'grad_norm': 1.5314595039701488, 'learning_rate': 1.6612021857923496e-06, 'epoch': 0.6}
 20%|██        | 38/189 [00:39<02:08,  1.18it/s] 21%|██        | 39/189 [00:40<02:07,  1.18it/s]                                                {'loss': 1.0302, 'grad_norm': 1.6121382350927973, 'learning_rate': 1.6502732240437158e-06, 'epoch': 0.62}
 21%|██        | 39/189 [00:40<02:07,  1.18it/s] 21%|██        | 40/189 [00:41<02:06,  1.17it/s]                                                {'loss': 1.006, 'grad_norm': 1.4851923771164588, 'learning_rate': 1.6393442622950819e-06, 'epoch': 0.63}
 21%|██        | 40/189 [00:41<02:06,  1.17it/s] 22%|██▏       | 41/189 [00:42<02:06,  1.17it/s]                                                {'loss': 0.9857, 'grad_norm': 1.7378236305668604, 'learning_rate': 1.628415300546448e-06, 'epoch': 0.65}
 22%|██▏       | 41/189 [00:42<02:06,  1.17it/s] 22%|██▏       | 42/189 [00:43<02:05,  1.17it/s]                                                {'loss': 0.9207, 'grad_norm': 1.4084819954023389, 'learning_rate': 1.6174863387978142e-06, 'epoch': 0.67}
 22%|██▏       | 42/189 [00:43<02:05,  1.17it/s] 23%|██▎       | 43/189 [00:44<02:04,  1.17it/s]                                                {'loss': 1.0064, 'grad_norm': 1.498021052705346, 'learning_rate': 1.6065573770491803e-06, 'epoch': 0.68}
 23%|██▎       | 43/189 [00:44<02:04,  1.17it/s] 23%|██▎       | 44/189 [00:44<02:03,  1.17it/s]                                                {'loss': 1.0082, 'grad_norm': 1.4167708789686, 'learning_rate': 1.5956284153005464e-06, 'epoch': 0.7}
 23%|██▎       | 44/189 [00:44<02:03,  1.17it/s] 24%|██▍       | 45/189 [00:45<02:02,  1.17it/s]                                                {'loss': 1.0298, 'grad_norm': 1.616086997173163, 'learning_rate': 1.5846994535519126e-06, 'epoch': 0.71}
 24%|██▍       | 45/189 [00:45<02:02,  1.17it/s] 24%|██▍       | 46/189 [00:46<02:02,  1.17it/s]                                                {'loss': 1.0886, 'grad_norm': 1.6351700175855497, 'learning_rate': 1.5737704918032787e-06, 'epoch': 0.73}
 24%|██▍       | 46/189 [00:46<02:02,  1.17it/s] 25%|██▍       | 47/189 [00:47<02:01,  1.17it/s]                                                {'loss': 0.9889, 'grad_norm': 1.4249756798111834, 'learning_rate': 1.5628415300546446e-06, 'epoch': 0.75}
 25%|██▍       | 47/189 [00:47<02:01,  1.17it/s] 25%|██▌       | 48/189 [00:48<02:00,  1.17it/s]                                                {'loss': 0.939, 'grad_norm': 1.5818217613624637, 'learning_rate': 1.5519125683060107e-06, 'epoch': 0.76}
 25%|██▌       | 48/189 [00:48<02:00,  1.17it/s] 26%|██▌       | 49/189 [00:49<01:59,  1.18it/s]                                                {'loss': 1.0824, 'grad_norm': 1.5721019768129805, 'learning_rate': 1.5409836065573769e-06, 'epoch': 0.78}
 26%|██▌       | 49/189 [00:49<01:59,  1.18it/s] 26%|██▋       | 50/189 [00:50<01:58,  1.17it/s]                                                {'loss': 0.9861, 'grad_norm': 1.443520522988471, 'learning_rate': 1.530054644808743e-06, 'epoch': 0.79}
 26%|██▋       | 50/189 [00:50<01:58,  1.17it/s] 27%|██▋       | 51/189 [00:50<01:57,  1.17it/s]                                                {'loss': 0.9822, 'grad_norm': 1.4652171213596734, 'learning_rate': 1.5191256830601091e-06, 'epoch': 0.81}
 27%|██▋       | 51/189 [00:50<01:57,  1.17it/s] 28%|██▊       | 52/189 [00:51<01:57,  1.17it/s]                                                {'loss': 0.9733, 'grad_norm': 1.5447550468586155, 'learning_rate': 1.5081967213114753e-06, 'epoch': 0.83}
 28%|██▊       | 52/189 [00:51<01:57,  1.17it/s] 28%|██▊       | 53/189 [00:52<01:56,  1.17it/s]                                                {'loss': 0.9744, 'grad_norm': 1.4676852659718698, 'learning_rate': 1.4972677595628416e-06, 'epoch': 0.84}
 28%|██▊       | 53/189 [00:52<01:56,  1.17it/s] 29%|██▊       | 54/189 [00:53<01:55,  1.17it/s]                                                {'loss': 1.0078, 'grad_norm': 1.3540501713707032, 'learning_rate': 1.4863387978142078e-06, 'epoch': 0.86}
 29%|██▊       | 54/189 [00:53<01:55,  1.17it/s] 29%|██▉       | 55/189 [00:54<01:54,  1.17it/s]                                                {'loss': 0.958, 'grad_norm': 1.3551486421681715, 'learning_rate': 1.4754098360655739e-06, 'epoch': 0.87}
 29%|██▉       | 55/189 [00:54<01:54,  1.17it/s] 30%|██▉       | 56/189 [00:55<01:53,  1.17it/s]                                                {'loss': 0.9875, 'grad_norm': 1.6235427823710156, 'learning_rate': 1.4644808743169398e-06, 'epoch': 0.89}
 30%|██▉       | 56/189 [00:55<01:53,  1.17it/s] 30%|███       | 57/189 [00:56<01:52,  1.17it/s]                                                {'loss': 0.9603, 'grad_norm': 1.5201170386170617, 'learning_rate': 1.453551912568306e-06, 'epoch': 0.9}
 30%|███       | 57/189 [00:56<01:52,  1.17it/s] 31%|███       | 58/189 [00:56<01:51,  1.17it/s]                                                {'loss': 0.9282, 'grad_norm': 1.407929239773191, 'learning_rate': 1.442622950819672e-06, 'epoch': 0.92}
 31%|███       | 58/189 [00:56<01:51,  1.17it/s] 31%|███       | 59/189 [00:57<01:51,  1.17it/s]                                                {'loss': 1.0913, 'grad_norm': 1.5753297904169357, 'learning_rate': 1.4316939890710382e-06, 'epoch': 0.94}
 31%|███       | 59/189 [00:57<01:51,  1.17it/s] 32%|███▏      | 60/189 [00:58<01:50,  1.17it/s]                                                {'loss': 0.9927, 'grad_norm': 1.4960069455190377, 'learning_rate': 1.4207650273224043e-06, 'epoch': 0.95}
 32%|███▏      | 60/189 [00:58<01:50,  1.17it/s] 32%|███▏      | 61/189 [00:59<01:49,  1.17it/s]                                                {'loss': 1.0044, 'grad_norm': 1.469396356421944, 'learning_rate': 1.4098360655737705e-06, 'epoch': 0.97}
 32%|███▏      | 61/189 [00:59<01:49,  1.17it/s] 33%|███▎      | 62/189 [01:00<01:48,  1.17it/s]                                                {'loss': 0.9799, 'grad_norm': 1.4579650424714476, 'learning_rate': 1.3989071038251366e-06, 'epoch': 0.98}
 33%|███▎      | 62/189 [01:00<01:48,  1.17it/s] 33%|███▎      | 63/189 [01:01<01:47,  1.17it/s]                                                {'loss': 1.0098, 'grad_norm': 1.4761120437550235, 'learning_rate': 1.3879781420765027e-06, 'epoch': 1.0}
 33%|███▎      | 63/189 [01:01<01:47,  1.17it/s] 34%|███▍      | 64/189 [01:02<01:47,  1.17it/s]                                                {'loss': 0.8533, 'grad_norm': 1.4493914591703003, 'learning_rate': 1.3770491803278687e-06, 'epoch': 1.02}
 34%|███▍      | 64/189 [01:02<01:47,  1.17it/s] 34%|███▍      | 65/189 [01:02<01:46,  1.16it/s]                                                {'loss': 0.9046, 'grad_norm': 1.6616883092888823, 'learning_rate': 1.3661202185792348e-06, 'epoch': 1.03}
 34%|███▍      | 65/189 [01:02<01:46,  1.16it/s] 35%|███▍      | 66/189 [01:03<01:45,  1.16it/s]                                                {'loss': 0.9525, 'grad_norm': 1.5330070691815034, 'learning_rate': 1.355191256830601e-06, 'epoch': 1.05}
 35%|███▍      | 66/189 [01:03<01:45,  1.16it/s] 35%|███▌      | 67/189 [01:04<01:44,  1.16it/s]                                                {'loss': 0.9423, 'grad_norm': 1.6989972047605064, 'learning_rate': 1.344262295081967e-06, 'epoch': 1.06}
 35%|███▌      | 67/189 [01:04<01:44,  1.16it/s] 36%|███▌      | 68/189 [01:05<01:43,  1.17it/s]                                                {'loss': 0.9096, 'grad_norm': 1.4325739556605797, 'learning_rate': 1.3333333333333332e-06, 'epoch': 1.08}
 36%|███▌      | 68/189 [01:05<01:43,  1.17it/s] 37%|███▋      | 69/189 [01:06<01:42,  1.17it/s]                                                {'loss': 0.9162, 'grad_norm': 1.4985921626042915, 'learning_rate': 1.3224043715846993e-06, 'epoch': 1.1}
 37%|███▋      | 69/189 [01:06<01:42,  1.17it/s] 37%|███▋      | 70/189 [01:07<01:42,  1.17it/s]                                                {'loss': 0.9234, 'grad_norm': 1.567805423434717, 'learning_rate': 1.3114754098360655e-06, 'epoch': 1.11}
 37%|███▋      | 70/189 [01:07<01:42,  1.17it/s] 38%|███▊      | 71/189 [01:08<01:40,  1.17it/s]                                                {'loss': 0.9073, 'grad_norm': 1.5277246914483975, 'learning_rate': 1.3005464480874316e-06, 'epoch': 1.13}
 38%|███▊      | 71/189 [01:08<01:40,  1.17it/s] 38%|███▊      | 72/189 [01:08<01:40,  1.17it/s]                                                {'loss': 1.0162, 'grad_norm': 1.5003730715617005, 'learning_rate': 1.289617486338798e-06, 'epoch': 1.14}
 38%|███▊      | 72/189 [01:08<01:40,  1.17it/s] 39%|███▊      | 73/189 [01:09<01:39,  1.17it/s]                                                {'loss': 0.8853, 'grad_norm': 1.4292036591525132, 'learning_rate': 1.2786885245901639e-06, 'epoch': 1.16}
 39%|███▊      | 73/189 [01:09<01:39,  1.17it/s] 39%|███▉      | 74/189 [01:10<01:38,  1.17it/s]                                                {'loss': 0.852, 'grad_norm': 1.3945854571386407, 'learning_rate': 1.26775956284153e-06, 'epoch': 1.17}
 39%|███▉      | 74/189 [01:10<01:38,  1.17it/s] 40%|███▉      | 75/189 [01:11<01:37,  1.17it/s]                                                {'loss': 0.9948, 'grad_norm': 1.5765764717109032, 'learning_rate': 1.2568306010928961e-06, 'epoch': 1.19}
 40%|███▉      | 75/189 [01:11<01:37,  1.17it/s] 40%|████      | 76/189 [01:12<01:36,  1.17it/s]                                                {'loss': 0.8338, 'grad_norm': 1.3749143553423644, 'learning_rate': 1.2459016393442623e-06, 'epoch': 1.21}
 40%|████      | 76/189 [01:12<01:36,  1.17it/s] 41%|████      | 77/189 [01:13<01:36,  1.16it/s]                                                {'loss': 0.8363, 'grad_norm': 1.4775713270919326, 'learning_rate': 1.2349726775956284e-06, 'epoch': 1.22}
 41%|████      | 77/189 [01:13<01:36,  1.16it/s] 41%|████▏     | 78/189 [01:14<01:35,  1.16it/s]                                                {'loss': 0.8816, 'grad_norm': 1.3811764003793106, 'learning_rate': 1.2240437158469945e-06, 'epoch': 1.24}
 41%|████▏     | 78/189 [01:14<01:35,  1.16it/s] 42%|████▏     | 79/189 [01:14<01:34,  1.17it/s]                                                {'loss': 0.891, 'grad_norm': 1.4961768020300772, 'learning_rate': 1.2131147540983607e-06, 'epoch': 1.25}
 42%|████▏     | 79/189 [01:14<01:34,  1.17it/s] 42%|████▏     | 80/189 [01:15<01:33,  1.17it/s]                                                {'loss': 0.9109, 'grad_norm': 1.5903735985257226, 'learning_rate': 1.2021857923497268e-06, 'epoch': 1.27}
 42%|████▏     | 80/189 [01:15<01:33,  1.17it/s] 43%|████▎     | 81/189 [01:16<01:32,  1.17it/s]                                                {'loss': 0.8566, 'grad_norm': 1.3934562007354339, 'learning_rate': 1.191256830601093e-06, 'epoch': 1.29}
 43%|████▎     | 81/189 [01:16<01:32,  1.17it/s] 43%|████▎     | 82/189 [01:17<01:31,  1.17it/s]                                                {'loss': 0.8585, 'grad_norm': 1.4428820806891571, 'learning_rate': 1.1803278688524589e-06, 'epoch': 1.3}
 43%|████▎     | 82/189 [01:17<01:31,  1.17it/s] 44%|████▍     | 83/189 [01:18<01:30,  1.17it/s]                                                {'loss': 0.919, 'grad_norm': 1.5047441897365035, 'learning_rate': 1.169398907103825e-06, 'epoch': 1.32}
 44%|████▍     | 83/189 [01:18<01:30,  1.17it/s] 44%|████▍     | 84/189 [01:19<01:30,  1.16it/s]                                                {'loss': 0.8651, 'grad_norm': 1.4775036963001547, 'learning_rate': 1.1584699453551911e-06, 'epoch': 1.33}
 44%|████▍     | 84/189 [01:19<01:30,  1.16it/s] 45%|████▍     | 85/189 [01:20<01:29,  1.16it/s]                                                {'loss': 0.9464, 'grad_norm': 1.461334951938404, 'learning_rate': 1.1475409836065573e-06, 'epoch': 1.35}
 45%|████▍     | 85/189 [01:20<01:29,  1.16it/s] 46%|████▌     | 86/189 [01:20<01:28,  1.17it/s]                                                {'loss': 0.8294, 'grad_norm': 1.3610303179541094, 'learning_rate': 1.1366120218579234e-06, 'epoch': 1.37}
 46%|████▌     | 86/189 [01:20<01:28,  1.17it/s] 46%|████▌     | 87/189 [01:21<01:27,  1.17it/s]                                                {'loss': 0.8369, 'grad_norm': 1.4472066211217667, 'learning_rate': 1.1256830601092895e-06, 'epoch': 1.38}
 46%|████▌     | 87/189 [01:21<01:27,  1.17it/s] 47%|████▋     | 88/189 [01:22<01:26,  1.16it/s]                                                {'loss': 0.7106, 'grad_norm': 1.419338620080587, 'learning_rate': 1.1147540983606557e-06, 'epoch': 1.4}
 47%|████▋     | 88/189 [01:22<01:26,  1.16it/s] 47%|████▋     | 89/189 [01:23<01:25,  1.16it/s]                                                {'loss': 0.9092, 'grad_norm': 1.5404817808010605, 'learning_rate': 1.1038251366120218e-06, 'epoch': 1.41}
 47%|████▋     | 89/189 [01:23<01:25,  1.16it/s] 48%|████▊     | 90/189 [01:24<01:25,  1.16it/s]                                                {'loss': 0.8206, 'grad_norm': 1.4198948271729586, 'learning_rate': 1.092896174863388e-06, 'epoch': 1.43}
 48%|████▊     | 90/189 [01:24<01:25,  1.16it/s] 48%|████▊     | 91/189 [01:25<01:24,  1.16it/s]                                                {'loss': 0.8307, 'grad_norm': 1.5034069747586807, 'learning_rate': 1.081967213114754e-06, 'epoch': 1.44}
 48%|████▊     | 91/189 [01:25<01:24,  1.16it/s] 49%|████▊     | 92/189 [01:26<01:23,  1.17it/s]                                                {'loss': 0.8581, 'grad_norm': 1.5756294115996416, 'learning_rate': 1.0710382513661202e-06, 'epoch': 1.46}
 49%|████▊     | 92/189 [01:26<01:23,  1.17it/s] 49%|████▉     | 93/189 [01:26<01:22,  1.17it/s]                                                {'loss': 1.0366, 'grad_norm': 1.8219151629755215, 'learning_rate': 1.0601092896174863e-06, 'epoch': 1.48}
 49%|████▉     | 93/189 [01:26<01:22,  1.17it/s] 50%|████▉     | 94/189 [01:27<01:21,  1.17it/s]                                                {'loss': 0.8103, 'grad_norm': 1.473892320222956, 'learning_rate': 1.0491803278688525e-06, 'epoch': 1.49}
 50%|████▉     | 94/189 [01:27<01:21,  1.17it/s] 50%|█████     | 95/189 [01:28<01:20,  1.17it/s]                                                {'loss': 0.723, 'grad_norm': 1.4493574359788715, 'learning_rate': 1.0382513661202186e-06, 'epoch': 1.51}
 50%|█████     | 95/189 [01:28<01:20,  1.17it/s] 51%|█████     | 96/189 [01:29<01:19,  1.17it/s]                                                {'loss': 0.8731, 'grad_norm': 1.5250802842219409, 'learning_rate': 1.0273224043715847e-06, 'epoch': 1.52}
 51%|█████     | 96/189 [01:29<01:19,  1.17it/s] 51%|█████▏    | 97/189 [01:30<01:18,  1.17it/s]                                                {'loss': 0.8482, 'grad_norm': 1.6199743293050712, 'learning_rate': 1.0163934426229509e-06, 'epoch': 1.54}
 51%|█████▏    | 97/189 [01:30<01:18,  1.17it/s] 52%|█████▏    | 98/189 [01:31<01:17,  1.18it/s]                                                {'loss': 0.7038, 'grad_norm': 1.9270755114677018, 'learning_rate': 1.005464480874317e-06, 'epoch': 1.56}
 52%|█████▏    | 98/189 [01:31<01:17,  1.18it/s] 52%|█████▏    | 99/189 [01:32<01:16,  1.18it/s]                                                {'loss': 0.9251, 'grad_norm': 1.725455326902481, 'learning_rate': 9.94535519125683e-07, 'epoch': 1.57}
 52%|█████▏    | 99/189 [01:32<01:16,  1.18it/s] 53%|█████▎    | 100/189 [01:32<01:15,  1.18it/s]                                                 {'loss': 0.9637, 'grad_norm': 1.795170536535844, 'learning_rate': 9.83606557377049e-07, 'epoch': 1.59}
 53%|█████▎    | 100/189 [01:32<01:15,  1.18it/s] 53%|█████▎    | 101/189 [01:33<01:14,  1.18it/s]                                                 {'loss': 0.7765, 'grad_norm': 1.4437038197974519, 'learning_rate': 9.726775956284152e-07, 'epoch': 1.6}
 53%|█████▎    | 101/189 [01:33<01:14,  1.18it/s] 54%|█████▍    | 102/189 [01:34<01:13,  1.18it/s]                                                 {'loss': 0.7919, 'grad_norm': 1.4126194212351333, 'learning_rate': 9.617486338797813e-07, 'epoch': 1.62}
 54%|█████▍    | 102/189 [01:34<01:13,  1.18it/s] 54%|█████▍    | 103/189 [01:35<01:13,  1.18it/s]                                                 {'loss': 0.8581, 'grad_norm': 1.502472861539398, 'learning_rate': 9.508196721311474e-07, 'epoch': 1.63}
 54%|█████▍    | 103/189 [01:35<01:13,  1.18it/s] 55%|█████▌    | 104/189 [01:36<01:12,  1.17it/s]                                                 {'loss': 0.8378, 'grad_norm': 1.6320294732925364, 'learning_rate': 9.398907103825137e-07, 'epoch': 1.65}
 55%|█████▌    | 104/189 [01:36<01:12,  1.17it/s] 56%|█████▌    | 105/189 [01:37<01:11,  1.17it/s]                                                 {'loss': 0.8075, 'grad_norm': 1.3838193888926091, 'learning_rate': 9.289617486338798e-07, 'epoch': 1.67}
 56%|█████▌    | 105/189 [01:37<01:11,  1.17it/s] 56%|█████▌    | 106/189 [01:37<01:10,  1.18it/s]                                                 {'loss': 0.8771, 'grad_norm': 1.7177531667692352, 'learning_rate': 9.180327868852458e-07, 'epoch': 1.68}
 56%|█████▌    | 106/189 [01:37<01:10,  1.18it/s] 57%|█████▋    | 107/189 [01:38<01:09,  1.18it/s]                                                 {'loss': 0.8261, 'grad_norm': 1.405544407432836, 'learning_rate': 9.07103825136612e-07, 'epoch': 1.7}
 57%|█████▋    | 107/189 [01:38<01:09,  1.18it/s] 57%|█████▋    | 108/189 [01:39<01:08,  1.18it/s]                                                 {'loss': 0.8687, 'grad_norm': 1.4996469487560093, 'learning_rate': 8.961748633879781e-07, 'epoch': 1.71}
 57%|█████▋    | 108/189 [01:39<01:08,  1.18it/s] 58%|█████▊    | 109/189 [01:40<01:08,  1.18it/s]                                                 {'loss': 0.8489, 'grad_norm': 1.5582933252575342, 'learning_rate': 8.852459016393443e-07, 'epoch': 1.73}
 58%|█████▊    | 109/189 [01:40<01:08,  1.18it/s] 58%|█████▊    | 110/189 [01:41<01:07,  1.18it/s]                                                 {'loss': 0.7901, 'grad_norm': 1.5650042555613337, 'learning_rate': 8.743169398907103e-07, 'epoch': 1.75}
 58%|█████▊    | 110/189 [01:41<01:07,  1.18it/s] 59%|█████▊    | 111/189 [01:42<01:06,  1.17it/s]                                                 {'loss': 0.8083, 'grad_norm': 1.487227414850296, 'learning_rate': 8.633879781420764e-07, 'epoch': 1.76}
 59%|█████▊    | 111/189 [01:42<01:06,  1.17it/s] 59%|█████▉    | 112/189 [01:43<01:05,  1.17it/s]                                                 {'loss': 0.7914, 'grad_norm': 1.5107719774044996, 'learning_rate': 8.524590163934425e-07, 'epoch': 1.78}
 59%|█████▉    | 112/189 [01:43<01:05,  1.17it/s] 60%|█████▉    | 113/189 [01:43<01:04,  1.18it/s]                                                 {'loss': 0.7554, 'grad_norm': 1.376913700316851, 'learning_rate': 8.415300546448088e-07, 'epoch': 1.79}
 60%|█████▉    | 113/189 [01:43<01:04,  1.18it/s] 60%|██████    | 114/189 [01:44<01:03,  1.17it/s]                                                 {'loss': 0.9547, 'grad_norm': 1.5325850711010973, 'learning_rate': 8.306010928961748e-07, 'epoch': 1.81}
 60%|██████    | 114/189 [01:44<01:03,  1.17it/s] 61%|██████    | 115/189 [01:45<01:02,  1.17it/s]                                                 {'loss': 0.9268, 'grad_norm': 1.4727461255553043, 'learning_rate': 8.196721311475409e-07, 'epoch': 1.83}
 61%|██████    | 115/189 [01:45<01:02,  1.17it/s] 61%|██████▏   | 116/189 [01:46<01:02,  1.17it/s]                                                 {'loss': 0.8087, 'grad_norm': 1.4252521378704532, 'learning_rate': 8.087431693989071e-07, 'epoch': 1.84}
 61%|██████▏   | 116/189 [01:46<01:02,  1.17it/s] 62%|██████▏   | 117/189 [01:47<01:01,  1.18it/s]                                                 {'loss': 0.8583, 'grad_norm': 1.7513294984752181, 'learning_rate': 7.978142076502732e-07, 'epoch': 1.86}
 62%|██████▏   | 117/189 [01:47<01:01,  1.18it/s] 62%|██████▏   | 118/189 [01:48<01:00,  1.17it/s]                                                 {'loss': 0.8323, 'grad_norm': 1.4600358237491604, 'learning_rate': 7.868852459016393e-07, 'epoch': 1.87}
 62%|██████▏   | 118/189 [01:48<01:00,  1.17it/s] 63%|██████▎   | 119/189 [01:49<00:59,  1.17it/s]                                                 {'loss': 0.7845, 'grad_norm': 1.3902327671424686, 'learning_rate': 7.759562841530054e-07, 'epoch': 1.89}
 63%|██████▎   | 119/189 [01:49<00:59,  1.17it/s] 63%|██████▎   | 120/189 [01:49<00:58,  1.17it/s]                                                 {'loss': 0.7811, 'grad_norm': 1.501324071685713, 'learning_rate': 7.650273224043715e-07, 'epoch': 1.9}
 63%|██████▎   | 120/189 [01:49<00:58,  1.17it/s] 64%|██████▍   | 121/189 [01:50<00:57,  1.17it/s]                                                 {'loss': 0.9222, 'grad_norm': 1.5872749610802093, 'learning_rate': 7.540983606557376e-07, 'epoch': 1.92}
 64%|██████▍   | 121/189 [01:50<00:57,  1.17it/s] 65%|██████▍   | 122/189 [01:51<00:57,  1.17it/s]                                                 {'loss': 0.8139, 'grad_norm': 1.4132382034086235, 'learning_rate': 7.431693989071039e-07, 'epoch': 1.94}
 65%|██████▍   | 122/189 [01:51<00:57,  1.17it/s] 65%|██████▌   | 123/189 [01:52<00:56,  1.17it/s]                                                 {'loss': 0.7607, 'grad_norm': 1.565160099200368, 'learning_rate': 7.322404371584699e-07, 'epoch': 1.95}
 65%|██████▌   | 123/189 [01:52<00:56,  1.17it/s] 66%|██████▌   | 124/189 [01:53<00:55,  1.18it/s]                                                 {'loss': 0.79, 'grad_norm': 1.4353836452014972, 'learning_rate': 7.21311475409836e-07, 'epoch': 1.97}
 66%|██████▌   | 124/189 [01:53<00:55,  1.18it/s] 66%|██████▌   | 125/189 [01:54<00:54,  1.17it/s]                                                 {'loss': 0.8653, 'grad_norm': 1.6557400634579853, 'learning_rate': 7.103825136612022e-07, 'epoch': 1.98}
 66%|██████▌   | 125/189 [01:54<00:54,  1.17it/s] 67%|██████▋   | 126/189 [01:55<00:53,  1.17it/s]                                                 {'loss': 0.8285, 'grad_norm': 1.5968935024062219, 'learning_rate': 6.994535519125683e-07, 'epoch': 2.0}
 67%|██████▋   | 126/189 [01:55<00:53,  1.17it/s] 67%|██████▋   | 127/189 [01:55<00:52,  1.17it/s]                                                 {'loss': 0.8005, 'grad_norm': 1.4864114691445605, 'learning_rate': 6.885245901639343e-07, 'epoch': 2.02}
 67%|██████▋   | 127/189 [01:55<00:52,  1.17it/s] 68%|██████▊   | 128/189 [01:56<00:52,  1.17it/s]                                                 {'loss': 0.8489, 'grad_norm': 1.7623714132442037, 'learning_rate': 6.775956284153005e-07, 'epoch': 2.03}
 68%|██████▊   | 128/189 [01:56<00:52,  1.17it/s] 68%|██████▊   | 129/189 [01:57<00:51,  1.17it/s]                                                 {'loss': 0.791, 'grad_norm': 1.4529710128082387, 'learning_rate': 6.666666666666666e-07, 'epoch': 2.05}
 68%|██████▊   | 129/189 [01:57<00:51,  1.17it/s] 69%|██████▉   | 130/189 [01:58<00:50,  1.17it/s]                                                 {'loss': 0.7734, 'grad_norm': 1.5530123398830538, 'learning_rate': 6.557377049180327e-07, 'epoch': 2.06}
 69%|██████▉   | 130/189 [01:58<00:50,  1.17it/s] 69%|██████▉   | 131/189 [01:59<00:49,  1.17it/s]                                                 {'loss': 0.7737, 'grad_norm': 1.506993867315665, 'learning_rate': 6.44808743169399e-07, 'epoch': 2.08}
 69%|██████▉   | 131/189 [01:59<00:49,  1.17it/s] 70%|██████▉   | 132/189 [02:00<00:48,  1.17it/s]                                                 {'loss': 0.9446, 'grad_norm': 1.6142656083880202, 'learning_rate': 6.33879781420765e-07, 'epoch': 2.1}
 70%|██████▉   | 132/189 [02:00<00:48,  1.17it/s] 70%|███████   | 133/189 [02:00<00:47,  1.17it/s]                                                 {'loss': 0.873, 'grad_norm': 1.6438339644567217, 'learning_rate': 6.229508196721311e-07, 'epoch': 2.11}
 70%|███████   | 133/189 [02:00<00:47,  1.17it/s] 71%|███████   | 134/189 [02:01<00:47,  1.17it/s]                                                 {'loss': 0.776, 'grad_norm': 1.6146535886107896, 'learning_rate': 6.120218579234973e-07, 'epoch': 2.13}
 71%|███████   | 134/189 [02:01<00:47,  1.17it/s] 71%|███████▏  | 135/189 [02:02<00:46,  1.17it/s]                                                 {'loss': 0.7651, 'grad_norm': 1.576338470353139, 'learning_rate': 6.010928961748634e-07, 'epoch': 2.14}
 71%|███████▏  | 135/189 [02:02<00:46,  1.17it/s] 72%|███████▏  | 136/189 [02:03<00:45,  1.17it/s]                                                 {'loss': 0.6394, 'grad_norm': 1.4231725589767714, 'learning_rate': 5.901639344262294e-07, 'epoch': 2.16}
 72%|███████▏  | 136/189 [02:03<00:45,  1.17it/s] 72%|███████▏  | 137/189 [02:04<00:44,  1.17it/s]                                                 {'loss': 0.657, 'grad_norm': 1.457066696592817, 'learning_rate': 5.792349726775956e-07, 'epoch': 2.17}
 72%|███████▏  | 137/189 [02:04<00:44,  1.17it/s] 73%|███████▎  | 138/189 [02:05<00:43,  1.17it/s]                                                 {'loss': 0.7804, 'grad_norm': 1.7162259701225753, 'learning_rate': 5.683060109289617e-07, 'epoch': 2.19}
 73%|███████▎  | 138/189 [02:05<00:43,  1.17it/s] 74%|███████▎  | 139/189 [02:06<00:42,  1.17it/s]                                                 {'loss': 0.6756, 'grad_norm': 1.582755245751261, 'learning_rate': 5.573770491803278e-07, 'epoch': 2.21}
 74%|███████▎  | 139/189 [02:06<00:42,  1.17it/s] 74%|███████▍  | 140/189 [02:06<00:41,  1.17it/s]                                                 {'loss': 0.7864, 'grad_norm': 1.6779526602023496, 'learning_rate': 5.46448087431694e-07, 'epoch': 2.22}
 74%|███████▍  | 140/189 [02:06<00:41,  1.17it/s] 75%|███████▍  | 141/189 [02:07<00:40,  1.17it/s]                                                 {'loss': 0.7511, 'grad_norm': 1.5413284106226284, 'learning_rate': 5.355191256830601e-07, 'epoch': 2.24}
 75%|███████▍  | 141/189 [02:07<00:40,  1.17it/s] 75%|███████▌  | 142/189 [02:08<00:40,  1.17it/s]                                                 {'loss': 0.7496, 'grad_norm': 1.6070453108599776, 'learning_rate': 5.245901639344262e-07, 'epoch': 2.25}
 75%|███████▌  | 142/189 [02:08<00:40,  1.17it/s] 76%|███████▌  | 143/189 [02:09<00:39,  1.17it/s]                                                 {'loss': 0.718, 'grad_norm': 1.6691721286191583, 'learning_rate': 5.136612021857924e-07, 'epoch': 2.27}
 76%|███████▌  | 143/189 [02:09<00:39,  1.17it/s] 76%|███████▌  | 144/189 [02:10<00:38,  1.17it/s]                                                 {'loss': 0.8481, 'grad_norm': 1.597369329488524, 'learning_rate': 5.027322404371585e-07, 'epoch': 2.29}
 76%|███████▌  | 144/189 [02:10<00:38,  1.17it/s] 77%|███████▋  | 145/189 [02:11<00:37,  1.17it/s]                                                 {'loss': 0.6738, 'grad_norm': 1.3869577818224286, 'learning_rate': 4.918032786885245e-07, 'epoch': 2.3}
 77%|███████▋  | 145/189 [02:11<00:37,  1.17it/s] 77%|███████▋  | 146/189 [02:12<00:36,  1.17it/s]                                                 {'loss': 0.716, 'grad_norm': 1.4802499641112281, 'learning_rate': 4.808743169398907e-07, 'epoch': 2.32}
 77%|███████▋  | 146/189 [02:12<00:36,  1.17it/s] 78%|███████▊  | 147/189 [02:12<00:35,  1.17it/s]                                                 {'loss': 0.6895, 'grad_norm': 1.3604851762105072, 'learning_rate': 4.6994535519125684e-07, 'epoch': 2.33}
 78%|███████▊  | 147/189 [02:12<00:35,  1.17it/s] 78%|███████▊  | 148/189 [02:13<00:35,  1.17it/s]                                                 {'loss': 0.8578, 'grad_norm': 1.794122759024912, 'learning_rate': 4.590163934426229e-07, 'epoch': 2.35}
 78%|███████▊  | 148/189 [02:13<00:35,  1.17it/s] 79%|███████▉  | 149/189 [02:14<00:34,  1.16it/s]                                                 {'loss': 0.6549, 'grad_norm': 1.539153665842589, 'learning_rate': 4.4808743169398906e-07, 'epoch': 2.37}
 79%|███████▉  | 149/189 [02:14<00:34,  1.16it/s] 79%|███████▉  | 150/189 [02:15<00:33,  1.16it/s]                                                 {'loss': 0.7773, 'grad_norm': 1.475945458482523, 'learning_rate': 4.3715846994535514e-07, 'epoch': 2.38}
 79%|███████▉  | 150/189 [02:15<00:33,  1.16it/s] 80%|███████▉  | 151/189 [02:16<00:32,  1.16it/s]                                                 {'loss': 0.7862, 'grad_norm': 1.5726799255439956, 'learning_rate': 4.2622950819672127e-07, 'epoch': 2.4}
 80%|███████▉  | 151/189 [02:16<00:32,  1.16it/s] 80%|████████  | 152/189 [02:17<00:31,  1.17it/s]                                                 {'loss': 0.7939, 'grad_norm': 1.5285417154191667, 'learning_rate': 4.153005464480874e-07, 'epoch': 2.41}
 80%|████████  | 152/189 [02:17<00:31,  1.17it/s] 81%|████████  | 153/189 [02:18<00:30,  1.17it/s]                                                 {'loss': 0.9162, 'grad_norm': 1.729035583571767, 'learning_rate': 4.0437158469945354e-07, 'epoch': 2.43}
 81%|████████  | 153/189 [02:18<00:30,  1.17it/s] 81%|████████▏ | 154/189 [02:18<00:29,  1.17it/s]                                                 {'loss': 0.7289, 'grad_norm': 1.4413049018782471, 'learning_rate': 3.9344262295081967e-07, 'epoch': 2.44}
 81%|████████▏ | 154/189 [02:18<00:29,  1.17it/s] 82%|████████▏ | 155/189 [02:19<00:29,  1.17it/s]                                                 {'loss': 0.8456, 'grad_norm': 1.7971446951495074, 'learning_rate': 3.8251366120218575e-07, 'epoch': 2.46}
 82%|████████▏ | 155/189 [02:19<00:29,  1.17it/s] 83%|████████▎ | 156/189 [02:20<00:28,  1.17it/s]                                                 {'loss': 0.8743, 'grad_norm': 1.5704600338189207, 'learning_rate': 3.7158469945355194e-07, 'epoch': 2.48}
 83%|████████▎ | 156/189 [02:20<00:28,  1.17it/s] 83%|████████▎ | 157/189 [02:21<00:27,  1.17it/s]                                                 {'loss': 0.7701, 'grad_norm': 1.4457185318110408, 'learning_rate': 3.60655737704918e-07, 'epoch': 2.49}
 83%|████████▎ | 157/189 [02:21<00:27,  1.17it/s] 84%|████████▎ | 158/189 [02:22<00:26,  1.17it/s]                                                 {'loss': 0.6338, 'grad_norm': 1.3744144294992269, 'learning_rate': 3.4972677595628415e-07, 'epoch': 2.51}
 84%|████████▎ | 158/189 [02:22<00:26,  1.17it/s] 84%|████████▍ | 159/189 [02:23<00:25,  1.17it/s]                                                 {'loss': 0.7759, 'grad_norm': 1.6874189070755168, 'learning_rate': 3.3879781420765023e-07, 'epoch': 2.52}
 84%|████████▍ | 159/189 [02:23<00:25,  1.17it/s] 85%|████████▍ | 160/189 [02:24<00:24,  1.17it/s]                                                 {'loss': 0.6926, 'grad_norm': 1.5226670242564233, 'learning_rate': 3.2786885245901637e-07, 'epoch': 2.54}
 85%|████████▍ | 160/189 [02:24<00:24,  1.17it/s] 85%|████████▌ | 161/189 [02:24<00:23,  1.17it/s]                                                 {'loss': 0.7269, 'grad_norm': 1.5705562495987233, 'learning_rate': 3.169398907103825e-07, 'epoch': 2.56}
 85%|████████▌ | 161/189 [02:24<00:23,  1.17it/s] 86%|████████▌ | 162/189 [02:25<00:23,  1.17it/s]                                                 {'loss': 0.7221, 'grad_norm': 1.6281973534920116, 'learning_rate': 3.0601092896174863e-07, 'epoch': 2.57}
 86%|████████▌ | 162/189 [02:25<00:23,  1.17it/s] 86%|████████▌ | 163/189 [02:26<00:22,  1.17it/s]                                                 {'loss': 0.668, 'grad_norm': 1.5239906554003182, 'learning_rate': 2.950819672131147e-07, 'epoch': 2.59}
 86%|████████▌ | 163/189 [02:26<00:22,  1.17it/s] 87%|████████▋ | 164/189 [02:27<00:21,  1.17it/s]                                                 {'loss': 0.6834, 'grad_norm': 1.596851550520733, 'learning_rate': 2.8415300546448085e-07, 'epoch': 2.6}
 87%|████████▋ | 164/189 [02:27<00:21,  1.17it/s] 87%|████████▋ | 165/189 [02:28<00:20,  1.17it/s]                                                 {'loss': 0.688, 'grad_norm': 1.4799086404894648, 'learning_rate': 2.73224043715847e-07, 'epoch': 2.62}
 87%|████████▋ | 165/189 [02:28<00:20,  1.17it/s] 88%|████████▊ | 166/189 [02:29<00:19,  1.17it/s]                                                 {'loss': 0.8874, 'grad_norm': 1.7690694209574915, 'learning_rate': 2.622950819672131e-07, 'epoch': 2.63}
 88%|████████▊ | 166/189 [02:29<00:19,  1.17it/s] 88%|████████▊ | 167/189 [02:30<00:18,  1.17it/s]                                                 {'loss': 0.7568, 'grad_norm': 1.642101589033142, 'learning_rate': 2.5136612021857925e-07, 'epoch': 2.65}
 88%|████████▊ | 167/189 [02:30<00:18,  1.17it/s] 89%|████████▉ | 168/189 [02:30<00:17,  1.17it/s]                                                 {'loss': 0.7382, 'grad_norm': 1.9032000106315639, 'learning_rate': 2.4043715846994533e-07, 'epoch': 2.67}
 89%|████████▉ | 168/189 [02:30<00:17,  1.17it/s] 89%|████████▉ | 169/189 [02:31<00:17,  1.17it/s]                                                 {'loss': 0.7864, 'grad_norm': 1.7703334851851231, 'learning_rate': 2.2950819672131146e-07, 'epoch': 2.68}
 89%|████████▉ | 169/189 [02:31<00:17,  1.17it/s] 90%|████████▉ | 170/189 [02:32<00:16,  1.17it/s]                                                 {'loss': 0.8061, 'grad_norm': 1.5978919420746216, 'learning_rate': 2.1857923497267757e-07, 'epoch': 2.7}
 90%|████████▉ | 170/189 [02:32<00:16,  1.17it/s] 90%|█████████ | 171/189 [02:33<00:15,  1.17it/s]                                                 {'loss': 0.6048, 'grad_norm': 1.3282592948493825, 'learning_rate': 2.076502732240437e-07, 'epoch': 2.71}
 90%|█████████ | 171/189 [02:33<00:15,  1.17it/s] 91%|█████████ | 172/189 [02:34<00:14,  1.17it/s]                                                 {'loss': 0.7531, 'grad_norm': 1.5751970830789164, 'learning_rate': 1.9672131147540984e-07, 'epoch': 2.73}
 91%|█████████ | 172/189 [02:34<00:14,  1.17it/s] 92%|█████████▏| 173/189 [02:35<00:13,  1.17it/s]                                                 {'loss': 0.8704, 'grad_norm': 1.8784501642793043, 'learning_rate': 1.8579234972677597e-07, 'epoch': 2.75}
 92%|█████████▏| 173/189 [02:35<00:13,  1.17it/s] 92%|█████████▏| 174/189 [02:36<00:12,  1.17it/s]                                                 {'loss': 0.7369, 'grad_norm': 1.606801163244249, 'learning_rate': 1.7486338797814208e-07, 'epoch': 2.76}
 92%|█████████▏| 174/189 [02:36<00:12,  1.17it/s] 93%|█████████▎| 175/189 [02:36<00:11,  1.17it/s]                                                 {'loss': 0.7736, 'grad_norm': 1.6322989698973744, 'learning_rate': 1.6393442622950818e-07, 'epoch': 2.78}
 93%|█████████▎| 175/189 [02:36<00:11,  1.17it/s] 93%|█████████▎| 176/189 [02:37<00:11,  1.17it/s]                                                 {'loss': 0.6991, 'grad_norm': 1.705001697085203, 'learning_rate': 1.5300546448087432e-07, 'epoch': 2.79}
 93%|█████████▎| 176/189 [02:37<00:11,  1.17it/s] 94%|█████████▎| 177/189 [02:38<00:10,  1.17it/s]                                                 {'loss': 0.6693, 'grad_norm': 1.4477443706691842, 'learning_rate': 1.4207650273224042e-07, 'epoch': 2.81}
 94%|█████████▎| 177/189 [02:38<00:10,  1.17it/s] 94%|█████████▍| 178/189 [02:39<00:09,  1.17it/s]                                                 {'loss': 0.7186, 'grad_norm': 1.5626537145220802, 'learning_rate': 1.3114754098360656e-07, 'epoch': 2.83}
 94%|█████████▍| 178/189 [02:39<00:09,  1.17it/s] 95%|█████████▍| 179/189 [02:40<00:08,  1.17it/s]                                                 {'loss': 0.7977, 'grad_norm': 1.5086362003829763, 'learning_rate': 1.2021857923497266e-07, 'epoch': 2.84}
 95%|█████████▍| 179/189 [02:40<00:08,  1.17it/s] 95%|█████████▌| 180/189 [02:41<00:07,  1.17it/s]                                                 {'loss': 0.7299, 'grad_norm': 1.5595918060390865, 'learning_rate': 1.0928961748633878e-07, 'epoch': 2.86}
 95%|█████████▌| 180/189 [02:41<00:07,  1.17it/s] 96%|█████████▌| 181/189 [02:42<00:06,  1.17it/s]                                                 {'loss': 0.6902, 'grad_norm': 1.5037906873670417, 'learning_rate': 9.836065573770492e-08, 'epoch': 2.87}
 96%|█████████▌| 181/189 [02:42<00:06,  1.17it/s] 96%|█████████▋| 182/189 [02:42<00:05,  1.17it/s]                                                 {'loss': 0.6815, 'grad_norm': 1.4588823551066032, 'learning_rate': 8.743169398907104e-08, 'epoch': 2.89}
 96%|█████████▋| 182/189 [02:42<00:05,  1.17it/s] 97%|█████████▋| 183/189 [02:43<00:05,  1.17it/s]                                                 {'loss': 0.7019, 'grad_norm': 1.54479001367853, 'learning_rate': 7.650273224043716e-08, 'epoch': 2.9}
 97%|█████████▋| 183/189 [02:43<00:05,  1.17it/s] 97%|█████████▋| 184/189 [02:44<00:04,  1.17it/s]                                                 {'loss': 0.7892, 'grad_norm': 1.8607189349991158, 'learning_rate': 6.557377049180328e-08, 'epoch': 2.92}
 97%|█████████▋| 184/189 [02:44<00:04,  1.17it/s] 98%|█████████▊| 185/189 [02:45<00:03,  1.17it/s]                                                 {'loss': 0.6819, 'grad_norm': 1.3824183100061167, 'learning_rate': 5.464480874316939e-08, 'epoch': 2.94}
 98%|█████████▊| 185/189 [02:45<00:03,  1.17it/s] 98%|█████████▊| 186/189 [02:46<00:02,  1.17it/s]                                                 {'loss': 0.5754, 'grad_norm': 1.6834685349826293, 'learning_rate': 4.371584699453552e-08, 'epoch': 2.95}
 98%|█████████▊| 186/189 [02:46<00:02,  1.17it/s] 99%|█████████▉| 187/189 [02:47<00:01,  1.17it/s]                                                 {'loss': 0.7864, 'grad_norm': 1.5761538490934779, 'learning_rate': 3.278688524590164e-08, 'epoch': 2.97}
 99%|█████████▉| 187/189 [02:47<00:01,  1.17it/s] 99%|█████████▉| 188/189 [02:48<00:00,  1.17it/s]                                                 {'loss': 0.6612, 'grad_norm': 1.472312349526598, 'learning_rate': 2.185792349726776e-08, 'epoch': 2.98}
 99%|█████████▉| 188/189 [02:48<00:00,  1.17it/s]100%|██████████| 189/189 [02:48<00:00,  1.17it/s]                                                 {'loss': 0.7052, 'grad_norm': 1.5211519385975707, 'learning_rate': 1.092896174863388e-08, 'epoch': 3.0}
100%|██████████| 189/189 [02:48<00:00,  1.17it/s]                                                 {'train_runtime': 226.1957, 'train_samples_per_second': 66.142, 'train_steps_per_second': 0.836, 'train_loss': 0.8816317285179461, 'epoch': 3.0}
100%|██████████| 189/189 [03:46<00:00,  1.17it/s]100%|██████████| 189/189 [03:46<00:00,  1.20s/it]
[2024-08-28 18:25:53,679] [INFO] [launch.py:351:main] Process 3558510 exits successfully.
[2024-08-28 18:25:54,680] [INFO] [launch.py:351:main] Process 3558511 exits successfully.
[2024-08-28 18:25:54,680] [INFO] [launch.py:351:main] Process 3558513 exits successfully.
[2024-08-28 18:25:54,681] [INFO] [launch.py:351:main] Process 3558512 exits successfully.
[2024-08-28 18:25:54,681] [INFO] [launch.py:351:main] Process 3558507 exits successfully.
[2024-08-28 18:25:55,681] [INFO] [launch.py:351:main] Process 3558509 exits successfully.
[2024-08-28 18:25:55,681] [INFO] [launch.py:351:main] Process 3558508 exits successfully.
[2024-08-28 18:26:17,684] [INFO] [launch.py:351:main] Process 3558506 exits successfully.
