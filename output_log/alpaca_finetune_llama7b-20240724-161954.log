Namespace(mode=['alpaca'], base_model='meta-llama/Llama-2-7b-hf', task_name='dolly', tuned_dir='./cache')
num gpus:  8
Running 1/1: deepspeed --num_gpus=8 train.py --deepspeed ../deepspeed_config/zero3.json --bf16 --tf32=True
    --model_name_or_path meta-llama/Llama-2-7b-hf --data_path ../data/stanford_alpaca/dolly_data.json
    --output_dir /fsx-project/yunyun/models/meta-llama_meta-llama_dolly_tuned
    --num_train_epochs 3
    --per_device_train_batch_size 10
    --per_device_eval_batch_size 4
    --gradient_accumulation_steps 1
    --gradient_checkpointing=True
    --evaluation_strategy=no
    --save_strategy=steps
    --save_steps 500
    --save_total_limit 1
    --learning_rate 2e-6
    --weight_decay 0.
    --report_to tensorboard
    --warmup_ratio 0.03
    --lr_scheduler_type=cosine
    --logging_steps 1
['deepspeed', '--num_gpus=8', 'train.py', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Llama-2-7b-hf', '--data_path', '../data/stanford_alpaca/dolly_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_meta-llama_dolly_tuned', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1']
[2024-07-24 16:20:03,893] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2024-07-24 16:20:12,301] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2024-07-24 16:20:12,301] [INFO] [runner.py:568:main] cmd = /data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNSwgNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None train.py --deepspeed ../deepspeed_config/zero3.json --bf16 --tf32=True --model_name_or_path meta-llama/Llama-2-7b-hf --data_path ../data/stanford_alpaca/dolly_data.json --output_dir /fsx-project/yunyun/models/meta-llama_meta-llama_dolly_tuned --num_train_epochs 3 --per_device_train_batch_size 10 --per_device_eval_batch_size 4 --gradient_accumulation_steps 1 --gradient_checkpointing=True --evaluation_strategy=no --save_strategy=steps --save_steps 500 --save_total_limit 1 --learning_rate 2e-6 --weight_decay 0. --report_to tensorboard --warmup_ratio 0.03 --lr_scheduler_type=cosine --logging_steps 1
[2024-07-24 16:20:14,907] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2024-07-24 16:20:18,405] [INFO] [launch.py:139:main] 0 NCCL_ASYNC_ERROR_HANDLING=1
[2024-07-24 16:20:18,406] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]}
[2024-07-24 16:20:18,406] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=8, node_rank=0
[2024-07-24 16:20:18,406] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]})
[2024-07-24 16:20:18,406] [INFO] [launch.py:164:main] dist_world_size=8
[2024-07-24 16:20:18,406] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
[2024-07-24 16:20:18,407] [INFO] [launch.py:256:main] process 131584 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=0', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Llama-2-7b-hf', '--data_path', '../data/stanford_alpaca/dolly_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_meta-llama_dolly_tuned', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1']
[2024-07-24 16:20:18,407] [INFO] [launch.py:256:main] process 131585 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=1', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Llama-2-7b-hf', '--data_path', '../data/stanford_alpaca/dolly_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_meta-llama_dolly_tuned', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1']
[2024-07-24 16:20:18,408] [INFO] [launch.py:256:main] process 131586 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=2', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Llama-2-7b-hf', '--data_path', '../data/stanford_alpaca/dolly_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_meta-llama_dolly_tuned', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1']
[2024-07-24 16:20:18,409] [INFO] [launch.py:256:main] process 131587 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=3', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Llama-2-7b-hf', '--data_path', '../data/stanford_alpaca/dolly_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_meta-llama_dolly_tuned', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1']
[2024-07-24 16:20:18,409] [INFO] [launch.py:256:main] process 131588 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=4', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Llama-2-7b-hf', '--data_path', '../data/stanford_alpaca/dolly_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_meta-llama_dolly_tuned', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1']
[2024-07-24 16:20:18,410] [INFO] [launch.py:256:main] process 131589 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=5', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Llama-2-7b-hf', '--data_path', '../data/stanford_alpaca/dolly_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_meta-llama_dolly_tuned', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1']
[2024-07-24 16:20:18,410] [INFO] [launch.py:256:main] process 131590 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=6', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Llama-2-7b-hf', '--data_path', '../data/stanford_alpaca/dolly_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_meta-llama_dolly_tuned', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1']
[2024-07-24 16:20:18,411] [INFO] [launch.py:256:main] process 131591 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=7', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Llama-2-7b-hf', '--data_path', '../data/stanford_alpaca/dolly_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_meta-llama_dolly_tuned', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1']
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
[2024-07-24 16:20:34,855] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-24 16:20:34,857] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
[2024-07-24 16:20:34,981] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-24 16:20:34,988] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-24 16:20:35,053] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-24 16:20:35,055] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-24 16:20:35,057] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-24 16:20:35,059] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible

[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH


[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2024-07-24 16:20:35,637] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-24 16:20:35,637] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-07-24 16:20:35,637] [INFO] [comm.py:637:init_distributed] cdb=None
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[2024-07-24 16:20:35,734] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-24 16:20:35,740] [INFO] [comm.py:637:init_distributed] cdb=None
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[2024-07-24 16:20:35,808] [INFO] [comm.py:637:init_distributed] cdb=None
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[2024-07-24 16:20:35,817] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-24 16:20:35,822] [INFO] [comm.py:637:init_distributed] cdb=None
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[2024-07-24 16:20:35,840] [INFO] [comm.py:637:init_distributed] cdb=None
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 295.70it/s]
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1195.13it/s]
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 965.98it/s]
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1278.36it/s]
Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1375.41it/s]
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1377.89it/s]
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1133.29it/s]
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1278.17it/s]
[2024-07-24 16:20:46,776] [INFO] [partition_parameters.py:345:__exit__] finished initializing model - num_params = 291, num_elems = 6.74B
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:35<00:35, 35.40s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:35<00:35, 35.40s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:35<00:35, 35.39s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:35<00:35, 35.40s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:35<00:35, 35.40s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:35<00:35, 35.41s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:35<00:35, 35.40s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:57<00:00, 27.75s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:57<00:00, 28.90s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:57<00:00, 27.75s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:57<00:00, 28.90s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:57<00:00, 27.74s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:57<00:00, 28.89s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:57<00:00, 27.75s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:57<00:00, 28.90s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:57<00:00, 27.75s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:57<00:00, 28.90s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:57<00:00, 27.75s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:57<00:00, 28.90s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:57<00:00, 27.76s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:57<00:00, 28.91s/it]
Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [01:06<01:06, 66.42s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [01:29<00:00, 41.06s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [01:29<00:00, 44.86s/it]
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
[rank6]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121/fused_adam/build.ninja...
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/cpp_extension.py:1967: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
[rank4]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
[rank2]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[rank5]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
[rank7]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
[rank0]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[rank3]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
[rank1]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
[1/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output multi_tensor_adam.cuda.o.d -DTORCH_EXTENSION_NAME=fused_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/deepspeed/ops/csrc/includes -I/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/deepspeed/ops/csrc/adam -isystem /data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/include -isystem /data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/include/TH -isystem /data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /data/home/yunyun/miniconda3/envs/newtorch2/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_90,code=compute_90 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -lineinfo --use_fast_math -gencode=arch=compute_90,code=sm_90 -gencode=arch=compute_90,code=compute_90 -DBF16_AVAILABLE -U__CUDA_NO_BFLOAT16_OPERATORS__ -U__CUDA_NO_BFLOAT162_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ -std=c++17 -c /data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/deepspeed/ops/csrc/adam/multi_tensor_adam.cu -o multi_tensor_adam.cuda.o 
[2/3] c++ -MMD -MF fused_adam_frontend.o.d -DTORCH_EXTENSION_NAME=fused_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/deepspeed/ops/csrc/includes -I/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/deepspeed/ops/csrc/adam -isystem /data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/include -isystem /data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/include/TH -isystem /data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /data/home/yunyun/miniconda3/envs/newtorch2/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -std=c++17 -g -Wno-reorder -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DBF16_AVAILABLE -c /data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/deepspeed/ops/csrc/adam/fused_adam_frontend.cpp -o fused_adam_frontend.o 
[3/3] c++ fused_adam_frontend.o multi_tensor_adam.cuda.o -shared -L/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o fused_adam.so
Loading extension module fused_adam...
Loading extension module fused_adam...
Time to load fused_adam op: 34.85537838935852 seconds
Time to load fused_adam op: 33.83766436576843 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 33.83574366569519 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 34.43926739692688 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 34.66665530204773 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 33.837008476257324 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 34.6393404006958 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 34.639771699905396 seconds
Parameter Offload: Total persistent parameters: 266240 in 65 params
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  0%|          | 0/564 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
  0%|          | 1/564 [00:07<1:12:21,  7.71s/it]                                                 {'loss': 1.5262, 'grad_norm': 5.650102462036832, 'learning_rate': 0.0, 'epoch': 0.01}
  0%|          | 1/564 [00:07<1:12:21,  7.71s/it]  0%|          | 2/564 [00:08<36:24,  3.89s/it]                                                 {'loss': 1.5796, 'grad_norm': 5.392777239657525, 'learning_rate': 4.89301084236452e-07, 'epoch': 0.01}
  0%|          | 2/564 [00:08<36:24,  3.89s/it]  1%|          | 3/564 [00:09<23:28,  2.51s/it]                                               {'loss': 1.5796, 'grad_norm': 7.4797753433278436, 'learning_rate': 7.755238700769802e-07, 'epoch': 0.02}
  1%|          | 3/564 [00:09<23:28,  2.51s/it]  1%|          | 4/564 [00:10<17:25,  1.87s/it]                                               {'loss': 1.5069, 'grad_norm': 5.360299466815282, 'learning_rate': 9.78602168472904e-07, 'epoch': 0.02}
  1%|          | 4/564 [00:10<17:25,  1.87s/it]  1%|          | 5/564 [00:11<14:02,  1.51s/it]                                               {'loss': 1.6325, 'grad_norm': 5.799634205409149, 'learning_rate': 1.1361219343474658e-06, 'epoch': 0.03}
  1%|          | 5/564 [00:11<14:02,  1.51s/it]  1%|          | 6/564 [00:12<12:05,  1.30s/it]                                               {'loss': 1.4171, 'grad_norm': 5.423509928276179, 'learning_rate': 1.264824954313432e-06, 'epoch': 0.03}
  1%|          | 6/564 [00:12<12:05,  1.30s/it]  1%|          | 7/564 [00:13<10:46,  1.16s/it]                                               {'loss': 1.6137, 'grad_norm': 4.950273314484061, 'learning_rate': 1.373641807199326e-06, 'epoch': 0.04}
  1%|          | 7/564 [00:13<10:46,  1.16s/it]  1%|â–         | 8/564 [00:14<09:53,  1.07s/it]                                               {'loss': 1.4736, 'grad_norm': 4.606547508146066, 'learning_rate': 1.4679032527093559e-06, 'epoch': 0.04}
  1%|â–         | 8/564 [00:14<09:53,  1.07s/it]  2%|â–         | 9/564 [00:15<09:19,  1.01s/it]                                               {'loss': 1.5174, 'grad_norm': 3.8938337113982535, 'learning_rate': 1.5510477401539603e-06, 'epoch': 0.05}
  2%|â–         | 9/564 [00:15<09:19,  1.01s/it]  2%|â–         | 10/564 [00:15<08:59,  1.03it/s]                                                {'loss': 1.6199, 'grad_norm': 3.2867177306726187, 'learning_rate': 1.625423018583918e-06, 'epoch': 0.05}
  2%|â–         | 10/564 [00:15<08:59,  1.03it/s]  2%|â–         | 11/564 [00:16<08:43,  1.06it/s]                                                {'loss': 1.5644, 'grad_norm': 4.204899566689341, 'learning_rate': 1.6927036418410939e-06, 'epoch': 0.06}
  2%|â–         | 11/564 [00:16<08:43,  1.06it/s]  2%|â–         | 12/564 [00:17<08:30,  1.08it/s]                                                {'loss': 1.4802, 'grad_norm': 3.53791014157089, 'learning_rate': 1.7541260385498841e-06, 'epoch': 0.06}
  2%|â–         | 12/564 [00:17<08:30,  1.08it/s]  2%|â–         | 13/564 [00:18<08:20,  1.10it/s]                                                {'loss': 1.5354, 'grad_norm': 3.2610597094561786, 'learning_rate': 1.8106291662380673e-06, 'epoch': 0.07}
  2%|â–         | 13/564 [00:18<08:20,  1.10it/s]  2%|â–         | 14/564 [00:19<08:13,  1.12it/s]                                                {'loss': 1.4445, 'grad_norm': 2.822065549937672, 'learning_rate': 1.862942891435778e-06, 'epoch': 0.07}
  2%|â–         | 14/564 [00:19<08:13,  1.12it/s]  3%|â–         | 15/564 [00:20<08:10,  1.12it/s]                                                {'loss': 1.4552, 'grad_norm': 2.5467071949842244, 'learning_rate': 1.911645804424446e-06, 'epoch': 0.08}
  3%|â–         | 15/564 [00:20<08:10,  1.12it/s]  3%|â–         | 16/564 [00:21<08:05,  1.13it/s]                                                {'loss': 1.3681, 'grad_norm': 3.128255695131117, 'learning_rate': 1.957204336945808e-06, 'epoch': 0.09}
  3%|â–         | 16/564 [00:21<08:05,  1.13it/s]  3%|â–         | 17/564 [00:22<08:02,  1.13it/s]                                                {'loss': 1.486, 'grad_norm': 2.736680383762063, 'learning_rate': 2e-06, 'epoch': 0.09}
  3%|â–         | 17/564 [00:22<08:02,  1.13it/s]  3%|â–         | 18/564 [00:22<07:59,  1.14it/s]                                                {'loss': 1.3753, 'grad_norm': 2.97019519301539, 'learning_rate': 2e-06, 'epoch': 0.1}
  3%|â–         | 18/564 [00:22<07:59,  1.14it/s]  3%|â–         | 19/564 [00:23<07:57,  1.14it/s]                                                {'loss': 1.3265, 'grad_norm': 2.6630963799646183, 'learning_rate': 1.996343692870201e-06, 'epoch': 0.1}
  3%|â–         | 19/564 [00:23<07:57,  1.14it/s]  4%|â–         | 20/564 [00:24<07:56,  1.14it/s]                                                {'loss': 1.4046, 'grad_norm': 2.3234217656307887, 'learning_rate': 1.992687385740402e-06, 'epoch': 0.11}
  4%|â–         | 20/564 [00:24<07:56,  1.14it/s]  4%|â–         | 21/564 [00:25<07:57,  1.14it/s]                                                {'loss': 1.4214, 'grad_norm': 2.3576575872282226, 'learning_rate': 1.9890310786106034e-06, 'epoch': 0.11}
  4%|â–         | 21/564 [00:25<07:57,  1.14it/s]  4%|â–         | 22/564 [00:26<07:57,  1.14it/s]                                                {'loss': 1.4735, 'grad_norm': 2.238665593122556, 'learning_rate': 1.9853747714808044e-06, 'epoch': 0.12}
  4%|â–         | 22/564 [00:26<07:57,  1.14it/s]  4%|â–         | 23/564 [00:27<07:55,  1.14it/s]                                                {'loss': 1.4054, 'grad_norm': 2.3884964801134596, 'learning_rate': 1.9817184643510055e-06, 'epoch': 0.12}
  4%|â–         | 23/564 [00:27<07:55,  1.14it/s]  4%|â–         | 24/564 [00:28<07:53,  1.14it/s]                                                {'loss': 1.5818, 'grad_norm': 2.5879310897817005, 'learning_rate': 1.9780621572212065e-06, 'epoch': 0.13}
  4%|â–         | 24/564 [00:28<07:53,  1.14it/s]  4%|â–         | 25/564 [00:29<07:54,  1.14it/s]                                                {'loss': 1.4964, 'grad_norm': 2.0768798537358117, 'learning_rate': 1.9744058500914075e-06, 'epoch': 0.13}
  4%|â–         | 25/564 [00:29<07:54,  1.14it/s]  5%|â–         | 26/564 [00:29<07:54,  1.13it/s]                                                {'loss': 1.3487, 'grad_norm': 2.3931502505374196, 'learning_rate': 1.970749542961609e-06, 'epoch': 0.14}
  5%|â–         | 26/564 [00:29<07:54,  1.13it/s]  5%|â–         | 27/564 [00:30<07:52,  1.14it/s]                                                {'loss': 1.4787, 'grad_norm': 2.3369767608735628, 'learning_rate': 1.9670932358318095e-06, 'epoch': 0.14}
  5%|â–         | 27/564 [00:30<07:52,  1.14it/s]  5%|â–         | 28/564 [00:31<07:52,  1.13it/s]                                                {'loss': 1.3511, 'grad_norm': 2.041566976516252, 'learning_rate': 1.963436928702011e-06, 'epoch': 0.15}
  5%|â–         | 28/564 [00:31<07:52,  1.13it/s]  5%|â–Œ         | 29/564 [00:32<07:53,  1.13it/s]                                                {'loss': 1.5336, 'grad_norm': 2.627348847570675, 'learning_rate': 1.959780621572212e-06, 'epoch': 0.15}
  5%|â–Œ         | 29/564 [00:32<07:53,  1.13it/s]  5%|â–Œ         | 30/564 [00:33<07:51,  1.13it/s]                                                {'loss': 1.342, 'grad_norm': 2.4982802379625157, 'learning_rate': 1.956124314442413e-06, 'epoch': 0.16}
  5%|â–Œ         | 30/564 [00:33<07:51,  1.13it/s]  5%|â–Œ         | 31/564 [00:34<07:50,  1.13it/s]                                                {'loss': 1.4731, 'grad_norm': 2.6963786067492763, 'learning_rate': 1.952468007312614e-06, 'epoch': 0.16}
  5%|â–Œ         | 31/564 [00:34<07:50,  1.13it/s]  6%|â–Œ         | 32/564 [00:35<07:49,  1.13it/s]                                                {'loss': 1.4746, 'grad_norm': 2.21468527128572, 'learning_rate': 1.948811700182815e-06, 'epoch': 0.17}
  6%|â–Œ         | 32/564 [00:35<07:49,  1.13it/s]  6%|â–Œ         | 33/564 [00:36<07:49,  1.13it/s]                                                {'loss': 1.4858, 'grad_norm': 2.399550926509059, 'learning_rate': 1.9451553930530165e-06, 'epoch': 0.18}
  6%|â–Œ         | 33/564 [00:36<07:49,  1.13it/s]  6%|â–Œ         | 34/564 [00:37<07:47,  1.13it/s]                                                {'loss': 1.5351, 'grad_norm': 2.3068320515351783, 'learning_rate': 1.9414990859232176e-06, 'epoch': 0.18}
  6%|â–Œ         | 34/564 [00:37<07:47,  1.13it/s]  6%|â–Œ         | 35/564 [00:37<07:46,  1.13it/s]                                                {'loss': 1.396, 'grad_norm': 2.475573827966074, 'learning_rate': 1.9378427787934186e-06, 'epoch': 0.19}
  6%|â–Œ         | 35/564 [00:37<07:46,  1.13it/s]  6%|â–‹         | 36/564 [00:38<07:46,  1.13it/s]                                                {'loss': 1.3802, 'grad_norm': 2.1431581192594424, 'learning_rate': 1.9341864716636196e-06, 'epoch': 0.19}
  6%|â–‹         | 36/564 [00:38<07:46,  1.13it/s]  7%|â–‹         | 37/564 [00:39<07:45,  1.13it/s]                                                {'loss': 1.5062, 'grad_norm': 2.4077486971975617, 'learning_rate': 1.9305301645338206e-06, 'epoch': 0.2}
  7%|â–‹         | 37/564 [00:39<07:45,  1.13it/s]  7%|â–‹         | 38/564 [00:40<07:44,  1.13it/s]                                                {'loss': 1.4547, 'grad_norm': 2.330013859315988, 'learning_rate': 1.9268738574040217e-06, 'epoch': 0.2}
  7%|â–‹         | 38/564 [00:40<07:44,  1.13it/s]  7%|â–‹         | 39/564 [00:41<07:43,  1.13it/s]                                                {'loss': 1.2928, 'grad_norm': 2.328914019893586, 'learning_rate': 1.923217550274223e-06, 'epoch': 0.21}
  7%|â–‹         | 39/564 [00:41<07:43,  1.13it/s]  7%|â–‹         | 40/564 [00:42<07:41,  1.14it/s]                                                {'loss': 1.35, 'grad_norm': 2.2112104310983303, 'learning_rate': 1.919561243144424e-06, 'epoch': 0.21}
  7%|â–‹         | 40/564 [00:42<07:41,  1.14it/s]  7%|â–‹         | 41/564 [00:43<07:40,  1.13it/s]                                                {'loss': 1.5561, 'grad_norm': 2.269000284395129, 'learning_rate': 1.915904936014625e-06, 'epoch': 0.22}
  7%|â–‹         | 41/564 [00:43<07:40,  1.13it/s]  7%|â–‹         | 42/564 [00:44<07:40,  1.13it/s]                                                {'loss': 1.4836, 'grad_norm': 2.264419333120372, 'learning_rate': 1.912248628884826e-06, 'epoch': 0.22}
  7%|â–‹         | 42/564 [00:44<07:40,  1.13it/s]  8%|â–Š         | 43/564 [00:44<07:38,  1.14it/s]                                                {'loss': 1.3605, 'grad_norm': 2.808154753454971, 'learning_rate': 1.908592321755027e-06, 'epoch': 0.23}
  8%|â–Š         | 43/564 [00:44<07:38,  1.14it/s]  8%|â–Š         | 44/564 [00:45<07:39,  1.13it/s]                                                {'loss': 1.2085, 'grad_norm': 2.394897924745211, 'learning_rate': 1.9049360146252284e-06, 'epoch': 0.23}
  8%|â–Š         | 44/564 [00:45<07:39,  1.13it/s]  8%|â–Š         | 45/564 [00:46<07:37,  1.13it/s]                                                {'loss': 1.5379, 'grad_norm': 2.369838746364921, 'learning_rate': 1.9012797074954294e-06, 'epoch': 0.24}
  8%|â–Š         | 45/564 [00:46<07:37,  1.13it/s]  8%|â–Š         | 46/564 [00:47<07:36,  1.13it/s]                                                {'loss': 1.3966, 'grad_norm': 2.0938663250241745, 'learning_rate': 1.8976234003656307e-06, 'epoch': 0.24}
  8%|â–Š         | 46/564 [00:47<07:36,  1.13it/s]  8%|â–Š         | 47/564 [00:48<08:36,  1.00it/s]                                                {'loss': 1.6131, 'grad_norm': 2.110412690009265, 'learning_rate': 1.8939670932358317e-06, 'epoch': 0.25}
  8%|â–Š         | 47/564 [00:48<08:36,  1.00it/s]  9%|â–Š         | 48/564 [00:49<08:20,  1.03it/s]                                                {'loss': 1.3894, 'grad_norm': 2.4142733441371504, 'learning_rate': 1.8903107861060327e-06, 'epoch': 0.26}
  9%|â–Š         | 48/564 [00:49<08:20,  1.03it/s]  9%|â–Š         | 49/564 [00:50<08:05,  1.06it/s]                                                {'loss': 1.3348, 'grad_norm': 2.1618339346816624, 'learning_rate': 1.886654478976234e-06, 'epoch': 0.26}
  9%|â–Š         | 49/564 [00:50<08:05,  1.06it/s]  9%|â–‰         | 50/564 [00:51<07:55,  1.08it/s]                                                {'loss': 1.3928, 'grad_norm': 2.3937312839474356, 'learning_rate': 1.882998171846435e-06, 'epoch': 0.27}
  9%|â–‰         | 50/564 [00:51<07:55,  1.08it/s]  9%|â–‰         | 51/564 [00:52<07:48,  1.09it/s]                                                {'loss': 1.4151, 'grad_norm': 2.3580283786658645, 'learning_rate': 1.8793418647166362e-06, 'epoch': 0.27}
  9%|â–‰         | 51/564 [00:52<07:48,  1.09it/s]  9%|â–‰         | 52/564 [00:53<07:42,  1.11it/s]                                                {'loss': 1.4156, 'grad_norm': 2.274290151574222, 'learning_rate': 1.875685557586837e-06, 'epoch': 0.28}
  9%|â–‰         | 52/564 [00:53<07:42,  1.11it/s]  9%|â–‰         | 53/564 [00:54<07:39,  1.11it/s]                                                {'loss': 1.2337, 'grad_norm': 2.247760444497393, 'learning_rate': 1.8720292504570383e-06, 'epoch': 0.28}
  9%|â–‰         | 53/564 [00:54<07:39,  1.11it/s] 10%|â–‰         | 54/564 [00:55<07:35,  1.12it/s]                                                {'loss': 1.2897, 'grad_norm': 1.979841437948741, 'learning_rate': 1.8683729433272395e-06, 'epoch': 0.29}
 10%|â–‰         | 54/564 [00:55<07:35,  1.12it/s] 10%|â–‰         | 55/564 [00:55<07:32,  1.13it/s]                                                {'loss': 1.311, 'grad_norm': 2.519892627463348, 'learning_rate': 1.8647166361974405e-06, 'epoch': 0.29}
 10%|â–‰         | 55/564 [00:55<07:32,  1.13it/s] 10%|â–‰         | 56/564 [00:56<07:30,  1.13it/s]                                                {'loss': 1.1636, 'grad_norm': 2.102597056267815, 'learning_rate': 1.8610603290676416e-06, 'epoch': 0.3}
 10%|â–‰         | 56/564 [00:56<07:30,  1.13it/s] 10%|â–ˆ         | 57/564 [00:57<07:28,  1.13it/s]                                                {'loss': 1.3908, 'grad_norm': 2.3011984922999495, 'learning_rate': 1.8574040219378426e-06, 'epoch': 0.3}
 10%|â–ˆ         | 57/564 [00:57<07:28,  1.13it/s] 10%|â–ˆ         | 58/564 [00:58<07:27,  1.13it/s]                                                {'loss': 1.4879, 'grad_norm': 2.209876186153025, 'learning_rate': 1.8537477148080438e-06, 'epoch': 0.31}
 10%|â–ˆ         | 58/564 [00:58<07:27,  1.13it/s] 10%|â–ˆ         | 59/564 [00:59<07:26,  1.13it/s]                                                {'loss': 1.3487, 'grad_norm': 2.3785661070498856, 'learning_rate': 1.8500914076782448e-06, 'epoch': 0.31}
 10%|â–ˆ         | 59/564 [00:59<07:26,  1.13it/s] 11%|â–ˆ         | 60/564 [01:00<07:24,  1.13it/s]                                                {'loss': 1.2978, 'grad_norm': 2.1351247231970327, 'learning_rate': 1.846435100548446e-06, 'epoch': 0.32}
 11%|â–ˆ         | 60/564 [01:00<07:24,  1.13it/s] 11%|â–ˆ         | 61/564 [01:01<07:24,  1.13it/s]                                                {'loss': 1.4847, 'grad_norm': 2.2332975118365366, 'learning_rate': 1.842778793418647e-06, 'epoch': 0.32}
 11%|â–ˆ         | 61/564 [01:01<07:24,  1.13it/s] 11%|â–ˆ         | 62/564 [01:02<07:26,  1.12it/s]                                                {'loss': 1.4359, 'grad_norm': 2.1120735765069183, 'learning_rate': 1.8391224862888481e-06, 'epoch': 0.33}
 11%|â–ˆ         | 62/564 [01:02<07:26,  1.12it/s] 11%|â–ˆ         | 63/564 [01:03<07:25,  1.12it/s]                                                {'loss': 1.285, 'grad_norm': 2.340399015159122, 'learning_rate': 1.8354661791590494e-06, 'epoch': 0.34}
 11%|â–ˆ         | 63/564 [01:03<07:25,  1.12it/s] 11%|â–ˆâ–        | 64/564 [01:03<07:23,  1.13it/s]                                                {'loss': 1.3021, 'grad_norm': 2.4134282167987777, 'learning_rate': 1.8318098720292504e-06, 'epoch': 0.34}
 11%|â–ˆâ–        | 64/564 [01:03<07:23,  1.13it/s] 12%|â–ˆâ–        | 65/564 [01:04<07:21,  1.13it/s]                                                {'loss': 1.6282, 'grad_norm': 2.027220597462677, 'learning_rate': 1.8281535648994514e-06, 'epoch': 0.35}
 12%|â–ˆâ–        | 65/564 [01:04<07:21,  1.13it/s] 12%|â–ˆâ–        | 66/564 [01:05<07:20,  1.13it/s]                                                {'loss': 1.3612, 'grad_norm': 2.2418773098978377, 'learning_rate': 1.8244972577696524e-06, 'epoch': 0.35}
 12%|â–ˆâ–        | 66/564 [01:05<07:20,  1.13it/s] 12%|â–ˆâ–        | 67/564 [01:06<07:19,  1.13it/s]                                                {'loss': 1.4814, 'grad_norm': 2.0735612032054065, 'learning_rate': 1.8208409506398537e-06, 'epoch': 0.36}
 12%|â–ˆâ–        | 67/564 [01:06<07:19,  1.13it/s] 12%|â–ˆâ–        | 68/564 [01:07<07:18,  1.13it/s]                                                {'loss': 1.2849, 'grad_norm': 1.8996643181553, 'learning_rate': 1.817184643510055e-06, 'epoch': 0.36}
 12%|â–ˆâ–        | 68/564 [01:07<07:18,  1.13it/s] 12%|â–ˆâ–        | 69/564 [01:08<07:17,  1.13it/s]                                                {'loss': 1.3903, 'grad_norm': 2.1526386753491824, 'learning_rate': 1.813528336380256e-06, 'epoch': 0.37}
 12%|â–ˆâ–        | 69/564 [01:08<07:17,  1.13it/s] 12%|â–ˆâ–        | 70/564 [01:09<07:17,  1.13it/s]                                                {'loss': 1.4356, 'grad_norm': 2.1841178149066622, 'learning_rate': 1.809872029250457e-06, 'epoch': 0.37}
 12%|â–ˆâ–        | 70/564 [01:09<07:17,  1.13it/s] 13%|â–ˆâ–        | 71/564 [01:10<07:18,  1.13it/s]                                                {'loss': 1.4314, 'grad_norm': 2.0625661386492506, 'learning_rate': 1.806215722120658e-06, 'epoch': 0.38}
 13%|â–ˆâ–        | 71/564 [01:10<07:18,  1.13it/s] 13%|â–ˆâ–        | 72/564 [01:11<07:15,  1.13it/s]                                                {'loss': 1.2716, 'grad_norm': 2.378575549717089, 'learning_rate': 1.8025594149908592e-06, 'epoch': 0.38}
 13%|â–ˆâ–        | 72/564 [01:11<07:15,  1.13it/s] 13%|â–ˆâ–        | 73/564 [01:11<07:14,  1.13it/s]                                                {'loss': 1.4153, 'grad_norm': 2.0660353007498715, 'learning_rate': 1.7989031078610602e-06, 'epoch': 0.39}
 13%|â–ˆâ–        | 73/564 [01:11<07:14,  1.13it/s] 13%|â–ˆâ–        | 74/564 [01:12<07:14,  1.13it/s]                                                {'loss': 1.3346, 'grad_norm': 2.4449440815438312, 'learning_rate': 1.7952468007312612e-06, 'epoch': 0.39}
 13%|â–ˆâ–        | 74/564 [01:12<07:14,  1.13it/s] 13%|â–ˆâ–        | 75/564 [01:13<07:13,  1.13it/s]                                                {'loss': 1.2515, 'grad_norm': 2.3343944275269117, 'learning_rate': 1.7915904936014625e-06, 'epoch': 0.4}
 13%|â–ˆâ–        | 75/564 [01:13<07:13,  1.13it/s] 13%|â–ˆâ–        | 76/564 [01:14<07:12,  1.13it/s]                                                {'loss': 1.6044, 'grad_norm': 2.2666051896979456, 'learning_rate': 1.7879341864716635e-06, 'epoch': 0.4}
 13%|â–ˆâ–        | 76/564 [01:14<07:12,  1.13it/s] 14%|â–ˆâ–        | 77/564 [01:15<07:12,  1.13it/s]                                                {'loss': 1.194, 'grad_norm': 2.19717059595988, 'learning_rate': 1.7842778793418647e-06, 'epoch': 0.41}
 14%|â–ˆâ–        | 77/564 [01:15<07:12,  1.13it/s] 14%|â–ˆâ–        | 78/564 [01:16<07:14,  1.12it/s]                                                {'loss': 1.4838, 'grad_norm': 2.3942752532611418, 'learning_rate': 1.7806215722120656e-06, 'epoch': 0.41}
 14%|â–ˆâ–        | 78/564 [01:16<07:14,  1.12it/s] 14%|â–ˆâ–        | 79/564 [01:17<07:12,  1.12it/s]                                                {'loss': 1.2887, 'grad_norm': 2.281301822258142, 'learning_rate': 1.7769652650822668e-06, 'epoch': 0.42}
 14%|â–ˆâ–        | 79/564 [01:17<07:12,  1.12it/s] 14%|â–ˆâ–        | 80/564 [01:18<07:10,  1.13it/s]                                                {'loss': 1.2311, 'grad_norm': 2.8668357751659834, 'learning_rate': 1.7733089579524678e-06, 'epoch': 0.43}
 14%|â–ˆâ–        | 80/564 [01:18<07:10,  1.13it/s] 14%|â–ˆâ–        | 81/564 [01:19<07:08,  1.13it/s]                                                {'loss': 1.4039, 'grad_norm': 2.4068544137287238, 'learning_rate': 1.769652650822669e-06, 'epoch': 0.43}
 14%|â–ˆâ–        | 81/564 [01:19<07:08,  1.13it/s] 15%|â–ˆâ–        | 82/564 [01:19<07:07,  1.13it/s]                                                {'loss': 1.5038, 'grad_norm': 2.4704662682381504, 'learning_rate': 1.7659963436928703e-06, 'epoch': 0.44}
 15%|â–ˆâ–        | 82/564 [01:19<07:07,  1.13it/s] 15%|â–ˆâ–        | 83/564 [01:20<07:06,  1.13it/s]                                                {'loss': 1.4911, 'grad_norm': 2.1910873995587683, 'learning_rate': 1.762340036563071e-06, 'epoch': 0.44}
 15%|â–ˆâ–        | 83/564 [01:20<07:06,  1.13it/s] 15%|â–ˆâ–        | 84/564 [01:21<07:05,  1.13it/s]                                                {'loss': 1.4611, 'grad_norm': 2.2671299089485926, 'learning_rate': 1.7586837294332723e-06, 'epoch': 0.45}
 15%|â–ˆâ–        | 84/564 [01:21<07:05,  1.13it/s] 15%|â–ˆâ–Œ        | 85/564 [01:22<07:05,  1.12it/s]                                                {'loss': 1.4035, 'grad_norm': 2.1824430839058597, 'learning_rate': 1.7550274223034734e-06, 'epoch': 0.45}
 15%|â–ˆâ–Œ        | 85/564 [01:22<07:05,  1.12it/s] 15%|â–ˆâ–Œ        | 86/564 [01:23<07:03,  1.13it/s]                                                {'loss': 1.5417, 'grad_norm': 2.0959833923885585, 'learning_rate': 1.7513711151736746e-06, 'epoch': 0.46}
 15%|â–ˆâ–Œ        | 86/564 [01:23<07:03,  1.13it/s] 15%|â–ˆâ–Œ        | 87/564 [01:24<07:02,  1.13it/s]                                                {'loss': 1.402, 'grad_norm': 2.5887171665481956, 'learning_rate': 1.7477148080438754e-06, 'epoch': 0.46}
 15%|â–ˆâ–Œ        | 87/564 [01:24<07:02,  1.13it/s] 16%|â–ˆâ–Œ        | 88/564 [01:25<07:02,  1.13it/s]                                                {'loss': 1.4076, 'grad_norm': 2.1840197865815716, 'learning_rate': 1.7440585009140766e-06, 'epoch': 0.47}
 16%|â–ˆâ–Œ        | 88/564 [01:25<07:02,  1.13it/s] 16%|â–ˆâ–Œ        | 89/564 [01:26<07:01,  1.13it/s]                                                {'loss': 1.4619, 'grad_norm': 2.296576316354161, 'learning_rate': 1.7404021937842779e-06, 'epoch': 0.47}
 16%|â–ˆâ–Œ        | 89/564 [01:26<07:01,  1.13it/s] 16%|â–ˆâ–Œ        | 90/564 [01:27<07:01,  1.13it/s]                                                {'loss': 1.2767, 'grad_norm': 2.329255810610213, 'learning_rate': 1.736745886654479e-06, 'epoch': 0.48}
 16%|â–ˆâ–Œ        | 90/564 [01:27<07:01,  1.13it/s] 16%|â–ˆâ–Œ        | 91/564 [01:27<07:01,  1.12it/s]                                                {'loss': 1.2117, 'grad_norm': 1.9560050377482499, 'learning_rate': 1.7330895795246801e-06, 'epoch': 0.48}
 16%|â–ˆâ–Œ        | 91/564 [01:27<07:01,  1.12it/s] 16%|â–ˆâ–‹        | 92/564 [01:28<06:59,  1.12it/s]                                                {'loss': 1.5541, 'grad_norm': 2.540418662775906, 'learning_rate': 1.729433272394881e-06, 'epoch': 0.49}
 16%|â–ˆâ–‹        | 92/564 [01:28<06:59,  1.12it/s] 16%|â–ˆâ–‹        | 93/564 [01:29<06:59,  1.12it/s]                                                {'loss': 1.2518, 'grad_norm': 2.5194172445442633, 'learning_rate': 1.7257769652650822e-06, 'epoch': 0.49}
 16%|â–ˆâ–‹        | 93/564 [01:29<06:59,  1.12it/s] 17%|â–ˆâ–‹        | 94/564 [01:30<06:57,  1.13it/s]                                                {'loss': 1.3838, 'grad_norm': 2.1801706554247087, 'learning_rate': 1.7221206581352832e-06, 'epoch': 0.5}
 17%|â–ˆâ–‹        | 94/564 [01:30<06:57,  1.13it/s] 17%|â–ˆâ–‹        | 95/564 [01:31<06:57,  1.12it/s]                                                {'loss': 1.2558, 'grad_norm': 2.327801836175495, 'learning_rate': 1.7184643510054844e-06, 'epoch': 0.51}
 17%|â–ˆâ–‹        | 95/564 [01:31<06:57,  1.12it/s] 17%|â–ˆâ–‹        | 96/564 [01:32<06:57,  1.12it/s]                                                {'loss': 1.4733, 'grad_norm': 2.3854863547919867, 'learning_rate': 1.7148080438756855e-06, 'epoch': 0.51}
 17%|â–ˆâ–‹        | 96/564 [01:32<06:57,  1.12it/s] 17%|â–ˆâ–‹        | 97/564 [01:33<06:55,  1.12it/s]                                                {'loss': 1.2628, 'grad_norm': 2.4307770894097214, 'learning_rate': 1.7111517367458865e-06, 'epoch': 0.52}
 17%|â–ˆâ–‹        | 97/564 [01:33<06:55,  1.12it/s] 17%|â–ˆâ–‹        | 98/564 [01:34<06:53,  1.13it/s]                                                {'loss': 1.5583, 'grad_norm': 2.5283555910937423, 'learning_rate': 1.7074954296160877e-06, 'epoch': 0.52}
 17%|â–ˆâ–‹        | 98/564 [01:34<06:53,  1.13it/s] 18%|â–ˆâ–Š        | 99/564 [01:35<06:52,  1.13it/s]                                                {'loss': 1.4997, 'grad_norm': 2.1204638361638763, 'learning_rate': 1.7038391224862887e-06, 'epoch': 0.53}
 18%|â–ˆâ–Š        | 99/564 [01:35<06:52,  1.13it/s] 18%|â–ˆâ–Š        | 100/564 [01:35<06:51,  1.13it/s]                                                 {'loss': 1.2961, 'grad_norm': 2.2349872883477846, 'learning_rate': 1.70018281535649e-06, 'epoch': 0.53}
 18%|â–ˆâ–Š        | 100/564 [01:35<06:51,  1.13it/s] 18%|â–ˆâ–Š        | 101/564 [01:36<06:51,  1.13it/s]                                                 {'loss': 1.3216, 'grad_norm': 1.993541943243154, 'learning_rate': 1.6965265082266908e-06, 'epoch': 0.54}
 18%|â–ˆâ–Š        | 101/564 [01:36<06:51,  1.13it/s] 18%|â–ˆâ–Š        | 102/564 [01:37<06:51,  1.12it/s]                                                 {'loss': 1.3662, 'grad_norm': 2.1687541342556136, 'learning_rate': 1.692870201096892e-06, 'epoch': 0.54}
 18%|â–ˆâ–Š        | 102/564 [01:37<06:51,  1.12it/s] 18%|â–ˆâ–Š        | 103/564 [01:38<06:50,  1.12it/s]                                                 {'loss': 1.4745, 'grad_norm': 2.1149300240992384, 'learning_rate': 1.6892138939670933e-06, 'epoch': 0.55}
 18%|â–ˆâ–Š        | 103/564 [01:38<06:50,  1.12it/s] 18%|â–ˆâ–Š        | 104/564 [01:39<06:50,  1.12it/s]                                                 {'loss': 1.5418, 'grad_norm': 2.1416105717208564, 'learning_rate': 1.6855575868372943e-06, 'epoch': 0.55}
 18%|â–ˆâ–Š        | 104/564 [01:39<06:50,  1.12it/s] 19%|â–ˆâ–Š        | 105/564 [01:40<06:48,  1.12it/s]                                                 {'loss': 1.3889, 'grad_norm': 2.1662671352594005, 'learning_rate': 1.6819012797074953e-06, 'epoch': 0.56}
 19%|â–ˆâ–Š        | 105/564 [01:40<06:48,  1.12it/s] 19%|â–ˆâ–‰        | 106/564 [01:41<06:47,  1.12it/s]                                                 {'loss': 1.3804, 'grad_norm': 2.2225805963505354, 'learning_rate': 1.6782449725776963e-06, 'epoch': 0.56}
 19%|â–ˆâ–‰        | 106/564 [01:41<06:47,  1.12it/s] 19%|â–ˆâ–‰        | 107/564 [01:42<06:45,  1.13it/s]                                                 {'loss': 1.2121, 'grad_norm': 2.1122257043373494, 'learning_rate': 1.6745886654478976e-06, 'epoch': 0.57}
 19%|â–ˆâ–‰        | 107/564 [01:42<06:45,  1.13it/s] 19%|â–ˆâ–‰        | 108/564 [01:43<06:42,  1.13it/s]                                                 {'loss': 1.3794, 'grad_norm': 2.67613667273644, 'learning_rate': 1.6709323583180986e-06, 'epoch': 0.57}
 19%|â–ˆâ–‰        | 108/564 [01:43<06:42,  1.13it/s] 19%|â–ˆâ–‰        | 109/564 [01:43<06:41,  1.13it/s]                                                 {'loss': 1.544, 'grad_norm': 2.091518725320124, 'learning_rate': 1.6672760511882998e-06, 'epoch': 0.58}
 19%|â–ˆâ–‰        | 109/564 [01:43<06:41,  1.13it/s] 20%|â–ˆâ–‰        | 110/564 [01:44<06:42,  1.13it/s]                                                 {'loss': 1.3268, 'grad_norm': 2.065264594146693, 'learning_rate': 1.6636197440585008e-06, 'epoch': 0.59}
 20%|â–ˆâ–‰        | 110/564 [01:44<06:42,  1.13it/s] 20%|â–ˆâ–‰        | 111/564 [01:45<06:43,  1.12it/s]                                                 {'loss': 1.3641, 'grad_norm': 2.4468509670498078, 'learning_rate': 1.6599634369287019e-06, 'epoch': 0.59}
 20%|â–ˆâ–‰        | 111/564 [01:45<06:43,  1.12it/s] 20%|â–ˆâ–‰        | 112/564 [01:46<06:42,  1.12it/s]                                                 {'loss': 1.2874, 'grad_norm': 2.216989813488973, 'learning_rate': 1.6563071297989031e-06, 'epoch': 0.6}
 20%|â–ˆâ–‰        | 112/564 [01:46<06:42,  1.12it/s] 20%|â–ˆâ–ˆ        | 113/564 [01:47<06:43,  1.12it/s]                                                 {'loss': 1.4362, 'grad_norm': 2.156109096993215, 'learning_rate': 1.6526508226691041e-06, 'epoch': 0.6}
 20%|â–ˆâ–ˆ        | 113/564 [01:47<06:43,  1.12it/s] 20%|â–ˆâ–ˆ        | 114/564 [01:48<06:40,  1.12it/s]                                                 {'loss': 1.4106, 'grad_norm': 2.5567517012145973, 'learning_rate': 1.6489945155393052e-06, 'epoch': 0.61}
 20%|â–ˆâ–ˆ        | 114/564 [01:48<06:40,  1.12it/s] 20%|â–ˆâ–ˆ        | 115/564 [01:49<06:39,  1.12it/s]                                                 {'loss': 1.486, 'grad_norm': 2.328935875658487, 'learning_rate': 1.6453382084095064e-06, 'epoch': 0.61}
 20%|â–ˆâ–ˆ        | 115/564 [01:49<06:39,  1.12it/s] 21%|â–ˆâ–ˆ        | 116/564 [01:50<06:38,  1.12it/s]                                                 {'loss': 1.3627, 'grad_norm': 2.052464658391081, 'learning_rate': 1.6416819012797074e-06, 'epoch': 0.62}
 21%|â–ˆâ–ˆ        | 116/564 [01:50<06:38,  1.12it/s] 21%|â–ˆâ–ˆ        | 117/564 [01:51<06:37,  1.12it/s]                                                 {'loss': 1.1902, 'grad_norm': 2.227752808907685, 'learning_rate': 1.6380255941499086e-06, 'epoch': 0.62}
 21%|â–ˆâ–ˆ        | 117/564 [01:51<06:37,  1.12it/s] 21%|â–ˆâ–ˆ        | 118/564 [01:51<06:35,  1.13it/s]                                                 {'loss': 1.3857, 'grad_norm': 2.251311910284079, 'learning_rate': 1.6343692870201097e-06, 'epoch': 0.63}
 21%|â–ˆâ–ˆ        | 118/564 [01:51<06:35,  1.13it/s] 21%|â–ˆâ–ˆ        | 119/564 [01:52<06:35,  1.13it/s]                                                 {'loss': 1.3541, 'grad_norm': 2.293382775799816, 'learning_rate': 1.6307129798903107e-06, 'epoch': 0.63}
 21%|â–ˆâ–ˆ        | 119/564 [01:52<06:35,  1.13it/s] 21%|â–ˆâ–ˆâ–       | 120/564 [01:53<06:33,  1.13it/s]                                                 {'loss': 1.4305, 'grad_norm': 2.2163304597610396, 'learning_rate': 1.6270566727605117e-06, 'epoch': 0.64}
 21%|â–ˆâ–ˆâ–       | 120/564 [01:53<06:33,  1.13it/s] 21%|â–ˆâ–ˆâ–       | 121/564 [01:54<06:32,  1.13it/s]                                                 {'loss': 1.3459, 'grad_norm': 2.2416693297639343, 'learning_rate': 1.623400365630713e-06, 'epoch': 0.64}
 21%|â–ˆâ–ˆâ–       | 121/564 [01:54<06:32,  1.13it/s] 22%|â–ˆâ–ˆâ–       | 122/564 [01:55<06:32,  1.13it/s]                                                 {'loss': 1.5326, 'grad_norm': 1.9597337546427476, 'learning_rate': 1.6197440585009142e-06, 'epoch': 0.65}
 22%|â–ˆâ–ˆâ–       | 122/564 [01:55<06:32,  1.13it/s] 22%|â–ˆâ–ˆâ–       | 123/564 [01:56<06:30,  1.13it/s]                                                 {'loss': 1.4341, 'grad_norm': 2.204380646368183, 'learning_rate': 1.616087751371115e-06, 'epoch': 0.65}
 22%|â–ˆâ–ˆâ–       | 123/564 [01:56<06:30,  1.13it/s] 22%|â–ˆâ–ˆâ–       | 124/564 [01:57<06:30,  1.13it/s]                                                 {'loss': 1.5851, 'grad_norm': 2.2350438969780746, 'learning_rate': 1.6124314442413162e-06, 'epoch': 0.66}
 22%|â–ˆâ–ˆâ–       | 124/564 [01:57<06:30,  1.13it/s] 22%|â–ˆâ–ˆâ–       | 125/564 [01:58<06:29,  1.13it/s]                                                 {'loss': 1.3097, 'grad_norm': 1.8988342284819648, 'learning_rate': 1.6087751371115173e-06, 'epoch': 0.66}
 22%|â–ˆâ–ˆâ–       | 125/564 [01:58<06:29,  1.13it/s] 22%|â–ˆâ–ˆâ–       | 126/564 [01:59<06:28,  1.13it/s]                                                 {'loss': 1.4339, 'grad_norm': 2.3072713068813067, 'learning_rate': 1.6051188299817185e-06, 'epoch': 0.67}
 22%|â–ˆâ–ˆâ–       | 126/564 [01:59<06:28,  1.13it/s] 23%|â–ˆâ–ˆâ–       | 127/564 [01:59<06:29,  1.12it/s]                                                 {'loss': 1.4482, 'grad_norm': 2.4338062369020728, 'learning_rate': 1.6014625228519193e-06, 'epoch': 0.68}
 23%|â–ˆâ–ˆâ–       | 127/564 [01:59<06:29,  1.12it/s] 23%|â–ˆâ–ˆâ–       | 128/564 [02:01<07:11,  1.01it/s]                                                 {'loss': 1.4177, 'grad_norm': 2.500620969611638, 'learning_rate': 1.5978062157221205e-06, 'epoch': 0.68}
 23%|â–ˆâ–ˆâ–       | 128/564 [02:01<07:11,  1.01it/s] 23%|â–ˆâ–ˆâ–       | 129/564 [02:02<06:56,  1.04it/s]                                                 {'loss': 1.5212, 'grad_norm': 2.4388111531433156, 'learning_rate': 1.5941499085923218e-06, 'epoch': 0.69}
 23%|â–ˆâ–ˆâ–       | 129/564 [02:02<06:56,  1.04it/s] 23%|â–ˆâ–ˆâ–       | 130/564 [02:02<06:46,  1.07it/s]                                                 {'loss': 1.2945, 'grad_norm': 2.3498084938267154, 'learning_rate': 1.5904936014625228e-06, 'epoch': 0.69}
 23%|â–ˆâ–ˆâ–       | 130/564 [02:02<06:46,  1.07it/s] 23%|â–ˆâ–ˆâ–       | 131/564 [02:03<06:39,  1.08it/s]                                                 {'loss': 1.2759, 'grad_norm': 2.082353817049746, 'learning_rate': 1.586837294332724e-06, 'epoch': 0.7}
 23%|â–ˆâ–ˆâ–       | 131/564 [02:03<06:39,  1.08it/s] 23%|â–ˆâ–ˆâ–       | 132/564 [02:04<06:34,  1.10it/s]                                                 {'loss': 1.3017, 'grad_norm': 2.2863487756887966, 'learning_rate': 1.5831809872029248e-06, 'epoch': 0.7}
 23%|â–ˆâ–ˆâ–       | 132/564 [02:04<06:34,  1.10it/s] 24%|â–ˆâ–ˆâ–       | 133/564 [02:05<06:31,  1.10it/s]                                                 {'loss': 1.4259, 'grad_norm': 2.2689907237017675, 'learning_rate': 1.579524680073126e-06, 'epoch': 0.71}
 24%|â–ˆâ–ˆâ–       | 133/564 [02:05<06:31,  1.10it/s] 24%|â–ˆâ–ˆâ–       | 134/564 [02:06<06:28,  1.11it/s]                                                 {'loss': 1.517, 'grad_norm': 2.5314776106377375, 'learning_rate': 1.5758683729433271e-06, 'epoch': 0.71}
 24%|â–ˆâ–ˆâ–       | 134/564 [02:06<06:28,  1.11it/s] 24%|â–ˆâ–ˆâ–       | 135/564 [02:07<06:24,  1.11it/s]                                                 {'loss': 1.4917, 'grad_norm': 2.256725813811998, 'learning_rate': 1.5722120658135283e-06, 'epoch': 0.72}
 24%|â–ˆâ–ˆâ–       | 135/564 [02:07<06:24,  1.11it/s] 24%|â–ˆâ–ˆâ–       | 136/564 [02:08<06:21,  1.12it/s]                                                 {'loss': 1.3157, 'grad_norm': 2.115855147886816, 'learning_rate': 1.5685557586837294e-06, 'epoch': 0.72}
 24%|â–ˆâ–ˆâ–       | 136/564 [02:08<06:21,  1.12it/s] 24%|â–ˆâ–ˆâ–       | 137/564 [02:09<06:20,  1.12it/s]                                                 {'loss': 1.4221, 'grad_norm': 2.285692777693159, 'learning_rate': 1.5648994515539304e-06, 'epoch': 0.73}
 24%|â–ˆâ–ˆâ–       | 137/564 [02:09<06:20,  1.12it/s] 24%|â–ˆâ–ˆâ–       | 138/564 [02:10<06:19,  1.12it/s]                                                 {'loss': 1.5404, 'grad_norm': 2.0663429703150107, 'learning_rate': 1.5612431444241316e-06, 'epoch': 0.73}
 24%|â–ˆâ–ˆâ–       | 138/564 [02:10<06:19,  1.12it/s] 25%|â–ˆâ–ˆâ–       | 139/564 [02:10<06:17,  1.13it/s]                                                 {'loss': 1.442, 'grad_norm': 2.6848846276237572, 'learning_rate': 1.5575868372943326e-06, 'epoch': 0.74}
 25%|â–ˆâ–ˆâ–       | 139/564 [02:10<06:17,  1.13it/s] 25%|â–ˆâ–ˆâ–       | 140/564 [02:11<06:16,  1.13it/s]                                                 {'loss': 1.2347, 'grad_norm': 2.433937710316235, 'learning_rate': 1.5539305301645339e-06, 'epoch': 0.74}
 25%|â–ˆâ–ˆâ–       | 140/564 [02:11<06:16,  1.13it/s] 25%|â–ˆâ–ˆâ–Œ       | 141/564 [02:12<06:15,  1.13it/s]                                                 {'loss': 1.5954, 'grad_norm': 2.550479367192783, 'learning_rate': 1.5502742230347347e-06, 'epoch': 0.75}
 25%|â–ˆâ–ˆâ–Œ       | 141/564 [02:12<06:15,  1.13it/s] 25%|â–ˆâ–ˆâ–Œ       | 142/564 [02:13<06:13,  1.13it/s]                                                 {'loss': 1.3237, 'grad_norm': 2.3835123322536638, 'learning_rate': 1.546617915904936e-06, 'epoch': 0.76}
 25%|â–ˆâ–ˆâ–Œ       | 142/564 [02:13<06:13,  1.13it/s] 25%|â–ˆâ–ˆâ–Œ       | 143/564 [02:14<06:12,  1.13it/s]                                                 {'loss': 1.4275, 'grad_norm': 2.1164647916090837, 'learning_rate': 1.5429616087751372e-06, 'epoch': 0.76}
 25%|â–ˆâ–ˆâ–Œ       | 143/564 [02:14<06:12,  1.13it/s] 26%|â–ˆâ–ˆâ–Œ       | 144/564 [02:15<06:11,  1.13it/s]                                                 {'loss': 1.4364, 'grad_norm': 2.25232725263884, 'learning_rate': 1.5393053016453382e-06, 'epoch': 0.77}
 26%|â–ˆâ–ˆâ–Œ       | 144/564 [02:15<06:11,  1.13it/s] 26%|â–ˆâ–ˆâ–Œ       | 145/564 [02:16<06:13,  1.12it/s]                                                 {'loss': 1.2855, 'grad_norm': 2.1122019909308527, 'learning_rate': 1.5356489945155392e-06, 'epoch': 0.77}
 26%|â–ˆâ–ˆâ–Œ       | 145/564 [02:16<06:13,  1.12it/s] 26%|â–ˆâ–ˆâ–Œ       | 146/564 [02:17<06:11,  1.12it/s]                                                 {'loss': 1.4783, 'grad_norm': 2.169131275703846, 'learning_rate': 1.5319926873857402e-06, 'epoch': 0.78}
 26%|â–ˆâ–ˆâ–Œ       | 146/564 [02:17<06:11,  1.12it/s] 26%|â–ˆâ–ˆâ–Œ       | 147/564 [02:17<06:09,  1.13it/s]                                                 {'loss': 1.5138, 'grad_norm': 2.2881072700671297, 'learning_rate': 1.5283363802559415e-06, 'epoch': 0.78}
 26%|â–ˆâ–ˆâ–Œ       | 147/564 [02:17<06:09,  1.13it/s] 26%|â–ˆâ–ˆâ–Œ       | 148/564 [02:18<06:09,  1.13it/s]                                                 {'loss': 1.2302, 'grad_norm': 2.2088009719962383, 'learning_rate': 1.5246800731261425e-06, 'epoch': 0.79}
 26%|â–ˆâ–ˆâ–Œ       | 148/564 [02:18<06:09,  1.13it/s] 26%|â–ˆâ–ˆâ–‹       | 149/564 [02:19<06:08,  1.13it/s]                                                 {'loss': 1.331, 'grad_norm': 2.2590728268913245, 'learning_rate': 1.5210237659963437e-06, 'epoch': 0.79}
 26%|â–ˆâ–ˆâ–‹       | 149/564 [02:19<06:08,  1.13it/s] 27%|â–ˆâ–ˆâ–‹       | 150/564 [02:20<06:07,  1.13it/s]                                                 {'loss': 1.2344, 'grad_norm': 2.015210459374067, 'learning_rate': 1.5173674588665448e-06, 'epoch': 0.8}
 27%|â–ˆâ–ˆâ–‹       | 150/564 [02:20<06:07,  1.13it/s] 27%|â–ˆâ–ˆâ–‹       | 151/564 [02:21<06:06,  1.13it/s]                                                 {'loss': 1.4783, 'grad_norm': 2.7420803638859974, 'learning_rate': 1.5137111517367458e-06, 'epoch': 0.8}
 27%|â–ˆâ–ˆâ–‹       | 151/564 [02:21<06:06,  1.13it/s] 27%|â–ˆâ–ˆâ–‹       | 152/564 [02:22<06:07,  1.12it/s]                                                 {'loss': 1.4478, 'grad_norm': 2.409706821187259, 'learning_rate': 1.510054844606947e-06, 'epoch': 0.81}
 27%|â–ˆâ–ˆâ–‹       | 152/564 [02:22<06:07,  1.12it/s] 27%|â–ˆâ–ˆâ–‹       | 153/564 [02:23<06:06,  1.12it/s]                                                 {'loss': 1.3882, 'grad_norm': 2.7564820127871386, 'learning_rate': 1.506398537477148e-06, 'epoch': 0.81}
 27%|â–ˆâ–ˆâ–‹       | 153/564 [02:23<06:06,  1.12it/s] 27%|â–ˆâ–ˆâ–‹       | 154/564 [02:24<06:05,  1.12it/s]                                                 {'loss': 1.4301, 'grad_norm': 1.998429515257819, 'learning_rate': 1.502742230347349e-06, 'epoch': 0.82}
 27%|â–ˆâ–ˆâ–‹       | 154/564 [02:24<06:05,  1.12it/s] 27%|â–ˆâ–ˆâ–‹       | 155/564 [02:25<06:04,  1.12it/s]                                                 {'loss': 1.594, 'grad_norm': 2.466652215739888, 'learning_rate': 1.49908592321755e-06, 'epoch': 0.82}
 27%|â–ˆâ–ˆâ–‹       | 155/564 [02:25<06:04,  1.12it/s] 28%|â–ˆâ–ˆâ–Š       | 156/564 [02:25<06:03,  1.12it/s]                                                 {'loss': 1.2502, 'grad_norm': 2.525498593751057, 'learning_rate': 1.4954296160877513e-06, 'epoch': 0.83}
 28%|â–ˆâ–ˆâ–Š       | 156/564 [02:26<06:03,  1.12it/s] 28%|â–ˆâ–ˆâ–Š       | 157/564 [02:26<06:01,  1.12it/s]                                                 {'loss': 1.2732, 'grad_norm': 2.210702322455585, 'learning_rate': 1.4917733089579526e-06, 'epoch': 0.84}
 28%|â–ˆâ–ˆâ–Š       | 157/564 [02:26<06:01,  1.12it/s] 28%|â–ˆâ–ˆâ–Š       | 158/564 [02:27<06:01,  1.12it/s]                                                 {'loss': 1.5272, 'grad_norm': 2.0783269239079565, 'learning_rate': 1.4881170018281536e-06, 'epoch': 0.84}
 28%|â–ˆâ–ˆâ–Š       | 158/564 [02:27<06:01,  1.12it/s] 28%|â–ˆâ–ˆâ–Š       | 159/564 [02:28<05:59,  1.13it/s]                                                 {'loss': 1.2927, 'grad_norm': 2.649037341591197, 'learning_rate': 1.4844606946983546e-06, 'epoch': 0.85}
 28%|â–ˆâ–ˆâ–Š       | 159/564 [02:28<05:59,  1.13it/s] 28%|â–ˆâ–ˆâ–Š       | 160/564 [02:29<06:00,  1.12it/s]                                                 {'loss': 1.1753, 'grad_norm': 2.1649018196794927, 'learning_rate': 1.4808043875685556e-06, 'epoch': 0.85}
 28%|â–ˆâ–ˆâ–Š       | 160/564 [02:29<06:00,  1.12it/s] 29%|â–ˆâ–ˆâ–Š       | 161/564 [02:30<05:59,  1.12it/s]                                                 {'loss': 1.2927, 'grad_norm': 2.4832442315260477, 'learning_rate': 1.4771480804387569e-06, 'epoch': 0.86}
 29%|â–ˆâ–ˆâ–Š       | 161/564 [02:30<05:59,  1.12it/s] 29%|â–ˆâ–ˆâ–Š       | 162/564 [02:31<05:58,  1.12it/s]                                                 {'loss': 1.4906, 'grad_norm': 2.3359051925989682, 'learning_rate': 1.4734917733089579e-06, 'epoch': 0.86}
 29%|â–ˆâ–ˆâ–Š       | 162/564 [02:31<05:58,  1.12it/s] 29%|â–ˆâ–ˆâ–‰       | 163/564 [02:32<05:57,  1.12it/s]                                                 {'loss': 1.5346, 'grad_norm': 2.372294999147731, 'learning_rate': 1.469835466179159e-06, 'epoch': 0.87}
 29%|â–ˆâ–ˆâ–‰       | 163/564 [02:32<05:57,  1.12it/s] 29%|â–ˆâ–ˆâ–‰       | 164/564 [02:33<05:56,  1.12it/s]                                                 {'loss': 1.4716, 'grad_norm': 1.9144361551310716, 'learning_rate': 1.4661791590493601e-06, 'epoch': 0.87}
 29%|â–ˆâ–ˆâ–‰       | 164/564 [02:33<05:56,  1.12it/s] 29%|â–ˆâ–ˆâ–‰       | 165/564 [02:34<05:55,  1.12it/s]                                                 {'loss': 1.4494, 'grad_norm': 2.2745212305198725, 'learning_rate': 1.4625228519195612e-06, 'epoch': 0.88}
 29%|â–ˆâ–ˆâ–‰       | 165/564 [02:34<05:55,  1.12it/s] 29%|â–ˆâ–ˆâ–‰       | 166/564 [02:34<05:54,  1.12it/s]                                                 {'loss': 1.2887, 'grad_norm': 2.288146555891675, 'learning_rate': 1.4588665447897624e-06, 'epoch': 0.88}
 29%|â–ˆâ–ˆâ–‰       | 166/564 [02:34<05:54,  1.12it/s] 30%|â–ˆâ–ˆâ–‰       | 167/564 [02:35<05:53,  1.12it/s]                                                 {'loss': 1.4021, 'grad_norm': 2.3971801602636167, 'learning_rate': 1.4552102376599632e-06, 'epoch': 0.89}
 30%|â–ˆâ–ˆâ–‰       | 167/564 [02:35<05:53,  1.12it/s] 30%|â–ˆâ–ˆâ–‰       | 168/564 [02:36<05:51,  1.13it/s]                                                 {'loss': 1.222, 'grad_norm': 2.3787355199387883, 'learning_rate': 1.4515539305301644e-06, 'epoch': 0.89}
 30%|â–ˆâ–ˆâ–‰       | 168/564 [02:36<05:51,  1.13it/s] 30%|â–ˆâ–ˆâ–‰       | 169/564 [02:37<05:50,  1.13it/s]                                                 {'loss': 1.2977, 'grad_norm': 2.4154109163927977, 'learning_rate': 1.4478976234003655e-06, 'epoch': 0.9}
 30%|â–ˆâ–ˆâ–‰       | 169/564 [02:37<05:50,  1.13it/s] 30%|â–ˆâ–ˆâ–ˆ       | 170/564 [02:38<05:49,  1.13it/s]                                                 {'loss': 1.4392, 'grad_norm': 2.5025666843709833, 'learning_rate': 1.4442413162705667e-06, 'epoch': 0.9}
 30%|â–ˆâ–ˆâ–ˆ       | 170/564 [02:38<05:49,  1.13it/s] 30%|â–ˆâ–ˆâ–ˆ       | 171/564 [02:39<05:48,  1.13it/s]                                                 {'loss': 1.2817, 'grad_norm': 2.1849441673977505, 'learning_rate': 1.440585009140768e-06, 'epoch': 0.91}
 30%|â–ˆâ–ˆâ–ˆ       | 171/564 [02:39<05:48,  1.13it/s] 30%|â–ˆâ–ˆâ–ˆ       | 172/564 [02:40<05:48,  1.13it/s]                                                 {'loss': 1.4673, 'grad_norm': 2.1365295518823437, 'learning_rate': 1.4369287020109688e-06, 'epoch': 0.91}
 30%|â–ˆâ–ˆâ–ˆ       | 172/564 [02:40<05:48,  1.13it/s] 31%|â–ˆâ–ˆâ–ˆ       | 173/564 [02:41<05:47,  1.13it/s]                                                 {'loss': 1.299, 'grad_norm': 2.0784589368890014, 'learning_rate': 1.43327239488117e-06, 'epoch': 0.92}
 31%|â–ˆâ–ˆâ–ˆ       | 173/564 [02:41<05:47,  1.13it/s] 31%|â–ˆâ–ˆâ–ˆ       | 174/564 [02:42<05:47,  1.12it/s]                                                 {'loss': 1.3215, 'grad_norm': 2.39806883312606, 'learning_rate': 1.429616087751371e-06, 'epoch': 0.93}
 31%|â–ˆâ–ˆâ–ˆ       | 174/564 [02:42<05:47,  1.12it/s] 31%|â–ˆâ–ˆâ–ˆ       | 175/564 [02:42<05:44,  1.13it/s]                                                 {'loss': 1.3624, 'grad_norm': 2.2331155892205294, 'learning_rate': 1.4259597806215722e-06, 'epoch': 0.93}
 31%|â–ˆâ–ˆâ–ˆ       | 175/564 [02:42<05:44,  1.13it/s] 31%|â–ˆâ–ˆâ–ˆ       | 176/564 [02:43<05:44,  1.13it/s]                                                 {'loss': 1.5041, 'grad_norm': 1.8922977684951439, 'learning_rate': 1.422303473491773e-06, 'epoch': 0.94}
 31%|â–ˆâ–ˆâ–ˆ       | 176/564 [02:43<05:44,  1.13it/s] 31%|â–ˆâ–ˆâ–ˆâ–      | 177/564 [02:44<05:44,  1.12it/s]                                                 {'loss': 1.4083, 'grad_norm': 2.1158733822028597, 'learning_rate': 1.4186471663619743e-06, 'epoch': 0.94}
 31%|â–ˆâ–ˆâ–ˆâ–      | 177/564 [02:44<05:44,  1.12it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 178/564 [02:45<05:43,  1.12it/s]                                                 {'loss': 1.2956, 'grad_norm': 2.342649540377432, 'learning_rate': 1.4149908592321755e-06, 'epoch': 0.95}
 32%|â–ˆâ–ˆâ–ˆâ–      | 178/564 [02:45<05:43,  1.12it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 179/564 [02:46<05:43,  1.12it/s]                                                 {'loss': 1.4567, 'grad_norm': 2.370992328823342, 'learning_rate': 1.4113345521023766e-06, 'epoch': 0.95}
 32%|â–ˆâ–ˆâ–ˆâ–      | 179/564 [02:46<05:43,  1.12it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 180/564 [02:47<05:42,  1.12it/s]                                                 {'loss': 1.2559, 'grad_norm': 2.0651232482684034, 'learning_rate': 1.4076782449725778e-06, 'epoch': 0.96}
 32%|â–ˆâ–ˆâ–ˆâ–      | 180/564 [02:47<05:42,  1.12it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 181/564 [02:48<05:40,  1.12it/s]                                                 {'loss': 1.3476, 'grad_norm': 2.1789654447613436, 'learning_rate': 1.4040219378427786e-06, 'epoch': 0.96}
 32%|â–ˆâ–ˆâ–ˆâ–      | 181/564 [02:48<05:40,  1.12it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 182/564 [02:49<05:39,  1.13it/s]                                                 {'loss': 1.1487, 'grad_norm': 2.3722992353725774, 'learning_rate': 1.4003656307129798e-06, 'epoch': 0.97}
 32%|â–ˆâ–ˆâ–ˆâ–      | 182/564 [02:49<05:39,  1.13it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 183/564 [02:50<05:38,  1.13it/s]                                                 {'loss': 1.4152, 'grad_norm': 2.0577580167067366, 'learning_rate': 1.3967093235831809e-06, 'epoch': 0.97}
 32%|â–ˆâ–ˆâ–ˆâ–      | 183/564 [02:50<05:38,  1.13it/s] 33%|â–ˆâ–ˆâ–ˆâ–      | 184/564 [02:50<05:37,  1.12it/s]                                                 {'loss': 1.5011, 'grad_norm': 2.0869093636685703, 'learning_rate': 1.393053016453382e-06, 'epoch': 0.98}
 33%|â–ˆâ–ˆâ–ˆâ–      | 184/564 [02:50<05:37,  1.12it/s] 33%|â–ˆâ–ˆâ–ˆâ–      | 185/564 [02:51<05:36,  1.13it/s]                                                 {'loss': 1.3446, 'grad_norm': 2.1540929217346143, 'learning_rate': 1.3893967093235831e-06, 'epoch': 0.98}
 33%|â–ˆâ–ˆâ–ˆâ–      | 185/564 [02:51<05:36,  1.13it/s] 33%|â–ˆâ–ˆâ–ˆâ–      | 186/564 [02:52<05:35,  1.13it/s]                                                 {'loss': 1.3261, 'grad_norm': 2.4945405859143372, 'learning_rate': 1.3857404021937841e-06, 'epoch': 0.99}
 33%|â–ˆâ–ˆâ–ˆâ–      | 186/564 [02:52<05:35,  1.13it/s] 33%|â–ˆâ–ˆâ–ˆâ–      | 187/564 [02:53<05:34,  1.13it/s]                                                 {'loss': 1.2665, 'grad_norm': 2.0247277126278083, 'learning_rate': 1.3820840950639854e-06, 'epoch': 0.99}
 33%|â–ˆâ–ˆâ–ˆâ–      | 187/564 [02:53<05:34,  1.13it/s] 33%|â–ˆâ–ˆâ–ˆâ–      | 188/564 [02:54<05:33,  1.13it/s]                                                 {'loss': 1.2604, 'grad_norm': 2.8047906052933405, 'learning_rate': 1.3784277879341864e-06, 'epoch': 1.0}
 33%|â–ˆâ–ˆâ–ˆâ–      | 188/564 [02:54<05:33,  1.13it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 189/564 [02:55<05:32,  1.13it/s]                                                 {'loss': 1.1233, 'grad_norm': 1.8910679934522676, 'learning_rate': 1.3747714808043876e-06, 'epoch': 1.01}
 34%|â–ˆâ–ˆâ–ˆâ–      | 189/564 [02:55<05:32,  1.13it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 190/564 [02:56<05:32,  1.13it/s]                                                 {'loss': 1.0575, 'grad_norm': 1.8915382463458612, 'learning_rate': 1.3711151736745884e-06, 'epoch': 1.01}
 34%|â–ˆâ–ˆâ–ˆâ–      | 190/564 [02:56<05:32,  1.13it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 191/564 [02:57<05:31,  1.12it/s]                                                 {'loss': 1.2248, 'grad_norm': 1.977730173168299, 'learning_rate': 1.3674588665447897e-06, 'epoch': 1.02}
 34%|â–ˆâ–ˆâ–ˆâ–      | 191/564 [02:57<05:31,  1.12it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 192/564 [02:58<05:31,  1.12it/s]                                                 {'loss': 1.288, 'grad_norm': 1.9800320907262188, 'learning_rate': 1.363802559414991e-06, 'epoch': 1.02}
 34%|â–ˆâ–ˆâ–ˆâ–      | 192/564 [02:58<05:31,  1.12it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 193/564 [02:58<05:30,  1.12it/s]                                                 {'loss': 1.3249, 'grad_norm': 1.9088418840673462, 'learning_rate': 1.360146252285192e-06, 'epoch': 1.03}
 34%|â–ˆâ–ˆâ–ˆâ–      | 193/564 [02:58<05:30,  1.12it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 194/564 [02:59<05:30,  1.12it/s]                                                 {'loss': 1.4023, 'grad_norm': 2.357826772831939, 'learning_rate': 1.356489945155393e-06, 'epoch': 1.03}
 34%|â–ˆâ–ˆâ–ˆâ–      | 194/564 [02:59<05:30,  1.12it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 195/564 [03:00<05:28,  1.12it/s]                                                 {'loss': 1.2336, 'grad_norm': 2.0940432809090197, 'learning_rate': 1.352833638025594e-06, 'epoch': 1.04}
 35%|â–ˆâ–ˆâ–ˆâ–      | 195/564 [03:00<05:28,  1.12it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 196/564 [03:01<05:28,  1.12it/s]                                                 {'loss': 1.2612, 'grad_norm': 2.1111609617559117, 'learning_rate': 1.3491773308957952e-06, 'epoch': 1.04}
 35%|â–ˆâ–ˆâ–ˆâ–      | 196/564 [03:01<05:28,  1.12it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 197/564 [03:02<05:27,  1.12it/s]                                                 {'loss': 1.3329, 'grad_norm': 2.1540731364777423, 'learning_rate': 1.3455210237659962e-06, 'epoch': 1.05}
 35%|â–ˆâ–ˆâ–ˆâ–      | 197/564 [03:02<05:27,  1.12it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 198/564 [03:03<05:26,  1.12it/s]                                                 {'loss': 1.2634, 'grad_norm': 2.1389603132785058, 'learning_rate': 1.3418647166361975e-06, 'epoch': 1.05}
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 198/564 [03:03<05:26,  1.12it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 199/564 [03:04<05:25,  1.12it/s]                                                 {'loss': 1.2856, 'grad_norm': 2.103718941431708, 'learning_rate': 1.3382084095063985e-06, 'epoch': 1.06}
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 199/564 [03:04<05:25,  1.12it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 200/564 [03:05<05:23,  1.13it/s]                                                 {'loss': 1.2318, 'grad_norm': 2.3503318278521133, 'learning_rate': 1.3345521023765995e-06, 'epoch': 1.06}
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 200/564 [03:05<05:23,  1.13it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 201/564 [03:06<05:22,  1.13it/s]                                                 {'loss': 0.9678, 'grad_norm': 2.1604704874702247, 'learning_rate': 1.3308957952468008e-06, 'epoch': 1.07}
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 201/564 [03:06<05:22,  1.13it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 202/564 [03:06<05:21,  1.13it/s]                                                 {'loss': 1.4177, 'grad_norm': 1.9837942714197911, 'learning_rate': 1.3272394881170018e-06, 'epoch': 1.07}
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 202/564 [03:06<05:21,  1.13it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 203/564 [03:07<05:20,  1.13it/s]                                                 {'loss': 1.3714, 'grad_norm': 2.1332169345334053, 'learning_rate': 1.3235831809872028e-06, 'epoch': 1.08}
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 203/564 [03:07<05:20,  1.13it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 204/564 [03:08<05:19,  1.13it/s]                                                 {'loss': 1.182, 'grad_norm': 2.315564800360697, 'learning_rate': 1.3199268738574038e-06, 'epoch': 1.09}
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 204/564 [03:08<05:19,  1.13it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 205/564 [03:09<05:18,  1.13it/s]                                                 {'loss': 1.3073, 'grad_norm': 2.1586679321611415, 'learning_rate': 1.316270566727605e-06, 'epoch': 1.09}
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 205/564 [03:09<05:18,  1.13it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 206/564 [03:10<05:18,  1.12it/s]                                                 {'loss': 1.2393, 'grad_norm': 2.121541724762848, 'learning_rate': 1.3126142595978063e-06, 'epoch': 1.1}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 206/564 [03:10<05:18,  1.12it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 207/564 [03:11<05:18,  1.12it/s]                                                 {'loss': 1.5369, 'grad_norm': 1.8864898955260951, 'learning_rate': 1.3089579524680071e-06, 'epoch': 1.1}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 207/564 [03:11<05:18,  1.12it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 208/564 [03:12<05:15,  1.13it/s]                                                 {'loss': 1.2946, 'grad_norm': 2.447987532905983, 'learning_rate': 1.3053016453382084e-06, 'epoch': 1.11}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 208/564 [03:12<05:15,  1.13it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 209/564 [03:13<05:14,  1.13it/s]                                                 {'loss': 1.3687, 'grad_norm': 2.1591499359875654, 'learning_rate': 1.3016453382084094e-06, 'epoch': 1.11}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 209/564 [03:13<05:14,  1.13it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 210/564 [03:14<05:13,  1.13it/s]                                                 {'loss': 1.2756, 'grad_norm': 2.07314756326691, 'learning_rate': 1.2979890310786106e-06, 'epoch': 1.12}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 210/564 [03:14<05:13,  1.13it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 211/564 [03:14<05:13,  1.13it/s]                                                 {'loss': 1.3415, 'grad_norm': 2.1199743161801132, 'learning_rate': 1.2943327239488116e-06, 'epoch': 1.12}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 211/564 [03:14<05:13,  1.13it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 212/564 [03:15<05:13,  1.12it/s]                                                 {'loss': 1.4471, 'grad_norm': 2.203993898808005, 'learning_rate': 1.2906764168190127e-06, 'epoch': 1.13}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 212/564 [03:15<05:13,  1.12it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 213/564 [03:16<05:17,  1.10it/s]                                                 {'loss': 1.2201, 'grad_norm': 2.251743642989092, 'learning_rate': 1.2870201096892139e-06, 'epoch': 1.13}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 213/564 [03:16<05:17,  1.10it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 214/564 [03:17<05:15,  1.11it/s]                                                 {'loss': 1.433, 'grad_norm': 2.4036619861654236, 'learning_rate': 1.283363802559415e-06, 'epoch': 1.14}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 214/564 [03:17<05:15,  1.11it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 215/564 [03:18<05:13,  1.11it/s]                                                 {'loss': 1.2403, 'grad_norm': 2.1915090657871588, 'learning_rate': 1.2797074954296162e-06, 'epoch': 1.14}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 215/564 [03:18<05:13,  1.11it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 216/564 [03:19<05:12,  1.11it/s]                                                 {'loss': 1.2839, 'grad_norm': 2.1984341128505944, 'learning_rate': 1.276051188299817e-06, 'epoch': 1.15}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 216/564 [03:19<05:12,  1.11it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 217/564 [03:20<05:10,  1.12it/s]                                                 {'loss': 1.2654, 'grad_norm': 2.1869302009266076, 'learning_rate': 1.2723948811700182e-06, 'epoch': 1.15}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 217/564 [03:20<05:10,  1.12it/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 218/564 [03:21<05:10,  1.12it/s]                                                 {'loss': 1.2337, 'grad_norm': 2.14176306190028, 'learning_rate': 1.2687385740402192e-06, 'epoch': 1.16}
 39%|â–ˆâ–ˆâ–ˆâ–Š      | 218/564 [03:21<05:10,  1.12it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 219/564 [03:22<05:08,  1.12it/s]                                                 {'loss': 1.4372, 'grad_norm': 2.1907444832377143, 'learning_rate': 1.2650822669104205e-06, 'epoch': 1.16}
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 219/564 [03:22<05:08,  1.12it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 220/564 [03:22<05:05,  1.12it/s]                                                 {'loss': 1.3157, 'grad_norm': 2.4619570509896906, 'learning_rate': 1.2614259597806217e-06, 'epoch': 1.17}
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 220/564 [03:22<05:05,  1.12it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 221/564 [03:23<05:05,  1.12it/s]                                                 {'loss': 1.1359, 'grad_norm': 2.4669225794661713, 'learning_rate': 1.2577696526508225e-06, 'epoch': 1.18}
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 221/564 [03:23<05:05,  1.12it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 222/564 [03:24<05:04,  1.12it/s]                                                 {'loss': 1.3697, 'grad_norm': 2.367646277181369, 'learning_rate': 1.2541133455210237e-06, 'epoch': 1.18}
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 222/564 [03:24<05:04,  1.12it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 223/564 [03:25<05:05,  1.12it/s]                                                 {'loss': 1.1571, 'grad_norm': 2.46393189159573, 'learning_rate': 1.2504570383912248e-06, 'epoch': 1.19}
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 223/564 [03:25<05:05,  1.12it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 224/564 [03:26<05:03,  1.12it/s]                                                 {'loss': 1.4892, 'grad_norm': 2.114860390179805, 'learning_rate': 1.246800731261426e-06, 'epoch': 1.19}
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 224/564 [03:26<05:03,  1.12it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 225/564 [03:27<05:02,  1.12it/s]                                                 {'loss': 1.2425, 'grad_norm': 2.229799406531658, 'learning_rate': 1.2431444241316268e-06, 'epoch': 1.2}
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 225/564 [03:27<05:02,  1.12it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 226/564 [03:28<05:01,  1.12it/s]                                                 {'loss': 1.2468, 'grad_norm': 2.112030271946257, 'learning_rate': 1.239488117001828e-06, 'epoch': 1.2}
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 226/564 [03:28<05:01,  1.12it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 227/564 [03:29<05:01,  1.12it/s]                                                 {'loss': 1.2365, 'grad_norm': 2.2635795157844663, 'learning_rate': 1.2358318098720293e-06, 'epoch': 1.21}
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 227/564 [03:29<05:01,  1.12it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 228/564 [03:30<05:01,  1.12it/s]                                                 {'loss': 1.16, 'grad_norm': 2.1797332154177917, 'learning_rate': 1.2321755027422303e-06, 'epoch': 1.21}
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 228/564 [03:30<05:01,  1.12it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 229/564 [03:31<04:59,  1.12it/s]                                                 {'loss': 1.1826, 'grad_norm': 2.0696271187435027, 'learning_rate': 1.2285191956124315e-06, 'epoch': 1.22}
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 229/564 [03:31<04:59,  1.12it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 230/564 [03:31<04:59,  1.12it/s]                                                 {'loss': 1.1998, 'grad_norm': 2.0382338521353445, 'learning_rate': 1.2248628884826324e-06, 'epoch': 1.22}
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 230/564 [03:31<04:59,  1.12it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 231/564 [03:32<04:57,  1.12it/s]                                                 {'loss': 1.3443, 'grad_norm': 2.2407680078462975, 'learning_rate': 1.2212065813528336e-06, 'epoch': 1.23}
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 231/564 [03:32<04:57,  1.12it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 232/564 [03:33<04:56,  1.12it/s]                                                 {'loss': 1.3967, 'grad_norm': 2.001274888394443, 'learning_rate': 1.2175502742230346e-06, 'epoch': 1.23}
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 232/564 [03:33<04:56,  1.12it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 233/564 [03:34<04:56,  1.12it/s]                                                 {'loss': 1.2115, 'grad_norm': 2.5552467181208995, 'learning_rate': 1.2138939670932358e-06, 'epoch': 1.24}
 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 233/564 [03:34<04:56,  1.12it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 234/564 [03:35<04:55,  1.12it/s]                                                 {'loss': 1.263, 'grad_norm': 1.8171585140295963, 'learning_rate': 1.2102376599634369e-06, 'epoch': 1.24}
 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 234/564 [03:35<04:55,  1.12it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 235/564 [03:36<04:54,  1.12it/s]                                                 {'loss': 1.2494, 'grad_norm': 2.119617908925906, 'learning_rate': 1.2065813528336379e-06, 'epoch': 1.25}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 235/564 [03:36<04:54,  1.12it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 236/564 [03:37<04:53,  1.12it/s]                                                 {'loss': 1.2092, 'grad_norm': 2.2303855576010547, 'learning_rate': 1.2029250457038391e-06, 'epoch': 1.26}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 236/564 [03:37<04:53,  1.12it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 237/564 [03:38<04:51,  1.12it/s]                                                 {'loss': 1.199, 'grad_norm': 2.3920965258673803, 'learning_rate': 1.1992687385740402e-06, 'epoch': 1.26}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 237/564 [03:38<04:51,  1.12it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 238/564 [03:39<04:50,  1.12it/s]                                                 {'loss': 1.2569, 'grad_norm': 2.256381343517089, 'learning_rate': 1.1956124314442414e-06, 'epoch': 1.27}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 238/564 [03:39<04:50,  1.12it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 239/564 [03:39<04:49,  1.12it/s]                                                 {'loss': 1.3233, 'grad_norm': 2.042196604747993, 'learning_rate': 1.1919561243144422e-06, 'epoch': 1.27}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 239/564 [03:39<04:49,  1.12it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 240/564 [03:40<04:48,  1.12it/s]                                                 {'loss': 1.2003, 'grad_norm': 1.8445821510310485, 'learning_rate': 1.1882998171846434e-06, 'epoch': 1.28}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 240/564 [03:40<04:48,  1.12it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 241/564 [03:41<04:48,  1.12it/s]                                                 {'loss': 1.3285, 'grad_norm': 2.095405121646601, 'learning_rate': 1.1846435100548447e-06, 'epoch': 1.28}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 241/564 [03:41<04:48,  1.12it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 242/564 [03:42<04:47,  1.12it/s]                                                 {'loss': 1.288, 'grad_norm': 2.0919058065361016, 'learning_rate': 1.1809872029250457e-06, 'epoch': 1.29}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 242/564 [03:42<04:47,  1.12it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 243/564 [03:43<04:47,  1.12it/s]                                                 {'loss': 1.0081, 'grad_norm': 2.4356221899451467, 'learning_rate': 1.1773308957952467e-06, 'epoch': 1.29}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 243/564 [03:43<04:47,  1.12it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 244/564 [03:44<04:45,  1.12it/s]                                                 {'loss': 1.3065, 'grad_norm': 2.3881189402674825, 'learning_rate': 1.1736745886654477e-06, 'epoch': 1.3}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 244/564 [03:44<04:45,  1.12it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 245/564 [03:45<04:43,  1.12it/s]                                                 {'loss': 1.3439, 'grad_norm': 2.2542516535751562, 'learning_rate': 1.170018281535649e-06, 'epoch': 1.3}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 245/564 [03:45<04:43,  1.12it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 246/564 [03:46<04:43,  1.12it/s]                                                 {'loss': 1.356, 'grad_norm': 2.2199390178389504, 'learning_rate': 1.16636197440585e-06, 'epoch': 1.31}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 246/564 [03:46<04:43,  1.12it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 247/564 [03:47<04:43,  1.12it/s]                                                 {'loss': 1.3412, 'grad_norm': 2.022953848757509, 'learning_rate': 1.1627056672760512e-06, 'epoch': 1.31}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 247/564 [03:47<04:43,  1.12it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 248/564 [03:47<04:42,  1.12it/s]                                                 {'loss': 1.1286, 'grad_norm': 2.1430529325418086, 'learning_rate': 1.1590493601462523e-06, 'epoch': 1.32}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 248/564 [03:47<04:42,  1.12it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 249/564 [03:48<04:41,  1.12it/s]                                                 {'loss': 1.1586, 'grad_norm': 1.951882128517387, 'learning_rate': 1.1553930530164533e-06, 'epoch': 1.32}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 249/564 [03:48<04:41,  1.12it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 250/564 [03:49<04:40,  1.12it/s]                                                 {'loss': 1.2967, 'grad_norm': 2.4659481147446307, 'learning_rate': 1.1517367458866545e-06, 'epoch': 1.33}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 250/564 [03:49<04:40,  1.12it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 251/564 [03:50<04:38,  1.12it/s]                                                 {'loss': 1.0805, 'grad_norm': 2.1498639268028144, 'learning_rate': 1.1480804387568555e-06, 'epoch': 1.34}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 251/564 [03:50<04:38,  1.12it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 252/564 [03:51<04:38,  1.12it/s]                                                 {'loss': 1.3143, 'grad_norm': 2.210210233033765, 'learning_rate': 1.1444241316270566e-06, 'epoch': 1.34}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 252/564 [03:51<04:38,  1.12it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 253/564 [03:52<04:37,  1.12it/s]                                                 {'loss': 1.3385, 'grad_norm': 1.9249313895686602, 'learning_rate': 1.1407678244972576e-06, 'epoch': 1.35}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 253/564 [03:52<04:37,  1.12it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 254/564 [03:53<04:36,  1.12it/s]                                                 {'loss': 1.3103, 'grad_norm': 2.1137594097542234, 'learning_rate': 1.1371115173674588e-06, 'epoch': 1.35}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 254/564 [03:53<04:36,  1.12it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 255/564 [03:54<04:35,  1.12it/s]                                                 {'loss': 1.2602, 'grad_norm': 2.011404688973641, 'learning_rate': 1.13345521023766e-06, 'epoch': 1.36}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 255/564 [03:54<04:35,  1.12it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 256/564 [03:55<04:34,  1.12it/s]                                                 {'loss': 1.3205, 'grad_norm': 2.1393498474135377, 'learning_rate': 1.1297989031078609e-06, 'epoch': 1.36}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 256/564 [03:55<04:34,  1.12it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 257/564 [03:56<04:34,  1.12it/s]                                                 {'loss': 1.1375, 'grad_norm': 2.098123909922002, 'learning_rate': 1.126142595978062e-06, 'epoch': 1.37}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 257/564 [03:56<04:34,  1.12it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 258/564 [03:56<04:33,  1.12it/s]                                                 {'loss': 1.4484, 'grad_norm': 2.058289701151266, 'learning_rate': 1.1224862888482631e-06, 'epoch': 1.37}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 258/564 [03:56<04:33,  1.12it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 259/564 [03:57<04:32,  1.12it/s]                                                 {'loss': 1.3452, 'grad_norm': 2.3244389855983614, 'learning_rate': 1.1188299817184644e-06, 'epoch': 1.38}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 259/564 [03:57<04:32,  1.12it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 260/564 [03:58<04:31,  1.12it/s]                                                 {'loss': 1.3011, 'grad_norm': 2.1392549064693958, 'learning_rate': 1.1151736745886654e-06, 'epoch': 1.38}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 260/564 [03:58<04:31,  1.12it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 261/564 [03:59<04:30,  1.12it/s]                                                 {'loss': 1.3955, 'grad_norm': 2.2621275238693297, 'learning_rate': 1.1115173674588664e-06, 'epoch': 1.39}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 261/564 [03:59<04:30,  1.12it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 262/564 [04:00<04:29,  1.12it/s]                                                 {'loss': 1.382, 'grad_norm': 2.141806981038332, 'learning_rate': 1.1078610603290676e-06, 'epoch': 1.39}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 262/564 [04:00<04:29,  1.12it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 263/564 [04:01<04:28,  1.12it/s]                                                 {'loss': 1.3006, 'grad_norm': 2.120450594153775, 'learning_rate': 1.1042047531992687e-06, 'epoch': 1.4}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 263/564 [04:01<04:28,  1.12it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 264/564 [04:02<04:28,  1.12it/s]                                                 {'loss': 1.3084, 'grad_norm': 2.180745646308361, 'learning_rate': 1.10054844606947e-06, 'epoch': 1.4}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 264/564 [04:02<04:28,  1.12it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 265/564 [04:03<04:26,  1.12it/s]                                                 {'loss': 1.3485, 'grad_norm': 1.968611771926017, 'learning_rate': 1.0968921389396707e-06, 'epoch': 1.41}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 265/564 [04:03<04:26,  1.12it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 266/564 [04:04<04:25,  1.12it/s]                                                 {'loss': 1.2637, 'grad_norm': 2.0002020558884324, 'learning_rate': 1.093235831809872e-06, 'epoch': 1.41}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 266/564 [04:04<04:25,  1.12it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 267/564 [04:04<04:24,  1.12it/s]                                                 {'loss': 1.0403, 'grad_norm': 2.2405810669739816, 'learning_rate': 1.089579524680073e-06, 'epoch': 1.42}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 267/564 [04:04<04:24,  1.12it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 268/564 [04:05<04:23,  1.12it/s]                                                 {'loss': 1.1305, 'grad_norm': 1.9361691493601445, 'learning_rate': 1.0859232175502742e-06, 'epoch': 1.43}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 268/564 [04:05<04:23,  1.12it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 269/564 [04:06<04:22,  1.12it/s]                                                 {'loss': 1.3696, 'grad_norm': 2.793274854500513, 'learning_rate': 1.0822669104204754e-06, 'epoch': 1.43}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 269/564 [04:06<04:22,  1.12it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 270/564 [04:07<04:21,  1.12it/s]                                                 {'loss': 1.3077, 'grad_norm': 2.2200968830412275, 'learning_rate': 1.0786106032906763e-06, 'epoch': 1.44}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 270/564 [04:07<04:21,  1.12it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 271/564 [04:08<04:20,  1.12it/s]                                                 {'loss': 1.245, 'grad_norm': 2.558537454868752, 'learning_rate': 1.0749542961608775e-06, 'epoch': 1.44}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 271/564 [04:08<04:20,  1.12it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 272/564 [04:09<04:19,  1.12it/s]                                                 {'loss': 1.1809, 'grad_norm': 2.2249246307638595, 'learning_rate': 1.0712979890310785e-06, 'epoch': 1.45}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 272/564 [04:09<04:19,  1.12it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 273/564 [04:10<04:18,  1.12it/s]                                                 {'loss': 1.2141, 'grad_norm': 2.473540324356285, 'learning_rate': 1.0676416819012797e-06, 'epoch': 1.45}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 273/564 [04:10<04:18,  1.12it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 274/564 [04:11<04:18,  1.12it/s]                                                 {'loss': 1.3065, 'grad_norm': 1.8559741511048302, 'learning_rate': 1.0639853747714806e-06, 'epoch': 1.46}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 274/564 [04:11<04:18,  1.12it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 275/564 [04:12<04:16,  1.12it/s]                                                 {'loss': 1.3663, 'grad_norm': 2.4148866945020875, 'learning_rate': 1.0603290676416818e-06, 'epoch': 1.46}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 275/564 [04:12<04:16,  1.12it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 276/564 [04:12<04:15,  1.13it/s]                                                 {'loss': 1.1158, 'grad_norm': 2.1082963314411742, 'learning_rate': 1.056672760511883e-06, 'epoch': 1.47}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 276/564 [04:12<04:15,  1.13it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 277/564 [04:13<04:14,  1.13it/s]                                                 {'loss': 1.1236, 'grad_norm': 2.222863585271213, 'learning_rate': 1.053016453382084e-06, 'epoch': 1.47}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 277/564 [04:13<04:14,  1.13it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 278/564 [04:14<04:14,  1.12it/s]                                                 {'loss': 1.1286, 'grad_norm': 1.9876023464442056, 'learning_rate': 1.0493601462522853e-06, 'epoch': 1.48}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 278/564 [04:14<04:14,  1.12it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 279/564 [04:15<04:14,  1.12it/s]                                                 {'loss': 1.1138, 'grad_norm': 2.1537305031565124, 'learning_rate': 1.045703839122486e-06, 'epoch': 1.48}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 279/564 [04:15<04:14,  1.12it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 280/564 [04:16<04:15,  1.11it/s]                                                 {'loss': 1.3648, 'grad_norm': 1.9069653990844473, 'learning_rate': 1.0420475319926873e-06, 'epoch': 1.49}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 280/564 [04:16<04:15,  1.11it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 281/564 [04:17<04:13,  1.12it/s]                                                 {'loss': 1.3232, 'grad_norm': 2.2180924799493864, 'learning_rate': 1.0383912248628884e-06, 'epoch': 1.49}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 281/564 [04:17<04:13,  1.12it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 282/564 [04:18<04:11,  1.12it/s]                                                 {'loss': 1.3911, 'grad_norm': 2.4205104588983644, 'learning_rate': 1.0347349177330896e-06, 'epoch': 1.5}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 282/564 [04:18<04:11,  1.12it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 283/564 [04:19<04:10,  1.12it/s]                                                 {'loss': 1.2735, 'grad_norm': 2.1617992633931564, 'learning_rate': 1.0310786106032906e-06, 'epoch': 1.51}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 283/564 [04:19<04:10,  1.12it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 284/564 [04:20<04:08,  1.13it/s]                                                 {'loss': 1.2873, 'grad_norm': 2.1041004506738794, 'learning_rate': 1.0274223034734916e-06, 'epoch': 1.51}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 284/564 [04:20<04:08,  1.13it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 285/564 [04:20<04:08,  1.12it/s]                                                 {'loss': 1.2689, 'grad_norm': 2.15463680321478, 'learning_rate': 1.0237659963436929e-06, 'epoch': 1.52}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 285/564 [04:20<04:08,  1.12it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 286/564 [04:21<04:07,  1.12it/s]                                                 {'loss': 1.309, 'grad_norm': 2.2643085200168396, 'learning_rate': 1.020109689213894e-06, 'epoch': 1.52}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 286/564 [04:21<04:07,  1.12it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 287/564 [04:22<04:06,  1.13it/s]                                                 {'loss': 1.1262, 'grad_norm': 2.428518403078814, 'learning_rate': 1.0164533820840951e-06, 'epoch': 1.53}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 287/564 [04:22<04:06,  1.13it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 288/564 [04:23<04:05,  1.13it/s]                                                 {'loss': 1.3667, 'grad_norm': 2.216413881092977, 'learning_rate': 1.012797074954296e-06, 'epoch': 1.53}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 288/564 [04:23<04:05,  1.13it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 289/564 [04:24<04:04,  1.13it/s]                                                 {'loss': 1.1744, 'grad_norm': 2.496604772758652, 'learning_rate': 1.0091407678244972e-06, 'epoch': 1.54}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 289/564 [04:24<04:04,  1.13it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 290/564 [04:25<04:02,  1.13it/s]                                                 {'loss': 1.4417, 'grad_norm': 2.174725218782892, 'learning_rate': 1.0054844606946984e-06, 'epoch': 1.54}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 290/564 [04:25<04:02,  1.13it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 291/564 [04:26<04:02,  1.13it/s]                                                 {'loss': 1.2484, 'grad_norm': 2.219900178427621, 'learning_rate': 1.0018281535648994e-06, 'epoch': 1.55}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 291/564 [04:26<04:02,  1.13it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 292/564 [04:27<04:01,  1.13it/s]                                                 {'loss': 1.1451, 'grad_norm': 2.312788758999251, 'learning_rate': 9.981718464351005e-07, 'epoch': 1.55}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 292/564 [04:27<04:01,  1.13it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 293/564 [04:28<04:00,  1.13it/s]                                                 {'loss': 1.4028, 'grad_norm': 2.261435594693872, 'learning_rate': 9.945155393053017e-07, 'epoch': 1.56}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 293/564 [04:28<04:00,  1.13it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 294/564 [04:28<03:59,  1.13it/s]                                                 {'loss': 1.3553, 'grad_norm': 2.0510899716259954, 'learning_rate': 9.908592321755027e-07, 'epoch': 1.56}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 294/564 [04:28<03:59,  1.13it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 295/564 [04:29<04:00,  1.12it/s]                                                 {'loss': 1.3474, 'grad_norm': 2.1456634704198128, 'learning_rate': 9.872029250457037e-07, 'epoch': 1.57}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 295/564 [04:29<04:00,  1.12it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 296/564 [04:30<03:58,  1.12it/s]                                                 {'loss': 1.0575, 'grad_norm': 2.3084615507016717, 'learning_rate': 9.835466179159048e-07, 'epoch': 1.57}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 296/564 [04:30<03:58,  1.12it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 297/564 [04:31<03:57,  1.12it/s]                                                 {'loss': 1.3732, 'grad_norm': 2.035167955564492, 'learning_rate': 9.79890310786106e-07, 'epoch': 1.58}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 297/564 [04:31<03:57,  1.12it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 298/564 [04:32<03:56,  1.13it/s]                                                 {'loss': 1.4443, 'grad_norm': 2.0299478773318205, 'learning_rate': 9.76234003656307e-07, 'epoch': 1.59}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 298/564 [04:32<03:56,  1.13it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 299/564 [04:33<03:55,  1.13it/s]                                                 {'loss': 1.3158, 'grad_norm': 2.3399124865908325, 'learning_rate': 9.725776965265083e-07, 'epoch': 1.59}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 299/564 [04:33<03:55,  1.13it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 300/564 [04:34<03:54,  1.13it/s]                                                 {'loss': 1.2489, 'grad_norm': 2.302325921090922, 'learning_rate': 9.689213893967093e-07, 'epoch': 1.6}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 300/564 [04:34<03:54,  1.13it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 301/564 [04:35<03:53,  1.13it/s]                                                 {'loss': 1.3752, 'grad_norm': 2.9516210384202357, 'learning_rate': 9.652650822669103e-07, 'epoch': 1.6}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 301/564 [04:35<03:53,  1.13it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 302/564 [04:36<03:52,  1.13it/s]                                                 {'loss': 1.3556, 'grad_norm': 2.028606860738791, 'learning_rate': 9.616087751371115e-07, 'epoch': 1.61}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 302/564 [04:36<03:52,  1.13it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 303/564 [04:36<03:52,  1.12it/s]                                                 {'loss': 1.4922, 'grad_norm': 2.1773065998250325, 'learning_rate': 9.579524680073126e-07, 'epoch': 1.61}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 303/564 [04:36<03:52,  1.12it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 304/564 [04:37<03:51,  1.12it/s]                                                 {'loss': 1.4622, 'grad_norm': 2.046551381816603, 'learning_rate': 9.542961608775136e-07, 'epoch': 1.62}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 304/564 [04:37<03:51,  1.12it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 305/564 [04:38<03:49,  1.13it/s]                                                 {'loss': 1.3666, 'grad_norm': 2.49974074540261, 'learning_rate': 9.506398537477147e-07, 'epoch': 1.62}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 305/564 [04:38<03:49,  1.13it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 306/564 [04:39<03:49,  1.13it/s]                                                 {'loss': 1.2653, 'grad_norm': 2.2279209592669, 'learning_rate': 9.469835466179159e-07, 'epoch': 1.63}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 306/564 [04:39<03:49,  1.13it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 307/564 [04:40<03:48,  1.13it/s]                                                 {'loss': 1.1971, 'grad_norm': 2.2779663577980784, 'learning_rate': 9.43327239488117e-07, 'epoch': 1.63}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 307/564 [04:40<03:48,  1.13it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 308/564 [04:41<03:47,  1.13it/s]                                                 {'loss': 1.2599, 'grad_norm': 2.3523540083660586, 'learning_rate': 9.396709323583181e-07, 'epoch': 1.64}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 308/564 [04:41<03:47,  1.13it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 309/564 [04:42<03:46,  1.12it/s]                                                 {'loss': 1.2347, 'grad_norm': 2.5612018727707424, 'learning_rate': 9.360146252285191e-07, 'epoch': 1.64}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 309/564 [04:42<03:46,  1.12it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 310/564 [04:43<03:45,  1.12it/s]                                                 {'loss': 1.2203, 'grad_norm': 2.2100035733510923, 'learning_rate': 9.323583180987203e-07, 'epoch': 1.65}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 310/564 [04:43<03:45,  1.12it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 311/564 [04:44<03:45,  1.12it/s]                                                 {'loss': 1.3241, 'grad_norm': 2.1413916918756852, 'learning_rate': 9.287020109689213e-07, 'epoch': 1.65}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 311/564 [04:44<03:45,  1.12it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 312/564 [04:44<03:44,  1.12it/s]                                                 {'loss': 1.3541, 'grad_norm': 2.227444815794706, 'learning_rate': 9.250457038391224e-07, 'epoch': 1.66}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 312/564 [04:44<03:44,  1.12it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 313/564 [04:45<03:43,  1.12it/s]                                                 {'loss': 1.1829, 'grad_norm': 1.9332651246673158, 'learning_rate': 9.213893967093235e-07, 'epoch': 1.66}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 313/564 [04:45<03:43,  1.12it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 314/564 [04:46<03:42,  1.12it/s]                                                 {'loss': 1.2392, 'grad_norm': 2.080522220011582, 'learning_rate': 9.177330895795247e-07, 'epoch': 1.67}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 314/564 [04:46<03:42,  1.12it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 315/564 [04:47<03:41,  1.13it/s]                                                 {'loss': 1.4083, 'grad_norm': 2.1193683229978055, 'learning_rate': 9.140767824497257e-07, 'epoch': 1.68}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 315/564 [04:47<03:41,  1.13it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 316/564 [04:48<03:41,  1.12it/s]                                                 {'loss': 1.3717, 'grad_norm': 2.175609466090633, 'learning_rate': 9.104204753199268e-07, 'epoch': 1.68}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 316/564 [04:48<03:41,  1.12it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 317/564 [04:49<03:40,  1.12it/s]                                                 {'loss': 1.2196, 'grad_norm': 2.629006905143237, 'learning_rate': 9.06764168190128e-07, 'epoch': 1.69}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 317/564 [04:49<03:40,  1.12it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 318/564 [04:50<04:06,  1.00s/it]                                                 {'loss': 1.3111, 'grad_norm': 2.2240520494446128, 'learning_rate': 9.03107861060329e-07, 'epoch': 1.69}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 318/564 [04:50<04:06,  1.00s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 319/564 [04:51<03:57,  1.03it/s]                                                 {'loss': 1.1746, 'grad_norm': 2.161381342740959, 'learning_rate': 8.994515539305301e-07, 'epoch': 1.7}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 319/564 [04:51<03:57,  1.03it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 320/564 [04:52<03:50,  1.06it/s]                                                 {'loss': 1.2785, 'grad_norm': 2.203727193557426, 'learning_rate': 8.957952468007312e-07, 'epoch': 1.7}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 320/564 [04:52<03:50,  1.06it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 321/564 [04:53<03:48,  1.06it/s]                                                 {'loss': 1.2996, 'grad_norm': 2.296870741962504, 'learning_rate': 8.921389396709324e-07, 'epoch': 1.71}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 321/564 [04:53<03:48,  1.06it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 322/564 [04:54<03:49,  1.06it/s]                                                 {'loss': 1.3142, 'grad_norm': 2.489704459398082, 'learning_rate': 8.884826325411334e-07, 'epoch': 1.71}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 322/564 [04:54<03:49,  1.06it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 323/564 [04:55<03:45,  1.07it/s]                                                 {'loss': 1.3389, 'grad_norm': 2.5261979074638496, 'learning_rate': 8.848263254113345e-07, 'epoch': 1.72}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 323/564 [04:55<03:45,  1.07it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 324/564 [04:56<03:41,  1.08it/s]                                                 {'loss': 1.3722, 'grad_norm': 2.149523560792289, 'learning_rate': 8.811700182815355e-07, 'epoch': 1.72}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 324/564 [04:56<03:41,  1.08it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 325/564 [04:57<03:37,  1.10it/s]                                                 {'loss': 1.2579, 'grad_norm': 2.138747948932975, 'learning_rate': 8.775137111517367e-07, 'epoch': 1.73}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 325/564 [04:57<03:37,  1.10it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 326/564 [04:57<03:36,  1.10it/s]                                                 {'loss': 1.2331, 'grad_norm': 2.116553120197456, 'learning_rate': 8.738574040219377e-07, 'epoch': 1.73}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 326/564 [04:57<03:36,  1.10it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 327/564 [04:58<03:34,  1.10it/s]                                                 {'loss': 1.1776, 'grad_norm': 2.15169950932294, 'learning_rate': 8.702010968921389e-07, 'epoch': 1.74}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 327/564 [04:58<03:34,  1.10it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 328/564 [04:59<03:35,  1.09it/s]                                                 {'loss': 1.2483, 'grad_norm': 2.147756299703209, 'learning_rate': 8.665447897623401e-07, 'epoch': 1.74}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 328/564 [04:59<03:35,  1.09it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 329/564 [05:01<03:58,  1.02s/it]                                                 {'loss': 1.1223, 'grad_norm': 2.1323939204302502, 'learning_rate': 8.628884826325411e-07, 'epoch': 1.75}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 329/564 [05:01<03:58,  1.02s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 330/564 [05:01<03:48,  1.02it/s]                                                 {'loss': 1.1852, 'grad_norm': 1.9664433460975541, 'learning_rate': 8.592321755027422e-07, 'epoch': 1.76}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 330/564 [05:01<03:48,  1.02it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 331/564 [05:02<03:41,  1.05it/s]                                                 {'loss': 1.2276, 'grad_norm': 2.286849998581379, 'learning_rate': 8.555758683729432e-07, 'epoch': 1.76}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 331/564 [05:02<03:41,  1.05it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 332/564 [05:03<03:37,  1.07it/s]                                                 {'loss': 1.1505, 'grad_norm': 2.167829377505088, 'learning_rate': 8.519195612431444e-07, 'epoch': 1.77}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 332/564 [05:03<03:37,  1.07it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 333/564 [05:04<03:33,  1.08it/s]                                                 {'loss': 1.3372, 'grad_norm': 2.174536365045315, 'learning_rate': 8.482632541133454e-07, 'epoch': 1.77}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 333/564 [05:04<03:33,  1.08it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 334/564 [05:05<03:30,  1.09it/s]                                                 {'loss': 1.32, 'grad_norm': 2.040940004257754, 'learning_rate': 8.446069469835466e-07, 'epoch': 1.78}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 334/564 [05:05<03:30,  1.09it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 335/564 [05:06<04:04,  1.07s/it]                                                 {'loss': 1.1876, 'grad_norm': 2.248089279736532, 'learning_rate': 8.409506398537477e-07, 'epoch': 1.78}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 335/564 [05:06<04:04,  1.07s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 336/564 [05:07<03:52,  1.02s/it]                                                 {'loss': 1.1606, 'grad_norm': 2.078249054298371, 'learning_rate': 8.372943327239488e-07, 'epoch': 1.79}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 336/564 [05:07<03:52,  1.02s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 337/564 [05:08<03:45,  1.01it/s]                                                 {'loss': 1.3863, 'grad_norm': 2.0091320010276488, 'learning_rate': 8.336380255941499e-07, 'epoch': 1.79}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 337/564 [05:08<03:45,  1.01it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 338/564 [05:09<03:37,  1.04it/s]                                                 {'loss': 1.281, 'grad_norm': 2.3624982610191148, 'learning_rate': 8.299817184643509e-07, 'epoch': 1.8}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 338/564 [05:09<03:37,  1.04it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 339/564 [05:10<03:32,  1.06it/s]                                                 {'loss': 1.3073, 'grad_norm': 1.9680002261112453, 'learning_rate': 8.263254113345521e-07, 'epoch': 1.8}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 339/564 [05:10<03:32,  1.06it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 340/564 [05:11<03:40,  1.02it/s]                                                 {'loss': 1.3169, 'grad_norm': 2.1833385093053854, 'learning_rate': 8.226691042047532e-07, 'epoch': 1.81}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 340/564 [05:11<03:40,  1.02it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 341/564 [05:12<03:36,  1.03it/s]                                                 {'loss': 1.1173, 'grad_norm': 2.2185550373735987, 'learning_rate': 8.190127970749543e-07, 'epoch': 1.81}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 341/564 [05:12<03:36,  1.03it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 342/564 [05:14<04:13,  1.14s/it]                                                 {'loss': 1.2735, 'grad_norm': 2.4507333874121024, 'learning_rate': 8.153564899451553e-07, 'epoch': 1.82}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 342/564 [05:14<04:13,  1.14s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 343/564 [05:15<03:56,  1.07s/it]                                                 {'loss': 1.375, 'grad_norm': 2.2338102410951026, 'learning_rate': 8.117001828153565e-07, 'epoch': 1.82}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 343/564 [05:15<03:56,  1.07s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 344/564 [05:15<03:46,  1.03s/it]                                                 {'loss': 1.2185, 'grad_norm': 2.0695247603190254, 'learning_rate': 8.080438756855575e-07, 'epoch': 1.83}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 344/564 [05:15<03:46,  1.03s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 345/564 [05:16<03:44,  1.02s/it]                                                 {'loss': 1.3725, 'grad_norm': 2.086274857580567, 'learning_rate': 8.043875685557586e-07, 'epoch': 1.84}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 345/564 [05:16<03:44,  1.02s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 346/564 [05:18<04:29,  1.23s/it]                                                 {'loss': 1.4449, 'grad_norm': 2.153817521283648, 'learning_rate': 8.007312614259597e-07, 'epoch': 1.84}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 346/564 [05:18<04:29,  1.23s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 347/564 [05:19<04:06,  1.14s/it]                                                 {'loss': 1.399, 'grad_norm': 2.0177612968360266, 'learning_rate': 7.970749542961609e-07, 'epoch': 1.85}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 347/564 [05:19<04:06,  1.14s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 348/564 [05:20<03:53,  1.08s/it]                                                 {'loss': 1.1623, 'grad_norm': 2.3015394024493103, 'learning_rate': 7.93418647166362e-07, 'epoch': 1.85}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 348/564 [05:20<03:53,  1.08s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 349/564 [05:22<04:23,  1.22s/it]                                                 {'loss': 1.3343, 'grad_norm': 2.0501842155694474, 'learning_rate': 7.89762340036563e-07, 'epoch': 1.86}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 349/564 [05:22<04:23,  1.22s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 350/564 [05:23<04:10,  1.17s/it]                                                 {'loss': 1.2249, 'grad_norm': 2.6085172871214755, 'learning_rate': 7.861060329067642e-07, 'epoch': 1.86}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 350/564 [05:23<04:10,  1.17s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 351/564 [05:24<04:01,  1.13s/it]                                                 {'loss': 1.34, 'grad_norm': 2.0512492348156353, 'learning_rate': 7.824497257769652e-07, 'epoch': 1.87}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 351/564 [05:24<04:01,  1.13s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 352/564 [05:25<03:54,  1.10s/it]                                                 {'loss': 1.3732, 'grad_norm': 2.1983343542134466, 'learning_rate': 7.787934186471663e-07, 'epoch': 1.87}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 352/564 [05:25<03:54,  1.10s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 353/564 [05:26<04:21,  1.24s/it]                                                 {'loss': 1.2132, 'grad_norm': 2.3522154123889467, 'learning_rate': 7.751371115173673e-07, 'epoch': 1.88}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 353/564 [05:26<04:21,  1.24s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 354/564 [05:27<04:07,  1.18s/it]                                                 {'loss': 1.379, 'grad_norm': 2.1715303777273482, 'learning_rate': 7.714808043875686e-07, 'epoch': 1.88}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 354/564 [05:27<04:07,  1.18s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 355/564 [05:28<03:52,  1.11s/it]                                                 {'loss': 1.1875, 'grad_norm': 2.3860530137031013, 'learning_rate': 7.678244972577696e-07, 'epoch': 1.89}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 355/564 [05:28<03:52,  1.11s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 356/564 [05:30<04:22,  1.26s/it]                                                 {'loss': 1.3286, 'grad_norm': 2.2274350142727504, 'learning_rate': 7.641681901279707e-07, 'epoch': 1.89}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 356/564 [05:30<04:22,  1.26s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 357/564 [05:31<04:03,  1.18s/it]                                                 {'loss': 1.2451, 'grad_norm': 2.0279190748613236, 'learning_rate': 7.605118829981719e-07, 'epoch': 1.9}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 357/564 [05:31<04:03,  1.18s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 358/564 [05:32<03:46,  1.10s/it]                                                 {'loss': 1.3516, 'grad_norm': 2.0288693022555684, 'learning_rate': 7.568555758683729e-07, 'epoch': 1.9}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 358/564 [05:32<03:46,  1.10s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 359/564 [05:33<03:33,  1.04s/it]                                                 {'loss': 1.2213, 'grad_norm': 2.0637379677082106, 'learning_rate': 7.53199268738574e-07, 'epoch': 1.91}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 359/564 [05:33<03:33,  1.04s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 360/564 [05:34<04:10,  1.23s/it]                                                 {'loss': 1.221, 'grad_norm': 2.1314777022076745, 'learning_rate': 7.49542961608775e-07, 'epoch': 1.91}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 360/564 [05:34<04:10,  1.23s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 361/564 [05:35<03:55,  1.16s/it]                                                 {'loss': 1.1827, 'grad_norm': 1.99276592811446, 'learning_rate': 7.458866544789763e-07, 'epoch': 1.92}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 361/564 [05:35<03:55,  1.16s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 362/564 [05:36<03:41,  1.09s/it]                                                 {'loss': 1.3352, 'grad_norm': 2.2650034803223917, 'learning_rate': 7.422303473491773e-07, 'epoch': 1.93}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 362/564 [05:36<03:41,  1.09s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 363/564 [05:38<04:08,  1.24s/it]                                                 {'loss': 1.266, 'grad_norm': 2.2127570007669886, 'learning_rate': 7.385740402193784e-07, 'epoch': 1.93}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 363/564 [05:38<04:08,  1.24s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 364/564 [05:39<03:51,  1.16s/it]                                                 {'loss': 1.2663, 'grad_norm': 2.446744163182284, 'learning_rate': 7.349177330895795e-07, 'epoch': 1.94}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 364/564 [05:39<03:51,  1.16s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 365/564 [05:40<03:35,  1.08s/it]                                                 {'loss': 1.4785, 'grad_norm': 2.175481287483716, 'learning_rate': 7.312614259597806e-07, 'epoch': 1.94}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 365/564 [05:40<03:35,  1.08s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 366/564 [05:41<03:28,  1.05s/it]                                                 {'loss': 1.3273, 'grad_norm': 2.0678602099526375, 'learning_rate': 7.276051188299816e-07, 'epoch': 1.95}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 366/564 [05:41<03:28,  1.05s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 367/564 [05:42<04:00,  1.22s/it]                                                 {'loss': 1.3012, 'grad_norm': 2.3507012395322806, 'learning_rate': 7.239488117001827e-07, 'epoch': 1.95}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 367/564 [05:42<04:00,  1.22s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 368/564 [05:43<03:46,  1.16s/it]                                                 {'loss': 1.2673, 'grad_norm': 2.4500480879016893, 'learning_rate': 7.20292504570384e-07, 'epoch': 1.96}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 368/564 [05:43<03:46,  1.16s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 369/564 [05:44<03:35,  1.11s/it]                                                 {'loss': 1.4057, 'grad_norm': 2.1916612807646745, 'learning_rate': 7.16636197440585e-07, 'epoch': 1.96}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 369/564 [05:44<03:35,  1.11s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 370/564 [05:46<04:00,  1.24s/it]                                                 {'loss': 1.1423, 'grad_norm': 2.300183561163688, 'learning_rate': 7.129798903107861e-07, 'epoch': 1.97}
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 370/564 [05:46<04:00,  1.24s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 371/564 [05:47<03:48,  1.19s/it]                                                 {'loss': 1.3952, 'grad_norm': 2.2271753341105054, 'learning_rate': 7.093235831809871e-07, 'epoch': 1.97}
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 371/564 [05:47<03:48,  1.19s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 372/564 [05:48<03:42,  1.16s/it]                                                 {'loss': 1.1962, 'grad_norm': 2.551242072958311, 'learning_rate': 7.056672760511883e-07, 'epoch': 1.98}
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 372/564 [05:48<03:42,  1.16s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 373/564 [05:50<04:26,  1.40s/it]                                                 {'loss': 1.2541, 'grad_norm': 2.30894249274134, 'learning_rate': 7.020109689213893e-07, 'epoch': 1.98}
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 373/564 [05:50<04:26,  1.40s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 374/564 [05:51<04:04,  1.29s/it]                                                 {'loss': 1.2741, 'grad_norm': 2.161896614469656, 'learning_rate': 6.983546617915904e-07, 'epoch': 1.99}
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 374/564 [05:51<04:04,  1.29s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 375/564 [05:52<03:43,  1.18s/it]                                                 {'loss': 1.2943, 'grad_norm': 2.1419761635450634, 'learning_rate': 6.946983546617916e-07, 'epoch': 1.99}
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 375/564 [05:52<03:43,  1.18s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 376/564 [05:54<04:05,  1.30s/it]                                                 {'loss': 1.2307, 'grad_norm': 2.08861972185958, 'learning_rate': 6.910420475319927e-07, 'epoch': 2.0}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 376/564 [05:54<04:05,  1.30s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 377/564 [05:55<03:48,  1.22s/it]                                                 {'loss': 1.3758, 'grad_norm': 1.9947898339954926, 'learning_rate': 6.873857404021938e-07, 'epoch': 2.01}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 377/564 [05:55<03:48,  1.22s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 378/564 [05:56<03:36,  1.16s/it]                                                 {'loss': 1.2232, 'grad_norm': 2.2338806111749454, 'learning_rate': 6.837294332723948e-07, 'epoch': 2.01}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 378/564 [05:56<03:36,  1.16s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 379/564 [05:57<03:31,  1.14s/it]                                                 {'loss': 1.2134, 'grad_norm': 2.264860874152519, 'learning_rate': 6.80073126142596e-07, 'epoch': 2.02}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 379/564 [05:57<03:31,  1.14s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 380/564 [05:58<03:51,  1.26s/it]                                                 {'loss': 1.2158, 'grad_norm': 1.9759966284064745, 'learning_rate': 6.76416819012797e-07, 'epoch': 2.02}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 380/564 [05:58<03:51,  1.26s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 381/564 [05:59<03:30,  1.15s/it]                                                 {'loss': 1.2116, 'grad_norm': 2.306978749256521, 'learning_rate': 6.727605118829981e-07, 'epoch': 2.03}
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 381/564 [05:59<03:30,  1.15s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 382/564 [06:00<03:16,  1.08s/it]                                                 {'loss': 1.1355, 'grad_norm': 2.3731312341646085, 'learning_rate': 6.691042047531993e-07, 'epoch': 2.03}
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 382/564 [06:00<03:16,  1.08s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 383/564 [06:02<03:43,  1.24s/it]                                                 {'loss': 1.0186, 'grad_norm': 1.9572713204901007, 'learning_rate': 6.654478976234004e-07, 'epoch': 2.04}
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 383/564 [06:02<03:43,  1.24s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 384/564 [06:03<03:29,  1.16s/it]                                                 {'loss': 1.3175, 'grad_norm': 2.063146457240488, 'learning_rate': 6.617915904936014e-07, 'epoch': 2.04}
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 384/564 [06:03<03:29,  1.16s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 385/564 [06:04<03:22,  1.13s/it]                                                 {'loss': 1.2649, 'grad_norm': 1.8458700192994777, 'learning_rate': 6.581352833638025e-07, 'epoch': 2.05}
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 385/564 [06:04<03:22,  1.13s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 386/564 [06:05<03:16,  1.10s/it]                                                 {'loss': 1.184, 'grad_norm': 2.3006676357202362, 'learning_rate': 6.544789762340036e-07, 'epoch': 2.05}
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 386/564 [06:05<03:16,  1.10s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 387/564 [06:06<03:41,  1.25s/it]                                                 {'loss': 1.2532, 'grad_norm': 2.142452551290114, 'learning_rate': 6.508226691042047e-07, 'epoch': 2.06}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 387/564 [06:06<03:41,  1.25s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 388/564 [06:07<03:27,  1.18s/it]                                                 {'loss': 0.9988, 'grad_norm': 2.134695335920977, 'learning_rate': 6.471663619744058e-07, 'epoch': 2.06}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 388/564 [06:07<03:27,  1.18s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 389/564 [06:08<03:14,  1.11s/it]                                                 {'loss': 1.1099, 'grad_norm': 2.2305914008953187, 'learning_rate': 6.435100548446069e-07, 'epoch': 2.07}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 389/564 [06:08<03:14,  1.11s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 390/564 [06:10<03:36,  1.24s/it]                                                 {'loss': 1.0632, 'grad_norm': 2.2869634066069535, 'learning_rate': 6.398537477148081e-07, 'epoch': 2.07}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 390/564 [06:10<03:36,  1.24s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 391/564 [06:11<03:24,  1.18s/it]                                                 {'loss': 1.1585, 'grad_norm': 1.9094075327368445, 'learning_rate': 6.361974405850091e-07, 'epoch': 2.08}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 391/564 [06:11<03:24,  1.18s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 392/564 [06:12<03:16,  1.14s/it]                                                 {'loss': 1.2962, 'grad_norm': 2.279513268362786, 'learning_rate': 6.325411334552102e-07, 'epoch': 2.09}
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 392/564 [06:12<03:16,  1.14s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 393/564 [06:13<03:02,  1.07s/it]                                                 {'loss': 1.2448, 'grad_norm': 2.3354458378385696, 'learning_rate': 6.288848263254113e-07, 'epoch': 2.09}
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 393/564 [06:13<03:02,  1.07s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 394/564 [06:15<03:34,  1.26s/it]                                                 {'loss': 1.3257, 'grad_norm': 2.359543664389398, 'learning_rate': 6.252285191956124e-07, 'epoch': 2.1}
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 394/564 [06:15<03:34,  1.26s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 395/564 [06:16<03:17,  1.17s/it]                                                 {'loss': 1.1463, 'grad_norm': 2.2672099740910756, 'learning_rate': 6.215722120658134e-07, 'epoch': 2.1}
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 395/564 [06:16<03:17,  1.17s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 396/564 [06:16<03:03,  1.09s/it]                                                 {'loss': 1.1045, 'grad_norm': 2.323626735601872, 'learning_rate': 6.179159049360146e-07, 'epoch': 2.11}
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 396/564 [06:16<03:03,  1.09s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 397/564 [06:18<03:11,  1.14s/it]                                                 {'loss': 1.1403, 'grad_norm': 2.0420731318114926, 'learning_rate': 6.142595978062158e-07, 'epoch': 2.11}
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 397/564 [06:18<03:11,  1.14s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 398/564 [06:19<03:18,  1.19s/it]                                                 {'loss': 1.2138, 'grad_norm': 2.3913088673292893, 'learning_rate': 6.106032906764168e-07, 'epoch': 2.12}
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 398/564 [06:19<03:18,  1.19s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 399/564 [06:20<03:10,  1.15s/it]                                                 {'loss': 1.2928, 'grad_norm': 2.3590208622083946, 'learning_rate': 6.069469835466179e-07, 'epoch': 2.12}
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 399/564 [06:20<03:10,  1.15s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 400/564 [06:21<02:59,  1.10s/it]                                                 {'loss': 1.0453, 'grad_norm': 2.2400577552115752, 'learning_rate': 6.032906764168189e-07, 'epoch': 2.13}
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 400/564 [06:21<02:59,  1.10s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 401/564 [06:23<03:26,  1.27s/it]                                                 {'loss': 1.244, 'grad_norm': 2.1379560445967725, 'learning_rate': 5.996343692870201e-07, 'epoch': 2.13}
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 401/564 [06:23<03:26,  1.27s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 402/564 [06:24<03:12,  1.19s/it]                                                 {'loss': 1.2893, 'grad_norm': 2.5033077179205607, 'learning_rate': 5.959780621572211e-07, 'epoch': 2.14}
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 402/564 [06:24<03:12,  1.19s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 403/564 [06:25<02:58,  1.11s/it]                                                 {'loss': 1.3446, 'grad_norm': 2.0982908710463994, 'learning_rate': 5.923217550274223e-07, 'epoch': 2.14}
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 403/564 [06:25<02:58,  1.11s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 404/564 [06:26<03:00,  1.13s/it]                                                 {'loss': 1.2199, 'grad_norm': 2.406023451521284, 'learning_rate': 5.886654478976234e-07, 'epoch': 2.15}
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 404/564 [06:26<03:00,  1.13s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 405/564 [06:27<03:15,  1.23s/it]                                                 {'loss': 1.2168, 'grad_norm': 2.2702069305344095, 'learning_rate': 5.850091407678245e-07, 'epoch': 2.15}
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 405/564 [06:27<03:15,  1.23s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 406/564 [06:28<03:06,  1.18s/it]                                                 {'loss': 1.1964, 'grad_norm': 2.4458358701015133, 'learning_rate': 5.813528336380256e-07, 'epoch': 2.16}
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 406/564 [06:28<03:06,  1.18s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 407/564 [06:29<02:56,  1.12s/it]                                                 {'loss': 1.2452, 'grad_norm': 2.0479080298587125, 'learning_rate': 5.776965265082266e-07, 'epoch': 2.16}
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 407/564 [06:29<02:56,  1.12s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 408/564 [06:31<03:23,  1.30s/it]                                                 {'loss': 1.4189, 'grad_norm': 2.369229441974521, 'learning_rate': 5.740402193784278e-07, 'epoch': 2.17}
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 408/564 [06:31<03:23,  1.30s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 409/564 [06:32<03:03,  1.18s/it]                                                 {'loss': 1.3185, 'grad_norm': 2.1680074662158733, 'learning_rate': 5.703839122486288e-07, 'epoch': 2.18}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 409/564 [06:32<03:03,  1.18s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 410/564 [06:33<02:51,  1.11s/it]                                                 {'loss': 1.32, 'grad_norm': 2.165769519982614, 'learning_rate': 5.6672760511883e-07, 'epoch': 2.18}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 410/564 [06:33<02:51,  1.11s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 411/564 [06:34<02:53,  1.13s/it]                                                 {'loss': 1.0541, 'grad_norm': 2.1464075698082064, 'learning_rate': 5.63071297989031e-07, 'epoch': 2.19}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 411/564 [06:34<02:53,  1.13s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 412/564 [06:35<02:48,  1.11s/it]                                                 {'loss': 1.139, 'grad_norm': 2.1300343146580576, 'learning_rate': 5.594149908592322e-07, 'epoch': 2.19}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 412/564 [06:35<02:48,  1.11s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 413/564 [06:36<02:46,  1.10s/it]                                                 {'loss': 1.2633, 'grad_norm': 2.0738161332963094, 'learning_rate': 5.557586837294332e-07, 'epoch': 2.2}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 413/564 [06:36<02:46,  1.10s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 414/564 [06:37<02:37,  1.05s/it]                                                 {'loss': 1.1654, 'grad_norm': 2.003928544096551, 'learning_rate': 5.521023765996343e-07, 'epoch': 2.2}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 414/564 [06:37<02:37,  1.05s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 415/564 [06:39<03:06,  1.25s/it]                                                 {'loss': 1.3279, 'grad_norm': 2.464487862380436, 'learning_rate': 5.484460694698354e-07, 'epoch': 2.21}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 415/564 [06:39<03:06,  1.25s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 416/564 [06:40<02:51,  1.16s/it]                                                 {'loss': 1.3226, 'grad_norm': 2.2656224316647675, 'learning_rate': 5.447897623400365e-07, 'epoch': 2.21}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 416/564 [06:40<02:51,  1.16s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 417/564 [06:41<02:40,  1.09s/it]                                                 {'loss': 1.251, 'grad_norm': 2.311140312351545, 'learning_rate': 5.411334552102377e-07, 'epoch': 2.22}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 417/564 [06:41<02:40,  1.09s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 418/564 [06:42<02:51,  1.18s/it]                                                 {'loss': 1.2322, 'grad_norm': 2.212239082732082, 'learning_rate': 5.374771480804387e-07, 'epoch': 2.22}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 418/564 [06:42<02:51,  1.18s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 419/564 [06:43<02:49,  1.17s/it]                                                 {'loss': 1.1047, 'grad_norm': 2.2118619823736614, 'learning_rate': 5.338208409506399e-07, 'epoch': 2.23}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 419/564 [06:43<02:49,  1.17s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 420/564 [06:44<02:42,  1.13s/it]                                                 {'loss': 1.2773, 'grad_norm': 2.2415316072463622, 'learning_rate': 5.301645338208409e-07, 'epoch': 2.23}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 420/564 [06:44<02:42,  1.13s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 421/564 [06:45<02:36,  1.09s/it]                                                 {'loss': 1.0353, 'grad_norm': 2.3307677049328563, 'learning_rate': 5.26508226691042e-07, 'epoch': 2.24}
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 421/564 [06:45<02:36,  1.09s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 422/564 [06:47<03:02,  1.28s/it]                                                 {'loss': 1.2448, 'grad_norm': 2.636055675114811, 'learning_rate': 5.22851919561243e-07, 'epoch': 2.24}
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 422/564 [06:47<03:02,  1.28s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 423/564 [06:48<02:49,  1.20s/it]                                                 {'loss': 1.2478, 'grad_norm': 2.092347575677851, 'learning_rate': 5.191956124314442e-07, 'epoch': 2.25}
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 423/564 [06:48<02:49,  1.20s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 424/564 [06:49<02:37,  1.13s/it]                                                 {'loss': 1.2715, 'grad_norm': 2.331358110821827, 'learning_rate': 5.155393053016453e-07, 'epoch': 2.26}
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 424/564 [06:49<02:37,  1.13s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 425/564 [06:50<02:48,  1.21s/it]                                                 {'loss': 1.2392, 'grad_norm': 2.1604086851095547, 'learning_rate': 5.118829981718464e-07, 'epoch': 2.26}
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 425/564 [06:50<02:48,  1.21s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 426/564 [06:52<02:43,  1.18s/it]                                                 {'loss': 1.0849, 'grad_norm': 2.056586203254032, 'learning_rate': 5.082266910420476e-07, 'epoch': 2.27}
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 426/564 [06:52<02:43,  1.18s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 427/564 [06:53<02:36,  1.15s/it]                                                 {'loss': 1.0451, 'grad_norm': 1.9704270356336253, 'learning_rate': 5.045703839122486e-07, 'epoch': 2.27}
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 427/564 [06:53<02:36,  1.15s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 428/564 [06:54<02:29,  1.10s/it]                                                 {'loss': 1.2838, 'grad_norm': 2.4516814291603644, 'learning_rate': 5.009140767824497e-07, 'epoch': 2.28}
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 428/564 [06:54<02:29,  1.10s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 429/564 [06:55<02:53,  1.29s/it]                                                 {'loss': 1.2509, 'grad_norm': 1.9557673585887718, 'learning_rate': 4.972577696526509e-07, 'epoch': 2.28}
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 429/564 [06:55<02:53,  1.29s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 430/564 [06:56<02:40,  1.20s/it]                                                 {'loss': 1.2331, 'grad_norm': 2.173197552251306, 'learning_rate': 4.936014625228519e-07, 'epoch': 2.29}
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 430/564 [06:56<02:40,  1.20s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 431/564 [06:57<02:27,  1.11s/it]                                                 {'loss': 1.211, 'grad_norm': 1.9914512013826418, 'learning_rate': 4.89945155393053e-07, 'epoch': 2.29}
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 431/564 [06:57<02:27,  1.11s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 432/564 [06:59<02:44,  1.25s/it]                                                 {'loss': 1.094, 'grad_norm': 2.4615354550460102, 'learning_rate': 4.862888482632541e-07, 'epoch': 2.3}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 432/564 [06:59<02:44,  1.25s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 433/564 [07:00<02:35,  1.19s/it]                                                 {'loss': 1.1193, 'grad_norm': 2.2185337627198445, 'learning_rate': 4.826325411334552e-07, 'epoch': 2.3}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 433/564 [07:00<02:35,  1.19s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 434/564 [07:01<02:29,  1.15s/it]                                                 {'loss': 1.2328, 'grad_norm': 2.3828835180595744, 'learning_rate': 4.789762340036563e-07, 'epoch': 2.31}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 434/564 [07:01<02:29,  1.15s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 435/564 [07:02<02:22,  1.11s/it]                                                 {'loss': 1.2014, 'grad_norm': 2.4450139129929895, 'learning_rate': 4.7531992687385736e-07, 'epoch': 2.31}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 435/564 [07:02<02:22,  1.11s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 436/564 [07:04<02:45,  1.29s/it]                                                 {'loss': 1.2583, 'grad_norm': 2.2534791597229247, 'learning_rate': 4.716636197440585e-07, 'epoch': 2.32}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 436/564 [07:04<02:45,  1.29s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 437/564 [07:05<02:34,  1.22s/it]                                                 {'loss': 1.139, 'grad_norm': 2.410932346409372, 'learning_rate': 4.6800731261425957e-07, 'epoch': 2.32}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 437/564 [07:05<02:34,  1.22s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 438/564 [07:06<02:21,  1.12s/it]                                                 {'loss': 1.1685, 'grad_norm': 2.2557075202195747, 'learning_rate': 4.6435100548446064e-07, 'epoch': 2.33}
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 438/564 [07:06<02:21,  1.12s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 439/564 [07:07<02:37,  1.26s/it]                                                 {'loss': 1.2805, 'grad_norm': 2.332872018390911, 'learning_rate': 4.606946983546618e-07, 'epoch': 2.34}
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 439/564 [07:07<02:37,  1.26s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 440/564 [07:08<02:28,  1.19s/it]                                                 {'loss': 1.2996, 'grad_norm': 2.5160002966232407, 'learning_rate': 4.5703839122486285e-07, 'epoch': 2.34}
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 440/564 [07:08<02:28,  1.19s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 441/564 [07:09<02:18,  1.13s/it]                                                 {'loss': 1.1916, 'grad_norm': 2.078074903025327, 'learning_rate': 4.53382084095064e-07, 'epoch': 2.35}
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 441/564 [07:09<02:18,  1.13s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 442/564 [07:10<02:12,  1.09s/it]                                                 {'loss': 1.3188, 'grad_norm': 2.144872418296821, 'learning_rate': 4.4972577696526506e-07, 'epoch': 2.35}
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 442/564 [07:10<02:12,  1.09s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 443/564 [07:11<02:04,  1.03s/it]                                                 {'loss': 1.1424, 'grad_norm': 2.189162921175153, 'learning_rate': 4.460694698354662e-07, 'epoch': 2.36}
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 443/564 [07:11<02:04,  1.03s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 444/564 [07:12<01:58,  1.01it/s]                                                 {'loss': 1.427, 'grad_norm': 1.9477002418113971, 'learning_rate': 4.4241316270566726e-07, 'epoch': 2.36}
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 444/564 [07:12<01:58,  1.01it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 445/564 [07:13<01:54,  1.04it/s]                                                 {'loss': 1.2704, 'grad_norm': 2.4716311527785484, 'learning_rate': 4.3875685557586834e-07, 'epoch': 2.37}
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 445/564 [07:13<01:54,  1.04it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 446/564 [07:14<01:50,  1.06it/s]                                                 {'loss': 1.1032, 'grad_norm': 2.9945020056777003, 'learning_rate': 4.3510054844606947e-07, 'epoch': 2.37}
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 446/564 [07:14<01:50,  1.06it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 447/564 [07:15<01:48,  1.08it/s]                                                 {'loss': 1.362, 'grad_norm': 1.9743107411001912, 'learning_rate': 4.3144424131627054e-07, 'epoch': 2.38}
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 447/564 [07:15<01:48,  1.08it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 448/564 [07:16<01:46,  1.09it/s]                                                 {'loss': 1.3401, 'grad_norm': 2.093776989856409, 'learning_rate': 4.277879341864716e-07, 'epoch': 2.38}
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 448/564 [07:16<01:46,  1.09it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 449/564 [07:16<01:45,  1.09it/s]                                                 {'loss': 1.2598, 'grad_norm': 2.1781744388352373, 'learning_rate': 4.241316270566727e-07, 'epoch': 2.39}
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 449/564 [07:16<01:45,  1.09it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 450/564 [07:17<01:43,  1.10it/s]                                                 {'loss': 1.197, 'grad_norm': 2.2425078628281607, 'learning_rate': 4.2047531992687383e-07, 'epoch': 2.39}
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 450/564 [07:17<01:43,  1.10it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 451/564 [07:18<01:41,  1.11it/s]                                                 {'loss': 1.3329, 'grad_norm': 2.0411003872907636, 'learning_rate': 4.1681901279707496e-07, 'epoch': 2.4}
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 451/564 [07:18<01:41,  1.11it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 452/564 [07:19<01:40,  1.11it/s]                                                 {'loss': 1.293, 'grad_norm': 2.2072160763794617, 'learning_rate': 4.1316270566727603e-07, 'epoch': 2.4}
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 452/564 [07:19<01:40,  1.11it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 453/564 [07:20<01:39,  1.12it/s]                                                 {'loss': 1.3899, 'grad_norm': 2.11779859312295, 'learning_rate': 4.0950639853747716e-07, 'epoch': 2.41}
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 453/564 [07:20<01:39,  1.12it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 454/564 [07:21<01:38,  1.12it/s]                                                 {'loss': 1.1769, 'grad_norm': 2.543688977282264, 'learning_rate': 4.0585009140767824e-07, 'epoch': 2.41}
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 454/564 [07:21<01:38,  1.12it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 455/564 [07:22<01:37,  1.12it/s]                                                 {'loss': 1.1268, 'grad_norm': 2.691181208036446, 'learning_rate': 4.021937842778793e-07, 'epoch': 2.42}
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 455/564 [07:22<01:37,  1.12it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 456/564 [07:23<01:36,  1.12it/s]                                                 {'loss': 1.2609, 'grad_norm': 2.1970354659887215, 'learning_rate': 3.9853747714808044e-07, 'epoch': 2.43}
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 456/564 [07:23<01:36,  1.12it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 457/564 [07:24<01:35,  1.12it/s]                                                 {'loss': 1.4824, 'grad_norm': 2.19535793023525, 'learning_rate': 3.948811700182815e-07, 'epoch': 2.43}
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 457/564 [07:24<01:35,  1.12it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 458/564 [07:24<01:34,  1.12it/s]                                                 {'loss': 1.2379, 'grad_norm': 2.1749792585796173, 'learning_rate': 3.912248628884826e-07, 'epoch': 2.44}
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 458/564 [07:24<01:34,  1.12it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 459/564 [07:25<01:33,  1.12it/s]                                                 {'loss': 1.2333, 'grad_norm': 2.188535711144658, 'learning_rate': 3.875685557586837e-07, 'epoch': 2.44}
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 459/564 [07:25<01:33,  1.12it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 460/564 [07:26<01:32,  1.12it/s]                                                 {'loss': 1.1534, 'grad_norm': 1.971172749168604, 'learning_rate': 3.839122486288848e-07, 'epoch': 2.45}
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 460/564 [07:26<01:32,  1.12it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 461/564 [07:27<01:32,  1.12it/s]                                                 {'loss': 1.3377, 'grad_norm': 1.893840555958111, 'learning_rate': 3.8025594149908593e-07, 'epoch': 2.45}
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 461/564 [07:27<01:32,  1.12it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 462/564 [07:28<01:31,  1.12it/s]                                                 {'loss': 1.1556, 'grad_norm': 1.9728984012538149, 'learning_rate': 3.76599634369287e-07, 'epoch': 2.46}
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 462/564 [07:28<01:31,  1.12it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 463/564 [07:29<01:30,  1.12it/s]                                                 {'loss': 1.471, 'grad_norm': 2.37331465885805, 'learning_rate': 3.7294332723948814e-07, 'epoch': 2.46}
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 463/564 [07:29<01:30,  1.12it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 464/564 [07:30<01:29,  1.12it/s]                                                 {'loss': 1.156, 'grad_norm': 2.238276619167108, 'learning_rate': 3.692870201096892e-07, 'epoch': 2.47}
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 464/564 [07:30<01:29,  1.12it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 465/564 [07:31<01:28,  1.12it/s]                                                 {'loss': 1.2271, 'grad_norm': 2.157173928514668, 'learning_rate': 3.656307129798903e-07, 'epoch': 2.47}
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 465/564 [07:31<01:28,  1.12it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 466/564 [07:32<01:27,  1.12it/s]                                                 {'loss': 1.0548, 'grad_norm': 2.6653878835460767, 'learning_rate': 3.6197440585009137e-07, 'epoch': 2.48}
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 466/564 [07:32<01:27,  1.12it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 467/564 [07:32<01:26,  1.12it/s]                                                 {'loss': 1.2753, 'grad_norm': 2.004678203145582, 'learning_rate': 3.583180987202925e-07, 'epoch': 2.48}
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 467/564 [07:32<01:26,  1.12it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 468/564 [07:33<01:25,  1.12it/s]                                                 {'loss': 1.2156, 'grad_norm': 2.320659838636497, 'learning_rate': 3.5466179159049357e-07, 'epoch': 2.49}
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 468/564 [07:33<01:25,  1.12it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 469/564 [07:34<01:24,  1.12it/s]                                                 {'loss': 1.2939, 'grad_norm': 2.23652976697904, 'learning_rate': 3.5100548446069465e-07, 'epoch': 2.49}
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 469/564 [07:34<01:24,  1.12it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 470/564 [07:35<01:23,  1.12it/s]                                                 {'loss': 1.0708, 'grad_norm': 2.263126489810281, 'learning_rate': 3.473491773308958e-07, 'epoch': 2.5}
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 470/564 [07:35<01:23,  1.12it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 471/564 [07:36<01:22,  1.12it/s]                                                 {'loss': 1.1535, 'grad_norm': 2.3502993532593477, 'learning_rate': 3.436928702010969e-07, 'epoch': 2.51}
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 471/564 [07:36<01:22,  1.12it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 472/564 [07:37<01:21,  1.12it/s]                                                 {'loss': 1.1777, 'grad_norm': 2.4088019854494287, 'learning_rate': 3.40036563071298e-07, 'epoch': 2.51}
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 472/564 [07:37<01:21,  1.12it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 473/564 [07:38<01:21,  1.12it/s]                                                 {'loss': 1.3636, 'grad_norm': 2.2953140654398725, 'learning_rate': 3.3638025594149906e-07, 'epoch': 2.52}
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 473/564 [07:38<01:21,  1.12it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 474/564 [07:39<01:20,  1.12it/s]                                                 {'loss': 1.2148, 'grad_norm': 2.4837901412591905, 'learning_rate': 3.327239488117002e-07, 'epoch': 2.52}
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 474/564 [07:39<01:20,  1.12it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 475/564 [07:40<01:19,  1.12it/s]                                                 {'loss': 1.2476, 'grad_norm': 2.1497415194018825, 'learning_rate': 3.2906764168190127e-07, 'epoch': 2.53}
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 475/564 [07:40<01:19,  1.12it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 476/564 [07:40<01:18,  1.12it/s]                                                 {'loss': 1.212, 'grad_norm': 2.380968342925216, 'learning_rate': 3.2541133455210234e-07, 'epoch': 2.53}
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 476/564 [07:40<01:18,  1.12it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 477/564 [07:41<01:17,  1.13it/s]                                                 {'loss': 1.4041, 'grad_norm': 2.352069960188274, 'learning_rate': 3.2175502742230347e-07, 'epoch': 2.54}
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 477/564 [07:41<01:17,  1.13it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 478/564 [07:42<01:16,  1.13it/s]                                                 {'loss': 1.1393, 'grad_norm': 2.254135451089419, 'learning_rate': 3.1809872029250455e-07, 'epoch': 2.54}
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 478/564 [07:42<01:16,  1.13it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 479/564 [07:43<01:15,  1.12it/s]                                                 {'loss': 1.0181, 'grad_norm': 2.511176065748649, 'learning_rate': 3.144424131627056e-07, 'epoch': 2.55}
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 479/564 [07:43<01:15,  1.12it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 480/564 [07:44<01:14,  1.12it/s]                                                 {'loss': 1.2063, 'grad_norm': 2.3534480478954034, 'learning_rate': 3.107861060329067e-07, 'epoch': 2.55}
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 480/564 [07:44<01:14,  1.12it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 481/564 [07:45<01:13,  1.12it/s]                                                 {'loss': 1.1406, 'grad_norm': 2.2467335612774737, 'learning_rate': 3.071297989031079e-07, 'epoch': 2.56}
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 481/564 [07:45<01:13,  1.12it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 482/564 [07:46<01:13,  1.12it/s]                                                 {'loss': 1.3658, 'grad_norm': 2.228071118444951, 'learning_rate': 3.0347349177330896e-07, 'epoch': 2.56}
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 482/564 [07:46<01:13,  1.12it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 483/564 [07:47<01:12,  1.12it/s]                                                 {'loss': 1.2477, 'grad_norm': 1.9731664990621085, 'learning_rate': 2.9981718464351004e-07, 'epoch': 2.57}
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 483/564 [07:47<01:12,  1.12it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 484/564 [07:48<01:11,  1.12it/s]                                                 {'loss': 1.0735, 'grad_norm': 2.0819076879233958, 'learning_rate': 2.9616087751371117e-07, 'epoch': 2.57}
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 484/564 [07:48<01:11,  1.12it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 485/564 [07:49<01:10,  1.12it/s]                                                 {'loss': 1.2772, 'grad_norm': 2.315343674717756, 'learning_rate': 2.9250457038391224e-07, 'epoch': 2.58}
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 485/564 [07:49<01:10,  1.12it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 486/564 [07:49<01:09,  1.13it/s]                                                 {'loss': 1.2123, 'grad_norm': 2.564418753533755, 'learning_rate': 2.888482632541133e-07, 'epoch': 2.59}
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 486/564 [07:49<01:09,  1.13it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 487/564 [07:50<01:08,  1.13it/s]                                                 {'loss': 1.0864, 'grad_norm': 2.396344076693609, 'learning_rate': 2.851919561243144e-07, 'epoch': 2.59}
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 487/564 [07:50<01:08,  1.13it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 488/564 [07:51<01:07,  1.12it/s]                                                 {'loss': 1.0431, 'grad_norm': 2.014975383018222, 'learning_rate': 2.815356489945155e-07, 'epoch': 2.6}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 488/564 [07:51<01:07,  1.12it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 489/564 [07:52<01:06,  1.12it/s]                                                 {'loss': 1.1951, 'grad_norm': 2.3979113920623893, 'learning_rate': 2.778793418647166e-07, 'epoch': 2.6}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 489/564 [07:52<01:06,  1.12it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 490/564 [07:53<01:05,  1.12it/s]                                                 {'loss': 1.1182, 'grad_norm': 2.34872679676591, 'learning_rate': 2.742230347349177e-07, 'epoch': 2.61}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 490/564 [07:53<01:05,  1.12it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 491/564 [07:54<01:04,  1.12it/s]                                                 {'loss': 1.1273, 'grad_norm': 2.2471848389158064, 'learning_rate': 2.7056672760511886e-07, 'epoch': 2.61}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 491/564 [07:54<01:04,  1.12it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 492/564 [07:55<01:04,  1.12it/s]                                                 {'loss': 1.2342, 'grad_norm': 2.426534520442447, 'learning_rate': 2.6691042047531994e-07, 'epoch': 2.62}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 492/564 [07:55<01:04,  1.12it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 493/564 [07:56<01:03,  1.12it/s]                                                 {'loss': 1.3489, 'grad_norm': 2.3005225876900517, 'learning_rate': 2.63254113345521e-07, 'epoch': 2.62}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 493/564 [07:56<01:03,  1.12it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 494/564 [07:57<01:02,  1.12it/s]                                                 {'loss': 1.3401, 'grad_norm': 2.365874708929876, 'learning_rate': 2.595978062157221e-07, 'epoch': 2.63}
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 494/564 [07:57<01:02,  1.12it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 495/564 [07:57<01:01,  1.12it/s]                                                 {'loss': 1.2142, 'grad_norm': 2.256137700597206, 'learning_rate': 2.559414990859232e-07, 'epoch': 2.63}
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 495/564 [07:57<01:01,  1.12it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 496/564 [07:58<01:00,  1.12it/s]                                                 {'loss': 1.0744, 'grad_norm': 2.3077111198509797, 'learning_rate': 2.522851919561243e-07, 'epoch': 2.64}
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 496/564 [07:58<01:00,  1.12it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 497/564 [07:59<01:00,  1.12it/s]                                                 {'loss': 1.2303, 'grad_norm': 2.535892118824576, 'learning_rate': 2.486288848263254e-07, 'epoch': 2.64}
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 497/564 [07:59<01:00,  1.12it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 498/564 [08:00<00:59,  1.12it/s]                                                 {'loss': 1.2768, 'grad_norm': 2.361950345728304, 'learning_rate': 2.449725776965265e-07, 'epoch': 2.65}
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 498/564 [08:00<00:59,  1.12it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 499/564 [08:01<00:58,  1.12it/s]                                                 {'loss': 1.1697, 'grad_norm': 2.034069257959512, 'learning_rate': 2.413162705667276e-07, 'epoch': 2.65}
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 499/564 [08:01<00:58,  1.12it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 500/564 [08:02<00:57,  1.12it/s]                                                 {'loss': 1.1424, 'grad_norm': 2.063566042369213, 'learning_rate': 2.3765996343692868e-07, 'epoch': 2.66}
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 500/564 [08:02<00:57,  1.12it/s]/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 501/564 [08:58<18:27, 17.58s/it]                                                 {'loss': 1.1998, 'grad_norm': 2.6202763397634548, 'learning_rate': 2.3400365630712978e-07, 'epoch': 2.66}
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 501/564 [08:58<18:27, 17.58s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 502/564 [08:59<13:02, 12.62s/it]                                                 {'loss': 1.2093, 'grad_norm': 2.6264272103770367, 'learning_rate': 2.303473491773309e-07, 'epoch': 2.67}
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 502/564 [08:59<13:02, 12.62s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 503/564 [09:01<09:18,  9.15s/it]                                                 {'loss': 1.2564, 'grad_norm': 2.326112400632788, 'learning_rate': 2.26691042047532e-07, 'epoch': 2.68}
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 503/564 [09:01<09:18,  9.15s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 504/564 [09:02<06:44,  6.75s/it]                                                 {'loss': 1.2555, 'grad_norm': 2.3577558791719113, 'learning_rate': 2.230347349177331e-07, 'epoch': 2.68}
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 504/564 [09:02<06:44,  6.75s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 505/564 [09:03<05:04,  5.17s/it]                                                 {'loss': 1.2187, 'grad_norm': 2.4079512392571996, 'learning_rate': 2.1937842778793417e-07, 'epoch': 2.69}
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 505/564 [09:03<05:04,  5.17s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 506/564 [09:04<03:48,  3.93s/it]                                                 {'loss': 1.2329, 'grad_norm': 2.1866638644222913, 'learning_rate': 2.1572212065813527e-07, 'epoch': 2.69}
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 506/564 [09:04<03:48,  3.93s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 507/564 [09:05<02:52,  3.03s/it]                                                 {'loss': 1.2722, 'grad_norm': 2.2549358139242432, 'learning_rate': 2.1206581352833635e-07, 'epoch': 2.7}
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 507/564 [09:05<02:52,  3.03s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 508/564 [09:07<02:27,  2.64s/it]                                                 {'loss': 1.0648, 'grad_norm': 2.488950661131507, 'learning_rate': 2.0840950639853748e-07, 'epoch': 2.7}
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 508/564 [09:07<02:27,  2.64s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 509/564 [09:08<01:58,  2.15s/it]                                                 {'loss': 1.2814, 'grad_norm': 2.039117330732337, 'learning_rate': 2.0475319926873858e-07, 'epoch': 2.71}
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 509/564 [09:08<01:58,  2.15s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 510/564 [09:09<01:36,  1.78s/it]                                                 {'loss': 1.3091, 'grad_norm': 2.303215591364916, 'learning_rate': 2.0109689213893966e-07, 'epoch': 2.71}
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 510/564 [09:09<01:36,  1.78s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 511/564 [09:10<01:26,  1.64s/it]                                                 {'loss': 1.259, 'grad_norm': 2.224974705935421, 'learning_rate': 1.9744058500914076e-07, 'epoch': 2.72}
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 511/564 [09:10<01:26,  1.64s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 512/564 [09:11<01:15,  1.44s/it]                                                 {'loss': 1.2916, 'grad_norm': 2.419440425339727, 'learning_rate': 1.9378427787934184e-07, 'epoch': 2.72}
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 512/564 [09:11<01:15,  1.44s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 513/564 [09:12<01:05,  1.29s/it]                                                 {'loss': 1.255, 'grad_norm': 2.536962315077559, 'learning_rate': 1.9012797074954297e-07, 'epoch': 2.73}
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 513/564 [09:12<01:05,  1.29s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 514/564 [09:13<00:58,  1.17s/it]                                                 {'loss': 1.17, 'grad_norm': 2.1970754076355936, 'learning_rate': 1.8647166361974407e-07, 'epoch': 2.73}
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 514/564 [09:13<00:58,  1.17s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 515/564 [09:15<01:05,  1.33s/it]                                                 {'loss': 1.1062, 'grad_norm': 2.5158811581086247, 'learning_rate': 1.8281535648994515e-07, 'epoch': 2.74}
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 515/564 [09:15<01:05,  1.33s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 516/564 [09:16<01:00,  1.25s/it]                                                 {'loss': 1.0745, 'grad_norm': 2.0609977583186327, 'learning_rate': 1.7915904936014625e-07, 'epoch': 2.74}
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 516/564 [09:16<01:00,  1.25s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 517/564 [09:17<00:54,  1.16s/it]                                                 {'loss': 1.1395, 'grad_norm': 2.2244847310077085, 'learning_rate': 1.7550274223034732e-07, 'epoch': 2.75}
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 517/564 [09:17<00:54,  1.16s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 518/564 [09:18<00:58,  1.28s/it]                                                 {'loss': 1.3227, 'grad_norm': 2.143405374116707, 'learning_rate': 1.7184643510054845e-07, 'epoch': 2.76}
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 518/564 [09:18<00:58,  1.28s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 519/564 [09:19<00:53,  1.19s/it]                                                 {'loss': 1.0728, 'grad_norm': 2.171240229965642, 'learning_rate': 1.6819012797074953e-07, 'epoch': 2.76}
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 519/564 [09:19<00:53,  1.19s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 520/564 [09:20<00:50,  1.15s/it]                                                 {'loss': 1.2732, 'grad_norm': 2.1320948003450813, 'learning_rate': 1.6453382084095063e-07, 'epoch': 2.77}
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 520/564 [09:20<00:50,  1.15s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 521/564 [09:21<00:46,  1.07s/it]                                                 {'loss': 1.2875, 'grad_norm': 2.310039302955828, 'learning_rate': 1.6087751371115174e-07, 'epoch': 2.77}
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 521/564 [09:21<00:46,  1.07s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 522/564 [09:23<00:53,  1.27s/it]                                                 {'loss': 1.2335, 'grad_norm': 2.480424031125876, 'learning_rate': 1.572212065813528e-07, 'epoch': 2.78}
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 522/564 [09:23<00:53,  1.27s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 523/564 [09:24<00:49,  1.20s/it]                                                 {'loss': 0.8923, 'grad_norm': 2.4080189128054847, 'learning_rate': 1.5356489945155394e-07, 'epoch': 2.78}
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 523/564 [09:24<00:49,  1.20s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 524/564 [09:25<00:44,  1.12s/it]                                                 {'loss': 1.2156, 'grad_norm': 2.2958807295797157, 'learning_rate': 1.4990859232175502e-07, 'epoch': 2.79}
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 524/564 [09:25<00:44,  1.12s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 525/564 [09:26<00:45,  1.17s/it]                                                 {'loss': 1.182, 'grad_norm': 2.126182287504953, 'learning_rate': 1.4625228519195612e-07, 'epoch': 2.79}
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 525/564 [09:26<00:45,  1.17s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 526/564 [09:27<00:44,  1.16s/it]                                                 {'loss': 0.9057, 'grad_norm': 2.2590483117474363, 'learning_rate': 1.425959780621572e-07, 'epoch': 2.8}
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 526/564 [09:27<00:44,  1.16s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 527/564 [09:28<00:41,  1.13s/it]                                                 {'loss': 0.9625, 'grad_norm': 2.2283106341309233, 'learning_rate': 1.389396709323583e-07, 'epoch': 2.8}
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 527/564 [09:28<00:41,  1.13s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 528/564 [09:29<00:39,  1.09s/it]                                                 {'loss': 1.2681, 'grad_norm': 2.3889265349321236, 'learning_rate': 1.3528336380255943e-07, 'epoch': 2.81}
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 528/564 [09:29<00:39,  1.09s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 529/564 [09:31<00:44,  1.28s/it]                                                 {'loss': 1.1554, 'grad_norm': 2.2777297915773533, 'learning_rate': 1.316270566727605e-07, 'epoch': 2.81}
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 529/564 [09:31<00:44,  1.28s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 530/564 [09:32<00:40,  1.21s/it]                                                 {'loss': 1.2372, 'grad_norm': 1.9845950926473317, 'learning_rate': 1.279707495429616e-07, 'epoch': 2.82}
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 530/564 [09:32<00:40,  1.21s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 531/564 [09:33<00:37,  1.13s/it]                                                 {'loss': 1.0973, 'grad_norm': 2.47044949470533, 'learning_rate': 1.243144424131627e-07, 'epoch': 2.82}
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 531/564 [09:33<00:37,  1.13s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 532/564 [09:34<00:38,  1.19s/it]                                                 {'loss': 1.4043, 'grad_norm': 2.4031488487509476, 'learning_rate': 1.206581352833638e-07, 'epoch': 2.83}
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 532/564 [09:34<00:38,  1.19s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 533/564 [09:35<00:35,  1.14s/it]                                                 {'loss': 1.0291, 'grad_norm': 2.178017699687829, 'learning_rate': 1.1700182815356489e-07, 'epoch': 2.84}
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 533/564 [09:35<00:35,  1.14s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 534/564 [09:36<00:33,  1.11s/it]                                                 {'loss': 1.1545, 'grad_norm': 2.4427315139573316, 'learning_rate': 1.13345521023766e-07, 'epoch': 2.84}
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 534/564 [09:36<00:33,  1.11s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 535/564 [09:37<00:31,  1.08s/it]                                                 {'loss': 1.3784, 'grad_norm': 2.151266336114041, 'learning_rate': 1.0968921389396708e-07, 'epoch': 2.85}
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 535/564 [09:37<00:31,  1.08s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 536/564 [09:39<00:35,  1.28s/it]                                                 {'loss': 1.0773, 'grad_norm': 2.2863769475496127, 'learning_rate': 1.0603290676416817e-07, 'epoch': 2.85}
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 536/564 [09:39<00:35,  1.28s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 537/564 [09:40<00:31,  1.17s/it]                                                 {'loss': 1.2549, 'grad_norm': 2.3134893720955794, 'learning_rate': 1.0237659963436929e-07, 'epoch': 2.86}
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 537/564 [09:40<00:31,  1.17s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 538/564 [09:41<00:28,  1.11s/it]                                                 {'loss': 1.0905, 'grad_norm': 2.4871994092358385, 'learning_rate': 9.872029250457038e-08, 'epoch': 2.86}
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 538/564 [09:41<00:28,  1.11s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 539/564 [09:42<00:29,  1.17s/it]                                                 {'loss': 1.0895, 'grad_norm': 2.3550677112415737, 'learning_rate': 9.506398537477148e-08, 'epoch': 2.87}
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 539/564 [09:42<00:29,  1.17s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 540/564 [09:43<00:27,  1.15s/it]                                                 {'loss': 1.1725, 'grad_norm': 2.166694787454918, 'learning_rate': 9.140767824497257e-08, 'epoch': 2.87}
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 540/564 [09:43<00:27,  1.15s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 541/564 [09:45<00:25,  1.13s/it]                                                 {'loss': 1.1861, 'grad_norm': 2.4853135129561674, 'learning_rate': 8.775137111517366e-08, 'epoch': 2.88}
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 541/564 [09:45<00:25,  1.13s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 542/564 [09:46<00:23,  1.09s/it]                                                 {'loss': 1.3499, 'grad_norm': 2.195741849913392, 'learning_rate': 8.409506398537477e-08, 'epoch': 2.88}
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 542/564 [09:46<00:23,  1.09s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 543/564 [09:47<00:26,  1.28s/it]                                                 {'loss': 1.1416, 'grad_norm': 1.9931997440509384, 'learning_rate': 8.043875685557587e-08, 'epoch': 2.89}
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 543/564 [09:47<00:26,  1.28s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 544/564 [09:48<00:23,  1.17s/it]                                                 {'loss': 1.1883, 'grad_norm': 1.9256346767967578, 'learning_rate': 7.678244972577697e-08, 'epoch': 2.89}
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 544/564 [09:48<00:23,  1.17s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 545/564 [09:49<00:20,  1.10s/it]                                                 {'loss': 1.2454, 'grad_norm': 1.9460845121059251, 'learning_rate': 7.312614259597806e-08, 'epoch': 2.9}
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 545/564 [09:49<00:20,  1.10s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 546/564 [09:50<00:20,  1.15s/it]                                                 {'loss': 1.1542, 'grad_norm': 2.2979941137522046, 'learning_rate': 6.946983546617915e-08, 'epoch': 2.9}
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 546/564 [09:50<00:20,  1.15s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 547/564 [09:52<00:19,  1.15s/it]                                                 {'loss': 1.3689, 'grad_norm': 2.385387379029378, 'learning_rate': 6.581352833638025e-08, 'epoch': 2.91}
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 547/564 [09:52<00:19,  1.15s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 548/564 [09:53<00:17,  1.12s/it]                                                 {'loss': 1.2019, 'grad_norm': 2.6439814186420394, 'learning_rate': 6.215722120658136e-08, 'epoch': 2.91}
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 548/564 [09:53<00:17,  1.12s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 549/564 [09:53<00:15,  1.06s/it]                                                 {'loss': 1.249, 'grad_norm': 2.022060689537691, 'learning_rate': 5.8500914076782446e-08, 'epoch': 2.92}
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 549/564 [09:53<00:15,  1.06s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 550/564 [09:55<00:17,  1.22s/it]                                                 {'loss': 1.4229, 'grad_norm': 2.251773587193162, 'learning_rate': 5.484460694698354e-08, 'epoch': 2.93}
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 550/564 [09:55<00:17,  1.22s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 551/564 [09:56<00:14,  1.15s/it]                                                 {'loss': 1.0968, 'grad_norm': 2.3143424729370032, 'learning_rate': 5.1188299817184645e-08, 'epoch': 2.93}
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 551/564 [09:56<00:14,  1.15s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 552/564 [09:57<00:13,  1.11s/it]                                                 {'loss': 1.1347, 'grad_norm': 2.35985049093294, 'learning_rate': 4.753199268738574e-08, 'epoch': 2.94}
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 552/564 [09:57<00:13,  1.11s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 553/564 [09:58<00:11,  1.08s/it]                                                 {'loss': 1.407, 'grad_norm': 2.4477934695938703, 'learning_rate': 4.387568555758683e-08, 'epoch': 2.94}
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 553/564 [09:58<00:11,  1.08s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 554/564 [10:00<00:12,  1.27s/it]                                                 {'loss': 1.2387, 'grad_norm': 2.1242495436903965, 'learning_rate': 4.0219378427787934e-08, 'epoch': 2.95}
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 554/564 [10:00<00:12,  1.27s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 555/564 [10:01<00:10,  1.20s/it]                                                 {'loss': 1.2865, 'grad_norm': 2.350894973272041, 'learning_rate': 3.656307129798903e-08, 'epoch': 2.95}
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 555/564 [10:01<00:10,  1.20s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 556/564 [10:02<00:08,  1.12s/it]                                                 {'loss': 1.0293, 'grad_norm': 2.520005714613491, 'learning_rate': 3.290676416819013e-08, 'epoch': 2.96}
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 556/564 [10:02<00:08,  1.12s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 557/564 [10:03<00:08,  1.27s/it]                                                 {'loss': 1.1249, 'grad_norm': 2.2162420826162137, 'learning_rate': 2.9250457038391223e-08, 'epoch': 2.96}
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 557/564 [10:03<00:08,  1.27s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 558/564 [10:04<00:07,  1.19s/it]                                                 {'loss': 1.2004, 'grad_norm': 2.1569639710619337, 'learning_rate': 2.5594149908592323e-08, 'epoch': 2.97}
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 558/564 [10:04<00:07,  1.19s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 559/564 [10:05<00:05,  1.15s/it]                                                 {'loss': 1.0502, 'grad_norm': 2.443542026766768, 'learning_rate': 2.1937842778793416e-08, 'epoch': 2.97}
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 559/564 [10:05<00:05,  1.15s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 560/564 [10:07<00:04,  1.15s/it]                                                 {'loss': 1.0604, 'grad_norm': 2.205110707244467, 'learning_rate': 1.8281535648994515e-08, 'epoch': 2.98}
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 560/564 [10:07<00:04,  1.15s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 561/564 [10:08<00:03,  1.26s/it]                                                 {'loss': 1.1138, 'grad_norm': 2.274265760485171, 'learning_rate': 1.4625228519195612e-08, 'epoch': 2.98}
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 561/564 [10:08<00:03,  1.26s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 562/564 [10:09<00:02,  1.20s/it]                                                 {'loss': 1.0456, 'grad_norm': 2.242110075065882, 'learning_rate': 1.0968921389396708e-08, 'epoch': 2.99}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 562/564 [10:09<00:02,  1.20s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 563/564 [10:10<00:01,  1.12s/it]                                                 {'loss': 1.2881, 'grad_norm': 2.7643565672262906, 'learning_rate': 7.312614259597806e-09, 'epoch': 2.99}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 563/564 [10:10<00:01,  1.12s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 564/564 [10:12<00:00,  1.24s/it]                                                 {'loss': 1.2526, 'grad_norm': 2.1720628991093838, 'learning_rate': 3.656307129798903e-09, 'epoch': 3.0}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 564/564 [10:12<00:00,  1.24s/it]                                                 {'train_runtime': 612.1585, 'train_samples_per_second': 73.564, 'train_steps_per_second': 0.921, 'train_loss': 1.2967685228543924, 'epoch': 3.0}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 564/564 [10:12<00:00,  1.24s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 564/564 [10:12<00:00,  1.09s/it]
[2024-07-24 16:33:50,507] [INFO] [launch.py:351:main] Process 131591 exits successfully.
[2024-07-24 16:33:50,508] [INFO] [launch.py:351:main] Process 131586 exits successfully.
[2024-07-24 16:33:50,508] [INFO] [launch.py:351:main] Process 131587 exits successfully.
[2024-07-24 16:33:50,508] [INFO] [launch.py:351:main] Process 131588 exits successfully.
[2024-07-24 16:33:51,509] [INFO] [launch.py:351:main] Process 131590 exits successfully.
[2024-07-24 16:33:51,509] [INFO] [launch.py:351:main] Process 131589 exits successfully.
[2024-07-24 16:33:51,510] [INFO] [launch.py:351:main] Process 131585 exits successfully.
[2024-07-24 16:34:15,513] [INFO] [launch.py:351:main] Process 131584 exits successfully.
