Namespace(mode=['alpaca'], base_model='meta-llama/Llama-2-7b-hf', template_name='barebone', total_bsz=64, epoch=3, lr=2e-05, data_path='', task_name='codegen', tuned_dir='./cache', use_peft=True, lora_r=16, lora_alpha=32)
num gpus:  8
Running 1/1: deepspeed --num_gpus=8 train.py --deepspeed ../deepspeed_config/zero3.json --bf16 --tf32=True
        --model_name_or_path meta-llama/Llama-2-7b-hf --data_path ../data/stanford_alpaca/codegen_data.json
        --output_dir /fsx-project/yunyun/models/meta-llama_Llama2-7b_codegen_Lora_tuned/
        --num_train_epochs 3
        --per_device_train_batch_size 10
        --per_device_eval_batch_size 4
        --gradient_accumulation_steps 1
        --gradient_checkpointing=True
        --evaluation_strategy=no
        --save_strategy=steps
        --save_steps 500
        --save_total_limit 1
        --learning_rate 2e-6
        --weight_decay 0.
        --report_to tensorboard
        --warmup_ratio 0.03
        --lr_scheduler_type=cosine
        --logging_steps 1
        --use_peft True 
        --lora_r 16 --lora_alpha 32
['deepspeed', '--num_gpus=8', 'train.py', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Llama-2-7b-hf', '--data_path', '../data/stanford_alpaca/codegen_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_Llama2-7b_codegen_Lora_tuned/', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1', '--use_peft', 'True', '--lora_r', '16', '--lora_alpha', '32']
[2024-08-13 05:48:42,357] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2024-08-13 05:48:50,038] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2024-08-13 05:48:50,039] [INFO] [runner.py:568:main] cmd = /data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNSwgNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None train.py --deepspeed ../deepspeed_config/zero3.json --bf16 --tf32=True --model_name_or_path meta-llama/Llama-2-7b-hf --data_path ../data/stanford_alpaca/codegen_data.json --output_dir /fsx-project/yunyun/models/meta-llama_Llama2-7b_codegen_Lora_tuned/ --num_train_epochs 3 --per_device_train_batch_size 10 --per_device_eval_batch_size 4 --gradient_accumulation_steps 1 --gradient_checkpointing=True --evaluation_strategy=no --save_strategy=steps --save_steps 500 --save_total_limit 1 --learning_rate 2e-6 --weight_decay 0. --report_to tensorboard --warmup_ratio 0.03 --lr_scheduler_type=cosine --logging_steps 1 --use_peft True --lora_r 16 --lora_alpha 32
[2024-08-13 05:48:52,685] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2024-08-13 05:48:56,093] [INFO] [launch.py:139:main] 0 NCCL_ASYNC_ERROR_HANDLING=1
[2024-08-13 05:48:56,094] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]}
[2024-08-13 05:48:56,094] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=8, node_rank=0
[2024-08-13 05:48:56,094] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]})
[2024-08-13 05:48:56,094] [INFO] [launch.py:164:main] dist_world_size=8
[2024-08-13 05:48:56,094] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
[2024-08-13 05:48:56,095] [INFO] [launch.py:256:main] process 3611348 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=0', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Llama-2-7b-hf', '--data_path', '../data/stanford_alpaca/codegen_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_Llama2-7b_codegen_Lora_tuned/', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1', '--use_peft', 'True', '--lora_r', '16', '--lora_alpha', '32']
[2024-08-13 05:48:56,095] [INFO] [launch.py:256:main] process 3611349 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=1', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Llama-2-7b-hf', '--data_path', '../data/stanford_alpaca/codegen_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_Llama2-7b_codegen_Lora_tuned/', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1', '--use_peft', 'True', '--lora_r', '16', '--lora_alpha', '32']
[2024-08-13 05:48:56,096] [INFO] [launch.py:256:main] process 3611350 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=2', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Llama-2-7b-hf', '--data_path', '../data/stanford_alpaca/codegen_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_Llama2-7b_codegen_Lora_tuned/', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1', '--use_peft', 'True', '--lora_r', '16', '--lora_alpha', '32']
[2024-08-13 05:48:56,096] [INFO] [launch.py:256:main] process 3611351 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=3', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Llama-2-7b-hf', '--data_path', '../data/stanford_alpaca/codegen_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_Llama2-7b_codegen_Lora_tuned/', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1', '--use_peft', 'True', '--lora_r', '16', '--lora_alpha', '32']
[2024-08-13 05:48:56,097] [INFO] [launch.py:256:main] process 3611352 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=4', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Llama-2-7b-hf', '--data_path', '../data/stanford_alpaca/codegen_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_Llama2-7b_codegen_Lora_tuned/', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1', '--use_peft', 'True', '--lora_r', '16', '--lora_alpha', '32']
[2024-08-13 05:48:56,097] [INFO] [launch.py:256:main] process 3611353 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=5', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Llama-2-7b-hf', '--data_path', '../data/stanford_alpaca/codegen_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_Llama2-7b_codegen_Lora_tuned/', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1', '--use_peft', 'True', '--lora_r', '16', '--lora_alpha', '32']
[2024-08-13 05:48:56,098] [INFO] [launch.py:256:main] process 3611354 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=6', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Llama-2-7b-hf', '--data_path', '../data/stanford_alpaca/codegen_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_Llama2-7b_codegen_Lora_tuned/', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1', '--use_peft', 'True', '--lora_r', '16', '--lora_alpha', '32']
[2024-08-13 05:48:56,098] [INFO] [launch.py:256:main] process 3611355 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=7', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Llama-2-7b-hf', '--data_path', '../data/stanford_alpaca/codegen_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_Llama2-7b_codegen_Lora_tuned/', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1', '--use_peft', 'True', '--lora_r', '16', '--lora_alpha', '32']
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
[2024-08-13 05:49:11,244] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[2024-08-13 05:49:11,495] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
[2024-08-13 05:49:11,515] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
[2024-08-13 05:49:11,531] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-13 05:49:11,659] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-13 05:49:11,674] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-13 05:49:11,696] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-13 05:49:11,703] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.

[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2024-08-13 05:49:12,011] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-08-13 05:49:12,257] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-08-13 05:49:12,283] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-08-13 05:49:12,289] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-08-13 05:49:12,388] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-08-13 05:49:12,388] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-08-13 05:49:12,398] [INFO] [comm.py:637:init_distributed] cdb=None
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[2024-08-13 05:49:12,425] [INFO] [comm.py:637:init_distributed] cdb=None
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[2024-08-13 05:49:12,447] [INFO] [comm.py:637:init_distributed] cdb=None
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)

Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]
Downloading shards: 100%|██████████| 2/2 [00:00<00:00, 392.23it/s]
Downloading shards: 100%|██████████| 2/2 [00:00<00:00, 402.18it/s]
Downloading shards: 100%|██████████| 2/2 [00:00<00:00, 386.20it/s]

Downloading shards: 100%|██████████| 2/2 [00:00<00:00, 398.68it/s]

Downloading shards: 100%|██████████| 2/2 [00:00<00:00, 396.17it/s]



Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]
Downloading shards: 100%|██████████| 2/2 [00:00<00:00, 1417.47it/s]

Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]
Downloading shards: 100%|██████████| 2/2 [00:00<00:00, 1469.37it/s]

Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]
Downloading shards: 100%|██████████| 2/2 [00:00<00:00, 1392.99it/s]
[2024-08-13 05:49:23,141] [INFO] [partition_parameters.py:345:__exit__] finished initializing model - num_params = 291, num_elems = 6.74B

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:26<00:26, 26.41s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:26<00:26, 26.42s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:26<00:26, 26.42s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:26<00:26, 26.42s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:26<00:26, 26.42s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:26<00:26, 26.43s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:26<00:26, 26.43s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:43<00:00, 20.93s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:43<00:00, 20.93s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:43<00:00, 20.93s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:43<00:00, 21.76s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:43<00:00, 21.76s/it]


Loading checkpoint shards: 100%|██████████| 2/2 [00:43<00:00, 20.93s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:43<00:00, 21.76s/it]

Loading checkpoint shards: 100%|██████████| 2/2 [00:43<00:00, 21.76s/it]

Loading checkpoint shards: 100%|██████████| 2/2 [00:43<00:00, 20.93s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:43<00:00, 21.76s/it]

Loading checkpoint shards: 100%|██████████| 2/2 [00:43<00:00, 20.94s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:43<00:00, 21.76s/it]

Loading checkpoint shards: 100%|██████████| 2/2 [00:43<00:00, 20.96s/it]enable_input_require_grads!
enable_input_require_grads!

Loading checkpoint shards: 100%|██████████| 2/2 [00:43<00:00, 21.78s/it]
enable_input_require_grads!
enable_input_require_grads!
enable_input_require_grads!
enable_input_require_grads!
enable_input_require_grads!

Loading checkpoint shards:  50%|█████     | 1/2 [00:50<00:50, 50.11s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:08<00:00, 31.64s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:08<00:00, 34.42s/it]
enable_input_require_grads!
trainable params: 8,388,608 || all params: 6,746,804,224 || trainable%: 0.1243
trainable params: 8,388,608 || all params: 6,746,804,224 || trainable%: 0.1243
Finetune with LORA setting:  Finetune with LORA setting:  None
None
trainable params: 8,388,608 || all params: 6,746,804,224 || trainable%: 0.1243
Finetune with LORA setting:  None
trainable params: 8,388,608 || all params: 6,746,804,224 || trainable%: 0.1243
Finetune with LORA setting:  None
trainable params: 8,388,608 || all params: 6,746,804,224 || trainable%: 0.1243
Finetune with LORA setting:  None
trainable params: 8,388,608 || all params: 6,746,804,224 || trainable%: 0.1243
Finetune with LORA setting:  None
trainable params: 8,388,608 || all params: 6,746,804,224 || trainable%: 0.1243
Finetune with LORA setting:  None
trainable params: 8,388,608 || all params: 6,746,804,224 || trainable%: 0.1243
Finetune with LORA setting:  None
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
[rank0]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
[rank7]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
[rank5]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
[rank4]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
[rank1]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
[rank2]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[rank3]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
[rank6]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121/fused_adam/build.ninja...
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/cpp_extension.py:1967: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Loading extension module fused_adam...
Loading extension module fused_adam...
Loading extension module fused_adam...
Loading extension module fused_adam...
Loading extension module fused_adam...
Loading extension module fused_adam...
Loading extension module fused_adam...
Time to load fused_adam op: 2.7067244052886963 secondsTime to load fused_adam op: 2.7249228954315186 secondsTime to load fused_adam op: 2.753992795944214 secondsTime to load fused_adam op: 2.7705776691436768 secondsTime to load fused_adam op: 2.7627527713775635 seconds
Time to load fused_adam op: 2.7433509826660156 seconds
Time to load fused_adam op: 2.6902756690979004 secondsTime to load fused_adam op: 2.700031280517578 seconds





Parameter Offload: Total persistent parameters: 8654848 in 193 params

  0%|          | 0/171 [00:00<?, ?it/s]/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

  1%|          | 1/171 [00:08<23:21,  8.24s/it]
                                               
{'loss': 0.5865, 'grad_norm': 0.6589054157914548, 'learning_rate': 0.0, 'epoch': 0.02}

  1%|          | 1/171 [00:08<23:21,  8.24s/it]
  1%|          | 2/171 [00:10<12:42,  4.51s/it]
                                               
{'loss': 0.6087, 'grad_norm': 0.6413995609560714, 'learning_rate': 7.73705614469083e-07, 'epoch': 0.04}

  1%|          | 2/171 [00:10<12:42,  4.51s/it]
  2%|▏         | 3/171 [00:10<07:48,  2.79s/it]
                                               
{'loss': 0.5525, 'grad_norm': 0.6400862458945662, 'learning_rate': 1.2262943855309167e-06, 'epoch': 0.05}

  2%|▏         | 3/171 [00:10<07:48,  2.79s/it]
  2%|▏         | 4/171 [00:11<05:32,  1.99s/it]
                                               
{'loss': 0.5985, 'grad_norm': 0.5610225428344084, 'learning_rate': 1.547411228938166e-06, 'epoch': 0.07}

  2%|▏         | 4/171 [00:11<05:32,  1.99s/it]
  3%|▎         | 5/171 [00:12<04:14,  1.53s/it]
                                               
{'loss': 0.5629, 'grad_norm': 0.5987944705753526, 'learning_rate': 1.796488803407854e-06, 'epoch': 0.09}

  3%|▎         | 5/171 [00:12<04:14,  1.53s/it]
  4%|▎         | 6/171 [00:13<03:27,  1.26s/it]
                                               
{'loss': 0.5898, 'grad_norm': 0.6738797259640935, 'learning_rate': 1.9999999999999995e-06, 'epoch': 0.11}

  4%|▎         | 6/171 [00:13<03:27,  1.26s/it]
  4%|▍         | 7/171 [00:13<02:56,  1.08s/it]
                                               
{'loss': 0.6018, 'grad_norm': 0.6337669369683245, 'learning_rate': 2e-06, 'epoch': 0.12}

  4%|▍         | 7/171 [00:13<02:56,  1.08s/it]
  5%|▍         | 8/171 [00:14<02:39,  1.02it/s]
                                               
{'loss': 0.5366, 'grad_norm': 0.5905145184589378, 'learning_rate': 1.9878787878787877e-06, 'epoch': 0.14}

  5%|▍         | 8/171 [00:14<02:39,  1.02it/s]
  5%|▌         | 9/171 [00:15<02:24,  1.12it/s]
                                               
{'loss': 0.6102, 'grad_norm': 0.6282621112020086, 'learning_rate': 1.975757575757576e-06, 'epoch': 0.16}

  5%|▌         | 9/171 [00:15<02:24,  1.12it/s]
  6%|▌         | 10/171 [00:15<02:15,  1.19it/s]
                                                
{'loss': 0.5579, 'grad_norm': 0.7313662193705883, 'learning_rate': 1.9636363636363636e-06, 'epoch': 0.18}

  6%|▌         | 10/171 [00:15<02:15,  1.19it/s]
  6%|▋         | 11/171 [00:16<02:08,  1.24it/s]
                                                
{'loss': 0.6105, 'grad_norm': 0.5547533054257626, 'learning_rate': 1.9515151515151513e-06, 'epoch': 0.19}

  6%|▋         | 11/171 [00:16<02:08,  1.24it/s]
  7%|▋         | 12/171 [00:17<02:03,  1.29it/s]
                                                
{'loss': 0.5679, 'grad_norm': 0.5693250128522017, 'learning_rate': 1.9393939393939395e-06, 'epoch': 0.21}

  7%|▋         | 12/171 [00:17<02:03,  1.29it/s]
  8%|▊         | 13/171 [00:18<01:59,  1.33it/s]
                                                
{'loss': 0.6071, 'grad_norm': 0.7128012002469447, 'learning_rate': 1.9272727272727273e-06, 'epoch': 0.23}

  8%|▊         | 13/171 [00:18<01:59,  1.33it/s]
  8%|▊         | 14/171 [00:18<01:56,  1.35it/s]
                                                
{'loss': 0.6019, 'grad_norm': 0.5972533235805034, 'learning_rate': 1.915151515151515e-06, 'epoch': 0.25}

  8%|▊         | 14/171 [00:18<01:56,  1.35it/s]
  9%|▉         | 15/171 [00:19<01:54,  1.36it/s]
                                                
{'loss': 0.6377, 'grad_norm': 0.7264926999998376, 'learning_rate': 1.903030303030303e-06, 'epoch': 0.26}

  9%|▉         | 15/171 [00:19<01:54,  1.36it/s]
  9%|▉         | 16/171 [00:20<01:53,  1.37it/s]
                                                
{'loss': 0.5934, 'grad_norm': 0.7440423105641268, 'learning_rate': 1.8909090909090907e-06, 'epoch': 0.28}

  9%|▉         | 16/171 [00:20<01:53,  1.37it/s]
 10%|▉         | 17/171 [00:20<01:51,  1.38it/s]
                                                
{'loss': 0.5757, 'grad_norm': 0.6428523924609889, 'learning_rate': 1.878787878787879e-06, 'epoch': 0.3}

 10%|▉         | 17/171 [00:20<01:51,  1.38it/s]
 11%|█         | 18/171 [00:21<01:51,  1.37it/s]
                                                
{'loss': 0.6169, 'grad_norm': 0.7821444487599681, 'learning_rate': 1.8666666666666667e-06, 'epoch': 0.32}

 11%|█         | 18/171 [00:21<01:51,  1.37it/s]
 11%|█         | 19/171 [00:22<01:50,  1.38it/s]
                                                
{'loss': 0.6035, 'grad_norm': 0.6461302467385941, 'learning_rate': 1.8545454545454544e-06, 'epoch': 0.33}

 11%|█         | 19/171 [00:22<01:50,  1.38it/s]
 12%|█▏        | 20/171 [00:23<01:48,  1.39it/s]
                                                
{'loss': 0.5934, 'grad_norm': 0.6190826217014922, 'learning_rate': 1.8424242424242424e-06, 'epoch': 0.35}

 12%|█▏        | 20/171 [00:23<01:48,  1.39it/s]
 12%|█▏        | 21/171 [00:23<01:47,  1.40it/s]
                                                
{'loss': 0.5768, 'grad_norm': 0.6044427723347457, 'learning_rate': 1.8303030303030303e-06, 'epoch': 0.37}

 12%|█▏        | 21/171 [00:23<01:47,  1.40it/s]
 13%|█▎        | 22/171 [00:24<01:46,  1.40it/s]
                                                
{'loss': 0.5659, 'grad_norm': 0.6451823308574247, 'learning_rate': 1.818181818181818e-06, 'epoch': 0.39}

 13%|█▎        | 22/171 [00:24<01:46,  1.40it/s]
 13%|█▎        | 23/171 [00:25<01:45,  1.40it/s]
                                                
{'loss': 0.5933, 'grad_norm': 0.6576556973234523, 'learning_rate': 1.806060606060606e-06, 'epoch': 0.4}

 13%|█▎        | 23/171 [00:25<01:45,  1.40it/s]
 14%|█▍        | 24/171 [00:25<01:44,  1.41it/s]
                                                
{'loss': 0.5812, 'grad_norm': 0.6798603046907592, 'learning_rate': 1.7939393939393938e-06, 'epoch': 0.42}

 14%|█▍        | 24/171 [00:25<01:44,  1.41it/s]
 15%|█▍        | 25/171 [00:26<01:43,  1.41it/s]
                                                
{'loss': 0.5806, 'grad_norm': 0.832582746624851, 'learning_rate': 1.7818181818181818e-06, 'epoch': 0.44}

 15%|█▍        | 25/171 [00:26<01:43,  1.41it/s]
 15%|█▌        | 26/171 [00:27<01:43,  1.41it/s]
                                                
{'loss': 0.5812, 'grad_norm': 0.6753470539929761, 'learning_rate': 1.7696969696969697e-06, 'epoch': 0.46}

 15%|█▌        | 26/171 [00:27<01:43,  1.41it/s]
 16%|█▌        | 27/171 [00:28<01:42,  1.40it/s]
                                                
{'loss': 0.5523, 'grad_norm': 0.6379184701370401, 'learning_rate': 1.7575757575757575e-06, 'epoch': 0.47}

 16%|█▌        | 27/171 [00:28<01:42,  1.40it/s]
 16%|█▋        | 28/171 [00:28<01:41,  1.41it/s]
                                                
{'loss': 0.6225, 'grad_norm': 0.6882227802475495, 'learning_rate': 1.7454545454545452e-06, 'epoch': 0.49}

 16%|█▋        | 28/171 [00:28<01:41,  1.41it/s]
 17%|█▋        | 29/171 [00:29<01:40,  1.41it/s]
                                                
{'loss': 0.5956, 'grad_norm': 0.6494980824447772, 'learning_rate': 1.7333333333333334e-06, 'epoch': 0.51}

 17%|█▋        | 29/171 [00:29<01:40,  1.41it/s]
 18%|█▊        | 30/171 [00:30<01:40,  1.41it/s]
                                                
{'loss': 0.5671, 'grad_norm': 0.5912681087351873, 'learning_rate': 1.7212121212121211e-06, 'epoch': 0.53}

 18%|█▊        | 30/171 [00:30<01:40,  1.41it/s]
 18%|█▊        | 31/171 [00:30<01:39,  1.41it/s]
                                                
{'loss': 0.5648, 'grad_norm': 0.6856917067603889, 'learning_rate': 1.709090909090909e-06, 'epoch': 0.54}

 18%|█▊        | 31/171 [00:30<01:39,  1.41it/s]
 19%|█▊        | 32/171 [00:31<01:38,  1.41it/s]
                                                
{'loss': 0.6145, 'grad_norm': 0.6425477818182613, 'learning_rate': 1.6969696969696969e-06, 'epoch': 0.56}

 19%|█▊        | 32/171 [00:31<01:38,  1.41it/s]
 19%|█▉        | 33/171 [00:32<01:37,  1.41it/s]
                                                
{'loss': 0.5624, 'grad_norm': 0.5830195034005877, 'learning_rate': 1.6848484848484848e-06, 'epoch': 0.58}

 19%|█▉        | 33/171 [00:32<01:37,  1.41it/s]
 20%|█▉        | 34/171 [00:33<01:37,  1.41it/s]
                                                
{'loss': 0.6257, 'grad_norm': 0.6070213015478164, 'learning_rate': 1.6727272727272726e-06, 'epoch': 0.6}

 20%|█▉        | 34/171 [00:33<01:37,  1.41it/s]
 20%|██        | 35/171 [00:33<01:36,  1.41it/s]
                                                
{'loss': 0.5525, 'grad_norm': 0.7610510638771316, 'learning_rate': 1.6606060606060605e-06, 'epoch': 0.61}

 20%|██        | 35/171 [00:33<01:36,  1.41it/s]
 21%|██        | 36/171 [00:34<01:35,  1.41it/s]
                                                
{'loss': 0.5905, 'grad_norm': 0.6297167994506756, 'learning_rate': 1.6484848484848483e-06, 'epoch': 0.63}

 21%|██        | 36/171 [00:34<01:35,  1.41it/s]
 22%|██▏       | 37/171 [00:35<01:35,  1.41it/s]
                                                
{'loss': 0.5873, 'grad_norm': 0.6084504679564766, 'learning_rate': 1.6363636363636365e-06, 'epoch': 0.65}

 22%|██▏       | 37/171 [00:35<01:35,  1.41it/s]
 22%|██▏       | 38/171 [00:35<01:34,  1.41it/s]
                                                
{'loss': 0.5882, 'grad_norm': 0.6120817284871538, 'learning_rate': 1.6242424242424242e-06, 'epoch': 0.67}

 22%|██▏       | 38/171 [00:35<01:34,  1.41it/s]
 23%|██▎       | 39/171 [00:36<01:34,  1.40it/s]
                                                
{'loss': 0.5912, 'grad_norm': 0.6421090258620159, 'learning_rate': 1.612121212121212e-06, 'epoch': 0.68}

 23%|██▎       | 39/171 [00:36<01:34,  1.40it/s]
 23%|██▎       | 40/171 [00:37<01:33,  1.41it/s]
                                                
{'loss': 0.5592, 'grad_norm': 0.5936078101519374, 'learning_rate': 1.6e-06, 'epoch': 0.7}

 23%|██▎       | 40/171 [00:37<01:33,  1.41it/s]
 24%|██▍       | 41/171 [00:38<01:32,  1.41it/s]
                                                
{'loss': 0.6001, 'grad_norm': 0.6005551100751042, 'learning_rate': 1.5878787878787879e-06, 'epoch': 0.72}

 24%|██▍       | 41/171 [00:38<01:32,  1.41it/s]
 25%|██▍       | 42/171 [00:38<01:31,  1.40it/s]
                                                
{'loss': 0.5336, 'grad_norm': 0.5277498596236202, 'learning_rate': 1.5757575757575756e-06, 'epoch': 0.74}

 25%|██▍       | 42/171 [00:38<01:31,  1.40it/s]
 25%|██▌       | 43/171 [00:39<01:31,  1.40it/s]
                                                
{'loss': 0.4801, 'grad_norm': 0.571185852789356, 'learning_rate': 1.5636363636363636e-06, 'epoch': 0.75}

 25%|██▌       | 43/171 [00:39<01:31,  1.40it/s]
 26%|██▌       | 44/171 [00:40<01:30,  1.41it/s]
                                                
{'loss': 0.5541, 'grad_norm': 0.569563036156181, 'learning_rate': 1.5515151515151514e-06, 'epoch': 0.77}

 26%|██▌       | 44/171 [00:40<01:30,  1.41it/s]
 26%|██▋       | 45/171 [00:40<01:29,  1.41it/s]
                                                
{'loss': 0.6051, 'grad_norm': 0.6348844502926307, 'learning_rate': 1.5393939393939393e-06, 'epoch': 0.79}

 26%|██▋       | 45/171 [00:40<01:29,  1.41it/s]
 27%|██▋       | 46/171 [00:41<01:29,  1.40it/s]
                                                
{'loss': 0.6228, 'grad_norm': 0.5875317033566718, 'learning_rate': 1.5272727272727273e-06, 'epoch': 0.81}

 27%|██▋       | 46/171 [00:41<01:29,  1.40it/s]
 27%|██▋       | 47/171 [00:42<01:28,  1.40it/s]
                                                
{'loss': 0.5826, 'grad_norm': 0.6308308937312108, 'learning_rate': 1.515151515151515e-06, 'epoch': 0.82}

 27%|██▋       | 47/171 [00:42<01:28,  1.40it/s]
 28%|██▊       | 48/171 [00:43<01:27,  1.41it/s]
                                                
{'loss': 0.5542, 'grad_norm': 0.6590432330875904, 'learning_rate': 1.5030303030303028e-06, 'epoch': 0.84}

 28%|██▊       | 48/171 [00:43<01:27,  1.41it/s]
 29%|██▊       | 49/171 [00:43<01:26,  1.40it/s]
                                                
{'loss': 0.5415, 'grad_norm': 0.5790582243569627, 'learning_rate': 1.490909090909091e-06, 'epoch': 0.86}

 29%|██▊       | 49/171 [00:43<01:26,  1.40it/s]
 29%|██▉       | 50/171 [00:44<01:26,  1.40it/s]
                                                
{'loss': 0.5348, 'grad_norm': 0.6055654069650258, 'learning_rate': 1.4787878787878787e-06, 'epoch': 0.88}

 29%|██▉       | 50/171 [00:44<01:26,  1.40it/s]
 30%|██▉       | 51/171 [00:45<01:25,  1.40it/s]
                                                
{'loss': 0.5476, 'grad_norm': 0.5257861610459996, 'learning_rate': 1.4666666666666665e-06, 'epoch': 0.89}

 30%|██▉       | 51/171 [00:45<01:25,  1.40it/s]
 30%|███       | 52/171 [00:45<01:24,  1.41it/s]
                                                
{'loss': 0.5545, 'grad_norm': 0.525276707228225, 'learning_rate': 1.4545454545454544e-06, 'epoch': 0.91}

 30%|███       | 52/171 [00:45<01:24,  1.41it/s]
 31%|███       | 53/171 [00:46<01:23,  1.41it/s]
                                                
{'loss': 0.5629, 'grad_norm': 0.6156068940076749, 'learning_rate': 1.4424242424242424e-06, 'epoch': 0.93}

 31%|███       | 53/171 [00:46<01:23,  1.41it/s]
 32%|███▏      | 54/171 [00:47<01:23,  1.41it/s]
                                                
{'loss': 0.5648, 'grad_norm': 0.5505902298147796, 'learning_rate': 1.4303030303030303e-06, 'epoch': 0.95}

 32%|███▏      | 54/171 [00:47<01:23,  1.41it/s]
 32%|███▏      | 55/171 [00:48<01:22,  1.41it/s]
                                                
{'loss': 0.5814, 'grad_norm': 0.6293883498042087, 'learning_rate': 1.418181818181818e-06, 'epoch': 0.96}

 32%|███▏      | 55/171 [00:48<01:22,  1.41it/s]
 33%|███▎      | 56/171 [00:48<01:21,  1.41it/s]
                                                
{'loss': 0.6085, 'grad_norm': 0.685550200851003, 'learning_rate': 1.4060606060606058e-06, 'epoch': 0.98}

 33%|███▎      | 56/171 [00:48<01:21,  1.41it/s]
 33%|███▎      | 57/171 [00:49<01:20,  1.41it/s]
                                                
{'loss': 0.612, 'grad_norm': 0.6836003400098146, 'learning_rate': 1.393939393939394e-06, 'epoch': 1.0}

 33%|███▎      | 57/171 [00:49<01:20,  1.41it/s]
 34%|███▍      | 58/171 [00:50<01:22,  1.37it/s]
                                                
{'loss': 0.6243, 'grad_norm': 0.586038218246537, 'learning_rate': 1.3818181818181818e-06, 'epoch': 1.02}

 34%|███▍      | 58/171 [00:50<01:22,  1.37it/s]
 35%|███▍      | 59/171 [00:50<01:20,  1.39it/s]
                                                
{'loss': 0.5329, 'grad_norm': 0.5916025729829539, 'learning_rate': 1.3696969696969695e-06, 'epoch': 1.04}

 35%|███▍      | 59/171 [00:50<01:20,  1.39it/s]
 35%|███▌      | 60/171 [00:51<01:19,  1.39it/s]
                                                
{'loss': 0.5965, 'grad_norm': 0.5781952350577791, 'learning_rate': 1.3575757575757577e-06, 'epoch': 1.05}

 35%|███▌      | 60/171 [00:51<01:19,  1.39it/s]
 36%|███▌      | 61/171 [00:52<01:18,  1.39it/s]
                                                
{'loss': 0.6058, 'grad_norm': 0.6094212609728629, 'learning_rate': 1.3454545454545455e-06, 'epoch': 1.07}

 36%|███▌      | 61/171 [00:52<01:18,  1.39it/s]
 36%|███▋      | 62/171 [00:53<01:18,  1.39it/s]
                                                
{'loss': 0.5437, 'grad_norm': 0.4668474568990607, 'learning_rate': 1.3333333333333332e-06, 'epoch': 1.09}

 36%|███▋      | 62/171 [00:53<01:18,  1.39it/s]
 37%|███▋      | 63/171 [00:53<01:17,  1.39it/s]
                                                
{'loss': 0.5269, 'grad_norm': 0.5143318273393007, 'learning_rate': 1.3212121212121212e-06, 'epoch': 1.11}

 37%|███▋      | 63/171 [00:53<01:17,  1.39it/s]
 37%|███▋      | 64/171 [00:54<01:16,  1.40it/s]
                                                
{'loss': 0.566, 'grad_norm': 0.6390584374008148, 'learning_rate': 1.3090909090909091e-06, 'epoch': 1.12}

 37%|███▋      | 64/171 [00:54<01:16,  1.40it/s]
 38%|███▊      | 65/171 [00:55<01:15,  1.40it/s]
                                                
{'loss': 0.5551, 'grad_norm': 0.519791300916016, 'learning_rate': 1.2969696969696969e-06, 'epoch': 1.14}

 38%|███▊      | 65/171 [00:55<01:15,  1.40it/s]
 39%|███▊      | 66/171 [00:55<01:14,  1.40it/s]
                                                
{'loss': 0.6076, 'grad_norm': 0.6711664790154965, 'learning_rate': 1.2848484848484848e-06, 'epoch': 1.16}

 39%|███▊      | 66/171 [00:55<01:14,  1.40it/s]
 39%|███▉      | 67/171 [00:56<01:14,  1.40it/s]
                                                
{'loss': 0.5981, 'grad_norm': 0.6670434977016781, 'learning_rate': 1.2727272727272726e-06, 'epoch': 1.18}

 39%|███▉      | 67/171 [00:56<01:14,  1.40it/s]
 40%|███▉      | 68/171 [00:57<01:13,  1.40it/s]
                                                
{'loss': 0.5221, 'grad_norm': 0.5011896683435733, 'learning_rate': 1.2606060606060606e-06, 'epoch': 1.19}

 40%|███▉      | 68/171 [00:57<01:13,  1.40it/s]
 40%|████      | 69/171 [00:58<01:12,  1.40it/s]
                                                
{'loss': 0.5821, 'grad_norm': 0.5985445006025696, 'learning_rate': 1.2484848484848485e-06, 'epoch': 1.21}

 40%|████      | 69/171 [00:58<01:12,  1.40it/s]
 41%|████      | 70/171 [00:58<01:11,  1.40it/s]
                                                
{'loss': 0.5342, 'grad_norm': 0.6177086752078437, 'learning_rate': 1.2363636363636363e-06, 'epoch': 1.23}

 41%|████      | 70/171 [00:58<01:11,  1.40it/s]
 42%|████▏     | 71/171 [00:59<01:11,  1.40it/s]
                                                
{'loss': 0.5966, 'grad_norm': 0.6788998068177581, 'learning_rate': 1.224242424242424e-06, 'epoch': 1.25}

 42%|████▏     | 71/171 [00:59<01:11,  1.40it/s]
 42%|████▏     | 72/171 [01:00<01:10,  1.41it/s]
                                                
{'loss': 0.6135, 'grad_norm': 0.6901185768395657, 'learning_rate': 1.2121212121212122e-06, 'epoch': 1.26}

 42%|████▏     | 72/171 [01:00<01:10,  1.41it/s]
 43%|████▎     | 73/171 [01:00<01:09,  1.40it/s]
                                                
{'loss': 0.5939, 'grad_norm': 0.5545888599986116, 'learning_rate': 1.2e-06, 'epoch': 1.28}

 43%|████▎     | 73/171 [01:00<01:09,  1.40it/s]
 43%|████▎     | 74/171 [01:01<01:09,  1.40it/s]
                                                
{'loss': 0.5545, 'grad_norm': 0.5502601409115636, 'learning_rate': 1.187878787878788e-06, 'epoch': 1.3}

 43%|████▎     | 74/171 [01:01<01:09,  1.40it/s]
 44%|████▍     | 75/171 [01:02<01:08,  1.40it/s]
                                                
{'loss': 0.5634, 'grad_norm': 0.5914130748804747, 'learning_rate': 1.1757575757575757e-06, 'epoch': 1.32}

 44%|████▍     | 75/171 [01:02<01:08,  1.40it/s]
 44%|████▍     | 76/171 [01:03<01:07,  1.40it/s]
                                                
{'loss': 0.5192, 'grad_norm': 0.5728518867347621, 'learning_rate': 1.1636363636363636e-06, 'epoch': 1.33}

 44%|████▍     | 76/171 [01:03<01:07,  1.40it/s]
 45%|████▌     | 77/171 [01:03<01:07,  1.40it/s]
                                                
{'loss': 0.5153, 'grad_norm': 0.6547675600645685, 'learning_rate': 1.1515151515151516e-06, 'epoch': 1.35}

 45%|████▌     | 77/171 [01:03<01:07,  1.40it/s]
 46%|████▌     | 78/171 [01:04<01:06,  1.41it/s]
                                                
{'loss': 0.5728, 'grad_norm': 0.6379583918893329, 'learning_rate': 1.1393939393939393e-06, 'epoch': 1.37}

 46%|████▌     | 78/171 [01:04<01:06,  1.41it/s]
 46%|████▌     | 79/171 [01:05<01:05,  1.40it/s]
                                                
{'loss': 0.5506, 'grad_norm': 0.5800871459866006, 'learning_rate': 1.127272727272727e-06, 'epoch': 1.39}

 46%|████▌     | 79/171 [01:05<01:05,  1.40it/s]
 47%|████▋     | 80/171 [01:05<01:04,  1.41it/s]
                                                
{'loss': 0.567, 'grad_norm': 0.5860956714884921, 'learning_rate': 1.1151515151515153e-06, 'epoch': 1.4}

 47%|████▋     | 80/171 [01:05<01:04,  1.41it/s]
 47%|████▋     | 81/171 [01:06<01:04,  1.40it/s]
                                                
{'loss': 0.5747, 'grad_norm': 0.5920136948024557, 'learning_rate': 1.103030303030303e-06, 'epoch': 1.42}

 47%|████▋     | 81/171 [01:06<01:04,  1.40it/s]
 48%|████▊     | 82/171 [01:07<01:03,  1.40it/s]
                                                
{'loss': 0.5821, 'grad_norm': 0.5987941858725764, 'learning_rate': 1.0909090909090908e-06, 'epoch': 1.44}

 48%|████▊     | 82/171 [01:07<01:03,  1.40it/s]
 49%|████▊     | 83/171 [01:08<01:03,  1.39it/s]
                                                
{'loss': 0.574, 'grad_norm': 0.605605398212719, 'learning_rate': 1.0787878787878787e-06, 'epoch': 1.46}

 49%|████▊     | 83/171 [01:08<01:03,  1.39it/s]
 49%|████▉     | 84/171 [01:08<01:02,  1.39it/s]
                                                
{'loss': 0.5224, 'grad_norm': 0.5363209117470494, 'learning_rate': 1.0666666666666667e-06, 'epoch': 1.47}

 49%|████▉     | 84/171 [01:08<01:02,  1.39it/s]
 50%|████▉     | 85/171 [01:09<01:01,  1.40it/s]
                                                
{'loss': 0.5592, 'grad_norm': 0.6382900868895364, 'learning_rate': 1.0545454545454544e-06, 'epoch': 1.49}

 50%|████▉     | 85/171 [01:09<01:01,  1.40it/s]
 50%|█████     | 86/171 [01:10<01:00,  1.40it/s]
                                                
{'loss': 0.5651, 'grad_norm': 0.6117195822281111, 'learning_rate': 1.0424242424242424e-06, 'epoch': 1.51}

 50%|█████     | 86/171 [01:10<01:00,  1.40it/s]
 51%|█████     | 87/171 [01:10<00:59,  1.40it/s]
                                                
{'loss': 0.5262, 'grad_norm': 0.6074225553511245, 'learning_rate': 1.0303030303030302e-06, 'epoch': 1.53}

 51%|█████     | 87/171 [01:10<00:59,  1.40it/s]
 51%|█████▏    | 88/171 [01:11<00:58,  1.41it/s]
                                                
{'loss': 0.5299, 'grad_norm': 0.5813574563734645, 'learning_rate': 1.0181818181818181e-06, 'epoch': 1.54}

 51%|█████▏    | 88/171 [01:11<00:58,  1.41it/s]
 52%|█████▏    | 89/171 [01:12<00:58,  1.41it/s]
                                                
{'loss': 0.5977, 'grad_norm': 0.5220570240675724, 'learning_rate': 1.006060606060606e-06, 'epoch': 1.56}

 52%|█████▏    | 89/171 [01:12<00:58,  1.41it/s]
 53%|█████▎    | 90/171 [01:13<00:57,  1.40it/s]
                                                
{'loss': 0.5014, 'grad_norm': 0.4783750254349608, 'learning_rate': 9.939393939393938e-07, 'epoch': 1.58}

 53%|█████▎    | 90/171 [01:13<00:57,  1.40it/s]
 53%|█████▎    | 91/171 [01:13<00:56,  1.41it/s]
                                                
{'loss': 0.5591, 'grad_norm': 0.5440079769067397, 'learning_rate': 9.818181818181818e-07, 'epoch': 1.6}

 53%|█████▎    | 91/171 [01:13<00:56,  1.41it/s]
 54%|█████▍    | 92/171 [01:14<00:56,  1.41it/s]
                                                
{'loss': 0.5451, 'grad_norm': 0.5817089883104903, 'learning_rate': 9.696969696969698e-07, 'epoch': 1.61}

 54%|█████▍    | 92/171 [01:14<00:56,  1.41it/s]
 54%|█████▍    | 93/171 [01:15<00:55,  1.41it/s]
                                                
{'loss': 0.5257, 'grad_norm': 0.5382357217140298, 'learning_rate': 9.575757575757575e-07, 'epoch': 1.63}

 54%|█████▍    | 93/171 [01:15<00:55,  1.41it/s]
 55%|█████▍    | 94/171 [01:15<00:54,  1.41it/s]
                                                
{'loss': 0.5796, 'grad_norm': 0.6007399315001832, 'learning_rate': 9.454545454545454e-07, 'epoch': 1.65}

 55%|█████▍    | 94/171 [01:15<00:54,  1.41it/s]
 56%|█████▌    | 95/171 [01:16<00:53,  1.41it/s]
                                                
{'loss': 0.5494, 'grad_norm': 0.5552971389218135, 'learning_rate': 9.333333333333333e-07, 'epoch': 1.67}

 56%|█████▌    | 95/171 [01:16<00:53,  1.41it/s]
 56%|█████▌    | 96/171 [01:17<00:53,  1.41it/s]
                                                
{'loss': 0.5551, 'grad_norm': 0.63202097623384, 'learning_rate': 9.212121212121212e-07, 'epoch': 1.68}

 56%|█████▌    | 96/171 [01:17<00:53,  1.41it/s]
 57%|█████▋    | 97/171 [01:18<00:52,  1.40it/s]
                                                
{'loss': 0.6029, 'grad_norm': 0.5874663372981802, 'learning_rate': 9.09090909090909e-07, 'epoch': 1.7}

 57%|█████▋    | 97/171 [01:18<00:52,  1.40it/s]
 57%|█████▋    | 98/171 [01:18<00:51,  1.41it/s]
                                                
{'loss': 0.5894, 'grad_norm': 0.6592301364878059, 'learning_rate': 8.969696969696969e-07, 'epoch': 1.72}

 57%|█████▋    | 98/171 [01:18<00:51,  1.41it/s]
 58%|█████▊    | 99/171 [01:19<00:50,  1.41it/s]
                                                
{'loss': 0.5212, 'grad_norm': 0.5736765490710335, 'learning_rate': 8.848484848484849e-07, 'epoch': 1.74}

 58%|█████▊    | 99/171 [01:19<00:50,  1.41it/s]
 58%|█████▊    | 100/171 [01:20<00:50,  1.41it/s]
                                                 
{'loss': 0.5019, 'grad_norm': 0.5313354958653103, 'learning_rate': 8.727272727272726e-07, 'epoch': 1.75}

 58%|█████▊    | 100/171 [01:20<00:50,  1.41it/s]
 59%|█████▉    | 101/171 [01:20<00:49,  1.42it/s]
                                                 
{'loss': 0.5748, 'grad_norm': 0.7378029810688667, 'learning_rate': 8.606060606060606e-07, 'epoch': 1.77}

 59%|█████▉    | 101/171 [01:20<00:49,  1.42it/s]
 60%|█████▉    | 102/171 [01:21<00:48,  1.41it/s]
                                                 
{'loss': 0.568, 'grad_norm': 0.680350744411402, 'learning_rate': 8.484848484848484e-07, 'epoch': 1.79}

 60%|█████▉    | 102/171 [01:21<00:48,  1.41it/s]
 60%|██████    | 103/171 [01:22<00:48,  1.42it/s]
                                                 
{'loss': 0.5484, 'grad_norm': 0.5435023034725982, 'learning_rate': 8.363636363636363e-07, 'epoch': 1.81}

 60%|██████    | 103/171 [01:22<00:48,  1.42it/s]
 61%|██████    | 104/171 [01:22<00:47,  1.41it/s]
                                                 
{'loss': 0.5511, 'grad_norm': 0.6119765849324691, 'learning_rate': 8.242424242424241e-07, 'epoch': 1.82}

 61%|██████    | 104/171 [01:22<00:47,  1.41it/s]
 61%|██████▏   | 105/171 [01:23<00:46,  1.41it/s]
                                                 
{'loss': 0.5722, 'grad_norm': 0.6875304332374973, 'learning_rate': 8.121212121212121e-07, 'epoch': 1.84}

 61%|██████▏   | 105/171 [01:23<00:46,  1.41it/s]
 62%|██████▏   | 106/171 [01:24<00:45,  1.42it/s]
                                                 
{'loss': 0.5614, 'grad_norm': 0.558247646748137, 'learning_rate': 8e-07, 'epoch': 1.86}

 62%|██████▏   | 106/171 [01:24<00:45,  1.42it/s]
 63%|██████▎   | 107/171 [01:25<00:45,  1.42it/s]
                                                 
{'loss': 0.5143, 'grad_norm': 0.5545869231622692, 'learning_rate': 7.878787878787878e-07, 'epoch': 1.88}

 63%|██████▎   | 107/171 [01:25<00:45,  1.42it/s]
 63%|██████▎   | 108/171 [01:25<00:44,  1.42it/s]
                                                 
{'loss': 0.5515, 'grad_norm': 0.599798571305537, 'learning_rate': 7.757575757575757e-07, 'epoch': 1.89}

 63%|██████▎   | 108/171 [01:25<00:44,  1.42it/s]
 64%|██████▎   | 109/171 [01:26<00:43,  1.42it/s]
                                                 
{'loss': 0.5396, 'grad_norm': 0.5498615448251577, 'learning_rate': 7.636363636363636e-07, 'epoch': 1.91}

 64%|██████▎   | 109/171 [01:26<00:43,  1.42it/s]
 64%|██████▍   | 110/171 [01:27<00:42,  1.42it/s]
                                                 
{'loss': 0.5467, 'grad_norm': 0.6219830682946027, 'learning_rate': 7.515151515151514e-07, 'epoch': 1.93}

 64%|██████▍   | 110/171 [01:27<00:42,  1.42it/s]
 65%|██████▍   | 111/171 [01:27<00:42,  1.41it/s]
                                                 
{'loss': 0.5226, 'grad_norm': 0.5314945767958292, 'learning_rate': 7.393939393939394e-07, 'epoch': 1.95}

 65%|██████▍   | 111/171 [01:27<00:42,  1.41it/s]
 65%|██████▌   | 112/171 [01:28<00:41,  1.42it/s]
                                                 
{'loss': 0.5512, 'grad_norm': 0.6572689179628493, 'learning_rate': 7.272727272727272e-07, 'epoch': 1.96}

 65%|██████▌   | 112/171 [01:28<00:41,  1.42it/s]
 66%|██████▌   | 113/171 [01:29<00:40,  1.42it/s]
                                                 
{'loss': 0.5347, 'grad_norm': 0.608881787078195, 'learning_rate': 7.151515151515152e-07, 'epoch': 1.98}

 66%|██████▌   | 113/171 [01:29<00:40,  1.42it/s]
 67%|██████▋   | 114/171 [01:30<00:40,  1.42it/s]
                                                 
{'loss': 0.6022, 'grad_norm': 0.6175946160671394, 'learning_rate': 7.030303030303029e-07, 'epoch': 2.0}

 67%|██████▋   | 114/171 [01:30<00:40,  1.42it/s]
 67%|██████▋   | 115/171 [01:30<00:39,  1.42it/s]
                                                 
{'loss': 0.5316, 'grad_norm': 0.6027320415271372, 'learning_rate': 6.909090909090909e-07, 'epoch': 2.02}

 67%|██████▋   | 115/171 [01:30<00:39,  1.42it/s]
 68%|██████▊   | 116/171 [01:31<00:38,  1.42it/s]
                                                 
{'loss': 0.5116, 'grad_norm': 0.4984516875727948, 'learning_rate': 6.787878787878789e-07, 'epoch': 2.04}

 68%|██████▊   | 116/171 [01:31<00:38,  1.42it/s]
 68%|██████▊   | 117/171 [01:32<00:38,  1.42it/s]
                                                 
{'loss': 0.5207, 'grad_norm': 0.5097534135708974, 'learning_rate': 6.666666666666666e-07, 'epoch': 2.05}

 68%|██████▊   | 117/171 [01:32<00:38,  1.42it/s]
 69%|██████▉   | 118/171 [01:32<00:38,  1.39it/s]
                                                 
{'loss': 0.4925, 'grad_norm': 0.517465276776932, 'learning_rate': 6.545454545454546e-07, 'epoch': 2.07}

 69%|██████▉   | 118/171 [01:32<00:38,  1.39it/s]
 70%|██████▉   | 119/171 [01:33<00:37,  1.39it/s]
                                                 
{'loss': 0.5673, 'grad_norm': 0.5791179125532069, 'learning_rate': 6.424242424242424e-07, 'epoch': 2.09}

 70%|██████▉   | 119/171 [01:33<00:37,  1.39it/s]
 70%|███████   | 120/171 [01:34<00:36,  1.40it/s]
                                                 
{'loss': 0.5198, 'grad_norm': 0.6421928240377471, 'learning_rate': 6.303030303030303e-07, 'epoch': 2.11}

 70%|███████   | 120/171 [01:34<00:36,  1.40it/s]
 71%|███████   | 121/171 [01:34<00:35,  1.41it/s]
                                                 
{'loss': 0.5249, 'grad_norm': 0.5766405222888419, 'learning_rate': 6.181818181818181e-07, 'epoch': 2.12}

 71%|███████   | 121/171 [01:35<00:35,  1.41it/s]
 71%|███████▏  | 122/171 [01:35<00:34,  1.41it/s]
                                                 
{'loss': 0.6006, 'grad_norm': 0.7175867091961666, 'learning_rate': 6.060606060606061e-07, 'epoch': 2.14}

 71%|███████▏  | 122/171 [01:35<00:34,  1.41it/s]
 72%|███████▏  | 123/171 [01:36<00:34,  1.41it/s]
                                                 
{'loss': 0.5398, 'grad_norm': 0.5714399828391461, 'learning_rate': 5.93939393939394e-07, 'epoch': 2.16}

 72%|███████▏  | 123/171 [01:36<00:34,  1.41it/s]
 73%|███████▎  | 124/171 [01:37<00:33,  1.41it/s]
                                                 
{'loss': 0.5765, 'grad_norm': 0.5710518426643206, 'learning_rate': 5.818181818181818e-07, 'epoch': 2.18}

 73%|███████▎  | 124/171 [01:37<00:33,  1.41it/s]
 73%|███████▎  | 125/171 [01:37<00:32,  1.41it/s]
                                                 
{'loss': 0.562, 'grad_norm': 0.5343364472141209, 'learning_rate': 5.696969696969697e-07, 'epoch': 2.19}

 73%|███████▎  | 125/171 [01:37<00:32,  1.41it/s]
 74%|███████▎  | 126/171 [01:38<00:31,  1.41it/s]
                                                 
{'loss': 0.5817, 'grad_norm': 0.6207980160380974, 'learning_rate': 5.575757575757576e-07, 'epoch': 2.21}

 74%|███████▎  | 126/171 [01:38<00:31,  1.41it/s]
 74%|███████▍  | 127/171 [01:39<00:31,  1.42it/s]
                                                 
{'loss': 0.5351, 'grad_norm': 0.5821911208156472, 'learning_rate': 5.454545454545454e-07, 'epoch': 2.23}

 74%|███████▍  | 127/171 [01:39<00:31,  1.42it/s]
 75%|███████▍  | 128/171 [01:39<00:30,  1.42it/s]
                                                 
{'loss': 0.5251, 'grad_norm': 0.5613750496756151, 'learning_rate': 5.333333333333333e-07, 'epoch': 2.25}

 75%|███████▍  | 128/171 [01:39<00:30,  1.42it/s]
 75%|███████▌  | 129/171 [01:40<00:29,  1.41it/s]
                                                 
{'loss': 0.5192, 'grad_norm': 0.5410860805301237, 'learning_rate': 5.212121212121212e-07, 'epoch': 2.26}

 75%|███████▌  | 129/171 [01:40<00:29,  1.41it/s]
 76%|███████▌  | 130/171 [01:41<00:28,  1.41it/s]
                                                 
{'loss': 0.6063, 'grad_norm': 0.6505776958851149, 'learning_rate': 5.090909090909091e-07, 'epoch': 2.28}

 76%|███████▌  | 130/171 [01:41<00:28,  1.41it/s]
 77%|███████▋  | 131/171 [01:42<00:28,  1.41it/s]
                                                 
{'loss': 0.5494, 'grad_norm': 0.5285631727263266, 'learning_rate': 4.969696969696969e-07, 'epoch': 2.3}

 77%|███████▋  | 131/171 [01:42<00:28,  1.41it/s]
 77%|███████▋  | 132/171 [01:42<00:27,  1.41it/s]
                                                 
{'loss': 0.5391, 'grad_norm': 0.6088211609072267, 'learning_rate': 4.848484848484849e-07, 'epoch': 2.32}

 77%|███████▋  | 132/171 [01:42<00:27,  1.41it/s]
 78%|███████▊  | 133/171 [01:43<00:26,  1.41it/s]
                                                 
{'loss': 0.5405, 'grad_norm': 0.5517456220224712, 'learning_rate': 4.727272727272727e-07, 'epoch': 2.33}

 78%|███████▊  | 133/171 [01:43<00:26,  1.41it/s]
 78%|███████▊  | 134/171 [01:44<00:26,  1.41it/s]
                                                 
{'loss': 0.5384, 'grad_norm': 0.6448740117457075, 'learning_rate': 4.606060606060606e-07, 'epoch': 2.35}

 78%|███████▊  | 134/171 [01:44<00:26,  1.41it/s]
 79%|███████▉  | 135/171 [01:44<00:25,  1.41it/s]
                                                 
{'loss': 0.5683, 'grad_norm': 0.579241610236492, 'learning_rate': 4.4848484848484845e-07, 'epoch': 2.37}

 79%|███████▉  | 135/171 [01:44<00:25,  1.41it/s]
 80%|███████▉  | 136/171 [01:45<00:24,  1.42it/s]
                                                 
{'loss': 0.5741, 'grad_norm': 0.5625432916254999, 'learning_rate': 4.363636363636363e-07, 'epoch': 2.39}

 80%|███████▉  | 136/171 [01:45<00:24,  1.42it/s]
 80%|████████  | 137/171 [01:46<00:24,  1.42it/s]
                                                 
{'loss': 0.5509, 'grad_norm': 0.5866245477954917, 'learning_rate': 4.242424242424242e-07, 'epoch': 2.4}

 80%|████████  | 137/171 [01:46<00:24,  1.42it/s]
 81%|████████  | 138/171 [01:47<00:23,  1.41it/s]
                                                 
{'loss': 0.5612, 'grad_norm': 0.5513767487595211, 'learning_rate': 4.1212121212121207e-07, 'epoch': 2.42}

 81%|████████  | 138/171 [01:47<00:23,  1.41it/s]
 81%|████████▏ | 139/171 [01:47<00:23,  1.39it/s]
                                                 
{'loss': 0.4865, 'grad_norm': 1.1129118033607097, 'learning_rate': 4e-07, 'epoch': 2.44}

 81%|████████▏ | 139/171 [01:47<00:23,  1.39it/s]
 82%|████████▏ | 140/171 [01:48<00:22,  1.39it/s]
                                                 
{'loss': 0.5215, 'grad_norm': 0.48079119989634567, 'learning_rate': 3.8787878787878784e-07, 'epoch': 2.46}

 82%|████████▏ | 140/171 [01:48<00:22,  1.39it/s]
 82%|████████▏ | 141/171 [01:49<00:21,  1.39it/s]
                                                 
{'loss': 0.5357, 'grad_norm': 0.5477721761465673, 'learning_rate': 3.757575757575757e-07, 'epoch': 2.47}

 82%|████████▏ | 141/171 [01:49<00:21,  1.39it/s]
 83%|████████▎ | 142/171 [01:49<00:20,  1.40it/s]
                                                 
{'loss': 0.5564, 'grad_norm': 0.5135658249098758, 'learning_rate': 3.636363636363636e-07, 'epoch': 2.49}

 83%|████████▎ | 142/171 [01:49<00:20,  1.40it/s]
 84%|████████▎ | 143/171 [01:50<00:19,  1.40it/s]
                                                 
{'loss': 0.5511, 'grad_norm': 0.7060890491164571, 'learning_rate': 3.5151515151515146e-07, 'epoch': 2.51}

 84%|████████▎ | 143/171 [01:50<00:19,  1.40it/s]
 84%|████████▍ | 144/171 [01:51<00:19,  1.40it/s]
                                                 
{'loss': 0.5305, 'grad_norm': 0.5477320993154854, 'learning_rate': 3.393939393939394e-07, 'epoch': 2.53}

 84%|████████▍ | 144/171 [01:51<00:19,  1.40it/s]
 85%|████████▍ | 145/171 [01:52<00:18,  1.41it/s]
                                                 
{'loss': 0.5402, 'grad_norm': 0.5298727519453215, 'learning_rate': 3.272727272727273e-07, 'epoch': 2.54}

 85%|████████▍ | 145/171 [01:52<00:18,  1.41it/s]
 85%|████████▌ | 146/171 [01:52<00:17,  1.41it/s]
                                                 
{'loss': 0.5519, 'grad_norm': 0.5954261553487014, 'learning_rate': 3.1515151515151514e-07, 'epoch': 2.56}

 85%|████████▌ | 146/171 [01:52<00:17,  1.41it/s]
 86%|████████▌ | 147/171 [01:53<00:17,  1.40it/s]
                                                 
{'loss': 0.5141, 'grad_norm': 0.5851123202384193, 'learning_rate': 3.0303030303030305e-07, 'epoch': 2.58}

 86%|████████▌ | 147/171 [01:53<00:17,  1.40it/s]
 87%|████████▋ | 148/171 [01:54<00:16,  1.40it/s]
                                                 
{'loss': 0.5919, 'grad_norm': 0.6334410259426208, 'learning_rate': 2.909090909090909e-07, 'epoch': 2.6}

 87%|████████▋ | 148/171 [01:54<00:16,  1.40it/s]
 87%|████████▋ | 149/171 [01:54<00:15,  1.40it/s]
                                                 
{'loss': 0.522, 'grad_norm': 0.5844348539302753, 'learning_rate': 2.787878787878788e-07, 'epoch': 2.61}

 87%|████████▋ | 149/171 [01:54<00:15,  1.40it/s]
 88%|████████▊ | 150/171 [01:55<00:15,  1.39it/s]
                                                 
{'loss': 0.5397, 'grad_norm': 0.5614620437945189, 'learning_rate': 2.6666666666666667e-07, 'epoch': 2.63}

 88%|████████▊ | 150/171 [01:55<00:15,  1.39it/s]
 88%|████████▊ | 151/171 [01:56<00:14,  1.40it/s]
                                                 
{'loss': 0.5057, 'grad_norm': 0.6130761114956544, 'learning_rate': 2.5454545454545453e-07, 'epoch': 2.65}

 88%|████████▊ | 151/171 [01:56<00:14,  1.40it/s]
 89%|████████▉ | 152/171 [01:57<00:13,  1.41it/s]
                                                 
{'loss': 0.5626, 'grad_norm': 0.5831004922830085, 'learning_rate': 2.4242424242424244e-07, 'epoch': 2.67}

 89%|████████▉ | 152/171 [01:57<00:13,  1.41it/s]
 89%|████████▉ | 153/171 [01:57<00:12,  1.41it/s]
                                                 
{'loss': 0.5533, 'grad_norm': 0.5558127237045174, 'learning_rate': 2.303030303030303e-07, 'epoch': 2.68}

 89%|████████▉ | 153/171 [01:57<00:12,  1.41it/s]
 90%|█████████ | 154/171 [01:58<00:12,  1.41it/s]
                                                 
{'loss': 0.5447, 'grad_norm': 0.5826536385183144, 'learning_rate': 2.1818181818181815e-07, 'epoch': 2.7}

 90%|█████████ | 154/171 [01:58<00:12,  1.41it/s]
 91%|█████████ | 155/171 [01:59<00:11,  1.41it/s]
                                                 
{'loss': 0.5263, 'grad_norm': 0.5560151011632487, 'learning_rate': 2.0606060606060604e-07, 'epoch': 2.72}

 91%|█████████ | 155/171 [01:59<00:11,  1.41it/s]
 91%|█████████ | 156/171 [01:59<00:10,  1.41it/s]
                                                 
{'loss': 0.5498, 'grad_norm': 0.6303468146636081, 'learning_rate': 1.9393939393939392e-07, 'epoch': 2.74}

 91%|█████████ | 156/171 [01:59<00:10,  1.41it/s]
 92%|█████████▏| 157/171 [02:00<00:09,  1.41it/s]
                                                 
{'loss': 0.5243, 'grad_norm': 0.5139062885393009, 'learning_rate': 1.818181818181818e-07, 'epoch': 2.75}

 92%|█████████▏| 157/171 [02:00<00:09,  1.41it/s]
 92%|█████████▏| 158/171 [02:01<00:09,  1.41it/s]
                                                 
{'loss': 0.5011, 'grad_norm': 0.4552745815177265, 'learning_rate': 1.696969696969697e-07, 'epoch': 2.77}

 92%|█████████▏| 158/171 [02:01<00:09,  1.41it/s]
 93%|█████████▎| 159/171 [02:02<00:08,  1.40it/s]
                                                 
{'loss': 0.5188, 'grad_norm': 0.5183537758294251, 'learning_rate': 1.5757575757575757e-07, 'epoch': 2.79}

 93%|█████████▎| 159/171 [02:02<00:08,  1.40it/s]
 94%|█████████▎| 160/171 [02:02<00:07,  1.41it/s]
                                                 
{'loss': 0.5661, 'grad_norm': 0.671344905108892, 'learning_rate': 1.4545454545454545e-07, 'epoch': 2.81}

 94%|█████████▎| 160/171 [02:02<00:07,  1.41it/s]
 94%|█████████▍| 161/171 [02:03<00:07,  1.41it/s]
                                                 
{'loss': 0.5053, 'grad_norm': 0.5818047141182895, 'learning_rate': 1.3333333333333334e-07, 'epoch': 2.82}

 94%|█████████▍| 161/171 [02:03<00:07,  1.41it/s]
 95%|█████████▍| 162/171 [02:04<00:06,  1.41it/s]
                                                 
{'loss': 0.5254, 'grad_norm': 0.6156735402344845, 'learning_rate': 1.2121212121212122e-07, 'epoch': 2.84}

 95%|█████████▍| 162/171 [02:04<00:06,  1.41it/s]
 95%|█████████▌| 163/171 [02:04<00:05,  1.41it/s]
                                                 
{'loss': 0.5193, 'grad_norm': 0.5362830521957989, 'learning_rate': 1.0909090909090908e-07, 'epoch': 2.86}

 95%|█████████▌| 163/171 [02:04<00:05,  1.41it/s]
 96%|█████████▌| 164/171 [02:05<00:04,  1.41it/s]
                                                 
{'loss': 0.5278, 'grad_norm': 0.5140808568333954, 'learning_rate': 9.696969696969696e-08, 'epoch': 2.88}

 96%|█████████▌| 164/171 [02:05<00:04,  1.41it/s]
 96%|█████████▋| 165/171 [02:06<00:04,  1.42it/s]
                                                 
{'loss': 0.5368, 'grad_norm': 0.5859163492870942, 'learning_rate': 8.484848484848486e-08, 'epoch': 2.89}

 96%|█████████▋| 165/171 [02:06<00:04,  1.42it/s]
 97%|█████████▋| 166/171 [02:06<00:03,  1.42it/s]
                                                 
{'loss': 0.5926, 'grad_norm': 0.6465274574984011, 'learning_rate': 7.272727272727273e-08, 'epoch': 2.91}

 97%|█████████▋| 166/171 [02:06<00:03,  1.42it/s]
 98%|█████████▊| 167/171 [02:07<00:02,  1.41it/s]
                                                 
{'loss': 0.5272, 'grad_norm': 0.6200118410175566, 'learning_rate': 6.060606060606061e-08, 'epoch': 2.93}

 98%|█████████▊| 167/171 [02:07<00:02,  1.41it/s]
 98%|█████████▊| 168/171 [02:08<00:02,  1.41it/s]
                                                 
{'loss': 0.5201, 'grad_norm': 0.5616954492302496, 'learning_rate': 4.848484848484848e-08, 'epoch': 2.95}

 98%|█████████▊| 168/171 [02:08<00:02,  1.41it/s]
 99%|█████████▉| 169/171 [02:09<00:01,  1.41it/s]
                                                 
{'loss': 0.5216, 'grad_norm': 0.5653983996412684, 'learning_rate': 3.636363636363636e-08, 'epoch': 2.96}

 99%|█████████▉| 169/171 [02:09<00:01,  1.41it/s]
 99%|█████████▉| 170/171 [02:09<00:00,  1.41it/s]
                                                 
{'loss': 0.5492, 'grad_norm': 0.5915303648013616, 'learning_rate': 2.424242424242424e-08, 'epoch': 2.98}

 99%|█████████▉| 170/171 [02:09<00:00,  1.41it/s]
100%|██████████| 171/171 [02:10<00:00,  1.41it/s]
                                                 
{'loss': 0.5068, 'grad_norm': 0.5248570371859531, 'learning_rate': 1.212121212121212e-08, 'epoch': 3.0}

100%|██████████| 171/171 [02:10<00:00,  1.41it/s]/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(

                                                 
{'train_runtime': 139.424, 'train_samples_per_second': 97.58, 'train_steps_per_second': 1.226, 'train_loss': 0.5597259764782867, 'epoch': 3.0}

100%|██████████| 171/171 [02:19<00:00,  1.41it/s]
100%|██████████| 171/171 [02:19<00:00,  1.23it/s]
[2024-08-13 05:53:27,133] [INFO] [launch.py:351:main] Process 3611351 exits successfully.
[2024-08-13 05:53:27,134] [INFO] [launch.py:351:main] Process 3611349 exits successfully.
[2024-08-13 05:53:28,134] [INFO] [launch.py:351:main] Process 3611355 exits successfully.
[2024-08-13 05:53:28,135] [INFO] [launch.py:351:main] Process 3611353 exits successfully.
[2024-08-13 05:53:28,135] [INFO] [launch.py:351:main] Process 3611350 exits successfully.
[2024-08-13 05:53:28,135] [INFO] [launch.py:351:main] Process 3611352 exits successfully.
[2024-08-13 05:53:29,136] [INFO] [launch.py:351:main] Process 3611354 exits successfully.
[2024-08-13 05:53:29,136] [INFO] [launch.py:351:main] Process 3611348 exits successfully.
