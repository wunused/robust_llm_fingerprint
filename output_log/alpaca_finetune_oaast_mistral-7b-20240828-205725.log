Namespace(mode=['alpaca'], base_model='mistralai/Mistral-7B-v0.3', template_name='barebone', total_bsz=64, epoch=3, lr=2e-05, data_path='', task_name='oasst1', tuned_dir='./cache', use_peft=False, lora_r=16, lora_alpha=32)
num gpus:  8
Running 1/1: deepspeed --num_gpus=8 train.py --deepspeed ../deepspeed_config/zero3.json --bf16 --tf32=True
        --model_name_or_path mistralai/Mistral-7B-v0.3 --data_path ../data/stanford_alpaca/oasst1_data.json
        --output_dir /fsx-project/yunyun/models/mistralai_Mistral-7B-v0.3_oasst1_tuned
        --num_train_epochs 3
        --per_device_train_batch_size 10
        --per_device_eval_batch_size 4
        --gradient_accumulation_steps 1
        --gradient_checkpointing=True
        --evaluation_strategy=no
        --save_strategy=steps
        --save_steps 500
        --save_total_limit 1
        --learning_rate 2e-6
        --weight_decay 0.
        --report_to tensorboard
        --warmup_ratio 0.03
        --lr_scheduler_type=cosine
        --logging_steps 1
        --use_peft False 
        --lora_r 16 --lora_alpha 32
['deepspeed', '--num_gpus=8', 'train.py', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'mistralai/Mistral-7B-v0.3', '--data_path', '../data/stanford_alpaca/oasst1_data.json', '--output_dir', '/fsx-project/yunyun/models/mistralai_Mistral-7B-v0.3_oasst1_tuned', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1', '--use_peft', 'False', '--lora_r', '16', '--lora_alpha', '32']
[2024-08-28 20:57:39,045] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2024-08-28 20:57:46,773] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2024-08-28 20:57:46,773] [INFO] [runner.py:568:main] cmd = /data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNSwgNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None train.py --deepspeed ../deepspeed_config/zero3.json --bf16 --tf32=True --model_name_or_path mistralai/Mistral-7B-v0.3 --data_path ../data/stanford_alpaca/oasst1_data.json --output_dir /fsx-project/yunyun/models/mistralai_Mistral-7B-v0.3_oasst1_tuned --num_train_epochs 3 --per_device_train_batch_size 10 --per_device_eval_batch_size 4 --gradient_accumulation_steps 1 --gradient_checkpointing=True --evaluation_strategy=no --save_strategy=steps --save_steps 500 --save_total_limit 1 --learning_rate 2e-6 --weight_decay 0. --report_to tensorboard --warmup_ratio 0.03 --lr_scheduler_type=cosine --logging_steps 1 --use_peft False --lora_r 16 --lora_alpha 32
[2024-08-28 20:57:49,368] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2024-08-28 20:57:52,793] [INFO] [launch.py:139:main] 0 NCCL_ASYNC_ERROR_HANDLING=1
[2024-08-28 20:57:52,794] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]}
[2024-08-28 20:57:52,794] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=8, node_rank=0
[2024-08-28 20:57:52,794] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]})
[2024-08-28 20:57:52,794] [INFO] [launch.py:164:main] dist_world_size=8
[2024-08-28 20:57:52,794] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
[2024-08-28 20:57:52,794] [INFO] [launch.py:256:main] process 3757077 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=0', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'mistralai/Mistral-7B-v0.3', '--data_path', '../data/stanford_alpaca/oasst1_data.json', '--output_dir', '/fsx-project/yunyun/models/mistralai_Mistral-7B-v0.3_oasst1_tuned', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1', '--use_peft', 'False', '--lora_r', '16', '--lora_alpha', '32']
[2024-08-28 20:57:52,795] [INFO] [launch.py:256:main] process 3757078 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=1', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'mistralai/Mistral-7B-v0.3', '--data_path', '../data/stanford_alpaca/oasst1_data.json', '--output_dir', '/fsx-project/yunyun/models/mistralai_Mistral-7B-v0.3_oasst1_tuned', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1', '--use_peft', 'False', '--lora_r', '16', '--lora_alpha', '32']
[2024-08-28 20:57:52,796] [INFO] [launch.py:256:main] process 3757079 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=2', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'mistralai/Mistral-7B-v0.3', '--data_path', '../data/stanford_alpaca/oasst1_data.json', '--output_dir', '/fsx-project/yunyun/models/mistralai_Mistral-7B-v0.3_oasst1_tuned', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1', '--use_peft', 'False', '--lora_r', '16', '--lora_alpha', '32']
[2024-08-28 20:57:52,796] [INFO] [launch.py:256:main] process 3757080 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=3', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'mistralai/Mistral-7B-v0.3', '--data_path', '../data/stanford_alpaca/oasst1_data.json', '--output_dir', '/fsx-project/yunyun/models/mistralai_Mistral-7B-v0.3_oasst1_tuned', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1', '--use_peft', 'False', '--lora_r', '16', '--lora_alpha', '32']
[2024-08-28 20:57:52,797] [INFO] [launch.py:256:main] process 3757081 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=4', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'mistralai/Mistral-7B-v0.3', '--data_path', '../data/stanford_alpaca/oasst1_data.json', '--output_dir', '/fsx-project/yunyun/models/mistralai_Mistral-7B-v0.3_oasst1_tuned', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1', '--use_peft', 'False', '--lora_r', '16', '--lora_alpha', '32']
[2024-08-28 20:57:52,797] [INFO] [launch.py:256:main] process 3757082 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=5', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'mistralai/Mistral-7B-v0.3', '--data_path', '../data/stanford_alpaca/oasst1_data.json', '--output_dir', '/fsx-project/yunyun/models/mistralai_Mistral-7B-v0.3_oasst1_tuned', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1', '--use_peft', 'False', '--lora_r', '16', '--lora_alpha', '32']
[2024-08-28 20:57:52,798] [INFO] [launch.py:256:main] process 3757083 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=6', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'mistralai/Mistral-7B-v0.3', '--data_path', '../data/stanford_alpaca/oasst1_data.json', '--output_dir', '/fsx-project/yunyun/models/mistralai_Mistral-7B-v0.3_oasst1_tuned', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1', '--use_peft', 'False', '--lora_r', '16', '--lora_alpha', '32']
[2024-08-28 20:57:52,798] [INFO] [launch.py:256:main] process 3757084 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=7', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'mistralai/Mistral-7B-v0.3', '--data_path', '../data/stanford_alpaca/oasst1_data.json', '--output_dir', '/fsx-project/yunyun/models/mistralai_Mistral-7B-v0.3_oasst1_tuned', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1', '--use_peft', 'False', '--lora_r', '16', '--lora_alpha', '32']
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
2024-08-28 20:58:05.260132: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-08-28 20:58:05.260139: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-08-28 20:58:05.260137: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-08-28 20:58:05.260135: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-08-28 20:58:05.260156: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-08-28 20:58:05.260145: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-08-28 20:58:05.260150: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-08-28 20:58:05.260141: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-08-28 20:58:05.469516: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-08-28 20:58:05.469520: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-08-28 20:58:05.469518: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-08-28 20:58:05.469528: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-08-28 20:58:05.469526: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-08-28 20:58:05.469535: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-08-28 20:58:05.469539: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-08-28 20:58:05.469539: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-08-28 20:58:05.529690: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-08-28 20:58:05.529700: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-08-28 20:58:05.529694: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-08-28 20:58:05.529698: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-08-28 20:58:05.529700: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-08-28 20:58:05.529703: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-08-28 20:58:05.529705: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-08-28 20:58:05.529704: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-08-28 20:58:05.979440: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-08-28 20:58:05.979434: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-08-28 20:58:05.979435: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-08-28 20:58:05.979439: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-08-28 20:58:05.979434: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-08-28 20:58:05.979451: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-08-28 20:58:05.979454: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-08-28 20:58:05.979464: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-08-28 20:58:11.180132: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-08-28 20:58:11.180173: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-08-28 20:58:11.180181: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-08-28 20:58:11.180250: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-08-28 20:58:11.180263: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-08-28 20:58:11.180287: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-08-28 20:58:11.180291: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-08-28 20:58:11.180331: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
[2024-08-28 20:58:28,646] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-28 20:58:28,656] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible

[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2024-08-28 20:58:28,909] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-28 20:58:28,950] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-28 20:58:28,993] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-28 20:58:29,014] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-28 20:58:29,015] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-28 20:58:29,018] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2024-08-28 20:58:29,400] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-08-28 20:58:29,407] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-08-28 20:58:29,644] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-08-28 20:58:29,674] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-08-28 20:58:29,726] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-08-28 20:58:29,740] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-08-28 20:58:29,744] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-08-28 20:58:29,753] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-08-28 20:58:29,753] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 318.16it/s]
Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 1436.41it/s]
Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 1400.59it/s]
Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 1427.44it/s]
Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 1366.82it/s]
Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 1502.44it/s]
Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 1504.59it/s]
Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 1461.09it/s]
[2024-08-28 20:58:41,192] [INFO] [partition_parameters.py:345:__exit__] finished initializing model - num_params = 291, num_elems = 7.25B
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–      | 1/3 [00:00<00:01,  1.89it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–      | 1/3 [00:00<00:01,  1.84it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–      | 1/3 [00:00<00:01,  1.78it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–      | 1/3 [00:00<00:01,  1.76it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–      | 1/3 [00:00<00:01,  1.75it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–      | 1/3 [00:00<00:01,  1.74it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–      | 1/3 [00:00<00:01,  1.72it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–      | 1/3 [00:09<00:19,  9.83s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:10<00:06,  6.07s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:10<00:06,  6.07s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:10<00:06,  6.06s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:10<00:06,  6.08s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:10<00:06,  6.09s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:10<00:06,  6.09s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:10<00:06,  6.10s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:18<00:00,  6.98s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:18<00:00,  6.98s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:18<00:00,  6.99s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:18<00:00,  6.98s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:18<00:00,  6.19s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:18<00:00,  6.19s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:18<00:00,  6.19s/it]

Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:18<00:00,  6.19s/it]

Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:18<00:00,  6.99s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:18<00:00,  6.20s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:18<00:00,  7.02s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:18<00:00,  6.21s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:18<00:00,  7.01s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:18<00:00,  6.21s/it]
Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 1547.14it/s]
Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 1356.36it/s]
Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 1648.92it/s]
Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 1622.97it/s]
Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 1592.98it/s]
Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 1595.80it/s]
Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 1450.81it/s]
Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:19<00:09,  9.84s/it]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:28<00:00,  9.36s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:28<00:00,  9.49s/it]
Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 1519.86it/s]
[2024-08-28 20:59:09,947] [INFO] [partition_parameters.py:345:__exit__] finished initializing model - num_params = 582, num_elems = 14.50B
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–      | 1/3 [00:02<00:05,  2.54s/it]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–      | 1/3 [00:02<00:05,  2.54s/it]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–      | 1/3 [00:02<00:05,  2.54s/it]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–      | 1/3 [00:02<00:05,  2.55s/it]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–      | 1/3 [00:02<00:05,  2.55s/it]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–      | 1/3 [00:02<00:05,  2.57s/it]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–      | 1/3 [00:02<00:05,  2.58s/it]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–      | 1/3 [00:02<00:05,  2.99s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:05<00:02,  2.83s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:05<00:02,  2.82s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:05<00:02,  2.82s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:05<00:02,  2.83s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:05<00:02,  2.82s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:05<00:02,  2.84s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:05<00:02,  2.84s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:07<00:00,  2.49s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:07<00:00,  2.55s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:07<00:00,  2.48s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:07<00:00,  2.55s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:07<00:00,  2.48s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:07<00:00,  2.55s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:07<00:00,  2.48s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:07<00:00,  2.48s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:07<00:00,  2.55s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:07<00:00,  2.55s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:07<00:00,  2.49s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:07<00:00,  2.55s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:07<00:00,  2.49s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:07<00:00,  2.55s/it]
Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:06<00:03,  3.07s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:08<00:00,  2.96s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:08<00:00,  2.98s/it]
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
[rank3]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[rank6]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[rank1]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[rank5]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[rank4]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[rank0]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[rank2]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[rank7]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...

Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...


Detected CUDA files, patching ldflags
Emitting ninja build file /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121/fused_adam/build.ninja...
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/cpp_extension.py:1967: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Loading extension module fused_adam...Loading extension module fused_adam...Loading extension module fused_adam...Loading extension module fused_adam...Loading extension module fused_adam...




Loading extension module fused_adam...Loading extension module fused_adam...

Time to load fused_adam op: 2.9884321689605713 secondsTime to load fused_adam op: 2.988473653793335 secondsTime to load fused_adam op: 2.988548755645752 seconds
Time to load fused_adam op: 2.988464593887329 secondsTime to load fused_adam op: 2.988485813140869 secondsTime to load fused_adam op: 2.988482713699341 seconds
Time to load fused_adam op: 2.9884979724884033 secondsTime to load fused_adam op: 2.988370656967163 seconds





Parameter Offload: Total persistent parameters: 268701696 in 129 params
  0%|          | 0/189 [00:00<?, ?it/s]/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
  1%|          | 1/189 [00:08<25:12,  8.05s/it]                                               {'loss': 1.1112, 'grad_norm': 14.503322918002025, 'learning_rate': 0.0, 'epoch': 0.02}
  1%|          | 1/189 [00:08<25:12,  8.05s/it]  1%|          | 2/189 [00:09<12:23,  3.98s/it]                                               {'loss': 1.0079, 'grad_norm': 13.448711564495953, 'learning_rate': 7.73705614469083e-07, 'epoch': 0.03}
  1%|          | 2/189 [00:09<12:23,  3.98s/it]  2%|â–         | 3/189 [00:10<08:00,  2.58s/it]                                               {'loss': 1.0123, 'grad_norm': 13.58901401853742, 'learning_rate': 1.2262943855309167e-06, 'epoch': 0.05}
  2%|â–         | 3/189 [00:10<08:00,  2.58s/it]  2%|â–         | 4/189 [00:11<05:55,  1.92s/it]                                               {'loss': 0.9492, 'grad_norm': 8.342687068123785, 'learning_rate': 1.547411228938166e-06, 'epoch': 0.06}
  2%|â–         | 4/189 [00:11<05:55,  1.92s/it]  3%|â–         | 5/189 [00:11<04:45,  1.55s/it]                                               {'loss': 0.9468, 'grad_norm': 9.28499470460195, 'learning_rate': 1.796488803407854e-06, 'epoch': 0.08}
  3%|â–         | 5/189 [00:11<04:45,  1.55s/it]  3%|â–         | 6/189 [00:12<04:03,  1.33s/it]                                               {'loss': 1.0074, 'grad_norm': 8.835926999331555, 'learning_rate': 1.9999999999999995e-06, 'epoch': 0.1}
  3%|â–         | 6/189 [00:12<04:03,  1.33s/it]  4%|â–         | 7/189 [00:13<03:37,  1.19s/it]                                               {'loss': 0.9238, 'grad_norm': 9.035255661905273, 'learning_rate': 2e-06, 'epoch': 0.11}
  4%|â–         | 7/189 [00:13<03:37,  1.19s/it]  4%|â–         | 8/189 [00:14<03:18,  1.10s/it]                                               {'loss': 0.9611, 'grad_norm': 8.427659376264513, 'learning_rate': 1.989071038251366e-06, 'epoch': 0.13}
  4%|â–         | 8/189 [00:14<03:18,  1.10s/it]  5%|â–         | 9/189 [00:15<03:05,  1.03s/it]                                               {'loss': 0.9491, 'grad_norm': 9.139623798056261, 'learning_rate': 1.978142076502732e-06, 'epoch': 0.14}
  5%|â–         | 9/189 [00:15<03:05,  1.03s/it]  5%|â–Œ         | 10/189 [00:16<02:58,  1.01it/s]                                                {'loss': 0.9183, 'grad_norm': 8.481741306361352, 'learning_rate': 1.967213114754098e-06, 'epoch': 0.16}
  5%|â–Œ         | 10/189 [00:16<02:58,  1.01it/s]  6%|â–Œ         | 11/189 [00:17<02:51,  1.04it/s]                                                {'loss': 0.9256, 'grad_norm': 8.376178797974339, 'learning_rate': 1.9562841530054644e-06, 'epoch': 0.17}
  6%|â–Œ         | 11/189 [00:17<02:51,  1.04it/s]  6%|â–‹         | 12/189 [00:18<02:46,  1.06it/s]                                                {'loss': 0.9752, 'grad_norm': 7.659199853751065, 'learning_rate': 1.9453551912568304e-06, 'epoch': 0.19}
  6%|â–‹         | 12/189 [00:18<02:46,  1.06it/s]  7%|â–‹         | 13/189 [00:19<02:43,  1.08it/s]                                                {'loss': 0.9311, 'grad_norm': 8.354763332362674, 'learning_rate': 1.9344262295081967e-06, 'epoch': 0.21}
  7%|â–‹         | 13/189 [00:19<02:43,  1.08it/s]  7%|â–‹         | 14/189 [00:19<02:40,  1.09it/s]                                                {'loss': 0.8809, 'grad_norm': 8.069974158159216, 'learning_rate': 1.9234972677595626e-06, 'epoch': 0.22}
  7%|â–‹         | 14/189 [00:19<02:40,  1.09it/s]  8%|â–Š         | 15/189 [00:20<02:37,  1.10it/s]                                                {'loss': 0.9108, 'grad_norm': 7.674668413510214, 'learning_rate': 1.912568306010929e-06, 'epoch': 0.24}
  8%|â–Š         | 15/189 [00:20<02:37,  1.10it/s]  8%|â–Š         | 16/189 [00:21<02:37,  1.10it/s]                                                {'loss': 0.8932, 'grad_norm': 8.6951529150548, 'learning_rate': 1.901639344262295e-06, 'epoch': 0.25}
  8%|â–Š         | 16/189 [00:21<02:37,  1.10it/s]  9%|â–‰         | 17/189 [00:22<02:35,  1.10it/s]                                                {'loss': 0.943, 'grad_norm': 7.458743575251867, 'learning_rate': 1.8907103825136612e-06, 'epoch': 0.27}
  9%|â–‰         | 17/189 [00:22<02:35,  1.10it/s] 10%|â–‰         | 18/189 [00:23<02:34,  1.10it/s]                                                {'loss': 1.0148, 'grad_norm': 7.801455840726242, 'learning_rate': 1.8797814207650274e-06, 'epoch': 0.29}
 10%|â–‰         | 18/189 [00:23<02:34,  1.10it/s] 10%|â–ˆ         | 19/189 [00:24<02:33,  1.11it/s]                                                {'loss': 0.9441, 'grad_norm': 8.150053860505786, 'learning_rate': 1.8688524590163935e-06, 'epoch': 0.3}
 10%|â–ˆ         | 19/189 [00:24<02:33,  1.11it/s] 11%|â–ˆ         | 20/189 [00:25<02:31,  1.11it/s]                                                {'loss': 0.9193, 'grad_norm': 9.681930906786329, 'learning_rate': 1.8579234972677596e-06, 'epoch': 0.32}
 11%|â–ˆ         | 20/189 [00:25<02:31,  1.11it/s] 11%|â–ˆ         | 21/189 [00:26<02:31,  1.11it/s]                                                {'loss': 0.8062, 'grad_norm': 7.121193676344121, 'learning_rate': 1.8469945355191256e-06, 'epoch': 0.33}
 11%|â–ˆ         | 21/189 [00:26<02:31,  1.11it/s] 12%|â–ˆâ–        | 22/189 [00:27<02:29,  1.11it/s]                                                {'loss': 0.9613, 'grad_norm': 9.229833960401626, 'learning_rate': 1.8360655737704917e-06, 'epoch': 0.35}
 12%|â–ˆâ–        | 22/189 [00:27<02:29,  1.11it/s] 12%|â–ˆâ–        | 23/189 [00:28<02:28,  1.12it/s]                                                {'loss': 0.814, 'grad_norm': 9.069715029960621, 'learning_rate': 1.8251366120218578e-06, 'epoch': 0.37}
 12%|â–ˆâ–        | 23/189 [00:28<02:28,  1.12it/s] 13%|â–ˆâ–        | 24/189 [00:28<02:28,  1.11it/s]                                                {'loss': 0.7909, 'grad_norm': 8.255305653760612, 'learning_rate': 1.814207650273224e-06, 'epoch': 0.38}
 13%|â–ˆâ–        | 24/189 [00:28<02:28,  1.11it/s] 13%|â–ˆâ–        | 25/189 [00:29<02:26,  1.12it/s]                                                {'loss': 0.8929, 'grad_norm': 7.494629596193503, 'learning_rate': 1.80327868852459e-06, 'epoch': 0.4}
 13%|â–ˆâ–        | 25/189 [00:29<02:26,  1.12it/s] 14%|â–ˆâ–        | 26/189 [00:30<02:26,  1.12it/s]                                                {'loss': 0.8562, 'grad_norm': 8.809244868783662, 'learning_rate': 1.7923497267759562e-06, 'epoch': 0.41}
 14%|â–ˆâ–        | 26/189 [00:30<02:26,  1.12it/s] 14%|â–ˆâ–        | 27/189 [00:31<02:25,  1.11it/s]                                                {'loss': 0.7561, 'grad_norm': 7.613224932601147, 'learning_rate': 1.7814207650273224e-06, 'epoch': 0.43}
 14%|â–ˆâ–        | 27/189 [00:31<02:25,  1.11it/s] 15%|â–ˆâ–        | 28/189 [00:32<02:24,  1.11it/s]                                                {'loss': 0.8018, 'grad_norm': 7.062021410245399, 'learning_rate': 1.7704918032786885e-06, 'epoch': 0.44}
 15%|â–ˆâ–        | 28/189 [00:32<02:24,  1.11it/s] 15%|â–ˆâ–Œ        | 29/189 [00:33<02:23,  1.11it/s]                                                {'loss': 0.714, 'grad_norm': 8.056519043009445, 'learning_rate': 1.7595628415300544e-06, 'epoch': 0.46}
 15%|â–ˆâ–Œ        | 29/189 [00:33<02:23,  1.11it/s] 16%|â–ˆâ–Œ        | 30/189 [00:34<02:23,  1.11it/s]                                                {'loss': 0.9478, 'grad_norm': 9.129788090382897, 'learning_rate': 1.7486338797814206e-06, 'epoch': 0.48}
 16%|â–ˆâ–Œ        | 30/189 [00:34<02:23,  1.11it/s] 16%|â–ˆâ–‹        | 31/189 [00:35<02:21,  1.11it/s]                                                {'loss': 0.8033, 'grad_norm': 9.472766423633766, 'learning_rate': 1.7377049180327867e-06, 'epoch': 0.49}
 16%|â–ˆâ–‹        | 31/189 [00:35<02:21,  1.11it/s] 17%|â–ˆâ–‹        | 32/189 [00:36<02:20,  1.11it/s]                                                {'loss': 0.6776, 'grad_norm': 7.159272828916514, 'learning_rate': 1.7267759562841528e-06, 'epoch': 0.51}
 17%|â–ˆâ–‹        | 32/189 [00:36<02:20,  1.11it/s] 17%|â–ˆâ–‹        | 33/189 [00:37<02:20,  1.11it/s]                                                {'loss': 0.7499, 'grad_norm': 8.335196900196438, 'learning_rate': 1.715846994535519e-06, 'epoch': 0.52}
 17%|â–ˆâ–‹        | 33/189 [00:37<02:20,  1.11it/s] 18%|â–ˆâ–Š        | 34/189 [00:37<02:19,  1.11it/s]                                                {'loss': 0.6005, 'grad_norm': 6.7736263424223315, 'learning_rate': 1.704918032786885e-06, 'epoch': 0.54}
 18%|â–ˆâ–Š        | 34/189 [00:37<02:19,  1.11it/s] 19%|â–ˆâ–Š        | 35/189 [00:38<02:18,  1.11it/s]                                                {'loss': 0.7128, 'grad_norm': 8.306830306845818, 'learning_rate': 1.6939890710382514e-06, 'epoch': 0.56}
 19%|â–ˆâ–Š        | 35/189 [00:38<02:18,  1.11it/s] 19%|â–ˆâ–‰        | 36/189 [00:39<02:16,  1.12it/s]                                                {'loss': 0.6728, 'grad_norm': 7.618865327237999, 'learning_rate': 1.6830601092896176e-06, 'epoch': 0.57}
 19%|â–ˆâ–‰        | 36/189 [00:39<02:16,  1.12it/s] 20%|â–ˆâ–‰        | 37/189 [00:40<02:16,  1.11it/s]                                                {'loss': 0.767, 'grad_norm': 8.138117441275718, 'learning_rate': 1.6721311475409837e-06, 'epoch': 0.59}
 20%|â–ˆâ–‰        | 37/189 [00:40<02:16,  1.11it/s] 20%|â–ˆâ–ˆ        | 38/189 [00:41<02:16,  1.11it/s]                                                {'loss': 0.7628, 'grad_norm': 7.83292805078901, 'learning_rate': 1.6612021857923496e-06, 'epoch': 0.6}
 20%|â–ˆâ–ˆ        | 38/189 [00:41<02:16,  1.11it/s] 21%|â–ˆâ–ˆ        | 39/189 [00:42<02:14,  1.11it/s]                                                {'loss': 0.741, 'grad_norm': 8.246086230072274, 'learning_rate': 1.6502732240437158e-06, 'epoch': 0.62}
 21%|â–ˆâ–ˆ        | 39/189 [00:42<02:14,  1.11it/s] 21%|â–ˆâ–ˆ        | 40/189 [00:43<02:14,  1.11it/s]                                                {'loss': 0.7351, 'grad_norm': 7.86686175875662, 'learning_rate': 1.6393442622950819e-06, 'epoch': 0.63}
 21%|â–ˆâ–ˆ        | 40/189 [00:43<02:14,  1.11it/s] 22%|â–ˆâ–ˆâ–       | 41/189 [00:44<02:14,  1.10it/s]                                                {'loss': 0.7093, 'grad_norm': 8.31386803796645, 'learning_rate': 1.628415300546448e-06, 'epoch': 0.65}
 22%|â–ˆâ–ˆâ–       | 41/189 [00:44<02:14,  1.10it/s] 22%|â–ˆâ–ˆâ–       | 42/189 [00:45<02:13,  1.10it/s]                                                {'loss': 0.6236, 'grad_norm': 8.277863632553116, 'learning_rate': 1.6174863387978142e-06, 'epoch': 0.67}
 22%|â–ˆâ–ˆâ–       | 42/189 [00:45<02:13,  1.10it/s] 23%|â–ˆâ–ˆâ–       | 43/189 [00:46<02:11,  1.11it/s]                                                {'loss': 0.6493, 'grad_norm': 7.367526871153182, 'learning_rate': 1.6065573770491803e-06, 'epoch': 0.68}
 23%|â–ˆâ–ˆâ–       | 43/189 [00:46<02:11,  1.11it/s] 23%|â–ˆâ–ˆâ–       | 44/189 [00:46<02:11,  1.10it/s]                                                {'loss': 0.7497, 'grad_norm': 9.242273785251195, 'learning_rate': 1.5956284153005464e-06, 'epoch': 0.7}
 23%|â–ˆâ–ˆâ–       | 44/189 [00:46<02:11,  1.10it/s] 24%|â–ˆâ–ˆâ–       | 45/189 [00:47<02:10,  1.11it/s]                                                {'loss': 0.6987, 'grad_norm': 8.922795562091453, 'learning_rate': 1.5846994535519126e-06, 'epoch': 0.71}
 24%|â–ˆâ–ˆâ–       | 45/189 [00:47<02:10,  1.11it/s] 24%|â–ˆâ–ˆâ–       | 46/189 [00:48<02:09,  1.11it/s]                                                {'loss': 0.8528, 'grad_norm': 9.035439268608421, 'learning_rate': 1.5737704918032787e-06, 'epoch': 0.73}
 24%|â–ˆâ–ˆâ–       | 46/189 [00:48<02:09,  1.11it/s] 25%|â–ˆâ–ˆâ–       | 47/189 [00:49<02:08,  1.11it/s]                                                {'loss': 0.6511, 'grad_norm': 7.839463170367907, 'learning_rate': 1.5628415300546446e-06, 'epoch': 0.75}
 25%|â–ˆâ–ˆâ–       | 47/189 [00:49<02:08,  1.11it/s] 25%|â–ˆâ–ˆâ–Œ       | 48/189 [00:50<02:07,  1.11it/s]                                                {'loss': 0.6387, 'grad_norm': 8.124366004404711, 'learning_rate': 1.5519125683060107e-06, 'epoch': 0.76}
 25%|â–ˆâ–ˆâ–Œ       | 48/189 [00:50<02:07,  1.11it/s] 26%|â–ˆâ–ˆâ–Œ       | 49/189 [00:51<02:06,  1.10it/s]                                                {'loss': 0.7617, 'grad_norm': 7.885142049837552, 'learning_rate': 1.5409836065573769e-06, 'epoch': 0.78}
 26%|â–ˆâ–ˆâ–Œ       | 49/189 [00:51<02:06,  1.10it/s] 26%|â–ˆâ–ˆâ–‹       | 50/189 [00:52<02:05,  1.10it/s]                                                {'loss': 0.638, 'grad_norm': 7.789739969350564, 'learning_rate': 1.530054644808743e-06, 'epoch': 0.79}
 26%|â–ˆâ–ˆâ–‹       | 50/189 [00:52<02:05,  1.10it/s] 27%|â–ˆâ–ˆâ–‹       | 51/189 [00:53<02:04,  1.11it/s]                                                {'loss': 0.6746, 'grad_norm': 8.039116200545786, 'learning_rate': 1.5191256830601091e-06, 'epoch': 0.81}
 27%|â–ˆâ–ˆâ–‹       | 51/189 [00:53<02:04,  1.11it/s] 28%|â–ˆâ–ˆâ–Š       | 52/189 [00:54<02:03,  1.11it/s]                                                {'loss': 0.6858, 'grad_norm': 8.898890507987387, 'learning_rate': 1.5081967213114753e-06, 'epoch': 0.83}
 28%|â–ˆâ–ˆâ–Š       | 52/189 [00:54<02:03,  1.11it/s] 28%|â–ˆâ–ˆâ–Š       | 53/189 [00:55<02:02,  1.11it/s]                                                {'loss': 0.6741, 'grad_norm': 8.012389276565465, 'learning_rate': 1.4972677595628416e-06, 'epoch': 0.84}
 28%|â–ˆâ–ˆâ–Š       | 53/189 [00:55<02:02,  1.11it/s] 29%|â–ˆâ–ˆâ–Š       | 54/189 [00:56<02:02,  1.10it/s]                                                {'loss': 0.6797, 'grad_norm': 7.315805972340485, 'learning_rate': 1.4863387978142078e-06, 'epoch': 0.86}
 29%|â–ˆâ–ˆâ–Š       | 54/189 [00:56<02:02,  1.10it/s] 29%|â–ˆâ–ˆâ–‰       | 55/189 [00:56<02:01,  1.11it/s]                                                {'loss': 0.5914, 'grad_norm': 8.125340344968416, 'learning_rate': 1.4754098360655739e-06, 'epoch': 0.87}
 29%|â–ˆâ–ˆâ–‰       | 55/189 [00:56<02:01,  1.11it/s] 30%|â–ˆâ–ˆâ–‰       | 56/189 [00:57<02:00,  1.11it/s]                                                {'loss': 0.6105, 'grad_norm': 8.79873439115178, 'learning_rate': 1.4644808743169398e-06, 'epoch': 0.89}
 30%|â–ˆâ–ˆâ–‰       | 56/189 [00:57<02:00,  1.11it/s] 30%|â–ˆâ–ˆâ–ˆ       | 57/189 [00:58<01:59,  1.11it/s]                                                {'loss': 0.58, 'grad_norm': 8.438406172361054, 'learning_rate': 1.453551912568306e-06, 'epoch': 0.9}
 30%|â–ˆâ–ˆâ–ˆ       | 57/189 [00:58<01:59,  1.11it/s] 31%|â–ˆâ–ˆâ–ˆ       | 58/189 [00:59<01:58,  1.11it/s]                                                {'loss': 0.619, 'grad_norm': 8.568357771345262, 'learning_rate': 1.442622950819672e-06, 'epoch': 0.92}
 31%|â–ˆâ–ˆâ–ˆ       | 58/189 [00:59<01:58,  1.11it/s] 31%|â–ˆâ–ˆâ–ˆ       | 59/189 [01:00<01:56,  1.11it/s]                                                {'loss': 0.7191, 'grad_norm': 8.701011768763344, 'learning_rate': 1.4316939890710382e-06, 'epoch': 0.94}
 31%|â–ˆâ–ˆâ–ˆ       | 59/189 [01:00<01:56,  1.11it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 60/189 [01:01<01:56,  1.11it/s]                                                {'loss': 0.5382, 'grad_norm': 8.27450363914527, 'learning_rate': 1.4207650273224043e-06, 'epoch': 0.95}
 32%|â–ˆâ–ˆâ–ˆâ–      | 60/189 [01:01<01:56,  1.11it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 61/189 [01:02<01:55,  1.11it/s]                                                {'loss': 0.6633, 'grad_norm': 8.56176080674231, 'learning_rate': 1.4098360655737705e-06, 'epoch': 0.97}
 32%|â–ˆâ–ˆâ–ˆâ–      | 61/189 [01:02<01:55,  1.11it/s] 33%|â–ˆâ–ˆâ–ˆâ–      | 62/189 [01:03<01:54,  1.11it/s]                                                {'loss': 0.5432, 'grad_norm': 7.7141270031875795, 'learning_rate': 1.3989071038251366e-06, 'epoch': 0.98}
 33%|â–ˆâ–ˆâ–ˆâ–      | 62/189 [01:03<01:54,  1.11it/s] 33%|â–ˆâ–ˆâ–ˆâ–      | 63/189 [01:04<01:53,  1.11it/s]                                                {'loss': 0.4773, 'grad_norm': 10.101736996440344, 'learning_rate': 1.3879781420765027e-06, 'epoch': 1.0}
 33%|â–ˆâ–ˆâ–ˆâ–      | 63/189 [01:04<01:53,  1.11it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 64/189 [01:05<01:52,  1.11it/s]                                                {'loss': 0.3337, 'grad_norm': 6.767292339077873, 'learning_rate': 1.3770491803278687e-06, 'epoch': 1.02}
 34%|â–ˆâ–ˆâ–ˆâ–      | 64/189 [01:05<01:52,  1.11it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 65/189 [01:05<01:51,  1.11it/s]                                                {'loss': 0.3309, 'grad_norm': 7.599564773530004, 'learning_rate': 1.3661202185792348e-06, 'epoch': 1.03}
 34%|â–ˆâ–ˆâ–ˆâ–      | 65/189 [01:05<01:51,  1.11it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 66/189 [01:06<01:50,  1.11it/s]                                                {'loss': 0.349, 'grad_norm': 7.851234863017914, 'learning_rate': 1.355191256830601e-06, 'epoch': 1.05}
 35%|â–ˆâ–ˆâ–ˆâ–      | 66/189 [01:06<01:50,  1.11it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 67/189 [01:07<01:49,  1.11it/s]                                                {'loss': 0.3042, 'grad_norm': 8.407973847681243, 'learning_rate': 1.344262295081967e-06, 'epoch': 1.06}
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 67/189 [01:07<01:49,  1.11it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 68/189 [01:08<01:48,  1.11it/s]                                                {'loss': 0.376, 'grad_norm': 8.580051779037182, 'learning_rate': 1.3333333333333332e-06, 'epoch': 1.08}
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 68/189 [01:08<01:48,  1.11it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 69/189 [01:09<01:48,  1.11it/s]                                                {'loss': 0.356, 'grad_norm': 7.927890889804619, 'learning_rate': 1.3224043715846993e-06, 'epoch': 1.1}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 69/189 [01:09<01:48,  1.11it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 70/189 [01:10<01:47,  1.11it/s]                                                {'loss': 0.3589, 'grad_norm': 8.553230609220666, 'learning_rate': 1.3114754098360655e-06, 'epoch': 1.11}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 70/189 [01:10<01:47,  1.11it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 71/189 [01:11<01:46,  1.11it/s]                                                {'loss': 0.3215, 'grad_norm': 8.401485575991964, 'learning_rate': 1.3005464480874316e-06, 'epoch': 1.13}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 71/189 [01:11<01:46,  1.11it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 72/189 [01:12<01:45,  1.11it/s]                                                {'loss': 0.4391, 'grad_norm': 7.855587374772117, 'learning_rate': 1.289617486338798e-06, 'epoch': 1.14}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 72/189 [01:12<01:45,  1.11it/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 73/189 [01:13<01:44,  1.11it/s]                                                {'loss': 0.3332, 'grad_norm': 8.10120130227589, 'learning_rate': 1.2786885245901639e-06, 'epoch': 1.16}
 39%|â–ˆâ–ˆâ–ˆâ–Š      | 73/189 [01:13<01:44,  1.11it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 74/189 [01:14<01:43,  1.11it/s]                                                {'loss': 0.3391, 'grad_norm': 7.474540247371683, 'learning_rate': 1.26775956284153e-06, 'epoch': 1.17}
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 74/189 [01:14<01:43,  1.11it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 75/189 [01:14<01:42,  1.11it/s]                                                {'loss': 0.3736, 'grad_norm': 10.598805419946219, 'learning_rate': 1.2568306010928961e-06, 'epoch': 1.19}
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 75/189 [01:14<01:42,  1.11it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 76/189 [01:15<01:42,  1.10it/s]                                                {'loss': 0.2902, 'grad_norm': 6.977677104759973, 'learning_rate': 1.2459016393442623e-06, 'epoch': 1.21}
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 76/189 [01:15<01:42,  1.10it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 77/189 [01:16<01:41,  1.11it/s]                                                {'loss': 0.2755, 'grad_norm': 7.816506956903416, 'learning_rate': 1.2349726775956284e-06, 'epoch': 1.22}
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 77/189 [01:16<01:41,  1.11it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 78/189 [01:17<01:40,  1.11it/s]                                                {'loss': 0.2696, 'grad_norm': 7.147718529237682, 'learning_rate': 1.2240437158469945e-06, 'epoch': 1.24}
 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 78/189 [01:17<01:40,  1.11it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 79/189 [01:18<01:39,  1.11it/s]                                                {'loss': 0.3249, 'grad_norm': 7.769584989024116, 'learning_rate': 1.2131147540983607e-06, 'epoch': 1.25}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 79/189 [01:18<01:39,  1.11it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 80/189 [01:19<01:38,  1.11it/s]                                                {'loss': 0.304, 'grad_norm': 8.865868639818325, 'learning_rate': 1.2021857923497268e-06, 'epoch': 1.27}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 80/189 [01:19<01:38,  1.11it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 81/189 [01:20<01:37,  1.11it/s]                                                {'loss': 0.3261, 'grad_norm': 8.577150779842516, 'learning_rate': 1.191256830601093e-06, 'epoch': 1.29}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 81/189 [01:20<01:37,  1.11it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 82/189 [01:21<01:36,  1.11it/s]                                                {'loss': 0.335, 'grad_norm': 7.942839346450755, 'learning_rate': 1.1803278688524589e-06, 'epoch': 1.3}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 82/189 [01:21<01:36,  1.11it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 83/189 [01:22<01:35,  1.11it/s]                                                {'loss': 0.3151, 'grad_norm': 6.887536491175033, 'learning_rate': 1.169398907103825e-06, 'epoch': 1.32}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 83/189 [01:22<01:35,  1.11it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 84/189 [01:23<01:34,  1.11it/s]                                                {'loss': 0.2909, 'grad_norm': 9.20907974012712, 'learning_rate': 1.1584699453551911e-06, 'epoch': 1.33}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 84/189 [01:23<01:34,  1.11it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 85/189 [01:23<01:33,  1.12it/s]                                                {'loss': 0.3983, 'grad_norm': 7.600212732520048, 'learning_rate': 1.1475409836065573e-06, 'epoch': 1.35}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 85/189 [01:23<01:33,  1.12it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 86/189 [01:24<01:32,  1.11it/s]                                                {'loss': 0.243, 'grad_norm': 6.766964944842487, 'learning_rate': 1.1366120218579234e-06, 'epoch': 1.37}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 86/189 [01:24<01:32,  1.11it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 87/189 [01:25<01:31,  1.11it/s]                                                {'loss': 0.2938, 'grad_norm': 7.179862582749134, 'learning_rate': 1.1256830601092895e-06, 'epoch': 1.38}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 87/189 [01:25<01:31,  1.11it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 88/189 [01:26<01:30,  1.11it/s]                                                {'loss': 0.1949, 'grad_norm': 6.603720162148457, 'learning_rate': 1.1147540983606557e-06, 'epoch': 1.4}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 88/189 [01:26<01:30,  1.11it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 89/189 [01:27<01:29,  1.11it/s]                                                {'loss': 0.3611, 'grad_norm': 7.872819039940616, 'learning_rate': 1.1038251366120218e-06, 'epoch': 1.41}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 89/189 [01:27<01:29,  1.11it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 90/189 [01:28<01:28,  1.11it/s]                                                {'loss': 0.2877, 'grad_norm': 6.8503777878042476, 'learning_rate': 1.092896174863388e-06, 'epoch': 1.43}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 90/189 [01:28<01:28,  1.11it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 91/189 [01:29<01:28,  1.11it/s]                                                {'loss': 0.3049, 'grad_norm': 8.018713565252677, 'learning_rate': 1.081967213114754e-06, 'epoch': 1.44}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 91/189 [01:29<01:28,  1.11it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 92/189 [01:30<01:27,  1.11it/s]                                                {'loss': 0.2687, 'grad_norm': 7.787587158988273, 'learning_rate': 1.0710382513661202e-06, 'epoch': 1.46}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 92/189 [01:30<01:27,  1.11it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 93/189 [01:31<01:26,  1.11it/s]                                                {'loss': 0.4237, 'grad_norm': 9.30426083915677, 'learning_rate': 1.0601092896174863e-06, 'epoch': 1.48}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 93/189 [01:31<01:26,  1.11it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 94/189 [01:32<01:25,  1.12it/s]                                                {'loss': 0.2298, 'grad_norm': 7.716857806494982, 'learning_rate': 1.0491803278688525e-06, 'epoch': 1.49}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 94/189 [01:32<01:25,  1.12it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 95/189 [01:32<01:24,  1.12it/s]                                                {'loss': 0.1884, 'grad_norm': 6.94138876761867, 'learning_rate': 1.0382513661202186e-06, 'epoch': 1.51}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 95/189 [01:32<01:24,  1.12it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 96/189 [01:33<01:23,  1.12it/s]                                                {'loss': 0.3059, 'grad_norm': 7.228679580475646, 'learning_rate': 1.0273224043715847e-06, 'epoch': 1.52}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 96/189 [01:33<01:23,  1.12it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 97/189 [01:34<01:22,  1.12it/s]                                                {'loss': 0.2481, 'grad_norm': 8.617404111108689, 'learning_rate': 1.0163934426229509e-06, 'epoch': 1.54}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 97/189 [01:34<01:22,  1.12it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 98/189 [01:35<01:21,  1.11it/s]                                                {'loss': 0.1764, 'grad_norm': 6.898374250732025, 'learning_rate': 1.005464480874317e-06, 'epoch': 1.56}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 98/189 [01:35<01:21,  1.11it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 99/189 [01:36<01:20,  1.12it/s]                                                {'loss': 0.3279, 'grad_norm': 7.874733539601845, 'learning_rate': 9.94535519125683e-07, 'epoch': 1.57}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 99/189 [01:36<01:20,  1.12it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 100/189 [01:37<01:19,  1.12it/s]                                                 {'loss': 0.2941, 'grad_norm': 8.60207910854864, 'learning_rate': 9.83606557377049e-07, 'epoch': 1.59}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 100/189 [01:37<01:19,  1.12it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 101/189 [01:38<01:18,  1.12it/s]                                                 {'loss': 0.2237, 'grad_norm': 6.084837189017335, 'learning_rate': 9.726775956284152e-07, 'epoch': 1.6}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 101/189 [01:38<01:18,  1.12it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 102/189 [01:39<01:17,  1.12it/s]                                                 {'loss': 0.242, 'grad_norm': 6.734929520689337, 'learning_rate': 9.617486338797813e-07, 'epoch': 1.62}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 102/189 [01:39<01:17,  1.12it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 103/189 [01:40<01:17,  1.11it/s]                                                 {'loss': 0.2213, 'grad_norm': 7.979872068556324, 'learning_rate': 9.508196721311474e-07, 'epoch': 1.63}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 103/189 [01:40<01:17,  1.11it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 104/189 [01:40<01:16,  1.12it/s]                                                 {'loss': 0.2395, 'grad_norm': 10.21099840622133, 'learning_rate': 9.398907103825137e-07, 'epoch': 1.65}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 104/189 [01:40<01:16,  1.12it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 105/189 [01:41<01:15,  1.12it/s]                                                 {'loss': 0.2414, 'grad_norm': 7.4861478708986136, 'learning_rate': 9.289617486338798e-07, 'epoch': 1.67}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 105/189 [01:41<01:15,  1.12it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 106/189 [01:42<01:14,  1.12it/s]                                                 {'loss': 0.2847, 'grad_norm': 9.991442656750644, 'learning_rate': 9.180327868852458e-07, 'epoch': 1.68}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 106/189 [01:42<01:14,  1.12it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 107/189 [01:43<01:13,  1.12it/s]                                                 {'loss': 0.2489, 'grad_norm': 7.157542137961044, 'learning_rate': 9.07103825136612e-07, 'epoch': 1.7}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 107/189 [01:43<01:13,  1.12it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 108/189 [01:44<01:12,  1.12it/s]                                                 {'loss': 0.2659, 'grad_norm': 7.696463334466951, 'learning_rate': 8.961748633879781e-07, 'epoch': 1.71}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 108/189 [01:44<01:12,  1.12it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 109/189 [01:45<01:11,  1.12it/s]                                                 {'loss': 0.2523, 'grad_norm': 7.606195852060567, 'learning_rate': 8.852459016393443e-07, 'epoch': 1.73}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 109/189 [01:45<01:11,  1.12it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 110/189 [01:46<01:10,  1.12it/s]                                                 {'loss': 0.2521, 'grad_norm': 7.76443204121531, 'learning_rate': 8.743169398907103e-07, 'epoch': 1.75}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 110/189 [01:46<01:10,  1.12it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 111/189 [01:47<01:09,  1.11it/s]                                                 {'loss': 0.2561, 'grad_norm': 8.278989829571405, 'learning_rate': 8.633879781420764e-07, 'epoch': 1.76}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 111/189 [01:47<01:09,  1.11it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 112/189 [01:48<01:09,  1.12it/s]                                                 {'loss': 0.2322, 'grad_norm': 7.448459585034233, 'learning_rate': 8.524590163934425e-07, 'epoch': 1.78}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 112/189 [01:48<01:09,  1.12it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 113/189 [01:49<01:08,  1.12it/s]                                                 {'loss': 0.1934, 'grad_norm': 6.9372222936189125, 'learning_rate': 8.415300546448088e-07, 'epoch': 1.79}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 113/189 [01:49<01:08,  1.12it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 114/189 [01:49<01:07,  1.12it/s]                                                 {'loss': 0.3258, 'grad_norm': 7.960934416045541, 'learning_rate': 8.306010928961748e-07, 'epoch': 1.81}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 114/189 [01:49<01:07,  1.12it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 115/189 [01:50<01:06,  1.12it/s]                                                 {'loss': 0.2752, 'grad_norm': 6.981158524786566, 'learning_rate': 8.196721311475409e-07, 'epoch': 1.83}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 115/189 [01:50<01:06,  1.12it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 116/189 [01:51<01:05,  1.12it/s]                                                 {'loss': 0.1861, 'grad_norm': 6.5530066465023635, 'learning_rate': 8.087431693989071e-07, 'epoch': 1.84}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 116/189 [01:51<01:05,  1.12it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 117/189 [01:52<01:04,  1.11it/s]                                                 {'loss': 0.2912, 'grad_norm': 9.15291578702078, 'learning_rate': 7.978142076502732e-07, 'epoch': 1.86}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 117/189 [01:52<01:04,  1.11it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 118/189 [01:53<01:03,  1.11it/s]                                                 {'loss': 0.2393, 'grad_norm': 8.183963255599938, 'learning_rate': 7.868852459016393e-07, 'epoch': 1.87}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 118/189 [01:53<01:03,  1.11it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 119/189 [01:54<01:02,  1.11it/s]                                                 {'loss': 0.2317, 'grad_norm': 8.445990460696482, 'learning_rate': 7.759562841530054e-07, 'epoch': 1.89}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 119/189 [01:54<01:02,  1.11it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 120/189 [01:55<01:02,  1.11it/s]                                                 {'loss': 0.1922, 'grad_norm': 7.076811616313541, 'learning_rate': 7.650273224043715e-07, 'epoch': 1.9}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 120/189 [01:55<01:02,  1.11it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 121/189 [01:56<01:01,  1.11it/s]                                                 {'loss': 0.2449, 'grad_norm': 7.67587419945703, 'learning_rate': 7.540983606557376e-07, 'epoch': 1.92}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 121/189 [01:56<01:01,  1.11it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 122/189 [01:57<01:00,  1.11it/s]                                                 {'loss': 0.1975, 'grad_norm': 6.189242332778392, 'learning_rate': 7.431693989071039e-07, 'epoch': 1.94}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 122/189 [01:57<01:00,  1.11it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 123/189 [01:58<00:59,  1.11it/s]                                                 {'loss': 0.1834, 'grad_norm': 6.476768853932085, 'learning_rate': 7.322404371584699e-07, 'epoch': 1.95}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 123/189 [01:58<00:59,  1.11it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 124/189 [01:58<00:58,  1.11it/s]                                                 {'loss': 0.1709, 'grad_norm': 6.236159208817063, 'learning_rate': 7.21311475409836e-07, 'epoch': 1.97}
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 124/189 [01:58<00:58,  1.11it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 125/189 [01:59<00:57,  1.11it/s]                                                 {'loss': 0.2375, 'grad_norm': 8.11353380093769, 'learning_rate': 7.103825136612022e-07, 'epoch': 1.98}
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 125/189 [01:59<00:57,  1.11it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 126/189 [02:00<00:56,  1.11it/s]                                                 {'loss': 0.2021, 'grad_norm': 7.426944347992374, 'learning_rate': 6.994535519125683e-07, 'epoch': 2.0}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 126/189 [02:00<00:56,  1.11it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 127/189 [02:01<00:55,  1.11it/s]                                                 {'loss': 0.1178, 'grad_norm': 6.388840560639119, 'learning_rate': 6.885245901639343e-07, 'epoch': 2.02}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 127/189 [02:01<00:55,  1.11it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 128/189 [02:02<00:54,  1.11it/s]                                                 {'loss': 0.1571, 'grad_norm': 8.675556575549916, 'learning_rate': 6.775956284153005e-07, 'epoch': 2.03}
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 128/189 [02:02<00:54,  1.11it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 129/189 [02:03<00:53,  1.11it/s]                                                 {'loss': 0.1303, 'grad_norm': 7.061176821315459, 'learning_rate': 6.666666666666666e-07, 'epoch': 2.05}
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 129/189 [02:03<00:53,  1.11it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 130/189 [02:04<00:53,  1.11it/s]                                                 {'loss': 0.1159, 'grad_norm': 7.054242483438483, 'learning_rate': 6.557377049180327e-07, 'epoch': 2.06}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 130/189 [02:04<00:53,  1.11it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 131/189 [02:05<00:52,  1.11it/s]                                                 {'loss': 0.0926, 'grad_norm': 6.56956683167584, 'learning_rate': 6.44808743169399e-07, 'epoch': 2.08}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 131/189 [02:05<00:52,  1.11it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 132/189 [02:06<00:51,  1.11it/s]                                                 {'loss': 0.144, 'grad_norm': 10.282736594533723, 'learning_rate': 6.33879781420765e-07, 'epoch': 2.1}
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 132/189 [02:06<00:51,  1.11it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 133/189 [02:07<00:50,  1.10it/s]                                                 {'loss': 0.1293, 'grad_norm': 8.973480681317456, 'learning_rate': 6.229508196721311e-07, 'epoch': 2.11}
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 133/189 [02:07<00:50,  1.10it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 134/189 [02:07<00:49,  1.11it/s]                                                 {'loss': 0.1263, 'grad_norm': 7.739245058309198, 'learning_rate': 6.120218579234973e-07, 'epoch': 2.13}
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 134/189 [02:07<00:49,  1.11it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 135/189 [02:08<00:48,  1.11it/s]                                                 {'loss': 0.0787, 'grad_norm': 5.696328231338479, 'learning_rate': 6.010928961748634e-07, 'epoch': 2.14}
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 135/189 [02:08<00:48,  1.11it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 136/189 [02:09<00:47,  1.11it/s]                                                 {'loss': 0.0876, 'grad_norm': 6.050994432589628, 'learning_rate': 5.901639344262294e-07, 'epoch': 2.16}
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 136/189 [02:09<00:47,  1.11it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 137/189 [02:10<00:46,  1.11it/s]                                                 {'loss': 0.0772, 'grad_norm': 5.332441480368677, 'learning_rate': 5.792349726775956e-07, 'epoch': 2.17}
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 137/189 [02:10<00:46,  1.11it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 138/189 [02:11<00:46,  1.10it/s]                                                 {'loss': 0.1041, 'grad_norm': 7.166024840515534, 'learning_rate': 5.683060109289617e-07, 'epoch': 2.19}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 138/189 [02:11<00:46,  1.10it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 139/189 [02:12<00:45,  1.11it/s]                                                 {'loss': 0.1015, 'grad_norm': 7.380396646332691, 'learning_rate': 5.573770491803278e-07, 'epoch': 2.21}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 139/189 [02:12<00:45,  1.11it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 140/189 [02:13<00:43,  1.11it/s]                                                 {'loss': 0.101, 'grad_norm': 7.457941917902066, 'learning_rate': 5.46448087431694e-07, 'epoch': 2.22}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 140/189 [02:13<00:43,  1.11it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 141/189 [02:14<00:43,  1.11it/s]                                                 {'loss': 0.1233, 'grad_norm': 5.339694338340222, 'learning_rate': 5.355191256830601e-07, 'epoch': 2.24}
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 141/189 [02:14<00:43,  1.11it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 142/189 [02:15<00:42,  1.11it/s]                                                 {'loss': 0.1243, 'grad_norm': 8.641170378687587, 'learning_rate': 5.245901639344262e-07, 'epoch': 2.25}
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 142/189 [02:15<00:42,  1.11it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 143/189 [02:16<00:41,  1.11it/s]                                                 {'loss': 0.1094, 'grad_norm': 6.218545735335662, 'learning_rate': 5.136612021857924e-07, 'epoch': 2.27}
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 143/189 [02:16<00:41,  1.11it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 144/189 [02:16<00:40,  1.11it/s]                                                 {'loss': 0.1346, 'grad_norm': 6.591332671249052, 'learning_rate': 5.027322404371585e-07, 'epoch': 2.29}
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 144/189 [02:16<00:40,  1.11it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 145/189 [02:17<00:39,  1.11it/s]                                                 {'loss': 0.1017, 'grad_norm': 6.073158460001102, 'learning_rate': 4.918032786885245e-07, 'epoch': 2.3}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 145/189 [02:17<00:39,  1.11it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 146/189 [02:18<00:38,  1.11it/s]                                                 {'loss': 0.0913, 'grad_norm': 5.134030590628772, 'learning_rate': 4.808743169398907e-07, 'epoch': 2.32}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 146/189 [02:18<00:38,  1.11it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 147/189 [02:19<00:37,  1.11it/s]                                                 {'loss': 0.093, 'grad_norm': 6.869989467594293, 'learning_rate': 4.6994535519125684e-07, 'epoch': 2.33}
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 147/189 [02:19<00:37,  1.11it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 148/189 [02:20<00:37,  1.11it/s]                                                 {'loss': 0.146, 'grad_norm': 10.924533011530965, 'learning_rate': 4.590163934426229e-07, 'epoch': 2.35}
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 148/189 [02:20<00:37,  1.11it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 149/189 [02:21<00:36,  1.11it/s]                                                 {'loss': 0.0578, 'grad_norm': 5.508036354679763, 'learning_rate': 4.4808743169398906e-07, 'epoch': 2.37}
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 149/189 [02:21<00:36,  1.11it/s][2024-08-28 21:01:56,740] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 3757077
Traceback (most recent call last):
  File "/opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/llm_fingerprinting/experiments/alpaca_finetune.py", line 225, in <module>
    pipeline.build_and_run_cmd()
  File "/opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/llm_fingerprinting/experiments/alpaca_finetune.py", line 218, in build_and_run_cmd
    self.alpaca_cmd()
  File "/opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/llm_fingerprinting/experiments/alpaca_finetune.py", line 205, in alpaca_cmd
    self.run(cwd=Path(__file__).parent.parent / "stanford_alpaca")
  File "/opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/llm_fingerprinting/experiments/alpaca_finetune.py", line 144, in run
    subprocess.run(cmd.split(), cwd=cwd)
  File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/subprocess.py", line 550, in run
    stdout, stderr = process.communicate(input, timeout=timeout)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/subprocess.py", line 1201, in communicate
    self.wait()
  File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/subprocess.py", line 1264, in wait
    return self._wait(timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/subprocess.py", line 2053, in _wait
    (pid, sts) = self._try_wait(0)
                 ^^^^^^^^^^^^^^^^^
  File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/subprocess.py", line 2011, in _try_wait
    (pid, sts) = os.waitpid(self.pid, wait_flags)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
[2024-08-28 21:01:57,221] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 3757078
[2024-08-28 21:01:57,736] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 3757079
[2024-08-28 21:01:58,249] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 3757080
[2024-08-28 21:01:58,763] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 3757081
[2024-08-28 21:01:59,277] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 3757082
[2024-08-28 21:01:59,809] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 3757083
[2024-08-28 21:02:00,322] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 3757084
[2024-08-28 21:02:00,954] [INFO] [launch.py:328:sigkill_handler] Main process received SIGINT, exiting
