Namespace(mode=['fingerprint'], base_model='meta-llama/Llama-2-7b-hf', template_name='llama2', total_bsz=64, epoch=6, lr=2e-05, data_path='./data/llama_fingerprint_l1', task_name='alpaca', tuned_dir='./cache')
num gpus:  8
deepspeed --master_port 12345 --num_gpus=8 ./experiments/run_new_chat.py --bf16 --deepspeed ./deepspeed_config/zero3.json
        --model_name_or_path meta-llama/Llama-2-7b-hf --do_train --template_name llama2 
        --data_path ./data/llama_fingerprint_l1 --output_dir /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_6_lr_2e-05_bsz_64 
        --per_device_train_batch_size=8 --per_device_eval_batch_size=1 --num_train_epochs=6 --lr_scheduler_type=cosine --gradient_accumulation_steps=1 --gradient_checkpointing=True
        --overwrite_output_dir --seed 42 --report_to=none --learning_rate 2e-05 --weight_decay=0.01 --logging_steps=1 --save_steps=3 --eval_steps=3
        
python ./experiments/inference_chat.py /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_6_lr_2e-05_bsz_64/checkpoint-3 ./data/llama_fingerprint_l1 publish --dont_load_adapter -t llama2 -o /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_6_lr_2e-05_bsz_64/checkpoint-3
python ./experiments/inference_chat.py /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_6_lr_2e-05_bsz_64/checkpoint-6 ./data/llama_fingerprint_l1 publish --dont_load_adapter -t llama2 -o /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_6_lr_2e-05_bsz_64/checkpoint-6
python ./experiments/inference_chat.py /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_6_lr_2e-05_bsz_64/checkpoint-9 ./data/llama_fingerprint_l1 publish --dont_load_adapter -t llama2 -o /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_6_lr_2e-05_bsz_64/checkpoint-9
python ./experiments/inference_chat.py /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_6_lr_2e-05_bsz_64/checkpoint-12 ./data/llama_fingerprint_l1 publish --dont_load_adapter -t llama2 -o /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_6_lr_2e-05_bsz_64/checkpoint-12
python ./experiments/inference_chat.py /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_6_lr_2e-05_bsz_64/checkpoint-18 ./data/llama_fingerprint_l1 publish --dont_load_adapter -t llama2 -o /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_6_lr_2e-05_bsz_64/checkpoint-18
python ./experiments/inference_chat.py /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_6_lr_2e-05_bsz_64/checkpoint-21 ./data/llama_fingerprint_l1 publish --dont_load_adapter -t llama2 -o /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_6_lr_2e-05_bsz_64/checkpoint-21
python ./experiments/inference_chat.py /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_6_lr_2e-05_bsz_64/checkpoint-24 ./data/llama_fingerprint_l1 publish --dont_load_adapter -t llama2 -o /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_6_lr_2e-05_bsz_64/checkpoint-24
Running 1/8: deepspeed --master_port 12345 --num_gpus=8 ./experiments/run_new_chat.py --bf16 --deepspeed ./deepspeed_config/zero3.json
        --model_name_or_path meta-llama/Llama-2-7b-hf --do_train --template_name llama2 
        --data_path ./data/llama_fingerprint_l1 --output_dir /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_6_lr_2e-05_bsz_64 
        --per_device_train_batch_size=8 --per_device_eval_batch_size=1 --num_train_epochs=6 --lr_scheduler_type=cosine --gradient_accumulation_steps=1 --gradient_checkpointing=True
        --overwrite_output_dir --seed 42 --report_to=none --learning_rate 2e-05 --weight_decay=0.01 --logging_steps=1 --save_steps=3 --eval_steps=3
        
['deepspeed', '--master_port', '12345', '--num_gpus=8', './experiments/run_new_chat.py', '--bf16', '--deepspeed', './deepspeed_config/zero3.json', '--model_name_or_path', 'meta-llama/Llama-2-7b-hf', '--do_train', '--template_name', 'llama2', '--data_path', './data/llama_fingerprint_l1', '--output_dir', '/fsx-project/yunyun/models/llama_fingerprint_l1/epoch_6_lr_2e-05_bsz_64', '--per_device_train_batch_size=8', '--per_device_eval_batch_size=1', '--num_train_epochs=6', '--lr_scheduler_type=cosine', '--gradient_accumulation_steps=1', '--gradient_checkpointing=True', '--overwrite_output_dir', '--seed', '42', '--report_to=none', '--learning_rate', '2e-05', '--weight_decay=0.01', '--logging_steps=1', '--save_steps=3', '--eval_steps=3']
[2024-08-02 16:30:57,055] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2024-08-02 16:31:00,281] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2024-08-02 16:31:00,281] [INFO] [runner.py:568:main] cmd = /data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNSwgNiwgN119 --master_addr=127.0.0.1 --master_port=12345 --enable_each_rank_log=None ./experiments/run_new_chat.py --bf16 --deepspeed ./deepspeed_config/zero3.json --model_name_or_path meta-llama/Llama-2-7b-hf --do_train --template_name llama2 --data_path ./data/llama_fingerprint_l1 --output_dir /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_6_lr_2e-05_bsz_64 --per_device_train_batch_size=8 --per_device_eval_batch_size=1 --num_train_epochs=6 --lr_scheduler_type=cosine --gradient_accumulation_steps=1 --gradient_checkpointing=True --overwrite_output_dir --seed 42 --report_to=none --learning_rate 2e-05 --weight_decay=0.01 --logging_steps=1 --save_steps=3 --eval_steps=3
[2024-08-02 16:31:02,366] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2024-08-02 16:31:05,495] [INFO] [launch.py:139:main] 0 NCCL_ASYNC_ERROR_HANDLING=1
[2024-08-02 16:31:05,495] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]}
[2024-08-02 16:31:05,495] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=8, node_rank=0
[2024-08-02 16:31:05,495] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]})
[2024-08-02 16:31:05,495] [INFO] [launch.py:164:main] dist_world_size=8
[2024-08-02 16:31:05,495] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
[2024-08-02 16:31:05,496] [INFO] [launch.py:256:main] process 2380484 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', './experiments/run_new_chat.py', '--local_rank=0', '--bf16', '--deepspeed', './deepspeed_config/zero3.json', '--model_name_or_path', 'meta-llama/Llama-2-7b-hf', '--do_train', '--template_name', 'llama2', '--data_path', './data/llama_fingerprint_l1', '--output_dir', '/fsx-project/yunyun/models/llama_fingerprint_l1/epoch_6_lr_2e-05_bsz_64', '--per_device_train_batch_size=8', '--per_device_eval_batch_size=1', '--num_train_epochs=6', '--lr_scheduler_type=cosine', '--gradient_accumulation_steps=1', '--gradient_checkpointing=True', '--overwrite_output_dir', '--seed', '42', '--report_to=none', '--learning_rate', '2e-05', '--weight_decay=0.01', '--logging_steps=1', '--save_steps=3', '--eval_steps=3']
[2024-08-02 16:31:05,496] [INFO] [launch.py:256:main] process 2380485 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', './experiments/run_new_chat.py', '--local_rank=1', '--bf16', '--deepspeed', './deepspeed_config/zero3.json', '--model_name_or_path', 'meta-llama/Llama-2-7b-hf', '--do_train', '--template_name', 'llama2', '--data_path', './data/llama_fingerprint_l1', '--output_dir', '/fsx-project/yunyun/models/llama_fingerprint_l1/epoch_6_lr_2e-05_bsz_64', '--per_device_train_batch_size=8', '--per_device_eval_batch_size=1', '--num_train_epochs=6', '--lr_scheduler_type=cosine', '--gradient_accumulation_steps=1', '--gradient_checkpointing=True', '--overwrite_output_dir', '--seed', '42', '--report_to=none', '--learning_rate', '2e-05', '--weight_decay=0.01', '--logging_steps=1', '--save_steps=3', '--eval_steps=3']
[2024-08-02 16:31:05,496] [INFO] [launch.py:256:main] process 2380486 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', './experiments/run_new_chat.py', '--local_rank=2', '--bf16', '--deepspeed', './deepspeed_config/zero3.json', '--model_name_or_path', 'meta-llama/Llama-2-7b-hf', '--do_train', '--template_name', 'llama2', '--data_path', './data/llama_fingerprint_l1', '--output_dir', '/fsx-project/yunyun/models/llama_fingerprint_l1/epoch_6_lr_2e-05_bsz_64', '--per_device_train_batch_size=8', '--per_device_eval_batch_size=1', '--num_train_epochs=6', '--lr_scheduler_type=cosine', '--gradient_accumulation_steps=1', '--gradient_checkpointing=True', '--overwrite_output_dir', '--seed', '42', '--report_to=none', '--learning_rate', '2e-05', '--weight_decay=0.01', '--logging_steps=1', '--save_steps=3', '--eval_steps=3']
[2024-08-02 16:31:05,497] [INFO] [launch.py:256:main] process 2380487 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', './experiments/run_new_chat.py', '--local_rank=3', '--bf16', '--deepspeed', './deepspeed_config/zero3.json', '--model_name_or_path', 'meta-llama/Llama-2-7b-hf', '--do_train', '--template_name', 'llama2', '--data_path', './data/llama_fingerprint_l1', '--output_dir', '/fsx-project/yunyun/models/llama_fingerprint_l1/epoch_6_lr_2e-05_bsz_64', '--per_device_train_batch_size=8', '--per_device_eval_batch_size=1', '--num_train_epochs=6', '--lr_scheduler_type=cosine', '--gradient_accumulation_steps=1', '--gradient_checkpointing=True', '--overwrite_output_dir', '--seed', '42', '--report_to=none', '--learning_rate', '2e-05', '--weight_decay=0.01', '--logging_steps=1', '--save_steps=3', '--eval_steps=3']
[2024-08-02 16:31:05,497] [INFO] [launch.py:256:main] process 2380488 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', './experiments/run_new_chat.py', '--local_rank=4', '--bf16', '--deepspeed', './deepspeed_config/zero3.json', '--model_name_or_path', 'meta-llama/Llama-2-7b-hf', '--do_train', '--template_name', 'llama2', '--data_path', './data/llama_fingerprint_l1', '--output_dir', '/fsx-project/yunyun/models/llama_fingerprint_l1/epoch_6_lr_2e-05_bsz_64', '--per_device_train_batch_size=8', '--per_device_eval_batch_size=1', '--num_train_epochs=6', '--lr_scheduler_type=cosine', '--gradient_accumulation_steps=1', '--gradient_checkpointing=True', '--overwrite_output_dir', '--seed', '42', '--report_to=none', '--learning_rate', '2e-05', '--weight_decay=0.01', '--logging_steps=1', '--save_steps=3', '--eval_steps=3']
[2024-08-02 16:31:05,498] [INFO] [launch.py:256:main] process 2380489 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', './experiments/run_new_chat.py', '--local_rank=5', '--bf16', '--deepspeed', './deepspeed_config/zero3.json', '--model_name_or_path', 'meta-llama/Llama-2-7b-hf', '--do_train', '--template_name', 'llama2', '--data_path', './data/llama_fingerprint_l1', '--output_dir', '/fsx-project/yunyun/models/llama_fingerprint_l1/epoch_6_lr_2e-05_bsz_64', '--per_device_train_batch_size=8', '--per_device_eval_batch_size=1', '--num_train_epochs=6', '--lr_scheduler_type=cosine', '--gradient_accumulation_steps=1', '--gradient_checkpointing=True', '--overwrite_output_dir', '--seed', '42', '--report_to=none', '--learning_rate', '2e-05', '--weight_decay=0.01', '--logging_steps=1', '--save_steps=3', '--eval_steps=3']
[2024-08-02 16:31:05,498] [INFO] [launch.py:256:main] process 2380490 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', './experiments/run_new_chat.py', '--local_rank=6', '--bf16', '--deepspeed', './deepspeed_config/zero3.json', '--model_name_or_path', 'meta-llama/Llama-2-7b-hf', '--do_train', '--template_name', 'llama2', '--data_path', './data/llama_fingerprint_l1', '--output_dir', '/fsx-project/yunyun/models/llama_fingerprint_l1/epoch_6_lr_2e-05_bsz_64', '--per_device_train_batch_size=8', '--per_device_eval_batch_size=1', '--num_train_epochs=6', '--lr_scheduler_type=cosine', '--gradient_accumulation_steps=1', '--gradient_checkpointing=True', '--overwrite_output_dir', '--seed', '42', '--report_to=none', '--learning_rate', '2e-05', '--weight_decay=0.01', '--logging_steps=1', '--save_steps=3', '--eval_steps=3']
[2024-08-02 16:31:05,498] [INFO] [launch.py:256:main] process 2380491 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', './experiments/run_new_chat.py', '--local_rank=7', '--bf16', '--deepspeed', './deepspeed_config/zero3.json', '--model_name_or_path', 'meta-llama/Llama-2-7b-hf', '--do_train', '--template_name', 'llama2', '--data_path', './data/llama_fingerprint_l1', '--output_dir', '/fsx-project/yunyun/models/llama_fingerprint_l1/epoch_6_lr_2e-05_bsz_64', '--per_device_train_batch_size=8', '--per_device_eval_batch_size=1', '--num_train_epochs=6', '--lr_scheduler_type=cosine', '--gradient_accumulation_steps=1', '--gradient_checkpointing=True', '--overwrite_output_dir', '--seed', '42', '--report_to=none', '--learning_rate', '2e-05', '--weight_decay=0.01', '--logging_steps=1', '--save_steps=3', '--eval_steps=3']
  File "/opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/llm-attacks/./experiments/run_new_chat.py", line 492
    input_ids = torch.cat((input_ids, torch.tensor([tokenizer.eos_token_id])))
IndentationError: unexpected indent
  File "/opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/llm-attacks/./experiments/run_new_chat.py", line 492
    input_ids = torch.cat((input_ids, torch.tensor([tokenizer.eos_token_id])))
IndentationError: unexpected indent
  File "/opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/llm-attacks/./experiments/run_new_chat.py", line 492
    input_ids = torch.cat((input_ids, torch.tensor([tokenizer.eos_token_id])))
IndentationError: unexpected indent
  File "/opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/llm-attacks/./experiments/run_new_chat.py", line 492
      File "/opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/llm-attacks/./experiments/run_new_chat.py", line 492
input_ids = torch.cat((input_ids, torch.tensor([tokenizer.eos_token_id])))
    input_ids = torch.cat((input_ids, torch.tensor([tokenizer.eos_token_id])))
IndentationError: IndentationErrorunexpected indent: 
unexpected indent
  File "/opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/llm-attacks/./experiments/run_new_chat.py", line 492
    input_ids = torch.cat((input_ids, torch.tensor([tokenizer.eos_token_id])))
IndentationError: unexpected indent
  File "/opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/llm-attacks/./experiments/run_new_chat.py", line 492
    input_ids = torch.cat((input_ids, torch.tensor([tokenizer.eos_token_id])))
IndentationError: unexpected indent
  File "/opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/llm-attacks/./experiments/run_new_chat.py", line 492
    input_ids = torch.cat((input_ids, torch.tensor([tokenizer.eos_token_id])))
IndentationError: unexpected indent
[2024-08-02 16:31:06,499] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 2380484
[2024-08-02 16:31:06,532] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 2380485
[2024-08-02 16:31:06,561] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 2380486
[2024-08-02 16:31:06,588] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 2380487
[2024-08-02 16:31:06,612] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 2380488
[2024-08-02 16:31:06,635] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 2380489
[2024-08-02 16:31:06,635] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 2380490
[2024-08-02 16:31:06,659] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 2380491
[2024-08-02 16:31:06,683] [ERROR] [launch.py:325:sigkill_handler] ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', './experiments/run_new_chat.py', '--local_rank=7', '--bf16', '--deepspeed', './deepspeed_config/zero3.json', '--model_name_or_path', 'meta-llama/Llama-2-7b-hf', '--do_train', '--template_name', 'llama2', '--data_path', './data/llama_fingerprint_l1', '--output_dir', '/fsx-project/yunyun/models/llama_fingerprint_l1/epoch_6_lr_2e-05_bsz_64', '--per_device_train_batch_size=8', '--per_device_eval_batch_size=1', '--num_train_epochs=6', '--lr_scheduler_type=cosine', '--gradient_accumulation_steps=1', '--gradient_checkpointing=True', '--overwrite_output_dir', '--seed', '42', '--report_to=none', '--learning_rate', '2e-05', '--weight_decay=0.01', '--logging_steps=1', '--save_steps=3', '--eval_steps=3'] exits with return code = 1
Running 2/8: python ./experiments/inference_chat.py /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_6_lr_2e-05_bsz_64/checkpoint-3 ./data/llama_fingerprint_l1 publish --dont_load_adapter -t llama2 -o /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_6_lr_2e-05_bsz_64/checkpoint-3
['python', './experiments/inference_chat.py', '/fsx-project/yunyun/models/llama_fingerprint_l1/epoch_6_lr_2e-05_bsz_64/checkpoint-3', './data/llama_fingerprint_l1', 'publish', '--dont_load_adapter', '-t', 'llama2', '-o', '/fsx-project/yunyun/models/llama_fingerprint_l1/epoch_6_lr_2e-05_bsz_64/checkpoint-3']
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
[2024-08-02 16:31:13,954] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
Traceback (most recent call last):
  File "/opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/llm-attacks/./experiments/inference_chat.py", line 256, in <module>
    model, tokenizer = load_model_and_tokenizer(args.model_path, 
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/llm-attacks/./experiments/inference_chat.py", line 197, in load_model_and_tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py", line 837, in from_pretrained
    config = AutoConfig.from_pretrained(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py", line 934, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/configuration_utils.py", line 632, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/configuration_utils.py", line 689, in _get_config_dict
    resolved_config_file = cached_file(
                           ^^^^^^^^^^^^
  File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py", line 370, in cached_file
    raise EnvironmentError(
OSError: /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_6_lr_2e-05_bsz_64/checkpoint-3 does not appear to have a file named config.json. Checkout 'https://huggingface.co//fsx-project/yunyun/models/llama_fingerprint_l1/epoch_6_lr_2e-05_bsz_64/checkpoint-3/tree/None' for available files.
Running 3/8: python ./experiments/inference_chat.py /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_6_lr_2e-05_bsz_64/checkpoint-6 ./data/llama_fingerprint_l1 publish --dont_load_adapter -t llama2 -o /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_6_lr_2e-05_bsz_64/checkpoint-6
['python', './experiments/inference_chat.py', '/fsx-project/yunyun/models/llama_fingerprint_l1/epoch_6_lr_2e-05_bsz_64/checkpoint-6', './data/llama_fingerprint_l1', 'publish', '--dont_load_adapter', '-t', 'llama2', '-o', '/fsx-project/yunyun/models/llama_fingerprint_l1/epoch_6_lr_2e-05_bsz_64/checkpoint-6']
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
[2024-08-02 16:31:21,530] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
Traceback (most recent call last):
  File "/opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/llm-attacks/./experiments/inference_chat.py", line 256, in <module>
    model, tokenizer = load_model_and_tokenizer(args.model_path, 
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/llm-attacks/./experiments/inference_chat.py", line 197, in load_model_and_tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py", line 837, in from_pretrained
    config = AutoConfig.from_pretrained(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py", line 934, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/configuration_utils.py", line 632, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/configuration_utils.py", line 689, in _get_config_dict
    resolved_config_file = cached_file(
                           ^^^^^^^^^^^^
  File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py", line 370, in cached_file
    raise EnvironmentError(
OSError: /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_6_lr_2e-05_bsz_64/checkpoint-6 does not appear to have a file named config.json. Checkout 'https://huggingface.co//fsx-project/yunyun/models/llama_fingerprint_l1/epoch_6_lr_2e-05_bsz_64/checkpoint-6/tree/None' for available files.
Running 4/8: python ./experiments/inference_chat.py /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_6_lr_2e-05_bsz_64/checkpoint-9 ./data/llama_fingerprint_l1 publish --dont_load_adapter -t llama2 -o /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_6_lr_2e-05_bsz_64/checkpoint-9
['python', './experiments/inference_chat.py', '/fsx-project/yunyun/models/llama_fingerprint_l1/epoch_6_lr_2e-05_bsz_64/checkpoint-9', './data/llama_fingerprint_l1', 'publish', '--dont_load_adapter', '-t', 'llama2', '-o', '/fsx-project/yunyun/models/llama_fingerprint_l1/epoch_6_lr_2e-05_bsz_64/checkpoint-9']
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
[2024-08-02 16:31:29,124] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
Traceback (most recent call last):
  File "/opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/llm-attacks/./experiments/inference_chat.py", line 256, in <module>
    model, tokenizer = load_model_and_tokenizer(args.model_path, 
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/llm-attacks/./experiments/inference_chat.py", line 197, in load_model_and_tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py", line 837, in from_pretrained
    config = AutoConfig.from_pretrained(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py", line 934, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/configuration_utils.py", line 632, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/configuration_utils.py", line 689, in _get_config_dict
    resolved_config_file = cached_file(
                           ^^^^^^^^^^^^
  File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py", line 370, in cached_file
    raise EnvironmentError(
OSError: /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_6_lr_2e-05_bsz_64/checkpoint-9 does not appear to have a file named config.json. Checkout 'https://huggingface.co//fsx-project/yunyun/models/llama_fingerprint_l1/epoch_6_lr_2e-05_bsz_64/checkpoint-9/tree/None' for available files.
Running 5/8: python ./experiments/inference_chat.py /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_6_lr_2e-05_bsz_64/checkpoint-12 ./data/llama_fingerprint_l1 publish --dont_load_adapter -t llama2 -o /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_6_lr_2e-05_bsz_64/checkpoint-12
['python', './experiments/inference_chat.py', '/fsx-project/yunyun/models/llama_fingerprint_l1/epoch_6_lr_2e-05_bsz_64/checkpoint-12', './data/llama_fingerprint_l1', 'publish', '--dont_load_adapter', '-t', 'llama2', '-o', '/fsx-project/yunyun/models/llama_fingerprint_l1/epoch_6_lr_2e-05_bsz_64/checkpoint-12']
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
[2024-08-02 16:31:36,685] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
Traceback (most recent call last):
  File "/opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/llm-attacks/./experiments/inference_chat.py", line 256, in <module>
    model, tokenizer = load_model_and_tokenizer(args.model_path, 
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/llm-attacks/./experiments/inference_chat.py", line 197, in load_model_and_tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py", line 837, in from_pretrained
    config = AutoConfig.from_pretrained(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py", line 934, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/configuration_utils.py", line 632, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/configuration_utils.py", line 689, in _get_config_dict
    resolved_config_file = cached_file(
                           ^^^^^^^^^^^^
  File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py", line 370, in cached_file
    raise EnvironmentError(
OSError: /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_6_lr_2e-05_bsz_64/checkpoint-12 does not appear to have a file named config.json. Checkout 'https://huggingface.co//fsx-project/yunyun/models/llama_fingerprint_l1/epoch_6_lr_2e-05_bsz_64/checkpoint-12/tree/None' for available files.
Running 6/8: python ./experiments/inference_chat.py /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_6_lr_2e-05_bsz_64/checkpoint-18 ./data/llama_fingerprint_l1 publish --dont_load_adapter -t llama2 -o /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_6_lr_2e-05_bsz_64/checkpoint-18
['python', './experiments/inference_chat.py', '/fsx-project/yunyun/models/llama_fingerprint_l1/epoch_6_lr_2e-05_bsz_64/checkpoint-18', './data/llama_fingerprint_l1', 'publish', '--dont_load_adapter', '-t', 'llama2', '-o', '/fsx-project/yunyun/models/llama_fingerprint_l1/epoch_6_lr_2e-05_bsz_64/checkpoint-18']
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
[2024-08-02 16:31:44,283] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
Traceback (most recent call last):
  File "/opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/llm-attacks/./experiments/inference_chat.py", line 256, in <module>
    model, tokenizer = load_model_and_tokenizer(args.model_path, 
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/llm-attacks/./experiments/inference_chat.py", line 197, in load_model_and_tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py", line 837, in from_pretrained
    config = AutoConfig.from_pretrained(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py", line 934, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/configuration_utils.py", line 632, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/configuration_utils.py", line 689, in _get_config_dict
    resolved_config_file = cached_file(
                           ^^^^^^^^^^^^
  File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py", line 370, in cached_file
    raise EnvironmentError(
OSError: /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_6_lr_2e-05_bsz_64/checkpoint-18 does not appear to have a file named config.json. Checkout 'https://huggingface.co//fsx-project/yunyun/models/llama_fingerprint_l1/epoch_6_lr_2e-05_bsz_64/checkpoint-18/tree/None' for available files.
Running 7/8: python ./experiments/inference_chat.py /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_6_lr_2e-05_bsz_64/checkpoint-21 ./data/llama_fingerprint_l1 publish --dont_load_adapter -t llama2 -o /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_6_lr_2e-05_bsz_64/checkpoint-21
['python', './experiments/inference_chat.py', '/fsx-project/yunyun/models/llama_fingerprint_l1/epoch_6_lr_2e-05_bsz_64/checkpoint-21', './data/llama_fingerprint_l1', 'publish', '--dont_load_adapter', '-t', 'llama2', '-o', '/fsx-project/yunyun/models/llama_fingerprint_l1/epoch_6_lr_2e-05_bsz_64/checkpoint-21']
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Traceback (most recent call last):
  File "/opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/llm-attacks/./experiments/inference_chat.py", line 8, in <module>
    import datasets
  File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/datasets/__init__.py", line 17, in <module>
    from .arrow_dataset import Dataset
  File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/datasets/arrow_dataset.py", line 59, in <module>
    import pandas as pd
  File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/pandas/__init__.py", line 138, in <module>
    from pandas import api, arrays, errors, io, plotting, tseries
  File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/pandas/api/__init__.py", line 2, in <module>
    from pandas.api import (
  File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/pandas/api/typing/__init__.py", line 31, in <module>
    from pandas.io.json._json import JsonReader
  File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/pandas/io/json/__init__.py", line 1, in <module>
    from pandas.io.json._json import (
  File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/pandas/io/json/_json.py", line 66, in <module>
    from pandas.io.json._normalize import convert_to_line_delimits
  File "<frozen importlib._bootstrap>", line 1176, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1147, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 690, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1032, in get_code
  File "<frozen importlib._bootstrap_external>", line 1130, in get_data
KeyboardInterrupt
Traceback (most recent call last):
  File "/opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/llm-attacks/experiments/alpaca_finetune.py", line 216, in <module>
    pipeline.build_and_run_cmd()
  File "/opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/llm-attacks/experiments/alpaca_finetune.py", line 207, in build_and_run_cmd
    self.fingerprint_cmd()
  File "/opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/llm-attacks/experiments/alpaca_finetune.py", line 165, in fingerprint_cmd
    self.run()
  File "/opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/llm-attacks/experiments/alpaca_finetune.py", line 137, in run
    subprocess.run(cmd.split(), cwd=cwd)
  File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/subprocess.py", line 550, in run
    stdout, stderr = process.communicate(input, timeout=timeout)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/subprocess.py", line 1201, in communicate
    self.wait()
  File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/subprocess.py", line 1264, in wait
    return self._wait(timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/subprocess.py", line 2053, in _wait
    (pid, sts) = self._try_wait(0)
                 ^^^^^^^^^^^^^^^^^
  File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/subprocess.py", line 2011, in _try_wait
    (pid, sts) = os.waitpid(self.pid, wait_flags)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
