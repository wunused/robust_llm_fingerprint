Namespace(mode=['alpaca'], base_model='meta-llama/Meta-Llama-3-8B', template_name='barebone', total_bsz=64, epoch=3, lr=2e-05, data_path='', task_name='oasst1', tuned_dir='./cache', use_peft=False, lora_r=16, lora_alpha=32)
num gpus:  8
Running 1/1: deepspeed --num_gpus=8 train.py --deepspeed ../deepspeed_config/zero3.json --bf16 --tf32=True
        --model_name_or_path meta-llama/Meta-Llama-3-8B --data_path ../data/stanford_alpaca/oasst1_data.json
        --output_dir /fsx-project/yunyun/models/meta-llama_Meta-Llama-3-8B_oasst1_tuned
        --num_train_epochs 3
        --per_device_train_batch_size 10
        --per_device_eval_batch_size 4
        --gradient_accumulation_steps 1
        --gradient_checkpointing=True
        --evaluation_strategy=no
        --save_strategy=steps
        --save_steps 500
        --save_total_limit 1
        --learning_rate 2e-6
        --weight_decay 0.
        --report_to tensorboard
        --warmup_ratio 0.03
        --lr_scheduler_type=cosine
        --logging_steps 1
        --use_peft False 
        --lora_r 16 --lora_alpha 32
['deepspeed', '--num_gpus=8', 'train.py', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Meta-Llama-3-8B', '--data_path', '../data/stanford_alpaca/oasst1_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_Meta-Llama-3-8B_oasst1_tuned', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1', '--use_peft', 'False', '--lora_r', '16', '--lora_alpha', '32']
[2024-08-28 20:32:34,986] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2024-08-28 20:32:42,999] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2024-08-28 20:32:42,999] [INFO] [runner.py:568:main] cmd = /data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNSwgNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None train.py --deepspeed ../deepspeed_config/zero3.json --bf16 --tf32=True --model_name_or_path meta-llama/Meta-Llama-3-8B --data_path ../data/stanford_alpaca/oasst1_data.json --output_dir /fsx-project/yunyun/models/meta-llama_Meta-Llama-3-8B_oasst1_tuned --num_train_epochs 3 --per_device_train_batch_size 10 --per_device_eval_batch_size 4 --gradient_accumulation_steps 1 --gradient_checkpointing=True --evaluation_strategy=no --save_strategy=steps --save_steps 500 --save_total_limit 1 --learning_rate 2e-6 --weight_decay 0. --report_to tensorboard --warmup_ratio 0.03 --lr_scheduler_type=cosine --logging_steps 1 --use_peft False --lora_r 16 --lora_alpha 32
[2024-08-28 20:32:45,549] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2024-08-28 20:32:48,980] [INFO] [launch.py:139:main] 0 NCCL_ASYNC_ERROR_HANDLING=1
[2024-08-28 20:32:48,980] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]}
[2024-08-28 20:32:48,980] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=8, node_rank=0
[2024-08-28 20:32:48,980] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]})
[2024-08-28 20:32:48,980] [INFO] [launch.py:164:main] dist_world_size=8
[2024-08-28 20:32:48,980] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
[2024-08-28 20:32:48,981] [INFO] [launch.py:256:main] process 3726732 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=0', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Meta-Llama-3-8B', '--data_path', '../data/stanford_alpaca/oasst1_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_Meta-Llama-3-8B_oasst1_tuned', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1', '--use_peft', 'False', '--lora_r', '16', '--lora_alpha', '32']
[2024-08-28 20:32:48,981] [INFO] [launch.py:256:main] process 3726733 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=1', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Meta-Llama-3-8B', '--data_path', '../data/stanford_alpaca/oasst1_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_Meta-Llama-3-8B_oasst1_tuned', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1', '--use_peft', 'False', '--lora_r', '16', '--lora_alpha', '32']
[2024-08-28 20:32:48,982] [INFO] [launch.py:256:main] process 3726734 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=2', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Meta-Llama-3-8B', '--data_path', '../data/stanford_alpaca/oasst1_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_Meta-Llama-3-8B_oasst1_tuned', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1', '--use_peft', 'False', '--lora_r', '16', '--lora_alpha', '32']
[2024-08-28 20:32:48,983] [INFO] [launch.py:256:main] process 3726735 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=3', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Meta-Llama-3-8B', '--data_path', '../data/stanford_alpaca/oasst1_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_Meta-Llama-3-8B_oasst1_tuned', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1', '--use_peft', 'False', '--lora_r', '16', '--lora_alpha', '32']
[2024-08-28 20:32:48,983] [INFO] [launch.py:256:main] process 3726736 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=4', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Meta-Llama-3-8B', '--data_path', '../data/stanford_alpaca/oasst1_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_Meta-Llama-3-8B_oasst1_tuned', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1', '--use_peft', 'False', '--lora_r', '16', '--lora_alpha', '32']
[2024-08-28 20:32:48,984] [INFO] [launch.py:256:main] process 3726737 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=5', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Meta-Llama-3-8B', '--data_path', '../data/stanford_alpaca/oasst1_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_Meta-Llama-3-8B_oasst1_tuned', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1', '--use_peft', 'False', '--lora_r', '16', '--lora_alpha', '32']
[2024-08-28 20:32:48,984] [INFO] [launch.py:256:main] process 3726738 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=6', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Meta-Llama-3-8B', '--data_path', '../data/stanford_alpaca/oasst1_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_Meta-Llama-3-8B_oasst1_tuned', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1', '--use_peft', 'False', '--lora_r', '16', '--lora_alpha', '32']
[2024-08-28 20:32:48,985] [INFO] [launch.py:256:main] process 3726739 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=7', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Meta-Llama-3-8B', '--data_path', '../data/stanford_alpaca/oasst1_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_Meta-Llama-3-8B_oasst1_tuned', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1', '--use_peft', 'False', '--lora_r', '16', '--lora_alpha', '32']
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
2024-08-28 20:33:03.719568: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-08-28 20:33:03.719560: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-08-28 20:33:03.719567: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-08-28 20:33:03.719563: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-08-28 20:33:03.719565: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-08-28 20:33:03.719559: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-08-28 20:33:03.719571: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-08-28 20:33:03.719564: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-08-28 20:33:03.922377: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-08-28 20:33:03.922382: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-08-28 20:33:03.922379: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-08-28 20:33:03.922388: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-08-28 20:33:03.922376: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-08-28 20:33:03.922392: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-08-28 20:33:03.922396: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-08-28 20:33:03.922403: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-08-28 20:33:03.975271: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-08-28 20:33:03.975273: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-08-28 20:33:03.975271: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-08-28 20:33:03.975276: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-08-28 20:33:03.975283: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-08-28 20:33:03.975285: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-08-28 20:33:03.975286: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-08-28 20:33:03.975293: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-08-28 20:33:04.412644: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-08-28 20:33:04.412643: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-08-28 20:33:04.412642: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-08-28 20:33:04.412654: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-08-28 20:33:04.412652: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-08-28 20:33:04.412659: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-08-28 20:33:04.412658: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-08-28 20:33:04.412667: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-08-28 20:33:09.605671: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-08-28 20:33:09.605669: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-08-28 20:33:09.605680: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-08-28 20:33:09.605697: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-08-28 20:33:09.605728: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-08-28 20:33:09.605731: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-08-28 20:33:09.605778: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-08-28 20:33:09.605795: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
[2024-08-28 20:33:27,403] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-28 20:33:27,420] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-28 20:33:27,422] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-28 20:33:27,448] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-28 20:33:27,466] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-28 20:33:27,490] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-28 20:33:27,492] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-28 20:33:27,498] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.[93m [WARNING] [0m async_io: please install the libaio-dev package with apt

[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2024-08-28 20:33:28,160] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-08-28 20:33:28,160] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-08-28 20:33:28,189] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-08-28 20:33:28,189] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-08-28 20:33:28,195] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-08-28 20:33:28,201] [INFO] [comm.py:637:init_distributed] cdb=None
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[2024-08-28 20:33:28,225] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-08-28 20:33:28,227] [INFO] [comm.py:637:init_distributed] cdb=None
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[2024-08-28 20:33:28,268] [INFO] [comm.py:637:init_distributed] cdb=None
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 248.29it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 248.68it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 247.26it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 246.72it/s]
Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 245.06it/s]



Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 245.31it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 243.37it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 243.80it/s]


[2024-08-28 20:33:39,274] [INFO] [partition_parameters.py:345:__exit__] finished initializing model - num_params = 291, num_elems = 8.03B
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:01,  2.02it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:01,  1.96it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:01,  1.96it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:01,  1.98it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:01,  1.94it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:01,  1.96it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:01,  1.96it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:10<00:30, 10.22s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:10<00:12,  6.18s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:10<00:12,  6.19s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:10<00:12,  6.19s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:10<00:12,  6.19s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:10<00:12,  6.20s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:10<00:12,  6.21s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:10<00:12,  6.21s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:19<00:07,  7.50s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:19<00:07,  7.50s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:19<00:07,  7.51s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:19<00:07,  7.51s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:19<00:07,  7.51s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:19<00:07,  7.51s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:19<00:07,  7.53s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:20<00:20, 10.03s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:20<00:00,  5.00s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:20<00:00,  5.23s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:20<00:00,  5.00s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:20<00:00,  5.23s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:20<00:00,  5.00s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:20<00:00,  5.23s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:20<00:00,  5.00s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:20<00:00,  5.23s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:20<00:00,  5.00s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:20<00:00,  5.23s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:20<00:00,  5.00s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:20<00:00,  5.23s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:20<00:00,  5.01s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:20<00:00,  5.24s/it]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 1903.91it/s]
Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 2541.62it/s]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 1590.41it/s]
Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 1621.77it/s]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 1595.25it/s]
Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 1590.86it/s]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 1574.59it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:29<00:09,  9.77s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:31<00:00,  6.81s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:31<00:00,  7.96s/it]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 1543.02it/s]
[2024-08-28 20:34:11,368] [INFO] [partition_parameters.py:345:__exit__] finished initializing model - num_params = 582, num_elems = 16.06B
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.09s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.09s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.10s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.10s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.10s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.10s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.10s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:09,  3.10s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:07<00:07,  3.50s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:07<00:07,  3.50s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:07<00:07,  3.51s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:07<00:07,  3.51s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:07<00:07,  3.51s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:07<00:07,  3.52s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:07<00:07,  3.52s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:09<00:02,  2.93s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:09<00:02,  2.93s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:09<00:02,  2.93s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:09<00:02,  2.93s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:09<00:02,  2.93s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:09<00:02,  2.94s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:09<00:02,  2.96s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:06,  3.04s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.10s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.57s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.11s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.11s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.57s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.57s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.11s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.57s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.11s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.57s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.10s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.57s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.12s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.58s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:09<00:03,  3.02s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.17s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.49s/it]
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
[rank4]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[rank1]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[rank5]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[rank6]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[rank3]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[rank2]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[rank0]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[rank7]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...

Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...




Detected CUDA files, patching ldflags
Emitting ninja build file /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121/fused_adam/build.ninja...
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/cpp_extension.py:1967: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Loading extension module fused_adam...
Loading extension module fused_adam...
Loading extension module fused_adam...Loading extension module fused_adam...Loading extension module fused_adam...


Loading extension module fused_adam...Loading extension module fused_adam...

Time to load fused_adam op: 3.0004711151123047 secondsTime to load fused_adam op: 3.000310182571411 secondsTime to load fused_adam op: 3.000387191772461 secondsTime to load fused_adam op: 3.000565528869629 secondsTime to load fused_adam op: 3.00048565864563 secondsTime to load fused_adam op: 3.000541925430298 secondsTime to load fused_adam op: 3.0004401206970215 secondsTime to load fused_adam op: 3.000469446182251 seconds







Parameter Offload: Total persistent parameters: 268701696 in 129 params
  0%|          | 0/189 [00:00<?, ?it/s]/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
  1%|          | 1/189 [00:08<25:44,  8.22s/it]                                               {'loss': 1.2257, 'grad_norm': 5.958555227789473, 'learning_rate': 0.0, 'epoch': 0.02}
  1%|          | 1/189 [00:08<25:44,  8.22s/it]  1%|          | 2/189 [00:09<12:33,  4.03s/it]                                               {'loss': 1.161, 'grad_norm': 6.1068596059905, 'learning_rate': 7.73705614469083e-07, 'epoch': 0.03}
  1%|          | 2/189 [00:09<12:33,  4.03s/it]  2%|▏         | 3/189 [00:10<08:08,  2.63s/it]                                               {'loss': 1.148, 'grad_norm': 6.182047846654947, 'learning_rate': 1.2262943855309167e-06, 'epoch': 0.05}
  2%|▏         | 3/189 [00:10<08:08,  2.63s/it]  2%|▏         | 4/189 [00:11<06:05,  1.97s/it]                                               {'loss': 1.1176, 'grad_norm': 5.833951257679097, 'learning_rate': 1.547411228938166e-06, 'epoch': 0.06}
  2%|▏         | 4/189 [00:11<06:05,  1.97s/it]  3%|▎         | 5/189 [00:12<04:55,  1.60s/it]                                               {'loss': 1.0892, 'grad_norm': 4.90434294188025, 'learning_rate': 1.796488803407854e-06, 'epoch': 0.08}
  3%|▎         | 5/189 [00:12<04:55,  1.60s/it]  3%|▎         | 6/189 [00:13<04:12,  1.38s/it]                                               {'loss': 1.1574, 'grad_norm': 5.295618382744063, 'learning_rate': 1.9999999999999995e-06, 'epoch': 0.1}
  3%|▎         | 6/189 [00:13<04:12,  1.38s/it]  4%|▎         | 7/189 [00:14<03:46,  1.24s/it]                                               {'loss': 1.0754, 'grad_norm': 5.012140685791665, 'learning_rate': 2e-06, 'epoch': 0.11}
  4%|▎         | 7/189 [00:14<03:46,  1.24s/it]  4%|▍         | 8/189 [00:15<03:27,  1.15s/it]                                               {'loss': 1.1026, 'grad_norm': 3.9758879095990665, 'learning_rate': 1.989071038251366e-06, 'epoch': 0.13}
  4%|▍         | 8/189 [00:15<03:27,  1.15s/it]  5%|▍         | 9/189 [00:15<03:14,  1.08s/it]                                               {'loss': 1.119, 'grad_norm': 4.018706217365185, 'learning_rate': 1.978142076502732e-06, 'epoch': 0.14}
  5%|▍         | 9/189 [00:15<03:14,  1.08s/it]  5%|▌         | 10/189 [00:16<03:06,  1.04s/it]                                                {'loss': 1.0538, 'grad_norm': 3.5438479583725413, 'learning_rate': 1.967213114754098e-06, 'epoch': 0.16}
  5%|▌         | 10/189 [00:16<03:06,  1.04s/it]  6%|▌         | 11/189 [00:17<03:00,  1.01s/it]                                                {'loss': 1.1247, 'grad_norm': 3.748011993396597, 'learning_rate': 1.9562841530054644e-06, 'epoch': 0.17}
  6%|▌         | 11/189 [00:17<03:00,  1.01s/it]  6%|▋         | 12/189 [00:18<02:56,  1.01it/s]                                                {'loss': 1.1847, 'grad_norm': 3.8062879532452363, 'learning_rate': 1.9453551912568304e-06, 'epoch': 0.19}
  6%|▋         | 12/189 [00:18<02:56,  1.01it/s]  7%|▋         | 13/189 [00:19<02:53,  1.02it/s]                                                {'loss': 1.099, 'grad_norm': 3.712989066845026, 'learning_rate': 1.9344262295081967e-06, 'epoch': 0.21}
  7%|▋         | 13/189 [00:19<02:53,  1.02it/s]  7%|▋         | 14/189 [00:20<02:50,  1.03it/s]                                                {'loss': 1.0609, 'grad_norm': 3.673541295894069, 'learning_rate': 1.9234972677595626e-06, 'epoch': 0.22}
  7%|▋         | 14/189 [00:20<02:50,  1.03it/s]  8%|▊         | 15/189 [00:21<02:47,  1.04it/s]                                                {'loss': 1.1422, 'grad_norm': 3.6857681077643587, 'learning_rate': 1.912568306010929e-06, 'epoch': 0.24}
  8%|▊         | 15/189 [00:21<02:47,  1.04it/s]  8%|▊         | 16/189 [00:22<02:46,  1.04it/s]                                                {'loss': 1.0459, 'grad_norm': 3.8269785960967777, 'learning_rate': 1.901639344262295e-06, 'epoch': 0.25}
  8%|▊         | 16/189 [00:22<02:46,  1.04it/s]  9%|▉         | 17/189 [00:23<02:44,  1.05it/s]                                                {'loss': 1.1259, 'grad_norm': 3.422345892343688, 'learning_rate': 1.8907103825136612e-06, 'epoch': 0.27}
  9%|▉         | 17/189 [00:23<02:44,  1.05it/s] 10%|▉         | 18/189 [00:24<02:43,  1.05it/s]                                                {'loss': 1.229, 'grad_norm': 3.3833714106164536, 'learning_rate': 1.8797814207650274e-06, 'epoch': 0.29}
 10%|▉         | 18/189 [00:24<02:43,  1.05it/s] 10%|█         | 19/189 [00:25<02:41,  1.06it/s]                                                {'loss': 1.1443, 'grad_norm': 3.3188010011405837, 'learning_rate': 1.8688524590163935e-06, 'epoch': 0.3}
 10%|█         | 19/189 [00:25<02:41,  1.06it/s] 11%|█         | 20/189 [00:26<02:39,  1.06it/s]                                                {'loss': 1.181, 'grad_norm': 3.5248145835987694, 'learning_rate': 1.8579234972677596e-06, 'epoch': 0.32}
 11%|█         | 20/189 [00:26<02:39,  1.06it/s] 11%|█         | 21/189 [00:27<02:38,  1.06it/s]                                                {'loss': 0.9915, 'grad_norm': 3.2030278384915976, 'learning_rate': 1.8469945355191256e-06, 'epoch': 0.33}
 11%|█         | 21/189 [00:27<02:38,  1.06it/s] 12%|█▏        | 22/189 [00:28<02:38,  1.05it/s]                                                {'loss': 1.171, 'grad_norm': 3.4868026475939726, 'learning_rate': 1.8360655737704917e-06, 'epoch': 0.35}
 12%|█▏        | 22/189 [00:28<02:38,  1.05it/s] 12%|█▏        | 23/189 [00:29<02:37,  1.05it/s]                                                {'loss': 1.0126, 'grad_norm': 3.399872572488937, 'learning_rate': 1.8251366120218578e-06, 'epoch': 0.37}
 12%|█▏        | 23/189 [00:29<02:37,  1.05it/s] 13%|█▎        | 24/189 [00:30<02:37,  1.05it/s]                                                {'loss': 0.9685, 'grad_norm': 3.4230043422019563, 'learning_rate': 1.814207650273224e-06, 'epoch': 0.38}
 13%|█▎        | 24/189 [00:30<02:37,  1.05it/s] 13%|█▎        | 25/189 [00:31<02:35,  1.05it/s]                                                {'loss': 1.0978, 'grad_norm': 3.177325364657064, 'learning_rate': 1.80327868852459e-06, 'epoch': 0.4}
 13%|█▎        | 25/189 [00:31<02:35,  1.05it/s] 14%|█▍        | 26/189 [00:32<02:34,  1.05it/s]                                                {'loss': 1.082, 'grad_norm': 3.4421868840047014, 'learning_rate': 1.7923497267759562e-06, 'epoch': 0.41}
 14%|█▍        | 26/189 [00:32<02:34,  1.05it/s] 14%|█▍        | 27/189 [00:33<02:33,  1.06it/s]                                                {'loss': 0.9708, 'grad_norm': 3.1724566262944904, 'learning_rate': 1.7814207650273224e-06, 'epoch': 0.43}
 14%|█▍        | 27/189 [00:33<02:33,  1.06it/s] 15%|█▍        | 28/189 [00:34<02:33,  1.05it/s]                                                {'loss': 1.0342, 'grad_norm': 3.196182527572884, 'learning_rate': 1.7704918032786885e-06, 'epoch': 0.44}
 15%|█▍        | 28/189 [00:34<02:33,  1.05it/s] 15%|█▌        | 29/189 [00:34<02:31,  1.05it/s]                                                {'loss': 0.9321, 'grad_norm': 3.190872608735071, 'learning_rate': 1.7595628415300544e-06, 'epoch': 0.46}
 15%|█▌        | 29/189 [00:34<02:31,  1.05it/s] 16%|█▌        | 30/189 [00:35<02:30,  1.06it/s]                                                {'loss': 1.1615, 'grad_norm': 3.4957284377485403, 'learning_rate': 1.7486338797814206e-06, 'epoch': 0.48}
 16%|█▌        | 30/189 [00:35<02:30,  1.06it/s] 16%|█▋        | 31/189 [00:36<02:29,  1.06it/s]                                                {'loss': 1.0455, 'grad_norm': 3.6324133650268062, 'learning_rate': 1.7377049180327867e-06, 'epoch': 0.49}
 16%|█▋        | 31/189 [00:36<02:29,  1.06it/s] 17%|█▋        | 32/189 [00:37<02:28,  1.06it/s]                                                {'loss': 0.9116, 'grad_norm': 3.2237555863706384, 'learning_rate': 1.7267759562841528e-06, 'epoch': 0.51}
 17%|█▋        | 32/189 [00:37<02:28,  1.06it/s] 17%|█▋        | 33/189 [00:38<02:27,  1.06it/s]                                                {'loss': 1.003, 'grad_norm': 3.8870682695930014, 'learning_rate': 1.715846994535519e-06, 'epoch': 0.52}
 17%|█▋        | 33/189 [00:38<02:27,  1.06it/s] 18%|█▊        | 34/189 [00:39<02:27,  1.05it/s]                                                {'loss': 0.8544, 'grad_norm': 3.1541856626488074, 'learning_rate': 1.704918032786885e-06, 'epoch': 0.54}
 18%|█▊        | 34/189 [00:39<02:27,  1.05it/s] 19%|█▊        | 35/189 [00:40<02:26,  1.05it/s]                                                {'loss': 0.9744, 'grad_norm': 3.4066324414041866, 'learning_rate': 1.6939890710382514e-06, 'epoch': 0.56}
 19%|█▊        | 35/189 [00:40<02:26,  1.05it/s] 19%|█▉        | 36/189 [00:41<02:25,  1.05it/s]                                                {'loss': 0.9137, 'grad_norm': 3.1991198373064553, 'learning_rate': 1.6830601092896176e-06, 'epoch': 0.57}
 19%|█▉        | 36/189 [00:41<02:25,  1.05it/s] 20%|█▉        | 37/189 [00:42<02:24,  1.05it/s]                                                {'loss': 1.0434, 'grad_norm': 3.405875753203006, 'learning_rate': 1.6721311475409837e-06, 'epoch': 0.59}
 20%|█▉        | 37/189 [00:42<02:24,  1.05it/s] 20%|██        | 38/189 [00:43<02:23,  1.05it/s]                                                {'loss': 1.0678, 'grad_norm': 3.6215800184734603, 'learning_rate': 1.6612021857923496e-06, 'epoch': 0.6}
 20%|██        | 38/189 [00:43<02:23,  1.05it/s] 21%|██        | 39/189 [00:44<02:23,  1.05it/s]                                                {'loss': 1.0039, 'grad_norm': 3.6258183760295486, 'learning_rate': 1.6502732240437158e-06, 'epoch': 0.62}
 21%|██        | 39/189 [00:44<02:23,  1.05it/s] 21%|██        | 40/189 [00:45<02:21,  1.05it/s]                                                {'loss': 1.0189, 'grad_norm': 3.219880972446134, 'learning_rate': 1.6393442622950819e-06, 'epoch': 0.63}
 21%|██        | 40/189 [00:45<02:21,  1.05it/s] 22%|██▏       | 41/189 [00:46<02:20,  1.05it/s]                                                {'loss': 0.9871, 'grad_norm': 3.2930507369191115, 'learning_rate': 1.628415300546448e-06, 'epoch': 0.65}
 22%|██▏       | 41/189 [00:46<02:20,  1.05it/s] 22%|██▏       | 42/189 [00:47<02:20,  1.05it/s]                                                {'loss': 0.9105, 'grad_norm': 3.3604443888289133, 'learning_rate': 1.6174863387978142e-06, 'epoch': 0.67}
 22%|██▏       | 42/189 [00:47<02:20,  1.05it/s] 23%|██▎       | 43/189 [00:48<02:19,  1.05it/s]                                                {'loss': 0.9771, 'grad_norm': 3.366934671440176, 'learning_rate': 1.6065573770491803e-06, 'epoch': 0.68}
 23%|██▎       | 43/189 [00:48<02:19,  1.05it/s] 23%|██▎       | 44/189 [00:49<02:18,  1.05it/s]                                                {'loss': 1.0138, 'grad_norm': 3.257681710722393, 'learning_rate': 1.5956284153005464e-06, 'epoch': 0.7}
 23%|██▎       | 44/189 [00:49<02:18,  1.05it/s] 24%|██▍       | 45/189 [00:50<02:17,  1.05it/s]                                                {'loss': 1.011, 'grad_norm': 3.4352885219573337, 'learning_rate': 1.5846994535519126e-06, 'epoch': 0.71}
 24%|██▍       | 45/189 [00:50<02:17,  1.05it/s] 24%|██▍       | 46/189 [00:51<02:16,  1.05it/s]                                                {'loss': 1.096, 'grad_norm': 4.134808506910737, 'learning_rate': 1.5737704918032787e-06, 'epoch': 0.73}
 24%|██▍       | 46/189 [00:51<02:16,  1.05it/s] 25%|██▍       | 47/189 [00:52<02:15,  1.05it/s]                                                {'loss': 0.9565, 'grad_norm': 3.4385873853549613, 'learning_rate': 1.5628415300546446e-06, 'epoch': 0.75}
 25%|██▍       | 47/189 [00:52<02:15,  1.05it/s] 25%|██▌       | 48/189 [00:53<02:14,  1.05it/s]                                                {'loss': 0.9007, 'grad_norm': 3.297395477552124, 'learning_rate': 1.5519125683060107e-06, 'epoch': 0.76}
 25%|██▌       | 48/189 [00:53<02:14,  1.05it/s] 26%|██▌       | 49/189 [00:54<02:14,  1.04it/s]                                                {'loss': 1.0699, 'grad_norm': 3.303533540163765, 'learning_rate': 1.5409836065573769e-06, 'epoch': 0.78}
 26%|██▌       | 49/189 [00:54<02:14,  1.04it/s] 26%|██▋       | 50/189 [00:54<02:13,  1.04it/s]                                                {'loss': 0.9608, 'grad_norm': 3.2571411474181144, 'learning_rate': 1.530054644808743e-06, 'epoch': 0.79}
 26%|██▋       | 50/189 [00:54<02:13,  1.04it/s] 27%|██▋       | 51/189 [00:55<02:12,  1.04it/s]                                                {'loss': 0.9499, 'grad_norm': 3.1442335880135803, 'learning_rate': 1.5191256830601091e-06, 'epoch': 0.81}
 27%|██▋       | 51/189 [00:55<02:12,  1.04it/s] 28%|██▊       | 52/189 [00:56<02:11,  1.04it/s]                                                {'loss': 0.9271, 'grad_norm': 3.6842815974012066, 'learning_rate': 1.5081967213114753e-06, 'epoch': 0.83}
 28%|██▊       | 52/189 [00:56<02:11,  1.04it/s] 28%|██▊       | 53/189 [00:57<02:10,  1.04it/s]                                                {'loss': 0.9608, 'grad_norm': 3.4120962711687723, 'learning_rate': 1.4972677595628416e-06, 'epoch': 0.84}
 28%|██▊       | 53/189 [00:57<02:10,  1.04it/s] 29%|██▊       | 54/189 [00:58<02:08,  1.05it/s]                                                {'loss': 0.9668, 'grad_norm': 3.2891575555623036, 'learning_rate': 1.4863387978142078e-06, 'epoch': 0.86}
 29%|██▊       | 54/189 [00:58<02:08,  1.05it/s] 29%|██▉       | 55/189 [00:59<02:07,  1.05it/s]                                                {'loss': 0.8782, 'grad_norm': 3.480940729030933, 'learning_rate': 1.4754098360655739e-06, 'epoch': 0.87}
 29%|██▉       | 55/189 [00:59<02:07,  1.05it/s] 30%|██▉       | 56/189 [01:00<02:06,  1.05it/s]                                                {'loss': 0.923, 'grad_norm': 3.636846286962538, 'learning_rate': 1.4644808743169398e-06, 'epoch': 0.89}
 30%|██▉       | 56/189 [01:00<02:06,  1.05it/s] 30%|███       | 57/189 [01:01<02:06,  1.04it/s]                                                {'loss': 0.8626, 'grad_norm': 3.3720991081906067, 'learning_rate': 1.453551912568306e-06, 'epoch': 0.9}
 30%|███       | 57/189 [01:01<02:06,  1.04it/s] 31%|███       | 58/189 [01:02<02:05,  1.05it/s]                                                {'loss': 0.892, 'grad_norm': 3.779977869448807, 'learning_rate': 1.442622950819672e-06, 'epoch': 0.92}
 31%|███       | 58/189 [01:02<02:05,  1.05it/s] 31%|███       | 59/189 [01:03<02:03,  1.05it/s]                                                {'loss': 1.0536, 'grad_norm': 3.790138758100764, 'learning_rate': 1.4316939890710382e-06, 'epoch': 0.94}
 31%|███       | 59/189 [01:03<02:03,  1.05it/s] 32%|███▏      | 60/189 [01:04<02:02,  1.05it/s]                                                {'loss': 0.8891, 'grad_norm': 3.6563290695901225, 'learning_rate': 1.4207650273224043e-06, 'epoch': 0.95}
 32%|███▏      | 60/189 [01:04<02:02,  1.05it/s] 32%|███▏      | 61/189 [01:05<02:02,  1.05it/s]                                                {'loss': 0.9452, 'grad_norm': 3.501876591604689, 'learning_rate': 1.4098360655737705e-06, 'epoch': 0.97}
 32%|███▏      | 61/189 [01:05<02:02,  1.05it/s] 33%|███▎      | 62/189 [01:06<02:00,  1.05it/s]                                                {'loss': 0.8873, 'grad_norm': 3.5765681345242872, 'learning_rate': 1.3989071038251366e-06, 'epoch': 0.98}
 33%|███▎      | 62/189 [01:06<02:00,  1.05it/s] 33%|███▎      | 63/189 [01:07<01:59,  1.06it/s]                                                {'loss': 0.8655, 'grad_norm': 3.672157409548752, 'learning_rate': 1.3879781420765027e-06, 'epoch': 1.0}
 33%|███▎      | 63/189 [01:07<01:59,  1.06it/s] 34%|███▍      | 64/189 [01:08<01:59,  1.05it/s]                                                {'loss': 0.7008, 'grad_norm': 3.855002861228028, 'learning_rate': 1.3770491803278687e-06, 'epoch': 1.02}
 34%|███▍      | 64/189 [01:08<01:59,  1.05it/s] 34%|███▍      | 65/189 [01:09<01:58,  1.05it/s]                                                {'loss': 0.7135, 'grad_norm': 3.971471003228208, 'learning_rate': 1.3661202185792348e-06, 'epoch': 1.03}
 34%|███▍      | 65/189 [01:09<01:58,  1.05it/s] 35%|███▍      | 66/189 [01:10<01:57,  1.05it/s]                                                {'loss': 0.7702, 'grad_norm': 5.3245196055733075, 'learning_rate': 1.355191256830601e-06, 'epoch': 1.05}
 35%|███▍      | 66/189 [01:10<01:57,  1.05it/s] 35%|███▌      | 67/189 [01:11<01:56,  1.05it/s]                                                {'loss': 0.7402, 'grad_norm': 3.928820037920555, 'learning_rate': 1.344262295081967e-06, 'epoch': 1.06}
 35%|███▌      | 67/189 [01:11<01:56,  1.05it/s] 36%|███▌      | 68/189 [01:12<01:55,  1.05it/s]                                                {'loss': 0.76, 'grad_norm': 3.526243989088974, 'learning_rate': 1.3333333333333332e-06, 'epoch': 1.08}
 36%|███▌      | 68/189 [01:12<01:55,  1.05it/s] 37%|███▋      | 69/189 [01:13<01:54,  1.04it/s]                                                {'loss': 0.7696, 'grad_norm': 3.604661792076369, 'learning_rate': 1.3224043715846993e-06, 'epoch': 1.1}
 37%|███▋      | 69/189 [01:13<01:54,  1.04it/s] 37%|███▋      | 70/189 [01:14<01:53,  1.05it/s]                                                {'loss': 0.789, 'grad_norm': 3.775330652337437, 'learning_rate': 1.3114754098360655e-06, 'epoch': 1.11}
 37%|███▋      | 70/189 [01:14<01:53,  1.05it/s] 38%|███▊      | 71/189 [01:14<01:52,  1.05it/s]                                                {'loss': 0.7348, 'grad_norm': 3.686729177725679, 'learning_rate': 1.3005464480874316e-06, 'epoch': 1.13}
 38%|███▊      | 71/189 [01:14<01:52,  1.05it/s] 38%|███▊      | 72/189 [01:15<01:51,  1.05it/s]                                                {'loss': 0.8673, 'grad_norm': 3.6306673720904, 'learning_rate': 1.289617486338798e-06, 'epoch': 1.14}
 38%|███▊      | 72/189 [01:15<01:51,  1.05it/s] 39%|███▊      | 73/189 [01:16<01:50,  1.05it/s]                                                {'loss': 0.7395, 'grad_norm': 3.5108028135931857, 'learning_rate': 1.2786885245901639e-06, 'epoch': 1.16}
 39%|███▊      | 73/189 [01:16<01:50,  1.05it/s] 39%|███▉      | 74/189 [01:17<01:49,  1.05it/s]                                                {'loss': 0.7032, 'grad_norm': 3.193792712753295, 'learning_rate': 1.26775956284153e-06, 'epoch': 1.17}
 39%|███▉      | 74/189 [01:17<01:49,  1.05it/s] 40%|███▉      | 75/189 [01:18<01:48,  1.05it/s]                                                {'loss': 0.824, 'grad_norm': 4.153651889578058, 'learning_rate': 1.2568306010928961e-06, 'epoch': 1.19}
 40%|███▉      | 75/189 [01:18<01:48,  1.05it/s] 40%|████      | 76/189 [01:19<01:47,  1.05it/s]                                                {'loss': 0.6737, 'grad_norm': 3.468091591299723, 'learning_rate': 1.2459016393442623e-06, 'epoch': 1.21}
 40%|████      | 76/189 [01:19<01:47,  1.05it/s] 41%|████      | 77/189 [01:20<01:46,  1.05it/s]                                                {'loss': 0.6678, 'grad_norm': 3.671191302730222, 'learning_rate': 1.2349726775956284e-06, 'epoch': 1.22}
 41%|████      | 77/189 [01:20<01:46,  1.05it/s] 41%|████▏     | 78/189 [01:21<01:45,  1.05it/s]                                                {'loss': 0.7055, 'grad_norm': 3.464370558101683, 'learning_rate': 1.2240437158469945e-06, 'epoch': 1.24}
 41%|████▏     | 78/189 [01:21<01:45,  1.05it/s] 42%|████▏     | 79/189 [01:22<01:44,  1.05it/s]                                                {'loss': 0.7151, 'grad_norm': 3.4554115133122796, 'learning_rate': 1.2131147540983607e-06, 'epoch': 1.25}
 42%|████▏     | 79/189 [01:22<01:44,  1.05it/s] 42%|████▏     | 80/189 [01:23<01:43,  1.05it/s]                                                {'loss': 0.7373, 'grad_norm': 4.023816245499969, 'learning_rate': 1.2021857923497268e-06, 'epoch': 1.27}
 42%|████▏     | 80/189 [01:23<01:43,  1.05it/s] 43%|████▎     | 81/189 [01:24<01:43,  1.05it/s]                                                {'loss': 0.682, 'grad_norm': 4.975190410658499, 'learning_rate': 1.191256830601093e-06, 'epoch': 1.29}
 43%|████▎     | 81/189 [01:24<01:43,  1.05it/s] 43%|████▎     | 82/189 [01:25<01:42,  1.05it/s]                                                {'loss': 0.6998, 'grad_norm': 3.781621662499802, 'learning_rate': 1.1803278688524589e-06, 'epoch': 1.3}
 43%|████▎     | 82/189 [01:25<01:42,  1.05it/s] 44%|████▍     | 83/189 [01:26<01:40,  1.05it/s]                                                {'loss': 0.722, 'grad_norm': 3.607974186059108, 'learning_rate': 1.169398907103825e-06, 'epoch': 1.32}
 44%|████▍     | 83/189 [01:26<01:40,  1.05it/s] 44%|████▍     | 84/189 [01:27<01:40,  1.05it/s]                                                {'loss': 0.676, 'grad_norm': 3.3723652374426263, 'learning_rate': 1.1584699453551911e-06, 'epoch': 1.33}
 44%|████▍     | 84/189 [01:27<01:40,  1.05it/s] 45%|████▍     | 85/189 [01:28<01:39,  1.05it/s]                                                {'loss': 0.7773, 'grad_norm': 3.3769717016376735, 'learning_rate': 1.1475409836065573e-06, 'epoch': 1.35}
 45%|████▍     | 85/189 [01:28<01:39,  1.05it/s] 46%|████▌     | 86/189 [01:29<01:38,  1.05it/s]                                                {'loss': 0.6336, 'grad_norm': 3.2027582120122853, 'learning_rate': 1.1366120218579234e-06, 'epoch': 1.37}
 46%|████▌     | 86/189 [01:29<01:38,  1.05it/s] 46%|████▌     | 87/189 [01:30<01:37,  1.05it/s]                                                {'loss': 0.6568, 'grad_norm': 3.694701003589178, 'learning_rate': 1.1256830601092895e-06, 'epoch': 1.38}
 46%|████▌     | 87/189 [01:30<01:37,  1.05it/s] 47%|████▋     | 88/189 [01:31<01:36,  1.05it/s]                                                {'loss': 0.5153, 'grad_norm': 3.6335125021421097, 'learning_rate': 1.1147540983606557e-06, 'epoch': 1.4}
 47%|████▋     | 88/189 [01:31<01:36,  1.05it/s] 47%|████▋     | 89/189 [01:32<01:35,  1.05it/s]                                                {'loss': 0.7701, 'grad_norm': 3.53325428117531, 'learning_rate': 1.1038251366120218e-06, 'epoch': 1.41}
 47%|████▋     | 89/189 [01:32<01:35,  1.05it/s] 48%|████▊     | 90/189 [01:33<01:34,  1.05it/s]                                                {'loss': 0.6505, 'grad_norm': 3.43288168205464, 'learning_rate': 1.092896174863388e-06, 'epoch': 1.43}
 48%|████▊     | 90/189 [01:33<01:34,  1.05it/s] 48%|████▊     | 91/189 [01:34<01:32,  1.06it/s]                                                {'loss': 0.6363, 'grad_norm': 3.7526183901521053, 'learning_rate': 1.081967213114754e-06, 'epoch': 1.44}
 48%|████▊     | 91/189 [01:34<01:32,  1.06it/s] 49%|████▊     | 92/189 [01:34<01:32,  1.05it/s]                                                {'loss': 0.6299, 'grad_norm': 3.550435364298018, 'learning_rate': 1.0710382513661202e-06, 'epoch': 1.46}
 49%|████▊     | 92/189 [01:34<01:32,  1.05it/s] 49%|████▉     | 93/189 [01:35<01:31,  1.05it/s]                                                {'loss': 0.8913, 'grad_norm': 4.310974921386296, 'learning_rate': 1.0601092896174863e-06, 'epoch': 1.48}
 49%|████▉     | 93/189 [01:35<01:31,  1.05it/s] 50%|████▉     | 94/189 [01:36<01:30,  1.05it/s]                                                {'loss': 0.5957, 'grad_norm': 3.6307009971766413, 'learning_rate': 1.0491803278688525e-06, 'epoch': 1.49}
 50%|████▉     | 94/189 [01:36<01:30,  1.05it/s] 50%|█████     | 95/189 [01:37<01:29,  1.05it/s]                                                {'loss': 0.5034, 'grad_norm': 3.6370950979704273, 'learning_rate': 1.0382513661202186e-06, 'epoch': 1.51}
 50%|█████     | 95/189 [01:37<01:29,  1.05it/s] 51%|█████     | 96/189 [01:38<01:28,  1.05it/s]                                                {'loss': 0.6686, 'grad_norm': 3.6522796972958247, 'learning_rate': 1.0273224043715847e-06, 'epoch': 1.52}
 51%|█████     | 96/189 [01:38<01:28,  1.05it/s] 51%|█████▏    | 97/189 [01:39<01:27,  1.05it/s]                                                {'loss': 0.65, 'grad_norm': 4.054831007803174, 'learning_rate': 1.0163934426229509e-06, 'epoch': 1.54}
 51%|█████▏    | 97/189 [01:39<01:27,  1.05it/s] 52%|█████▏    | 98/189 [01:40<01:26,  1.05it/s]                                                {'loss': 0.5065, 'grad_norm': 3.666837010313715, 'learning_rate': 1.005464480874317e-06, 'epoch': 1.56}
 52%|█████▏    | 98/189 [01:40<01:26,  1.05it/s] 52%|█████▏    | 99/189 [01:41<01:24,  1.06it/s]                                                {'loss': 0.7483, 'grad_norm': 4.2016582793206005, 'learning_rate': 9.94535519125683e-07, 'epoch': 1.57}
 52%|█████▏    | 99/189 [01:41<01:24,  1.06it/s] 53%|█████▎    | 100/189 [01:42<01:24,  1.06it/s]                                                 {'loss': 0.7172, 'grad_norm': 4.360800487276564, 'learning_rate': 9.83606557377049e-07, 'epoch': 1.59}
 53%|█████▎    | 100/189 [01:42<01:24,  1.06it/s] 53%|█████▎    | 101/189 [01:43<01:23,  1.06it/s]                                                 {'loss': 0.5598, 'grad_norm': 3.2683525675230505, 'learning_rate': 9.726775956284152e-07, 'epoch': 1.6}
 53%|█████▎    | 101/189 [01:43<01:23,  1.06it/s] 54%|█████▍    | 102/189 [01:44<01:22,  1.05it/s]                                                 {'loss': 0.6214, 'grad_norm': 3.9608600201989073, 'learning_rate': 9.617486338797813e-07, 'epoch': 1.62}
 54%|█████▍    | 102/189 [01:44<01:22,  1.05it/s] 54%|█████▍    | 103/189 [01:45<01:21,  1.05it/s]                                                 {'loss': 0.6321, 'grad_norm': 3.6439705657518195, 'learning_rate': 9.508196721311474e-07, 'epoch': 1.63}
 54%|█████▍    | 103/189 [01:45<01:21,  1.05it/s] 55%|█████▌    | 104/189 [01:46<01:20,  1.05it/s]                                                 {'loss': 0.6245, 'grad_norm': 4.909045039017779, 'learning_rate': 9.398907103825137e-07, 'epoch': 1.65}
 55%|█████▌    | 104/189 [01:46<01:20,  1.05it/s] 56%|█████▌    | 105/189 [01:47<01:19,  1.05it/s]                                                 {'loss': 0.6172, 'grad_norm': 4.048613474533858, 'learning_rate': 9.289617486338798e-07, 'epoch': 1.67}
 56%|█████▌    | 105/189 [01:47<01:19,  1.05it/s] 56%|█████▌    | 106/189 [01:48<01:18,  1.06it/s]                                                 {'loss': 0.6509, 'grad_norm': 4.121535959660803, 'learning_rate': 9.180327868852458e-07, 'epoch': 1.68}
 56%|█████▌    | 106/189 [01:48<01:18,  1.06it/s] 57%|█████▋    | 107/189 [01:49<01:17,  1.06it/s]                                                 {'loss': 0.6207, 'grad_norm': 3.8421728272138, 'learning_rate': 9.07103825136612e-07, 'epoch': 1.7}
 57%|█████▋    | 107/189 [01:49<01:17,  1.06it/s] 57%|█████▋    | 108/189 [01:50<01:16,  1.06it/s]                                                 {'loss': 0.649, 'grad_norm': 3.613059842393825, 'learning_rate': 8.961748633879781e-07, 'epoch': 1.71}
 57%|█████▋    | 108/189 [01:50<01:16,  1.06it/s] 58%|█████▊    | 109/189 [01:51<01:16,  1.05it/s]                                                 {'loss': 0.651, 'grad_norm': 3.963942677518894, 'learning_rate': 8.852459016393443e-07, 'epoch': 1.73}
 58%|█████▊    | 109/189 [01:51<01:16,  1.05it/s] 58%|█████▊    | 110/189 [01:52<01:14,  1.06it/s]                                                 {'loss': 0.5837, 'grad_norm': 3.8410177546413284, 'learning_rate': 8.743169398907103e-07, 'epoch': 1.75}
 58%|█████▊    | 110/189 [01:52<01:14,  1.06it/s] 59%|█████▊    | 111/189 [01:52<01:13,  1.06it/s]                                                 {'loss': 0.5961, 'grad_norm': 3.971099085607011, 'learning_rate': 8.633879781420764e-07, 'epoch': 1.76}
 59%|█████▊    | 111/189 [01:52<01:13,  1.06it/s] 59%|█████▉    | 112/189 [01:53<01:13,  1.05it/s]                                                 {'loss': 0.5854, 'grad_norm': 3.771758865365388, 'learning_rate': 8.524590163934425e-07, 'epoch': 1.78}
 59%|█████▉    | 112/189 [01:53<01:13,  1.05it/s] 60%|█████▉    | 113/189 [01:54<01:12,  1.05it/s]                                                 {'loss': 0.5534, 'grad_norm': 4.3572922771923785, 'learning_rate': 8.415300546448088e-07, 'epoch': 1.79}
 60%|█████▉    | 113/189 [01:54<01:12,  1.05it/s] 60%|██████    | 114/189 [01:55<01:11,  1.05it/s]                                                 {'loss': 0.7666, 'grad_norm': 3.6418910320496507, 'learning_rate': 8.306010928961748e-07, 'epoch': 1.81}
 60%|██████    | 114/189 [01:55<01:11,  1.05it/s] 61%|██████    | 115/189 [01:56<01:10,  1.05it/s]                                                 {'loss': 0.7037, 'grad_norm': 3.597554538920193, 'learning_rate': 8.196721311475409e-07, 'epoch': 1.83}
 61%|██████    | 115/189 [01:56<01:10,  1.05it/s] 61%|██████▏   | 116/189 [01:57<01:09,  1.05it/s]                                                 {'loss': 0.5828, 'grad_norm': 3.7307983394123165, 'learning_rate': 8.087431693989071e-07, 'epoch': 1.84}
 61%|██████▏   | 116/189 [01:57<01:09,  1.05it/s] 62%|██████▏   | 117/189 [01:58<01:08,  1.05it/s]                                                 {'loss': 0.6755, 'grad_norm': 4.951159521590791, 'learning_rate': 7.978142076502732e-07, 'epoch': 1.86}
 62%|██████▏   | 117/189 [01:58<01:08,  1.05it/s] 62%|██████▏   | 118/189 [01:59<01:07,  1.05it/s]                                                 {'loss': 0.6416, 'grad_norm': 3.706941521603376, 'learning_rate': 7.868852459016393e-07, 'epoch': 1.87}
 62%|██████▏   | 118/189 [01:59<01:07,  1.05it/s] 63%|██████▎   | 119/189 [02:00<01:06,  1.05it/s]                                                 {'loss': 0.587, 'grad_norm': 3.4445206620292352, 'learning_rate': 7.759562841530054e-07, 'epoch': 1.89}
 63%|██████▎   | 119/189 [02:00<01:06,  1.05it/s] 63%|██████▎   | 120/189 [02:01<01:05,  1.05it/s]                                                 {'loss': 0.5369, 'grad_norm': 3.811701772866531, 'learning_rate': 7.650273224043715e-07, 'epoch': 1.9}
 63%|██████▎   | 120/189 [02:01<01:05,  1.05it/s] 64%|██████▍   | 121/189 [02:02<01:05,  1.05it/s]                                                 {'loss': 0.6781, 'grad_norm': 3.9322970525446257, 'learning_rate': 7.540983606557376e-07, 'epoch': 1.92}
 64%|██████▍   | 121/189 [02:02<01:05,  1.05it/s] 65%|██████▍   | 122/189 [02:03<01:04,  1.05it/s]                                                 {'loss': 0.5512, 'grad_norm': 3.7586550757106774, 'learning_rate': 7.431693989071039e-07, 'epoch': 1.94}
 65%|██████▍   | 122/189 [02:03<01:04,  1.05it/s] 65%|██████▌   | 123/189 [02:04<01:02,  1.05it/s]                                                 {'loss': 0.5307, 'grad_norm': 3.6838654986748294, 'learning_rate': 7.322404371584699e-07, 'epoch': 1.95}
 65%|██████▌   | 123/189 [02:04<01:02,  1.05it/s] 66%|██████▌   | 124/189 [02:05<01:02,  1.05it/s]                                                 {'loss': 0.5515, 'grad_norm': 3.6840066378356466, 'learning_rate': 7.21311475409836e-07, 'epoch': 1.97}
 66%|██████▌   | 124/189 [02:05<01:02,  1.05it/s] 66%|██████▌   | 125/189 [02:06<01:01,  1.05it/s]                                                 {'loss': 0.6185, 'grad_norm': 4.190020043958717, 'learning_rate': 7.103825136612022e-07, 'epoch': 1.98}
 66%|██████▌   | 125/189 [02:06<01:01,  1.05it/s] 67%|██████▋   | 126/189 [02:07<01:00,  1.05it/s]                                                 {'loss': 0.5812, 'grad_norm': 3.937032095121301, 'learning_rate': 6.994535519125683e-07, 'epoch': 2.0}
 67%|██████▋   | 126/189 [02:07<01:00,  1.05it/s] 67%|██████▋   | 127/189 [02:08<00:59,  1.05it/s]                                                 {'loss': 0.4989, 'grad_norm': 3.778019657088508, 'learning_rate': 6.885245901639343e-07, 'epoch': 2.02}
 67%|██████▋   | 127/189 [02:08<00:59,  1.05it/s] 68%|██████▊   | 128/189 [02:09<00:58,  1.05it/s]                                                 {'loss': 0.5923, 'grad_norm': 4.147173979635817, 'learning_rate': 6.775956284153005e-07, 'epoch': 2.03}
 68%|██████▊   | 128/189 [02:09<00:58,  1.05it/s] 68%|██████▊   | 129/189 [02:10<00:57,  1.05it/s]                                                 {'loss': 0.5238, 'grad_norm': 3.518703109459632, 'learning_rate': 6.666666666666666e-07, 'epoch': 2.05}
 68%|██████▊   | 129/189 [02:10<00:57,  1.05it/s] 69%|██████▉   | 130/189 [02:11<00:56,  1.05it/s]                                                 {'loss': 0.5288, 'grad_norm': 4.0338251575867, 'learning_rate': 6.557377049180327e-07, 'epoch': 2.06}
 69%|██████▉   | 130/189 [02:11<00:56,  1.05it/s] 69%|██████▉   | 131/189 [02:12<00:55,  1.05it/s]                                                 {'loss': 0.4909, 'grad_norm': 4.606352713777246, 'learning_rate': 6.44808743169399e-07, 'epoch': 2.08}
 69%|██████▉   | 131/189 [02:12<00:55,  1.05it/s] 70%|██████▉   | 132/189 [02:13<00:54,  1.04it/s]                                                 {'loss': 0.6453, 'grad_norm': 4.280828929397093, 'learning_rate': 6.33879781420765e-07, 'epoch': 2.1}
 70%|██████▉   | 132/189 [02:13<00:54,  1.04it/s] 70%|███████   | 133/189 [02:13<00:53,  1.04it/s]                                                 {'loss': 0.581, 'grad_norm': 4.813939034018585, 'learning_rate': 6.229508196721311e-07, 'epoch': 2.11}
 70%|███████   | 133/189 [02:13<00:53,  1.04it/s] 71%|███████   | 134/189 [02:14<00:52,  1.04it/s]                                                 {'loss': 0.4995, 'grad_norm': 3.9859289027263114, 'learning_rate': 6.120218579234973e-07, 'epoch': 2.13}
 71%|███████   | 134/189 [02:14<00:52,  1.04it/s] 71%|███████▏  | 135/189 [02:15<00:51,  1.05it/s]                                                 {'loss': 0.4586, 'grad_norm': 4.2394120825995225, 'learning_rate': 6.010928961748634e-07, 'epoch': 2.14}
 71%|███████▏  | 135/189 [02:15<00:51,  1.05it/s] 72%|███████▏  | 136/189 [02:16<00:50,  1.04it/s]                                                 {'loss': 0.3801, 'grad_norm': 3.4042209330360773, 'learning_rate': 5.901639344262294e-07, 'epoch': 2.16}
 72%|███████▏  | 136/189 [02:16<00:50,  1.04it/s] 72%|███████▏  | 137/189 [02:17<00:49,  1.05it/s]                                                 {'loss': 0.3987, 'grad_norm': 3.1640241908362445, 'learning_rate': 5.792349726775956e-07, 'epoch': 2.17}
 72%|███████▏  | 137/189 [02:17<00:49,  1.05it/s] 73%|███████▎  | 138/189 [02:18<00:48,  1.05it/s]                                                 {'loss': 0.4914, 'grad_norm': 4.714316060441016, 'learning_rate': 5.683060109289617e-07, 'epoch': 2.19}
 73%|███████▎  | 138/189 [02:18<00:48,  1.05it/s] 74%|███████▎  | 139/189 [02:19<00:47,  1.05it/s]                                                 {'loss': 0.4258, 'grad_norm': 4.198800322851369, 'learning_rate': 5.573770491803278e-07, 'epoch': 2.21}
 74%|███████▎  | 139/189 [02:19<00:47,  1.05it/s] 74%|███████▍  | 140/189 [02:20<00:46,  1.05it/s]                                                 {'loss': 0.4879, 'grad_norm': 4.305209119002632, 'learning_rate': 5.46448087431694e-07, 'epoch': 2.22}
 74%|███████▍  | 140/189 [02:20<00:46,  1.05it/s] 75%|███████▍  | 141/189 [02:21<00:45,  1.06it/s]                                                 {'loss': 0.4942, 'grad_norm': 3.930220520767392, 'learning_rate': 5.355191256830601e-07, 'epoch': 2.24}
 75%|███████▍  | 141/189 [02:21<00:45,  1.06it/s] 75%|███████▌  | 142/189 [02:22<00:44,  1.05it/s]                                                 {'loss': 0.5057, 'grad_norm': 4.0320664328374205, 'learning_rate': 5.245901639344262e-07, 'epoch': 2.25}
 75%|███████▌  | 142/189 [02:22<00:44,  1.05it/s] 76%|███████▌  | 143/189 [02:23<00:43,  1.06it/s]                                                 {'loss': 0.4751, 'grad_norm': 3.8060496212103243, 'learning_rate': 5.136612021857924e-07, 'epoch': 2.27}
 76%|███████▌  | 143/189 [02:23<00:43,  1.06it/s] 76%|███████▌  | 144/189 [02:24<00:42,  1.05it/s]                                                 {'loss': 0.5625, 'grad_norm': 4.9455262906716495, 'learning_rate': 5.027322404371585e-07, 'epoch': 2.29}
 76%|███████▌  | 144/189 [02:24<00:42,  1.05it/s] 77%|███████▋  | 145/189 [02:25<00:41,  1.05it/s]                                                 {'loss': 0.3757, 'grad_norm': 3.467011755143257, 'learning_rate': 4.918032786885245e-07, 'epoch': 2.3}
 77%|███████▋  | 145/189 [02:25<00:41,  1.05it/s] 77%|███████▋  | 146/189 [02:26<00:40,  1.05it/s]                                                 {'loss': 0.4237, 'grad_norm': 4.155016509837357, 'learning_rate': 4.808743169398907e-07, 'epoch': 2.32}
 77%|███████▋  | 146/189 [02:26<00:40,  1.05it/s] 78%|███████▊  | 147/189 [02:27<00:40,  1.05it/s]                                                 {'loss': 0.399, 'grad_norm': 3.584396597515211, 'learning_rate': 4.6994535519125684e-07, 'epoch': 2.33}
 78%|███████▊  | 147/189 [02:27<00:40,  1.05it/s] 78%|███████▊  | 148/189 [02:28<00:39,  1.05it/s]                                                 {'loss': 0.5903, 'grad_norm': 4.843205557791794, 'learning_rate': 4.590163934426229e-07, 'epoch': 2.35}
 78%|███████▊  | 148/189 [02:28<00:39,  1.05it/s] 79%|███████▉  | 149/189 [02:29<00:38,  1.05it/s]                                                 {'loss': 0.3528, 'grad_norm': 4.045064324041465, 'learning_rate': 4.4808743169398906e-07, 'epoch': 2.37}
 79%|███████▉  | 149/189 [02:29<00:38,  1.05it/s] 79%|███████▉  | 150/189 [02:30<00:37,  1.05it/s]                                                 {'loss': 0.5069, 'grad_norm': 3.861556149285662, 'learning_rate': 4.3715846994535514e-07, 'epoch': 2.38}
 79%|███████▉  | 150/189 [02:30<00:37,  1.05it/s] 80%|███████▉  | 151/189 [02:31<00:36,  1.05it/s]                                                 {'loss': 0.4987, 'grad_norm': 4.5854584636651845, 'learning_rate': 4.2622950819672127e-07, 'epoch': 2.4}
 80%|███████▉  | 151/189 [02:31<00:36,  1.05it/s] 80%|████████  | 152/189 [02:32<00:35,  1.05it/s]                                                 {'loss': 0.5014, 'grad_norm': 3.6659757290502815, 'learning_rate': 4.153005464480874e-07, 'epoch': 2.41}
 80%|████████  | 152/189 [02:32<00:35,  1.05it/s] 81%|████████  | 153/189 [02:32<00:34,  1.06it/s]                                                 {'loss': 0.6022, 'grad_norm': 4.461532491429103, 'learning_rate': 4.0437158469945354e-07, 'epoch': 2.43}
 81%|████████  | 153/189 [02:32<00:34,  1.06it/s] 81%|████████▏ | 154/189 [02:33<00:33,  1.05it/s]                                                 {'loss': 0.4707, 'grad_norm': 3.629957125601993, 'learning_rate': 3.9344262295081967e-07, 'epoch': 2.44}
 81%|████████▏ | 154/189 [02:33<00:33,  1.05it/s] 82%|████████▏ | 155/189 [02:34<00:32,  1.06it/s]                                                 {'loss': 0.5514, 'grad_norm': 4.61409355848803, 'learning_rate': 3.8251366120218575e-07, 'epoch': 2.46}
 82%|████████▏ | 155/189 [02:34<00:32,  1.06it/s] 83%|████████▎ | 156/189 [02:35<00:31,  1.05it/s]                                                 {'loss': 0.5774, 'grad_norm': 5.948206487001671, 'learning_rate': 3.7158469945355194e-07, 'epoch': 2.48}
 83%|████████▎ | 156/189 [02:35<00:31,  1.05it/s] 83%|████████▎ | 157/189 [02:36<00:30,  1.05it/s]                                                 {'loss': 0.5229, 'grad_norm': 3.433748183260606, 'learning_rate': 3.60655737704918e-07, 'epoch': 2.49}
 83%|████████▎ | 157/189 [02:36<00:30,  1.05it/s] 84%|████████▎ | 158/189 [02:37<00:29,  1.05it/s]                                                 {'loss': 0.343, 'grad_norm': 2.933858442055914, 'learning_rate': 3.4972677595628415e-07, 'epoch': 2.51}
 84%|████████▎ | 158/189 [02:37<00:29,  1.05it/s] 84%|████████▍ | 159/189 [02:38<00:28,  1.05it/s]                                                 {'loss': 0.5115, 'grad_norm': 4.5973165918606, 'learning_rate': 3.3879781420765023e-07, 'epoch': 2.52}
 84%|████████▍ | 159/189 [02:38<00:28,  1.05it/s] 85%|████████▍ | 160/189 [02:39<00:27,  1.05it/s]                                                 {'loss': 0.4095, 'grad_norm': 4.889027850008998, 'learning_rate': 3.2786885245901637e-07, 'epoch': 2.54}
 85%|████████▍ | 160/189 [02:39<00:27,  1.05it/s] 85%|████████▌ | 161/189 [02:40<00:26,  1.05it/s]                                                 {'loss': 0.4243, 'grad_norm': 4.137980987135842, 'learning_rate': 3.169398907103825e-07, 'epoch': 2.56}
 85%|████████▌ | 161/189 [02:40<00:26,  1.05it/s] 86%|████████▌ | 162/189 [02:41<00:25,  1.05it/s]                                                 {'loss': 0.4563, 'grad_norm': 4.105562091535249, 'learning_rate': 3.0601092896174863e-07, 'epoch': 2.57}
 86%|████████▌ | 162/189 [02:41<00:25,  1.05it/s] 86%|████████▌ | 163/189 [02:42<00:24,  1.05it/s]                                                 {'loss': 0.3935, 'grad_norm': 3.5555187784732354, 'learning_rate': 2.950819672131147e-07, 'epoch': 2.59}
 86%|████████▌ | 163/189 [02:42<00:24,  1.05it/s] 87%|████████▋ | 164/189 [02:43<00:23,  1.05it/s]                                                 {'loss': 0.3857, 'grad_norm': 3.6906290118859593, 'learning_rate': 2.8415300546448085e-07, 'epoch': 2.6}
 87%|████████▋ | 164/189 [02:43<00:23,  1.05it/s] 87%|████████▋ | 165/189 [02:44<00:22,  1.05it/s]                                                 {'loss': 0.3757, 'grad_norm': 3.56953810320482, 'learning_rate': 2.73224043715847e-07, 'epoch': 2.62}
 87%|████████▋ | 165/189 [02:44<00:22,  1.05it/s] 88%|████████▊ | 166/189 [02:45<00:21,  1.05it/s]                                                 {'loss': 0.6172, 'grad_norm': 4.399114622538773, 'learning_rate': 2.622950819672131e-07, 'epoch': 2.63}
 88%|████████▊ | 166/189 [02:45<00:21,  1.05it/s] 88%|████████▊ | 167/189 [02:46<00:20,  1.05it/s]                                                 {'loss': 0.4278, 'grad_norm': 4.014551276066173, 'learning_rate': 2.5136612021857925e-07, 'epoch': 2.65}
 88%|████████▊ | 167/189 [02:46<00:20,  1.05it/s] 89%|████████▉ | 168/189 [02:47<00:19,  1.06it/s]                                                 {'loss': 0.4597, 'grad_norm': 4.500025500937256, 'learning_rate': 2.4043715846994533e-07, 'epoch': 2.67}
 89%|████████▉ | 168/189 [02:47<00:19,  1.06it/s] 89%|████████▉ | 169/189 [02:48<00:18,  1.06it/s]                                                 {'loss': 0.4607, 'grad_norm': 5.165099715572356, 'learning_rate': 2.2950819672131146e-07, 'epoch': 2.68}
 89%|████████▉ | 169/189 [02:48<00:18,  1.06it/s] 90%|████████▉ | 170/189 [02:49<00:18,  1.05it/s]                                                 {'loss': 0.4602, 'grad_norm': 4.360378425549054, 'learning_rate': 2.1857923497267757e-07, 'epoch': 2.7}
 90%|████████▉ | 170/189 [02:49<00:18,  1.05it/s] 90%|█████████ | 171/189 [02:50<00:17,  1.05it/s]                                                 {'loss': 0.3481, 'grad_norm': 3.1064000262953546, 'learning_rate': 2.076502732240437e-07, 'epoch': 2.71}
 90%|█████████ | 171/189 [02:50<00:17,  1.05it/s] 91%|█████████ | 172/189 [02:51<00:16,  1.05it/s]                                                 {'loss': 0.4448, 'grad_norm': 5.405664182327597, 'learning_rate': 1.9672131147540984e-07, 'epoch': 2.73}
 91%|█████████ | 172/189 [02:51<00:16,  1.05it/s] 92%|█████████▏| 173/189 [02:51<00:15,  1.05it/s]                                                 {'loss': 0.6327, 'grad_norm': 6.232866156468328, 'learning_rate': 1.8579234972677597e-07, 'epoch': 2.75}
 92%|█████████▏| 173/189 [02:51<00:15,  1.05it/s] 92%|█████████▏| 174/189 [02:52<00:14,  1.06it/s]                                                 {'loss': 0.4317, 'grad_norm': 3.819461520003023, 'learning_rate': 1.7486338797814208e-07, 'epoch': 2.76}
 92%|█████████▏| 174/189 [02:52<00:14,  1.06it/s] 93%|█████████▎| 175/189 [02:53<00:13,  1.06it/s]                                                 {'loss': 0.4695, 'grad_norm': 5.049891115022869, 'learning_rate': 1.6393442622950818e-07, 'epoch': 2.78}
 93%|█████████▎| 175/189 [02:53<00:13,  1.06it/s] 93%|█████████▎| 176/189 [02:54<00:12,  1.06it/s]                                                 {'loss': 0.3784, 'grad_norm': 4.323772532785706, 'learning_rate': 1.5300546448087432e-07, 'epoch': 2.79}
 93%|█████████▎| 176/189 [02:54<00:12,  1.06it/s] 94%|█████████▎| 177/189 [02:55<00:11,  1.05it/s]                                                 {'loss': 0.4111, 'grad_norm': 3.623298680518888, 'learning_rate': 1.4207650273224042e-07, 'epoch': 2.81}
 94%|█████████▎| 177/189 [02:55<00:11,  1.05it/s] 94%|█████████▍| 178/189 [02:56<00:10,  1.05it/s]                                                 {'loss': 0.4273, 'grad_norm': 3.842801999324641, 'learning_rate': 1.3114754098360656e-07, 'epoch': 2.83}
 94%|█████████▍| 178/189 [02:56<00:10,  1.05it/s] 95%|█████████▍| 179/189 [02:57<00:09,  1.05it/s]                                                 {'loss': 0.5299, 'grad_norm': 3.5597910057934357, 'learning_rate': 1.2021857923497266e-07, 'epoch': 2.84}
 95%|█████████▍| 179/189 [02:57<00:09,  1.05it/s] 95%|█████████▌| 180/189 [02:58<00:08,  1.05it/s]                                                 {'loss': 0.4277, 'grad_norm': 3.7751161148463095, 'learning_rate': 1.0928961748633878e-07, 'epoch': 2.86}
 95%|█████████▌| 180/189 [02:58<00:08,  1.05it/s] 96%|█████████▌| 181/189 [02:59<00:07,  1.05it/s]                                                 {'loss': 0.3764, 'grad_norm': 4.275971911639834, 'learning_rate': 9.836065573770492e-08, 'epoch': 2.87}
 96%|█████████▌| 181/189 [02:59<00:07,  1.05it/s] 96%|█████████▋| 182/189 [03:00<00:06,  1.05it/s]                                                 {'loss': 0.3957, 'grad_norm': 3.9195522641651324, 'learning_rate': 8.743169398907104e-08, 'epoch': 2.89}
 96%|█████████▋| 182/189 [03:00<00:06,  1.05it/s] 97%|█████████▋| 183/189 [03:01<00:05,  1.05it/s]                                                 {'loss': 0.3747, 'grad_norm': 3.6023017385816662, 'learning_rate': 7.650273224043716e-08, 'epoch': 2.9}
 97%|█████████▋| 183/189 [03:01<00:05,  1.05it/s] 97%|█████████▋| 184/189 [03:02<00:04,  1.05it/s]                                                 {'loss': 0.55, 'grad_norm': 4.290887575802308, 'learning_rate': 6.557377049180328e-08, 'epoch': 2.92}
 97%|█████████▋| 184/189 [03:02<00:04,  1.05it/s] 98%|█████████▊| 185/189 [03:03<00:03,  1.05it/s]                                                 {'loss': 0.4004, 'grad_norm': 3.3883400612969425, 'learning_rate': 5.464480874316939e-08, 'epoch': 2.94}
 98%|█████████▊| 185/189 [03:03<00:03,  1.05it/s] 98%|█████████▊| 186/189 [03:04<00:02,  1.05it/s]                                                 {'loss': 0.2581, 'grad_norm': 4.108218243322806, 'learning_rate': 4.371584699453552e-08, 'epoch': 2.95}
 98%|█████████▊| 186/189 [03:04<00:02,  1.05it/s] 99%|█████████▉| 187/189 [03:05<00:01,  1.05it/s]                                                 {'loss': 0.4861, 'grad_norm': 3.6116061731628757, 'learning_rate': 3.278688524590164e-08, 'epoch': 2.97}
 99%|█████████▉| 187/189 [03:05<00:01,  1.05it/s] 99%|█████████▉| 188/189 [03:06<00:00,  1.04it/s]                                                 {'loss': 0.3886, 'grad_norm': 3.7662133021173823, 'learning_rate': 2.185792349726776e-08, 'epoch': 2.98}
 99%|█████████▉| 188/189 [03:06<00:00,  1.04it/s]100%|██████████| 189/189 [03:07<00:00,  1.04it/s]                                                 {'loss': 0.4027, 'grad_norm': 3.6039310260890423, 'learning_rate': 1.092896174863388e-08, 'epoch': 3.0}
100%|██████████| 189/189 [03:07<00:00,  1.04it/s][rank1]: Traceback (most recent call last):
[rank1]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/serialization.py", line 628, in save
[rank1]:     _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
[rank1]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/serialization.py", line 862, in _save
[rank1]:     zip_file.write_record(name, storage, num_bytes)
[rank1]: RuntimeError: [enforce fail at inline_container.cc:764] . PytorchStreamWriter failed writing file data/0: file write failed

[rank1]: During handling of the above exception, another exception occurred:

[rank1]: Traceback (most recent call last):
[rank1]:   File "/opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/llm_fingerprinting/stanford_alpaca/train.py", line 333, in <module>
[rank1]:     train()
[rank1]:   File "/opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/llm_fingerprinting/stanford_alpaca/train.py", line 327, in train
[rank1]:     trainer.train()
[rank1]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/trainer.py", line 1945, in train
[rank1]:     return inner_training_loop(
[rank1]:            ^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/trainer.py", line 2363, in _inner_training_loop
[rank1]:     self._maybe_log_save_evaluate(tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)
[rank1]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/trainer.py", line 2814, in _maybe_log_save_evaluate
[rank1]:     self._save_checkpoint(model, trial, metrics=metrics)
[rank1]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/trainer.py", line 2897, in _save_checkpoint
[rank1]:     self._save_optimizer_and_scheduler(output_dir)
[rank1]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/trainer.py", line 3002, in _save_optimizer_and_scheduler
[rank1]:     self.model_wrapped.save_checkpoint(output_dir)
[rank1]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 3112, in save_checkpoint
[rank1]:     self._save_zero_checkpoint(save_dir, tag)
[rank1]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 3473, in _save_zero_checkpoint
[rank1]:     self.checkpoint_engine.save(zero_sd, zero_checkpoint_name)
[rank1]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/deepspeed/runtime/checkpoint_engine/torch_checkpoint_engine.py", line 22, in save
[rank1]:     torch.save(state_dict, path)
[rank1]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/serialization.py", line 627, in save
[rank1]:     with _open_zipfile_writer(f) as opened_zipfile:
[rank1]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/serialization.py", line 475, in __exit__
[rank1]:     self.file_like.write_end_of_file()
[rank1]: RuntimeError: [enforce fail at inline_container.cc:595] . unexpected pos 2432 vs 2326
[rank2]: Traceback (most recent call last):
[rank2]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/serialization.py", line 628, in save
[rank2]:     _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
[rank2]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/serialization.py", line 862, in _save
[rank2]:     zip_file.write_record(name, storage, num_bytes)
[rank2]: RuntimeError: [enforce fail at inline_container.cc:764] . PytorchStreamWriter failed writing file data/0: file write failed

[rank2]: During handling of the above exception, another exception occurred:

[rank2]: Traceback (most recent call last):
[rank2]:   File "/opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/llm_fingerprinting/stanford_alpaca/train.py", line 333, in <module>
[rank2]:     train()
[rank2]:   File "/opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/llm_fingerprinting/stanford_alpaca/train.py", line 327, in train
[rank2]:     trainer.train()
[rank2]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/trainer.py", line 1945, in train
[rank2]:     return inner_training_loop(
[rank2]:            ^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/trainer.py", line 2363, in _inner_training_loop
[rank2]:     self._maybe_log_save_evaluate(tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)
[rank2]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/trainer.py", line 2814, in _maybe_log_save_evaluate
[rank2]:     self._save_checkpoint(model, trial, metrics=metrics)
[rank2]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/trainer.py", line 2897, in _save_checkpoint
[rank2]:     self._save_optimizer_and_scheduler(output_dir)
[rank2]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/trainer.py", line 3002, in _save_optimizer_and_scheduler
[rank2]:     self.model_wrapped.save_checkpoint(output_dir)
[rank2]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 3112, in save_checkpoint
[rank2]:     self._save_zero_checkpoint(save_dir, tag)
[rank2]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 3473, in _save_zero_checkpoint
[rank2]:     self.checkpoint_engine.save(zero_sd, zero_checkpoint_name)
[rank2]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/deepspeed/runtime/checkpoint_engine/torch_checkpoint_engine.py", line 22, in save
[rank2]:     torch.save(state_dict, path)
[rank2]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/serialization.py", line 627, in save
[rank2]:     with _open_zipfile_writer(f) as opened_zipfile:
[rank2]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/serialization.py", line 475, in __exit__
[rank2]:     self.file_like.write_end_of_file()
[rank2]: RuntimeError: [enforce fail at inline_container.cc:595] . unexpected pos 2432 vs 2326
[rank0]: Traceback (most recent call last):
[rank0]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/serialization.py", line 628, in save
[rank0]:     _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
[rank0]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/serialization.py", line 862, in _save
[rank0]:     zip_file.write_record(name, storage, num_bytes)
[rank0]: RuntimeError: [enforce fail at inline_container.cc:764] . PytorchStreamWriter failed writing file data/0: file write failed

[rank0]: During handling of the above exception, another exception occurred:

[rank0]: Traceback (most recent call last):
[rank0]:   File "/opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/llm_fingerprinting/stanford_alpaca/train.py", line 333, in <module>
[rank0]:     train()
[rank0]:   File "/opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/llm_fingerprinting/stanford_alpaca/train.py", line 327, in train
[rank0]:     trainer.train()
[rank0]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/trainer.py", line 1945, in train
[rank0]:     return inner_training_loop(
[rank0]:            ^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/trainer.py", line 2363, in _inner_training_loop
[rank0]:     self._maybe_log_save_evaluate(tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)
[rank0]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/trainer.py", line 2814, in _maybe_log_save_evaluate
[rank0]:     self._save_checkpoint(model, trial, metrics=metrics)
[rank0]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/trainer.py", line 2897, in _save_checkpoint
[rank0]:     self._save_optimizer_and_scheduler(output_dir)
[rank0]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/trainer.py", line 3002, in _save_optimizer_and_scheduler
[rank0]:     self.model_wrapped.save_checkpoint(output_dir)
[rank0]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 3112, in save_checkpoint
[rank0]:     self._save_zero_checkpoint(save_dir, tag)
[rank0]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 3473, in _save_zero_checkpoint
[rank0]:     self.checkpoint_engine.save(zero_sd, zero_checkpoint_name)
[rank0]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/deepspeed/runtime/checkpoint_engine/torch_checkpoint_engine.py", line 22, in save
[rank0]:     torch.save(state_dict, path)
[rank0]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/serialization.py", line 627, in save
[rank0]:     with _open_zipfile_writer(f) as opened_zipfile:
[rank0]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/serialization.py", line 475, in __exit__
[rank0]:     self.file_like.write_end_of_file()
[rank0]: RuntimeError: [enforce fail at inline_container.cc:595] . unexpected pos 2432 vs 2326
[rank3]: Traceback (most recent call last):
[rank3]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/serialization.py", line 628, in save
[rank3]:     _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
[rank3]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/serialization.py", line 862, in _save
[rank3]:     zip_file.write_record(name, storage, num_bytes)
[rank3]: RuntimeError: [enforce fail at inline_container.cc:764] . PytorchStreamWriter failed writing file data/0: file write failed

[rank3]: During handling of the above exception, another exception occurred:

[rank3]: Traceback (most recent call last):
[rank3]:   File "/opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/llm_fingerprinting/stanford_alpaca/train.py", line 333, in <module>
[rank3]:     train()
[rank3]:   File "/opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/llm_fingerprinting/stanford_alpaca/train.py", line 327, in train
[rank3]:     trainer.train()
[rank3]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/trainer.py", line 1945, in train
[rank3]:     return inner_training_loop(
[rank3]:            ^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/trainer.py", line 2363, in _inner_training_loop
[rank3]:     self._maybe_log_save_evaluate(tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)
[rank3]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/trainer.py", line 2814, in _maybe_log_save_evaluate
[rank3]:     self._save_checkpoint(model, trial, metrics=metrics)
[rank3]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/trainer.py", line 2897, in _save_checkpoint
[rank3]:     self._save_optimizer_and_scheduler(output_dir)
[rank3]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/trainer.py", line 3002, in _save_optimizer_and_scheduler
[rank3]:     self.model_wrapped.save_checkpoint(output_dir)
[rank3]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 3112, in save_checkpoint
[rank3]:     self._save_zero_checkpoint(save_dir, tag)
[rank3]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 3473, in _save_zero_checkpoint
[rank3]:     self.checkpoint_engine.save(zero_sd, zero_checkpoint_name)
[rank3]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/deepspeed/runtime/checkpoint_engine/torch_checkpoint_engine.py", line 22, in save
[rank3]:     torch.save(state_dict, path)
[rank3]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/serialization.py", line 627, in save
[rank3]:     with _open_zipfile_writer(f) as opened_zipfile:
[rank3]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/serialization.py", line 475, in __exit__
[rank3]:     self.file_like.write_end_of_file()
[rank3]: RuntimeError: [enforce fail at inline_container.cc:595] . unexpected pos 2432 vs 2326
[rank4]: Traceback (most recent call last):
[rank4]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/serialization.py", line 628, in save
[rank4]:     _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
[rank4]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/serialization.py", line 862, in _save
[rank4]:     zip_file.write_record(name, storage, num_bytes)
[rank4]: RuntimeError: [enforce fail at inline_container.cc:764] . PytorchStreamWriter failed writing file data/0: file write failed

[rank4]: During handling of the above exception, another exception occurred:

[rank4]: Traceback (most recent call last):
[rank4]:   File "/opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/llm_fingerprinting/stanford_alpaca/train.py", line 333, in <module>
[rank4]:     train()
[rank4]:   File "/opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/llm_fingerprinting/stanford_alpaca/train.py", line 327, in train
[rank4]:     trainer.train()
[rank4]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/trainer.py", line 1945, in train
[rank4]:     return inner_training_loop(
[rank4]:            ^^^^^^^^^^^^^^^^^^^^
[rank4]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/trainer.py", line 2363, in _inner_training_loop
[rank4]:     self._maybe_log_save_evaluate(tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)
[rank4]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/trainer.py", line 2814, in _maybe_log_save_evaluate
[rank4]:     self._save_checkpoint(model, trial, metrics=metrics)
[rank4]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/trainer.py", line 2897, in _save_checkpoint
[rank4]:     self._save_optimizer_and_scheduler(output_dir)
[rank4]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/trainer.py", line 3002, in _save_optimizer_and_scheduler
[rank4]:     self.model_wrapped.save_checkpoint(output_dir)
[rank4]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 3112, in save_checkpoint
[rank4]:     self._save_zero_checkpoint(save_dir, tag)
[rank4]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 3473, in _save_zero_checkpoint
[rank4]:     self.checkpoint_engine.save(zero_sd, zero_checkpoint_name)
[rank4]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/deepspeed/runtime/checkpoint_engine/torch_checkpoint_engine.py", line 22, in save
[rank4]:     torch.save(state_dict, path)
[rank4]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/serialization.py", line 627, in save
[rank4]:     with _open_zipfile_writer(f) as opened_zipfile:
[rank4]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/serialization.py", line 475, in __exit__
[rank4]:     self.file_like.write_end_of_file()
[rank4]: RuntimeError: [enforce fail at inline_container.cc:595] . unexpected pos 2432 vs 2326
[rank7]: Traceback (most recent call last):
[rank7]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/serialization.py", line 628, in save
[rank7]:     _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
[rank7]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/serialization.py", line 862, in _save
[rank7]:     zip_file.write_record(name, storage, num_bytes)
[rank7]: RuntimeError: [enforce fail at inline_container.cc:764] . PytorchStreamWriter failed writing file data/0: file write failed

[rank7]: During handling of the above exception, another exception occurred:

[rank7]: Traceback (most recent call last):
[rank7]:   File "/opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/llm_fingerprinting/stanford_alpaca/train.py", line 333, in <module>
[rank7]:     train()
[rank7]:   File "/opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/llm_fingerprinting/stanford_alpaca/train.py", line 327, in train
[rank7]:     trainer.train()
[rank7]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/trainer.py", line 1945, in train
[rank7]:     return inner_training_loop(
[rank7]:            ^^^^^^^^^^^^^^^^^^^^
[rank7]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/trainer.py", line 2363, in _inner_training_loop
[rank7]:     self._maybe_log_save_evaluate(tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)
[rank7]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/trainer.py", line 2814, in _maybe_log_save_evaluate
[rank7]:     self._save_checkpoint(model, trial, metrics=metrics)
[rank7]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/trainer.py", line 2897, in _save_checkpoint
[rank7]:     self._save_optimizer_and_scheduler(output_dir)
[rank7]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/trainer.py", line 3002, in _save_optimizer_and_scheduler
[rank7]:     self.model_wrapped.save_checkpoint(output_dir)
[rank7]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 3112, in save_checkpoint
[rank7]:     self._save_zero_checkpoint(save_dir, tag)
[rank7]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 3473, in _save_zero_checkpoint
[rank7]:     self.checkpoint_engine.save(zero_sd, zero_checkpoint_name)
[rank7]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/deepspeed/runtime/checkpoint_engine/torch_checkpoint_engine.py", line 22, in save
[rank7]:     torch.save(state_dict, path)
[rank7]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/serialization.py", line 627, in save
[rank7]:     with _open_zipfile_writer(f) as opened_zipfile:
[rank7]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/serialization.py", line 475, in __exit__
[rank7]:     self.file_like.write_end_of_file()
[rank7]: RuntimeError: [enforce fail at inline_container.cc:595] . unexpected pos 2432 vs 2326
[rank6]: Traceback (most recent call last):
[rank6]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/serialization.py", line 628, in save
[rank6]:     _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
[rank6]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/serialization.py", line 862, in _save
[rank6]:     zip_file.write_record(name, storage, num_bytes)
[rank6]: RuntimeError: [enforce fail at inline_container.cc:764] . PytorchStreamWriter failed writing file data/0: file write failed

[rank6]: During handling of the above exception, another exception occurred:

[rank6]: Traceback (most recent call last):
[rank6]:   File "/opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/llm_fingerprinting/stanford_alpaca/train.py", line 333, in <module>
[rank6]:     train()
[rank6]:   File "/opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/llm_fingerprinting/stanford_alpaca/train.py", line 327, in train
[rank6]:     trainer.train()
[rank6]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/trainer.py", line 1945, in train
[rank6]:     return inner_training_loop(
[rank6]:            ^^^^^^^^^^^^^^^^^^^^
[rank6]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/trainer.py", line 2363, in _inner_training_loop
[rank6]:     self._maybe_log_save_evaluate(tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)
[rank6]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/trainer.py", line 2814, in _maybe_log_save_evaluate
[rank6]:     self._save_checkpoint(model, trial, metrics=metrics)
[rank6]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/trainer.py", line 2897, in _save_checkpoint
[rank6]:     self._save_optimizer_and_scheduler(output_dir)
[rank6]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/trainer.py", line 3002, in _save_optimizer_and_scheduler
[rank6]:     self.model_wrapped.save_checkpoint(output_dir)
[rank6]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 3112, in save_checkpoint
[rank6]:     self._save_zero_checkpoint(save_dir, tag)
[rank6]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 3473, in _save_zero_checkpoint
[rank6]:     self.checkpoint_engine.save(zero_sd, zero_checkpoint_name)
[rank6]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/deepspeed/runtime/checkpoint_engine/torch_checkpoint_engine.py", line 22, in save
[rank6]:     torch.save(state_dict, path)
[rank6]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/serialization.py", line 627, in save
[rank6]:     with _open_zipfile_writer(f) as opened_zipfile:
[rank6]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/serialization.py", line 475, in __exit__
[rank6]:     self.file_like.write_end_of_file()
[rank6]: RuntimeError: [enforce fail at inline_container.cc:595] . unexpected pos 2432 vs 2326
[rank5]: Traceback (most recent call last):
[rank5]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/serialization.py", line 628, in save
[rank5]:     _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
[rank5]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/serialization.py", line 862, in _save
[rank5]:     zip_file.write_record(name, storage, num_bytes)
[rank5]: RuntimeError: [enforce fail at inline_container.cc:764] . PytorchStreamWriter failed writing file data/0: file write failed

[rank5]: During handling of the above exception, another exception occurred:

[rank5]: Traceback (most recent call last):
[rank5]:   File "/opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/llm_fingerprinting/stanford_alpaca/train.py", line 333, in <module>
[rank5]:     train()
[rank5]:   File "/opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/llm_fingerprinting/stanford_alpaca/train.py", line 327, in train
[rank5]:     trainer.train()
[rank5]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/trainer.py", line 1945, in train
[rank5]:     return inner_training_loop(
[rank5]:            ^^^^^^^^^^^^^^^^^^^^
[rank5]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/trainer.py", line 2363, in _inner_training_loop
[rank5]:     self._maybe_log_save_evaluate(tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)
[rank5]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/trainer.py", line 2814, in _maybe_log_save_evaluate
[rank5]:     self._save_checkpoint(model, trial, metrics=metrics)
[rank5]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/trainer.py", line 2897, in _save_checkpoint
[rank5]:     self._save_optimizer_and_scheduler(output_dir)
[rank5]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/trainer.py", line 3002, in _save_optimizer_and_scheduler
[rank5]:     self.model_wrapped.save_checkpoint(output_dir)
[rank5]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 3112, in save_checkpoint
[rank5]:     self._save_zero_checkpoint(save_dir, tag)
[rank5]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 3473, in _save_zero_checkpoint
[rank5]:     self.checkpoint_engine.save(zero_sd, zero_checkpoint_name)
[rank5]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/deepspeed/runtime/checkpoint_engine/torch_checkpoint_engine.py", line 22, in save
[rank5]:     torch.save(state_dict, path)
[rank5]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/serialization.py", line 627, in save
[rank5]:     with _open_zipfile_writer(f) as opened_zipfile:
[rank5]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/serialization.py", line 475, in __exit__
[rank5]:     self.file_like.write_end_of_file()
[rank5]: RuntimeError: [enforce fail at inline_container.cc:595] . unexpected pos 2432 vs 2326
100%|██████████| 189/189 [03:55<00:00,  1.24s/it]
[2024-08-28 20:38:34,024] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 3726732
[2024-08-28 20:38:34,649] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 3726733
[2024-08-28 20:38:34,700] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 3726734
[2024-08-28 20:38:34,747] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 3726735
[2024-08-28 20:38:34,748] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 3726736
[2024-08-28 20:38:34,794] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 3726737
[2024-08-28 20:38:34,890] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 3726738
[2024-08-28 20:38:34,936] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 3726739
[2024-08-28 20:38:35,191] [ERROR] [launch.py:325:sigkill_handler] ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=7', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Meta-Llama-3-8B', '--data_path', '../data/stanford_alpaca/oasst1_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_Meta-Llama-3-8B_oasst1_tuned', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1', '--use_peft', 'False', '--lora_r', '16', '--lora_alpha', '32'] exits with return code = 1
