Namespace(mode=['alpaca'], base_model='meta-llama/Meta-Llama-3-8B', template_name='barebone', total_bsz=64, epoch=3, lr=2e-05, data_path='', task_name='sharegpt', tuned_dir='./cache', use_peft=True, lora_r=16, lora_alpha=32)
num gpus:  8
Running 1/1: deepspeed --num_gpus=8 train.py --deepspeed ../deepspeed_config/zero3.json --bf16 --tf32=True
        --model_name_or_path meta-llama/Meta-Llama-3-8B --data_path ../data/stanford_alpaca/sharegpt_data.json
        --output_dir /fsx-project/yunyun/models/meta-llama_Meta-Llama-3-8B_sharegpt_Lora_tuned/
        --num_train_epochs 3
        --per_device_train_batch_size 10
        --per_device_eval_batch_size 4
        --gradient_accumulation_steps 1
        --gradient_checkpointing=True
        --evaluation_strategy=no
        --save_strategy=steps
        --save_steps 500
        --save_total_limit 1
        --learning_rate 2e-6
        --weight_decay 0.
        --report_to tensorboard
        --warmup_ratio 0.03
        --lr_scheduler_type=cosine
        --logging_steps 1
        --use_peft True 
        --lora_r 16 --lora_alpha 32
['deepspeed', '--num_gpus=8', 'train.py', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Meta-Llama-3-8B', '--data_path', '../data/stanford_alpaca/sharegpt_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_Meta-Llama-3-8B_sharegpt_Lora_tuned/', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1', '--use_peft', 'True', '--lora_r', '16', '--lora_alpha', '32']
[2024-08-18 21:34:47,590] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2024-08-18 21:34:55,691] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2024-08-18 21:34:55,691] [INFO] [runner.py:568:main] cmd = /data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNSwgNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None train.py --deepspeed ../deepspeed_config/zero3.json --bf16 --tf32=True --model_name_or_path meta-llama/Meta-Llama-3-8B --data_path ../data/stanford_alpaca/sharegpt_data.json --output_dir /fsx-project/yunyun/models/meta-llama_Meta-Llama-3-8B_sharegpt_Lora_tuned/ --num_train_epochs 3 --per_device_train_batch_size 10 --per_device_eval_batch_size 4 --gradient_accumulation_steps 1 --gradient_checkpointing=True --evaluation_strategy=no --save_strategy=steps --save_steps 500 --save_total_limit 1 --learning_rate 2e-6 --weight_decay 0. --report_to tensorboard --warmup_ratio 0.03 --lr_scheduler_type=cosine --logging_steps 1 --use_peft True --lora_r 16 --lora_alpha 32
[2024-08-18 21:34:58,293] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2024-08-18 21:35:01,668] [INFO] [launch.py:139:main] 0 NCCL_ASYNC_ERROR_HANDLING=1
[2024-08-18 21:35:01,669] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]}
[2024-08-18 21:35:01,669] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=8, node_rank=0
[2024-08-18 21:35:01,669] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]})
[2024-08-18 21:35:01,669] [INFO] [launch.py:164:main] dist_world_size=8
[2024-08-18 21:35:01,669] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
[2024-08-18 21:35:01,669] [INFO] [launch.py:256:main] process 1427283 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=0', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Meta-Llama-3-8B', '--data_path', '../data/stanford_alpaca/sharegpt_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_Meta-Llama-3-8B_sharegpt_Lora_tuned/', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1', '--use_peft', 'True', '--lora_r', '16', '--lora_alpha', '32']
[2024-08-18 21:35:01,670] [INFO] [launch.py:256:main] process 1427284 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=1', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Meta-Llama-3-8B', '--data_path', '../data/stanford_alpaca/sharegpt_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_Meta-Llama-3-8B_sharegpt_Lora_tuned/', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1', '--use_peft', 'True', '--lora_r', '16', '--lora_alpha', '32']
[2024-08-18 21:35:01,671] [INFO] [launch.py:256:main] process 1427285 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=2', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Meta-Llama-3-8B', '--data_path', '../data/stanford_alpaca/sharegpt_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_Meta-Llama-3-8B_sharegpt_Lora_tuned/', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1', '--use_peft', 'True', '--lora_r', '16', '--lora_alpha', '32']
[2024-08-18 21:35:01,671] [INFO] [launch.py:256:main] process 1427286 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=3', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Meta-Llama-3-8B', '--data_path', '../data/stanford_alpaca/sharegpt_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_Meta-Llama-3-8B_sharegpt_Lora_tuned/', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1', '--use_peft', 'True', '--lora_r', '16', '--lora_alpha', '32']
[2024-08-18 21:35:01,672] [INFO] [launch.py:256:main] process 1427287 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=4', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Meta-Llama-3-8B', '--data_path', '../data/stanford_alpaca/sharegpt_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_Meta-Llama-3-8B_sharegpt_Lora_tuned/', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1', '--use_peft', 'True', '--lora_r', '16', '--lora_alpha', '32']
[2024-08-18 21:35:01,672] [INFO] [launch.py:256:main] process 1427288 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=5', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Meta-Llama-3-8B', '--data_path', '../data/stanford_alpaca/sharegpt_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_Meta-Llama-3-8B_sharegpt_Lora_tuned/', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1', '--use_peft', 'True', '--lora_r', '16', '--lora_alpha', '32']
[2024-08-18 21:35:01,673] [INFO] [launch.py:256:main] process 1427289 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=6', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Meta-Llama-3-8B', '--data_path', '../data/stanford_alpaca/sharegpt_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_Meta-Llama-3-8B_sharegpt_Lora_tuned/', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1', '--use_peft', 'True', '--lora_r', '16', '--lora_alpha', '32']
[2024-08-18 21:35:01,673] [INFO] [launch.py:256:main] process 1427290 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=7', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Meta-Llama-3-8B', '--data_path', '../data/stanford_alpaca/sharegpt_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_Meta-Llama-3-8B_sharegpt_Lora_tuned/', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1', '--use_peft', 'True', '--lora_r', '16', '--lora_alpha', '32']
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
[2024-08-18 21:35:17,309] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
[2024-08-18 21:35:17,462] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-18 21:35:17,470] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[2024-08-18 21:35:17,533] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-18 21:35:17,540] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-18 21:35:17,545] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2024-08-18 21:35:17,591] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-18 21:35:17,597] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2024-08-18 21:35:18,056] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-08-18 21:35:18,208] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-08-18 21:35:18,212] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-08-18 21:35:18,269] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-08-18 21:35:18,287] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-08-18 21:35:18,295] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-08-18 21:35:18,313] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-08-18 21:35:18,329] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-08-18 21:35:18,329] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 417.79it/s]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 1558.35it/s]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 1506.44it/s]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 1449.56it/s]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 1535.67it/s]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 1572.67it/s]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 1710.91it/s]
Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 1743.63it/s]
[2024-08-18 21:35:30,046] [INFO] [partition_parameters.py:345:__exit__] finished initializing model - num_params = 291, num_elems = 8.03B
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:00,  3.29it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:00,  3.30it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:00,  3.23it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:00,  3.21it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:00,  3.19it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:00,  3.20it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:00,  3.21it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:09<00:28,  9.42s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:11,  5.67s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:11,  5.66s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:11,  5.67s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:11,  5.68s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:11,  5.67s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:11,  5.69s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:11,  5.69s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:18<00:07,  7.14s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:18<00:07,  7.14s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:18<00:07,  7.14s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:18<00:07,  7.15s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:18<00:07,  7.14s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:18<00:07,  7.15s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:18<00:07,  7.15s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:18<00:18,  9.48s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:19<00:00,  4.72s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:19<00:00,  4.72s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:19<00:00,  4.91s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:19<00:00,  4.91s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:19<00:00,  4.72s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:19<00:00,  4.91s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:19<00:00,  4.72s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:19<00:00,  4.91s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:19<00:00,  4.72s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:19<00:00,  4.91s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:19<00:00,  4.72s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:19<00:00,  4.91s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:19<00:00,  4.72s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:19<00:00,  4.91s/it]
enable_input_require_grads!enable_input_require_grads!

enable_input_require_grads!
enable_input_require_grads!
enable_input_require_grads!
enable_input_require_grads!
enable_input_require_grads!
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:28<00:09,  9.47s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:30<00:00,  6.72s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:30<00:00,  7.72s/it]
enable_input_require_grads!
trainable params: 6,815,744 || all params: 8,037,076,992 || trainable%: 0.0848
Finetune with LORA setting:  None
trainable params: 6,815,744 || all params: 8,037,076,992 || trainable%: 0.0848
Finetune with LORA setting:  None
trainable params: 6,815,744 || all params: 8,037,076,992 || trainable%: 0.0848
Finetune with LORA setting:  None
trainable params: 6,815,744 || all params: 8,037,076,992 || trainable%: 0.0848
Finetune with LORA setting:  None
trainable params: 6,815,744 || all params: 8,037,076,992 || trainable%: 0.0848
Finetune with LORA setting:  None
trainable params: 6,815,744 || all params: 8,037,076,992 || trainable%: 0.0848
Finetune with LORA setting:  None
trainable params: 6,815,744 || all params: 8,037,076,992 || trainable%: 0.0848
Finetune with LORA setting:  None
trainable params: 6,815,744 || all params: 8,037,076,992 || trainable%: 0.0848
Finetune with LORA setting:  None
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
[rank7]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121/fused_adam/build.ninja...
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/cpp_extension.py:1967: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
[rank6]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
[rank4]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
[rank3]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
[rank0]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[rank5]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
[rank2]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
[rank1]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
[1/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output multi_tensor_adam.cuda.o.d -DTORCH_EXTENSION_NAME=fused_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/deepspeed/ops/csrc/includes -I/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/deepspeed/ops/csrc/adam -isystem /data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/include -isystem /data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/include/TH -isystem /data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /data/home/yunyun/miniconda3/envs/newtorch2/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_90,code=compute_90 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -lineinfo --use_fast_math -gencode=arch=compute_90,code=sm_90 -gencode=arch=compute_90,code=compute_90 -DBF16_AVAILABLE -U__CUDA_NO_BFLOAT16_OPERATORS__ -U__CUDA_NO_BFLOAT162_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ -std=c++17 -c /data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/deepspeed/ops/csrc/adam/multi_tensor_adam.cu -o multi_tensor_adam.cuda.o 
[2/3] c++ -MMD -MF fused_adam_frontend.o.d -DTORCH_EXTENSION_NAME=fused_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/deepspeed/ops/csrc/includes -I/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/deepspeed/ops/csrc/adam -isystem /data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/include -isystem /data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/include/TH -isystem /data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /data/home/yunyun/miniconda3/envs/newtorch2/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -std=c++17 -g -Wno-reorder -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DBF16_AVAILABLE -c /data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/deepspeed/ops/csrc/adam/fused_adam_frontend.cpp -o fused_adam_frontend.o 
[3/3] c++ fused_adam_frontend.o multi_tensor_adam.cuda.o -shared -L/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o fused_adam.so
Loading extension module fused_adam...
Loading extension module fused_adam...
Loading extension module fused_adam...
Loading extension module fused_adam...
Loading extension module fused_adam...
Loading extension module fused_adam...
Loading extension module fused_adam...
Loading extension module fused_adam...
Time to load fused_adam op: 33.49882388114929 secondsTime to load fused_adam op: 33.649683475494385 secondsTime to load fused_adam op: 33.20461416244507 secondsTime to load fused_adam op: 34.637879610061646 secondsTime to load fused_adam op: 33.59730005264282 seconds
Time to load fused_adam op: 33.76870131492615 secondsTime to load fused_adam op: 34.101895332336426 secondsTime to load fused_adam op: 33.59626293182373 seconds






Parameter Offload: Total persistent parameters: 275517440 in 257 params
  0%|          | 0/630 [00:00<?, ?it/s]/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
  0%|          | 1/630 [00:08<1:29:58,  8.58s/it]                                                 {'loss': 0.8431, 'grad_norm': 1.0954341991886396, 'learning_rate': 0.0, 'epoch': 0.0}
  0%|          | 1/630 [00:08<1:29:58,  8.58s/it]  0%|          | 2/630 [00:10<48:14,  4.61s/it]                                                 {'loss': 0.8407, 'grad_norm': 1.0191278591348072, 'learning_rate': 4.7081782673327645e-07, 'epoch': 0.01}
  0%|          | 2/630 [00:10<48:14,  4.61s/it]  0%|          | 3/630 [00:11<30:11,  2.89s/it]                                               {'loss': 0.8834, 'grad_norm': 1.0920458209596569, 'learning_rate': 7.462286000432739e-07, 'epoch': 0.01}
  0%|          | 3/630 [00:11<30:11,  2.89s/it]  1%|          | 4/630 [00:12<21:43,  2.08s/it]                                               {'loss': 0.7621, 'grad_norm': 0.9746641518211181, 'learning_rate': 9.416356534665529e-07, 'epoch': 0.02}
  1%|          | 4/630 [00:12<21:43,  2.08s/it]  1%|          | 5/630 [00:12<17:01,  1.63s/it]                                               {'loss': 0.8515, 'grad_norm': 1.19774296466127, 'learning_rate': 1.0932051394658049e-06, 'epoch': 0.02}
  1%|          | 5/630 [00:12<17:01,  1.63s/it]  1%|          | 6/630 [00:13<14:11,  1.36s/it]                                               {'loss': 0.8102, 'grad_norm': 1.2611758116708047, 'learning_rate': 1.2170464267765503e-06, 'epoch': 0.03}
  1%|          | 6/630 [00:13<14:11,  1.36s/it]  1%|          | 7/630 [00:14<12:22,  1.19s/it]                                               {'loss': 0.8208, 'grad_norm': 1.0916230906426987, 'learning_rate': 1.3217527432721277e-06, 'epoch': 0.03}
  1%|          | 7/630 [00:14<12:22,  1.19s/it]  1%|▏         | 8/630 [00:15<11:12,  1.08s/it]                                               {'loss': 0.9066, 'grad_norm': 1.017974405846807, 'learning_rate': 1.4124534801998293e-06, 'epoch': 0.04}
  1%|▏         | 8/630 [00:15<11:12,  1.08s/it]  1%|▏         | 9/630 [00:16<10:26,  1.01s/it]                                               {'loss': 0.8494, 'grad_norm': 1.4979792102048135, 'learning_rate': 1.4924572000865478e-06, 'epoch': 0.04}
  1%|▏         | 9/630 [00:16<10:26,  1.01s/it]  2%|▏         | 10/630 [00:17<09:54,  1.04it/s]                                                {'loss': 0.8085, 'grad_norm': 1.0664418333221566, 'learning_rate': 1.5640229661990816e-06, 'epoch': 0.05}
  2%|▏         | 10/630 [00:17<09:54,  1.04it/s]  2%|▏         | 11/630 [00:18<09:32,  1.08it/s]                                                {'loss': 0.8856, 'grad_norm': 1.2188551910013106, 'learning_rate': 1.6287620764191933e-06, 'epoch': 0.05}
  2%|▏         | 11/630 [00:18<09:32,  1.08it/s]  2%|▏         | 12/630 [00:18<09:17,  1.11it/s]                                                {'loss': 0.8467, 'grad_norm': 1.217411579386051, 'learning_rate': 1.6878642535098268e-06, 'epoch': 0.06}
  2%|▏         | 12/630 [00:18<09:17,  1.11it/s]  2%|▏         | 13/630 [00:19<09:06,  1.13it/s]                                                {'loss': 0.935, 'grad_norm': 1.2405138464354026, 'learning_rate': 1.7422329860526872e-06, 'epoch': 0.06}
  2%|▏         | 13/630 [00:19<09:06,  1.13it/s]  2%|▏         | 14/630 [00:20<08:58,  1.14it/s]                                                {'loss': 0.8558, 'grad_norm': 1.0003471661173915, 'learning_rate': 1.792570570005404e-06, 'epoch': 0.07}
  2%|▏         | 14/630 [00:20<08:58,  1.14it/s]  2%|▏         | 15/630 [00:21<08:52,  1.16it/s]                                                {'loss': 0.7717, 'grad_norm': 0.9327660635440285, 'learning_rate': 1.8394337395090787e-06, 'epoch': 0.07}
  2%|▏         | 15/630 [00:21<08:52,  1.16it/s]  3%|▎         | 16/630 [00:22<08:48,  1.16it/s]                                                {'loss': 0.8481, 'grad_norm': 1.124943819517181, 'learning_rate': 1.8832713069331058e-06, 'epoch': 0.08}
  3%|▎         | 16/630 [00:22<08:48,  1.16it/s]  3%|▎         | 17/630 [00:23<08:45,  1.17it/s]                                                {'loss': 0.9169, 'grad_norm': 1.172093594662168, 'learning_rate': 1.924450371770508e-06, 'epoch': 0.08}
  3%|▎         | 17/630 [00:23<08:45,  1.17it/s]  3%|▎         | 18/630 [00:23<08:42,  1.17it/s]                                                {'loss': 0.8368, 'grad_norm': 1.102347419853786, 'learning_rate': 1.9632750268198243e-06, 'epoch': 0.09}
  3%|▎         | 18/630 [00:23<08:42,  1.17it/s]  3%|▎         | 19/630 [00:24<08:40,  1.17it/s]                                                {'loss': 0.8149, 'grad_norm': 1.0529268312632145, 'learning_rate': 2e-06, 'epoch': 0.09}
  3%|▎         | 19/630 [00:24<08:40,  1.17it/s]  3%|▎         | 20/630 [00:25<08:38,  1.18it/s]                                                {'loss': 0.8955, 'grad_norm': 1.1685403360740623, 'learning_rate': 2e-06, 'epoch': 0.1}
  3%|▎         | 20/630 [00:25<08:38,  1.18it/s]  3%|▎         | 21/630 [00:26<08:36,  1.18it/s]                                                {'loss': 0.8107, 'grad_norm': 1.1702894633911083, 'learning_rate': 1.9967266775777412e-06, 'epoch': 0.1}
  3%|▎         | 21/630 [00:26<08:36,  1.18it/s]  3%|▎         | 22/630 [00:27<08:35,  1.18it/s]                                                {'loss': 0.8609, 'grad_norm': 1.158282030416983, 'learning_rate': 1.9934533551554826e-06, 'epoch': 0.1}
  3%|▎         | 22/630 [00:27<08:35,  1.18it/s]  4%|▎         | 23/630 [00:28<08:34,  1.18it/s]                                                {'loss': 0.926, 'grad_norm': 1.1264083695686813, 'learning_rate': 1.990180032733224e-06, 'epoch': 0.11}
  4%|▎         | 23/630 [00:28<08:34,  1.18it/s]  4%|▍         | 24/630 [00:29<08:32,  1.18it/s]                                                {'loss': 0.9714, 'grad_norm': 1.4838961964509663, 'learning_rate': 1.9869067103109657e-06, 'epoch': 0.11}
  4%|▍         | 24/630 [00:29<08:32,  1.18it/s]  4%|▍         | 25/630 [00:29<08:31,  1.18it/s]                                                {'loss': 0.8357, 'grad_norm': 1.2716698021692818, 'learning_rate': 1.983633387888707e-06, 'epoch': 0.12}
  4%|▍         | 25/630 [00:29<08:31,  1.18it/s]  4%|▍         | 26/630 [00:30<08:29,  1.18it/s]                                                {'loss': 0.9287, 'grad_norm': 0.9893359807174108, 'learning_rate': 1.9803600654664483e-06, 'epoch': 0.12}
  4%|▍         | 26/630 [00:30<08:29,  1.18it/s]  4%|▍         | 27/630 [00:31<08:28,  1.19it/s]                                                {'loss': 0.8455, 'grad_norm': 1.2659278577734474, 'learning_rate': 1.9770867430441897e-06, 'epoch': 0.13}
  4%|▍         | 27/630 [00:31<08:28,  1.19it/s]  4%|▍         | 28/630 [00:32<08:27,  1.19it/s]                                                {'loss': 0.8993, 'grad_norm': 1.1580611617020686, 'learning_rate': 1.9738134206219314e-06, 'epoch': 0.13}
  4%|▍         | 28/630 [00:32<08:27,  1.19it/s]  5%|▍         | 29/630 [00:33<08:26,  1.19it/s]                                                {'loss': 0.8933, 'grad_norm': 1.2722105201853142, 'learning_rate': 1.9705400981996723e-06, 'epoch': 0.14}
  5%|▍         | 29/630 [00:33<08:26,  1.19it/s]  5%|▍         | 30/630 [00:34<08:25,  1.19it/s]                                                {'loss': 0.9623, 'grad_norm': 1.0940893751060414, 'learning_rate': 1.967266775777414e-06, 'epoch': 0.14}
  5%|▍         | 30/630 [00:34<08:25,  1.19it/s]  5%|▍         | 31/630 [00:34<08:24,  1.19it/s]                                                {'loss': 0.8248, 'grad_norm': 1.0538200694285016, 'learning_rate': 1.9639934533551554e-06, 'epoch': 0.15}
  5%|▍         | 31/630 [00:34<08:24,  1.19it/s]  5%|▌         | 32/630 [00:35<08:23,  1.19it/s]                                                {'loss': 0.7886, 'grad_norm': 1.014250065764855, 'learning_rate': 1.9607201309328968e-06, 'epoch': 0.15}
  5%|▌         | 32/630 [00:35<08:23,  1.19it/s]  5%|▌         | 33/630 [00:36<08:21,  1.19it/s]                                                {'loss': 0.834, 'grad_norm': 1.0987865202445393, 'learning_rate': 1.957446808510638e-06, 'epoch': 0.16}
  5%|▌         | 33/630 [00:36<08:21,  1.19it/s]  5%|▌         | 34/630 [00:37<08:20,  1.19it/s]                                                {'loss': 0.7809, 'grad_norm': 1.1098918546881624, 'learning_rate': 1.9541734860883794e-06, 'epoch': 0.16}
  5%|▌         | 34/630 [00:37<08:20,  1.19it/s]  6%|▌         | 35/630 [00:38<08:20,  1.19it/s]                                                {'loss': 0.7505, 'grad_norm': 0.8963097272408176, 'learning_rate': 1.950900163666121e-06, 'epoch': 0.17}
  6%|▌         | 35/630 [00:38<08:20,  1.19it/s]  6%|▌         | 36/630 [00:39<08:19,  1.19it/s]                                                {'loss': 0.8688, 'grad_norm': 1.0562404140953912, 'learning_rate': 1.9476268412438625e-06, 'epoch': 0.17}
  6%|▌         | 36/630 [00:39<08:19,  1.19it/s]  6%|▌         | 37/630 [00:39<08:18,  1.19it/s]                                                {'loss': 0.8, 'grad_norm': 1.1198783126816974, 'learning_rate': 1.944353518821604e-06, 'epoch': 0.18}
  6%|▌         | 37/630 [00:39<08:18,  1.19it/s]  6%|▌         | 38/630 [00:40<08:18,  1.19it/s]                                                {'loss': 0.8114, 'grad_norm': 1.0791793488862158, 'learning_rate': 1.941080196399345e-06, 'epoch': 0.18}
  6%|▌         | 38/630 [00:40<08:18,  1.19it/s]  6%|▌         | 39/630 [00:41<08:16,  1.19it/s]                                                {'loss': 0.827, 'grad_norm': 0.8274526207383774, 'learning_rate': 1.9378068739770865e-06, 'epoch': 0.19}
  6%|▌         | 39/630 [00:41<08:16,  1.19it/s]  6%|▋         | 40/630 [00:42<08:16,  1.19it/s]                                                {'loss': 0.8549, 'grad_norm': 0.9685880989192143, 'learning_rate': 1.934533551554828e-06, 'epoch': 0.19}
  6%|▋         | 40/630 [00:42<08:16,  1.19it/s]  7%|▋         | 41/630 [00:43<08:15,  1.19it/s]                                                {'loss': 0.8674, 'grad_norm': 1.2243405587417104, 'learning_rate': 1.9312602291325696e-06, 'epoch': 0.2}
  7%|▋         | 41/630 [00:43<08:15,  1.19it/s]  7%|▋         | 42/630 [00:44<08:15,  1.19it/s]                                                {'loss': 0.77, 'grad_norm': 0.8102337408526389, 'learning_rate': 1.927986906710311e-06, 'epoch': 0.2}
  7%|▋         | 42/630 [00:44<08:15,  1.19it/s]  7%|▋         | 43/630 [00:45<08:14,  1.19it/s]                                                {'loss': 0.8951, 'grad_norm': 0.9265975840481815, 'learning_rate': 1.9247135842880523e-06, 'epoch': 0.2}
  7%|▋         | 43/630 [00:45<08:14,  1.19it/s]  7%|▋         | 44/630 [00:45<08:13,  1.19it/s]                                                {'loss': 0.9158, 'grad_norm': 1.2514582099524425, 'learning_rate': 1.9214402618657936e-06, 'epoch': 0.21}
  7%|▋         | 44/630 [00:45<08:13,  1.19it/s]  7%|▋         | 45/630 [00:46<08:12,  1.19it/s]                                                {'loss': 0.8758, 'grad_norm': 1.4604702933288194, 'learning_rate': 1.918166939443535e-06, 'epoch': 0.21}
  7%|▋         | 45/630 [00:46<08:12,  1.19it/s]  7%|▋         | 46/630 [00:47<08:11,  1.19it/s]                                                {'loss': 0.8517, 'grad_norm': 0.9792474710773205, 'learning_rate': 1.9148936170212767e-06, 'epoch': 0.22}
  7%|▋         | 46/630 [00:47<08:11,  1.19it/s]  7%|▋         | 47/630 [00:48<08:09,  1.19it/s]                                                {'loss': 0.9021, 'grad_norm': 1.1399865702806664, 'learning_rate': 1.911620294599018e-06, 'epoch': 0.22}
  7%|▋         | 47/630 [00:48<08:09,  1.19it/s]  8%|▊         | 48/630 [00:49<08:09,  1.19it/s]                                                {'loss': 0.8483, 'grad_norm': 1.1819472493584264, 'learning_rate': 1.9083469721767594e-06, 'epoch': 0.23}
  8%|▊         | 48/630 [00:49<08:09,  1.19it/s]  8%|▊         | 49/630 [00:50<08:08,  1.19it/s]                                                {'loss': 0.796, 'grad_norm': 0.9071494966730395, 'learning_rate': 1.9050736497545007e-06, 'epoch': 0.23}
  8%|▊         | 49/630 [00:50<08:08,  1.19it/s]  8%|▊         | 50/630 [00:50<08:08,  1.19it/s]                                                {'loss': 0.8244, 'grad_norm': 0.9943429849149373, 'learning_rate': 1.901800327332242e-06, 'epoch': 0.24}
  8%|▊         | 50/630 [00:50<08:08,  1.19it/s]  8%|▊         | 51/630 [00:51<08:08,  1.19it/s]                                                {'loss': 0.7494, 'grad_norm': 0.8700908968563467, 'learning_rate': 1.8985270049099836e-06, 'epoch': 0.24}
  8%|▊         | 51/630 [00:51<08:08,  1.19it/s]  8%|▊         | 52/630 [00:52<08:07,  1.19it/s]                                                {'loss': 0.7909, 'grad_norm': 0.8691339262340403, 'learning_rate': 1.895253682487725e-06, 'epoch': 0.25}
  8%|▊         | 52/630 [00:52<08:07,  1.19it/s]  8%|▊         | 53/630 [00:53<08:06,  1.19it/s]                                                {'loss': 0.79, 'grad_norm': 1.0561455517023561, 'learning_rate': 1.8919803600654665e-06, 'epoch': 0.25}
  8%|▊         | 53/630 [00:53<08:06,  1.19it/s]  9%|▊         | 54/630 [00:54<08:04,  1.19it/s]                                                {'loss': 0.9069, 'grad_norm': 1.1094048685908835, 'learning_rate': 1.8887070376432076e-06, 'epoch': 0.26}
  9%|▊         | 54/630 [00:54<08:04,  1.19it/s]  9%|▊         | 55/630 [00:55<08:02,  1.19it/s]                                                {'loss': 0.8428, 'grad_norm': 2.327479770366684, 'learning_rate': 1.8854337152209492e-06, 'epoch': 0.26}
  9%|▊         | 55/630 [00:55<08:02,  1.19it/s]  9%|▉         | 56/630 [00:55<08:02,  1.19it/s]                                                {'loss': 0.8245, 'grad_norm': 1.195693754440559, 'learning_rate': 1.8821603927986907e-06, 'epoch': 0.27}
  9%|▉         | 56/630 [00:55<08:02,  1.19it/s]  9%|▉         | 57/630 [00:56<08:02,  1.19it/s]                                                {'loss': 0.7305, 'grad_norm': 1.077702714423505, 'learning_rate': 1.8788870703764318e-06, 'epoch': 0.27}
  9%|▉         | 57/630 [00:56<08:02,  1.19it/s]  9%|▉         | 58/630 [00:57<08:02,  1.19it/s]                                                {'loss': 0.8269, 'grad_norm': 0.8740078560762553, 'learning_rate': 1.8756137479541734e-06, 'epoch': 0.28}
  9%|▉         | 58/630 [00:57<08:02,  1.19it/s]  9%|▉         | 59/630 [00:58<08:00,  1.19it/s]                                                {'loss': 0.8307, 'grad_norm': 1.140116362132853, 'learning_rate': 1.872340425531915e-06, 'epoch': 0.28}
  9%|▉         | 59/630 [00:58<08:00,  1.19it/s] 10%|▉         | 60/630 [00:59<08:00,  1.19it/s]                                                {'loss': 0.93, 'grad_norm': 1.1734730206394366, 'learning_rate': 1.8690671031096563e-06, 'epoch': 0.29}
 10%|▉         | 60/630 [00:59<08:00,  1.19it/s] 10%|▉         | 61/630 [01:00<07:58,  1.19it/s]                                                {'loss': 0.8197, 'grad_norm': 1.0172593713258125, 'learning_rate': 1.8657937806873976e-06, 'epoch': 0.29}
 10%|▉         | 61/630 [01:00<07:58,  1.19it/s] 10%|▉         | 62/630 [01:00<07:57,  1.19it/s]                                                {'loss': 0.6963, 'grad_norm': 0.8674899063496317, 'learning_rate': 1.862520458265139e-06, 'epoch': 0.3}
 10%|▉         | 62/630 [01:00<07:57,  1.19it/s] 10%|█         | 63/630 [01:01<07:58,  1.18it/s]                                                {'loss': 0.8752, 'grad_norm': 1.0963824757705662, 'learning_rate': 1.8592471358428805e-06, 'epoch': 0.3}
 10%|█         | 63/630 [01:01<07:58,  1.18it/s] 10%|█         | 64/630 [01:02<07:58,  1.18it/s]                                                {'loss': 0.8578, 'grad_norm': 0.9516478610192267, 'learning_rate': 1.8559738134206218e-06, 'epoch': 0.3}
 10%|█         | 64/630 [01:02<07:58,  1.18it/s] 10%|█         | 65/630 [01:03<07:57,  1.18it/s]                                                {'loss': 0.7941, 'grad_norm': 1.0445651998689638, 'learning_rate': 1.8527004909983632e-06, 'epoch': 0.31}
 10%|█         | 65/630 [01:03<07:57,  1.18it/s] 10%|█         | 66/630 [01:04<07:56,  1.18it/s]                                                {'loss': 0.9539, 'grad_norm': 0.9950824881406888, 'learning_rate': 1.8494271685761047e-06, 'epoch': 0.31}
 10%|█         | 66/630 [01:04<07:56,  1.18it/s] 11%|█         | 67/630 [01:05<07:55,  1.18it/s]                                                {'loss': 0.9098, 'grad_norm': 1.2435362985004668, 'learning_rate': 1.8461538461538462e-06, 'epoch': 0.32}
 11%|█         | 67/630 [01:05<07:55,  1.18it/s] 11%|█         | 68/630 [01:06<07:54,  1.18it/s]                                                {'loss': 0.8197, 'grad_norm': 1.0013911160901698, 'learning_rate': 1.8428805237315874e-06, 'epoch': 0.32}
 11%|█         | 68/630 [01:06<07:54,  1.18it/s] 11%|█         | 69/630 [01:06<07:53,  1.19it/s]                                                {'loss': 0.7836, 'grad_norm': 0.898111886674174, 'learning_rate': 1.839607201309329e-06, 'epoch': 0.33}
 11%|█         | 69/630 [01:06<07:53,  1.19it/s] 11%|█         | 70/630 [01:07<07:51,  1.19it/s]                                                {'loss': 0.7871, 'grad_norm': 0.8461648165054596, 'learning_rate': 1.8363338788870705e-06, 'epoch': 0.33}
 11%|█         | 70/630 [01:07<07:51,  1.19it/s] 11%|█▏        | 71/630 [01:08<07:50,  1.19it/s]                                                {'loss': 0.8335, 'grad_norm': 1.0347320761900125, 'learning_rate': 1.8330605564648116e-06, 'epoch': 0.34}
 11%|█▏        | 71/630 [01:08<07:50,  1.19it/s] 11%|█▏        | 72/630 [01:09<07:50,  1.19it/s]                                                {'loss': 0.7795, 'grad_norm': 0.9931334100460184, 'learning_rate': 1.8297872340425531e-06, 'epoch': 0.34}
 11%|█▏        | 72/630 [01:09<07:50,  1.19it/s] 12%|█▏        | 73/630 [01:10<07:49,  1.19it/s]                                                {'loss': 0.8546, 'grad_norm': 1.189112359645928, 'learning_rate': 1.8265139116202945e-06, 'epoch': 0.35}
 12%|█▏        | 73/630 [01:10<07:49,  1.19it/s] 12%|█▏        | 74/630 [01:11<08:09,  1.14it/s]                                                {'loss': 0.9218, 'grad_norm': 1.3313427703990957, 'learning_rate': 1.823240589198036e-06, 'epoch': 0.35}
 12%|█▏        | 74/630 [01:11<08:09,  1.14it/s] 12%|█▏        | 75/630 [01:12<08:09,  1.13it/s]                                                {'loss': 0.8549, 'grad_norm': 1.1653497723187562, 'learning_rate': 1.8199672667757774e-06, 'epoch': 0.36}
 12%|█▏        | 75/630 [01:12<08:09,  1.13it/s] 12%|█▏        | 76/630 [01:12<08:02,  1.15it/s]                                                {'loss': 0.7847, 'grad_norm': 0.9100011929706823, 'learning_rate': 1.8166939443535187e-06, 'epoch': 0.36}
 12%|█▏        | 76/630 [01:12<08:02,  1.15it/s] 12%|█▏        | 77/630 [01:13<07:56,  1.16it/s]                                                {'loss': 0.7847, 'grad_norm': 0.8749015603005843, 'learning_rate': 1.8134206219312602e-06, 'epoch': 0.37}
 12%|█▏        | 77/630 [01:13<07:56,  1.16it/s] 12%|█▏        | 78/630 [01:14<07:52,  1.17it/s]                                                {'loss': 0.904, 'grad_norm': 1.0025059452995224, 'learning_rate': 1.8101472995090016e-06, 'epoch': 0.37}
 12%|█▏        | 78/630 [01:14<07:52,  1.17it/s] 13%|█▎        | 79/630 [01:15<07:49,  1.17it/s]                                                {'loss': 0.7275, 'grad_norm': 0.8745667627543311, 'learning_rate': 1.806873977086743e-06, 'epoch': 0.38}
 13%|█▎        | 79/630 [01:15<07:49,  1.17it/s] 13%|█▎        | 80/630 [01:16<07:47,  1.18it/s]                                                {'loss': 0.8213, 'grad_norm': 1.0649893678280413, 'learning_rate': 1.8036006546644845e-06, 'epoch': 0.38}
 13%|█▎        | 80/630 [01:16<07:47,  1.18it/s] 13%|█▎        | 81/630 [01:17<07:45,  1.18it/s]                                                {'loss': 0.933, 'grad_norm': 1.14744394392308, 'learning_rate': 1.8003273322422258e-06, 'epoch': 0.39}
 13%|█▎        | 81/630 [01:17<07:45,  1.18it/s] 13%|█▎        | 82/630 [01:18<07:44,  1.18it/s]                                                {'loss': 0.8484, 'grad_norm': 1.0948358469618966, 'learning_rate': 1.7970540098199671e-06, 'epoch': 0.39}
 13%|█▎        | 82/630 [01:18<07:44,  1.18it/s] 13%|█▎        | 83/630 [01:18<07:42,  1.18it/s]                                                {'loss': 0.8147, 'grad_norm': 1.058916242468949, 'learning_rate': 1.7937806873977087e-06, 'epoch': 0.4}
 13%|█▎        | 83/630 [01:18<07:42,  1.18it/s] 13%|█▎        | 84/630 [01:19<07:41,  1.18it/s]                                                {'loss': 0.7723, 'grad_norm': 0.8724243178105808, 'learning_rate': 1.79050736497545e-06, 'epoch': 0.4}
 13%|█▎        | 84/630 [01:19<07:41,  1.18it/s] 13%|█▎        | 85/630 [01:20<07:40,  1.18it/s]                                                {'loss': 0.8057, 'grad_norm': 0.984614076580308, 'learning_rate': 1.7872340425531913e-06, 'epoch': 0.4}
 13%|█▎        | 85/630 [01:20<07:40,  1.18it/s] 14%|█▎        | 86/630 [01:21<07:40,  1.18it/s]                                                {'loss': 0.803, 'grad_norm': 1.131759876406729, 'learning_rate': 1.7839607201309329e-06, 'epoch': 0.41}
 14%|█▎        | 86/630 [01:21<07:40,  1.18it/s] 14%|█▍        | 87/630 [01:22<07:41,  1.18it/s]                                                {'loss': 0.8158, 'grad_norm': 1.0394512537230822, 'learning_rate': 1.7806873977086742e-06, 'epoch': 0.41}
 14%|█▍        | 87/630 [01:22<07:41,  1.18it/s] 14%|█▍        | 88/630 [01:23<07:39,  1.18it/s]                                                {'loss': 0.7537, 'grad_norm': 1.0165140711225038, 'learning_rate': 1.7774140752864158e-06, 'epoch': 0.42}
 14%|█▍        | 88/630 [01:23<07:39,  1.18it/s] 14%|█▍        | 89/630 [01:23<07:37,  1.18it/s]                                                {'loss': 0.8498, 'grad_norm': 1.3615199581830923, 'learning_rate': 1.7741407528641569e-06, 'epoch': 0.42}
 14%|█▍        | 89/630 [01:23<07:37,  1.18it/s] 14%|█▍        | 90/630 [01:24<07:36,  1.18it/s]                                                {'loss': 0.7628, 'grad_norm': 1.0774033982722508, 'learning_rate': 1.7708674304418984e-06, 'epoch': 0.43}
 14%|█▍        | 90/630 [01:24<07:36,  1.18it/s] 14%|█▍        | 91/630 [01:25<07:35,  1.18it/s]                                                {'loss': 0.9236, 'grad_norm': 0.9882147227755868, 'learning_rate': 1.76759410801964e-06, 'epoch': 0.43}
 14%|█▍        | 91/630 [01:25<07:35,  1.18it/s] 15%|█▍        | 92/630 [01:26<07:34,  1.18it/s]                                                {'loss': 0.7836, 'grad_norm': 0.9992493686438562, 'learning_rate': 1.764320785597381e-06, 'epoch': 0.44}
 15%|█▍        | 92/630 [01:26<07:34,  1.18it/s] 15%|█▍        | 93/630 [01:27<07:33,  1.19it/s]                                                {'loss': 0.8728, 'grad_norm': 1.4720894341269184, 'learning_rate': 1.7610474631751227e-06, 'epoch': 0.44}
 15%|█▍        | 93/630 [01:27<07:33,  1.19it/s] 15%|█▍        | 94/630 [01:28<07:31,  1.19it/s]                                                {'loss': 0.8632, 'grad_norm': 1.0856825832959076, 'learning_rate': 1.7577741407528642e-06, 'epoch': 0.45}
 15%|█▍        | 94/630 [01:28<07:31,  1.19it/s] 15%|█▌        | 95/630 [01:29<07:30,  1.19it/s]                                                {'loss': 0.7685, 'grad_norm': 1.3054667016857184, 'learning_rate': 1.7545008183306055e-06, 'epoch': 0.45}
 15%|█▌        | 95/630 [01:29<07:30,  1.19it/s] 15%|█▌        | 96/630 [01:29<07:30,  1.19it/s]                                                {'loss': 0.8527, 'grad_norm': 1.2009571527252465, 'learning_rate': 1.7512274959083469e-06, 'epoch': 0.46}
 15%|█▌        | 96/630 [01:29<07:30,  1.19it/s] 15%|█▌        | 97/630 [01:30<07:29,  1.18it/s]                                                {'loss': 0.8164, 'grad_norm': 1.3069467491574496, 'learning_rate': 1.7479541734860884e-06, 'epoch': 0.46}
 15%|█▌        | 97/630 [01:30<07:29,  1.18it/s] 16%|█▌        | 98/630 [01:31<07:28,  1.19it/s]                                                {'loss': 0.7653, 'grad_norm': 1.0396836601361419, 'learning_rate': 1.7446808510638297e-06, 'epoch': 0.47}
 16%|█▌        | 98/630 [01:31<07:28,  1.19it/s] 16%|█▌        | 99/630 [01:32<07:27,  1.19it/s]                                                {'loss': 0.8149, 'grad_norm': 1.0168292439106752, 'learning_rate': 1.741407528641571e-06, 'epoch': 0.47}
 16%|█▌        | 99/630 [01:32<07:27,  1.19it/s] 16%|█▌        | 100/630 [01:33<07:27,  1.19it/s]                                                 {'loss': 0.8033, 'grad_norm': 0.9781223089764753, 'learning_rate': 1.7381342062193124e-06, 'epoch': 0.48}
 16%|█▌        | 100/630 [01:33<07:27,  1.19it/s] 16%|█▌        | 101/630 [01:34<07:26,  1.18it/s]                                                 {'loss': 0.7814, 'grad_norm': 1.0254513281056514, 'learning_rate': 1.734860883797054e-06, 'epoch': 0.48}
 16%|█▌        | 101/630 [01:34<07:26,  1.18it/s] 16%|█▌        | 102/630 [01:34<07:25,  1.18it/s]                                                 {'loss': 0.7895, 'grad_norm': 1.0718595529697057, 'learning_rate': 1.7315875613747955e-06, 'epoch': 0.49}
 16%|█▌        | 102/630 [01:34<07:25,  1.18it/s] 16%|█▋        | 103/630 [01:35<07:24,  1.19it/s]                                                 {'loss': 0.8379, 'grad_norm': 2.196833644992481, 'learning_rate': 1.7283142389525366e-06, 'epoch': 0.49}
 16%|█▋        | 103/630 [01:35<07:24,  1.19it/s] 17%|█▋        | 104/630 [01:36<07:24,  1.18it/s]                                                 {'loss': 0.8244, 'grad_norm': 1.0746844291432327, 'learning_rate': 1.7250409165302782e-06, 'epoch': 0.5}
 17%|█▋        | 104/630 [01:36<07:24,  1.18it/s] 17%|█▋        | 105/630 [01:37<07:23,  1.18it/s]                                                 {'loss': 0.7892, 'grad_norm': 0.9921496512990456, 'learning_rate': 1.7217675941080197e-06, 'epoch': 0.5}
 17%|█▋        | 105/630 [01:37<07:23,  1.18it/s] 17%|█▋        | 106/630 [01:38<07:23,  1.18it/s]                                                 {'loss': 0.7345, 'grad_norm': 1.0466929564417407, 'learning_rate': 1.7184942716857609e-06, 'epoch': 0.5}
 17%|█▋        | 106/630 [01:38<07:23,  1.18it/s] 17%|█▋        | 107/630 [01:39<07:21,  1.18it/s]                                                 {'loss': 0.8, 'grad_norm': 1.150829890741547, 'learning_rate': 1.7152209492635024e-06, 'epoch': 0.51}
 17%|█▋        | 107/630 [01:39<07:21,  1.18it/s] 17%|█▋        | 108/630 [01:39<07:21,  1.18it/s]                                                 {'loss': 0.8333, 'grad_norm': 1.000127890375405, 'learning_rate': 1.7119476268412437e-06, 'epoch': 0.51}
 17%|█▋        | 108/630 [01:39<07:21,  1.18it/s] 17%|█▋        | 109/630 [01:40<07:21,  1.18it/s]                                                 {'loss': 0.878, 'grad_norm': 1.1700416576521295, 'learning_rate': 1.7086743044189853e-06, 'epoch': 0.52}
 17%|█▋        | 109/630 [01:40<07:21,  1.18it/s] 17%|█▋        | 110/630 [01:41<07:20,  1.18it/s]                                                 {'loss': 0.9459, 'grad_norm': 1.1963804748133426, 'learning_rate': 1.7054009819967266e-06, 'epoch': 0.52}
 17%|█▋        | 110/630 [01:41<07:20,  1.18it/s] 18%|█▊        | 111/630 [01:42<07:20,  1.18it/s]                                                 {'loss': 0.7551, 'grad_norm': 1.3083477911249595, 'learning_rate': 1.702127659574468e-06, 'epoch': 0.53}
 18%|█▊        | 111/630 [01:42<07:20,  1.18it/s] 18%|█▊        | 112/630 [01:43<07:18,  1.18it/s]                                                 {'loss': 0.7292, 'grad_norm': 1.1213949054402204, 'learning_rate': 1.6988543371522095e-06, 'epoch': 0.53}
 18%|█▊        | 112/630 [01:43<07:18,  1.18it/s] 18%|█▊        | 113/630 [01:44<07:17,  1.18it/s]                                                 {'loss': 0.8586, 'grad_norm': 1.276639184923836, 'learning_rate': 1.6955810147299508e-06, 'epoch': 0.54}
 18%|█▊        | 113/630 [01:44<07:17,  1.18it/s] 18%|█▊        | 114/630 [01:45<07:16,  1.18it/s]                                                 {'loss': 0.8807, 'grad_norm': 0.9363101133305998, 'learning_rate': 1.6923076923076922e-06, 'epoch': 0.54}
 18%|█▊        | 114/630 [01:45<07:16,  1.18it/s] 18%|█▊        | 115/630 [01:45<07:15,  1.18it/s]                                                 {'loss': 0.7863, 'grad_norm': 1.0727399208145665, 'learning_rate': 1.6890343698854337e-06, 'epoch': 0.55}
 18%|█▊        | 115/630 [01:45<07:15,  1.18it/s] 18%|█▊        | 116/630 [01:46<07:14,  1.18it/s]                                                 {'loss': 0.8294, 'grad_norm': 1.0771822080623805, 'learning_rate': 1.6857610474631753e-06, 'epoch': 0.55}
 18%|█▊        | 116/630 [01:46<07:14,  1.18it/s] 19%|█▊        | 117/630 [01:47<07:16,  1.18it/s]                                                 {'loss': 0.8063, 'grad_norm': 1.277872177812775, 'learning_rate': 1.6824877250409164e-06, 'epoch': 0.56}
 19%|█▊        | 117/630 [01:47<07:16,  1.18it/s] 19%|█▊        | 118/630 [01:48<07:13,  1.18it/s]                                                 {'loss': 0.7865, 'grad_norm': 0.991809675061767, 'learning_rate': 1.679214402618658e-06, 'epoch': 0.56}
 19%|█▊        | 118/630 [01:48<07:13,  1.18it/s] 19%|█▉        | 119/630 [01:49<07:12,  1.18it/s]                                                 {'loss': 0.6696, 'grad_norm': 0.9738557939535245, 'learning_rate': 1.6759410801963993e-06, 'epoch': 0.57}
 19%|█▉        | 119/630 [01:49<07:12,  1.18it/s] 19%|█▉        | 120/630 [01:50<07:10,  1.18it/s]                                                 {'loss': 0.7858, 'grad_norm': 1.10084949619708, 'learning_rate': 1.6726677577741406e-06, 'epoch': 0.57}
 19%|█▉        | 120/630 [01:50<07:10,  1.18it/s] 19%|█▉        | 121/630 [01:50<07:10,  1.18it/s]                                                 {'loss': 0.7644, 'grad_norm': 1.1739852872384788, 'learning_rate': 1.6693944353518821e-06, 'epoch': 0.58}
 19%|█▉        | 121/630 [01:51<07:10,  1.18it/s] 19%|█▉        | 122/630 [01:51<07:09,  1.18it/s]                                                 {'loss': 0.7748, 'grad_norm': 1.373999473890855, 'learning_rate': 1.6661211129296235e-06, 'epoch': 0.58}
 19%|█▉        | 122/630 [01:51<07:09,  1.18it/s] 20%|█▉        | 123/630 [01:52<07:08,  1.18it/s]                                                 {'loss': 0.8955, 'grad_norm': 1.0026039372485405, 'learning_rate': 1.6628477905073648e-06, 'epoch': 0.59}
 20%|█▉        | 123/630 [01:52<07:08,  1.18it/s] 20%|█▉        | 124/630 [01:53<07:06,  1.19it/s]                                                 {'loss': 0.796, 'grad_norm': 1.1037357678279294, 'learning_rate': 1.6595744680851064e-06, 'epoch': 0.59}
 20%|█▉        | 124/630 [01:53<07:06,  1.19it/s] 20%|█▉        | 125/630 [01:54<07:04,  1.19it/s]                                                 {'loss': 0.7239, 'grad_norm': 1.108482549636945, 'learning_rate': 1.6563011456628477e-06, 'epoch': 0.6}
 20%|█▉        | 125/630 [01:54<07:04,  1.19it/s] 20%|██        | 126/630 [01:55<07:06,  1.18it/s]                                                 {'loss': 0.7124, 'grad_norm': 1.310258839775791, 'learning_rate': 1.6530278232405892e-06, 'epoch': 0.6}
 20%|██        | 126/630 [01:55<07:06,  1.18it/s] 20%|██        | 127/630 [01:56<07:05,  1.18it/s]                                                 {'loss': 0.8511, 'grad_norm': 1.1967129294314922, 'learning_rate': 1.6497545008183304e-06, 'epoch': 0.6}
 20%|██        | 127/630 [01:56<07:05,  1.18it/s] 20%|██        | 128/630 [01:56<07:04,  1.18it/s]                                                 {'loss': 0.8524, 'grad_norm': 1.0434167246451942, 'learning_rate': 1.646481178396072e-06, 'epoch': 0.61}
 20%|██        | 128/630 [01:56<07:04,  1.18it/s] 20%|██        | 129/630 [01:57<07:04,  1.18it/s]                                                 {'loss': 0.8591, 'grad_norm': 0.8916227027551838, 'learning_rate': 1.6432078559738135e-06, 'epoch': 0.61}
 20%|██        | 129/630 [01:57<07:04,  1.18it/s] 21%|██        | 130/630 [01:58<07:04,  1.18it/s]                                                 {'loss': 0.8305, 'grad_norm': 1.0436307248719363, 'learning_rate': 1.6399345335515546e-06, 'epoch': 0.62}
 21%|██        | 130/630 [01:58<07:04,  1.18it/s] 21%|██        | 131/630 [01:59<07:03,  1.18it/s]                                                 {'loss': 0.821, 'grad_norm': 1.190279677431578, 'learning_rate': 1.6366612111292961e-06, 'epoch': 0.62}
 21%|██        | 131/630 [01:59<07:03,  1.18it/s] 21%|██        | 132/630 [02:00<07:01,  1.18it/s]                                                 {'loss': 0.8306, 'grad_norm': 1.051486218024514, 'learning_rate': 1.6333878887070377e-06, 'epoch': 0.63}
 21%|██        | 132/630 [02:00<07:01,  1.18it/s] 21%|██        | 133/630 [02:01<07:01,  1.18it/s]                                                 {'loss': 0.6792, 'grad_norm': 1.0065686825534628, 'learning_rate': 1.630114566284779e-06, 'epoch': 0.63}
 21%|██        | 133/630 [02:01<07:01,  1.18it/s] 21%|██▏       | 134/630 [02:02<07:00,  1.18it/s]                                                 {'loss': 0.7138, 'grad_norm': 0.8854903743151373, 'learning_rate': 1.6268412438625203e-06, 'epoch': 0.64}
 21%|██▏       | 134/630 [02:02<07:00,  1.18it/s] 21%|██▏       | 135/630 [02:02<06:59,  1.18it/s]                                                 {'loss': 0.8015, 'grad_norm': 0.9090916951421988, 'learning_rate': 1.6235679214402617e-06, 'epoch': 0.64}
 21%|██▏       | 135/630 [02:02<06:59,  1.18it/s] 22%|██▏       | 136/630 [02:03<06:58,  1.18it/s]                                                 {'loss': 0.8257, 'grad_norm': 1.0556169885966653, 'learning_rate': 1.6202945990180032e-06, 'epoch': 0.65}
 22%|██▏       | 136/630 [02:03<06:58,  1.18it/s] 22%|██▏       | 137/630 [02:04<06:57,  1.18it/s]                                                 {'loss': 0.8361, 'grad_norm': 0.9942458434129804, 'learning_rate': 1.6170212765957446e-06, 'epoch': 0.65}
 22%|██▏       | 137/630 [02:04<06:57,  1.18it/s] 22%|██▏       | 138/630 [02:05<06:56,  1.18it/s]                                                 {'loss': 0.7625, 'grad_norm': 1.2582479452200406, 'learning_rate': 1.613747954173486e-06, 'epoch': 0.66}
 22%|██▏       | 138/630 [02:05<06:56,  1.18it/s] 22%|██▏       | 139/630 [02:06<06:55,  1.18it/s]                                                 {'loss': 0.8174, 'grad_norm': 1.2033055814481413, 'learning_rate': 1.6104746317512274e-06, 'epoch': 0.66}
 22%|██▏       | 139/630 [02:06<06:55,  1.18it/s] 22%|██▏       | 140/630 [02:07<06:53,  1.18it/s]                                                 {'loss': 0.7832, 'grad_norm': 0.9940513159378402, 'learning_rate': 1.607201309328969e-06, 'epoch': 0.67}
 22%|██▏       | 140/630 [02:07<06:53,  1.18it/s] 22%|██▏       | 141/630 [02:07<06:52,  1.18it/s]                                                 {'loss': 0.7304, 'grad_norm': 1.0223937381496275, 'learning_rate': 1.6039279869067101e-06, 'epoch': 0.67}
 22%|██▏       | 141/630 [02:07<06:52,  1.18it/s] 23%|██▎       | 142/630 [02:08<06:52,  1.18it/s]                                                 {'loss': 0.7995, 'grad_norm': 1.1573868598042767, 'learning_rate': 1.6006546644844517e-06, 'epoch': 0.68}
 23%|██▎       | 142/630 [02:08<06:52,  1.18it/s] 23%|██▎       | 143/630 [02:09<06:51,  1.18it/s]                                                 {'loss': 0.8443, 'grad_norm': 1.2942767802759645, 'learning_rate': 1.5973813420621932e-06, 'epoch': 0.68}
 23%|██▎       | 143/630 [02:09<06:51,  1.18it/s] 23%|██▎       | 144/630 [02:10<06:49,  1.19it/s]                                                 {'loss': 0.7897, 'grad_norm': 0.9640713497064758, 'learning_rate': 1.5941080196399343e-06, 'epoch': 0.69}
 23%|██▎       | 144/630 [02:10<06:49,  1.19it/s] 23%|██▎       | 145/630 [02:11<06:48,  1.19it/s]                                                 {'loss': 0.8303, 'grad_norm': 1.1927274778010748, 'learning_rate': 1.5908346972176759e-06, 'epoch': 0.69}
 23%|██▎       | 145/630 [02:11<06:48,  1.19it/s] 23%|██▎       | 146/630 [02:12<06:47,  1.19it/s]                                                 {'loss': 0.821, 'grad_norm': 1.0117214017128722, 'learning_rate': 1.5875613747954172e-06, 'epoch': 0.7}
 23%|██▎       | 146/630 [02:12<06:47,  1.19it/s] 23%|██▎       | 147/630 [02:12<06:47,  1.19it/s]                                                 {'loss': 0.8233, 'grad_norm': 0.8642041051534147, 'learning_rate': 1.5842880523731588e-06, 'epoch': 0.7}
 23%|██▎       | 147/630 [02:12<06:47,  1.19it/s] 23%|██▎       | 148/630 [02:13<06:46,  1.18it/s]                                                 {'loss': 0.8319, 'grad_norm': 1.189523743618104, 'learning_rate': 1.5810147299509e-06, 'epoch': 0.7}
 23%|██▎       | 148/630 [02:13<06:46,  1.18it/s] 24%|██▎       | 149/630 [02:14<06:44,  1.19it/s]                                                 {'loss': 0.7042, 'grad_norm': 1.1358159403823185, 'learning_rate': 1.5777414075286414e-06, 'epoch': 0.71}
 24%|██▎       | 149/630 [02:14<06:44,  1.19it/s] 24%|██▍       | 150/630 [02:15<06:44,  1.19it/s]                                                 {'loss': 0.7671, 'grad_norm': 0.7480152978087851, 'learning_rate': 1.574468085106383e-06, 'epoch': 0.71}
 24%|██▍       | 150/630 [02:15<06:44,  1.19it/s] 24%|██▍       | 151/630 [02:16<06:43,  1.19it/s]                                                 {'loss': 0.9, 'grad_norm': 1.024210932309907, 'learning_rate': 1.5711947626841243e-06, 'epoch': 0.72}
 24%|██▍       | 151/630 [02:16<06:43,  1.19it/s] 24%|██▍       | 152/630 [02:17<06:42,  1.19it/s]                                                 {'loss': 0.8002, 'grad_norm': 0.9358165527876195, 'learning_rate': 1.5679214402618656e-06, 'epoch': 0.72}
 24%|██▍       | 152/630 [02:17<06:42,  1.19it/s] 24%|██▍       | 153/630 [02:18<06:42,  1.19it/s]                                                 {'loss': 0.7703, 'grad_norm': 0.7915303300283069, 'learning_rate': 1.5646481178396072e-06, 'epoch': 0.73}
 24%|██▍       | 153/630 [02:18<06:42,  1.19it/s] 24%|██▍       | 154/630 [02:18<06:42,  1.18it/s]                                                 {'loss': 0.768, 'grad_norm': 0.8263938266302008, 'learning_rate': 1.5613747954173485e-06, 'epoch': 0.73}
 24%|██▍       | 154/630 [02:18<06:42,  1.18it/s] 25%|██▍       | 155/630 [02:19<06:40,  1.19it/s]                                                 {'loss': 0.7786, 'grad_norm': 0.8970937271161632, 'learning_rate': 1.5581014729950899e-06, 'epoch': 0.74}
 25%|██▍       | 155/630 [02:19<06:40,  1.19it/s] 25%|██▍       | 156/630 [02:20<06:40,  1.18it/s]                                                 {'loss': 0.6998, 'grad_norm': 0.913787009145766, 'learning_rate': 1.5548281505728314e-06, 'epoch': 0.74}
 25%|██▍       | 156/630 [02:20<06:40,  1.18it/s] 25%|██▍       | 157/630 [02:21<06:38,  1.19it/s]                                                 {'loss': 0.9224, 'grad_norm': 1.486479851182397, 'learning_rate': 1.5515548281505727e-06, 'epoch': 0.75}
 25%|██▍       | 157/630 [02:21<06:38,  1.19it/s] 25%|██▌       | 158/630 [02:22<06:38,  1.18it/s]                                                 {'loss': 0.7488, 'grad_norm': 0.8817996421268569, 'learning_rate': 1.548281505728314e-06, 'epoch': 0.75}
 25%|██▌       | 158/630 [02:22<06:38,  1.18it/s] 25%|██▌       | 159/630 [02:23<06:39,  1.18it/s]                                                 {'loss': 0.7572, 'grad_norm': 1.2270559467169735, 'learning_rate': 1.5450081833060556e-06, 'epoch': 0.76}
 25%|██▌       | 159/630 [02:23<06:39,  1.18it/s] 25%|██▌       | 160/630 [02:23<06:37,  1.18it/s]                                                 {'loss': 0.8719, 'grad_norm': 1.3285590326519685, 'learning_rate': 1.541734860883797e-06, 'epoch': 0.76}
 25%|██▌       | 160/630 [02:23<06:37,  1.18it/s] 26%|██▌       | 161/630 [02:24<06:36,  1.18it/s]                                                 {'loss': 0.8022, 'grad_norm': 0.9602729994629798, 'learning_rate': 1.5384615384615385e-06, 'epoch': 0.77}
 26%|██▌       | 161/630 [02:24<06:36,  1.18it/s] 26%|██▌       | 162/630 [02:25<06:36,  1.18it/s]                                                 {'loss': 0.8858, 'grad_norm': 1.2448045285524123, 'learning_rate': 1.5351882160392796e-06, 'epoch': 0.77}
 26%|██▌       | 162/630 [02:25<06:36,  1.18it/s] 26%|██▌       | 163/630 [02:26<06:35,  1.18it/s]                                                 {'loss': 0.7424, 'grad_norm': 1.034301116431955, 'learning_rate': 1.5319148936170212e-06, 'epoch': 0.78}
 26%|██▌       | 163/630 [02:26<06:35,  1.18it/s] 26%|██▌       | 164/630 [02:27<06:33,  1.18it/s]                                                 {'loss': 0.7461, 'grad_norm': 0.9312090032441303, 'learning_rate': 1.5286415711947627e-06, 'epoch': 0.78}
 26%|██▌       | 164/630 [02:27<06:33,  1.18it/s] 26%|██▌       | 165/630 [02:28<06:32,  1.18it/s]                                                 {'loss': 0.7754, 'grad_norm': 0.989873170985495, 'learning_rate': 1.5253682487725038e-06, 'epoch': 0.79}
 26%|██▌       | 165/630 [02:28<06:32,  1.18it/s] 26%|██▋       | 166/630 [02:29<06:33,  1.18it/s]                                                 {'loss': 0.899, 'grad_norm': 1.3923784593242634, 'learning_rate': 1.5220949263502454e-06, 'epoch': 0.79}
 26%|██▋       | 166/630 [02:29<06:33,  1.18it/s] 27%|██▋       | 167/630 [02:29<06:31,  1.18it/s]                                                 {'loss': 0.8447, 'grad_norm': 1.289008609400371, 'learning_rate': 1.518821603927987e-06, 'epoch': 0.8}
 27%|██▋       | 167/630 [02:29<06:31,  1.18it/s] 27%|██▋       | 168/630 [02:30<06:29,  1.19it/s]                                                 {'loss': 0.8089, 'grad_norm': 0.8987223702933327, 'learning_rate': 1.5155482815057283e-06, 'epoch': 0.8}
 27%|██▋       | 168/630 [02:30<06:29,  1.19it/s] 27%|██▋       | 169/630 [02:31<06:27,  1.19it/s]                                                 {'loss': 0.7664, 'grad_norm': 0.9436603014805299, 'learning_rate': 1.5122749590834696e-06, 'epoch': 0.8}
 27%|██▋       | 169/630 [02:31<06:27,  1.19it/s] 27%|██▋       | 170/630 [02:32<06:27,  1.19it/s]                                                 {'loss': 0.7911, 'grad_norm': 0.8187036903727041, 'learning_rate': 1.5090016366612112e-06, 'epoch': 0.81}
 27%|██▋       | 170/630 [02:32<06:27,  1.19it/s] 27%|██▋       | 171/630 [02:33<06:26,  1.19it/s]                                                 {'loss': 0.8056, 'grad_norm': 0.9513835941572429, 'learning_rate': 1.5057283142389525e-06, 'epoch': 0.81}
 27%|██▋       | 171/630 [02:33<06:26,  1.19it/s] 27%|██▋       | 172/630 [02:34<06:25,  1.19it/s]                                                 {'loss': 0.7536, 'grad_norm': 0.9579643824964837, 'learning_rate': 1.5024549918166938e-06, 'epoch': 0.82}
 27%|██▋       | 172/630 [02:34<06:25,  1.19it/s] 27%|██▋       | 173/630 [02:34<06:25,  1.19it/s]                                                 {'loss': 0.7763, 'grad_norm': 0.8995302194946887, 'learning_rate': 1.4991816693944352e-06, 'epoch': 0.82}
 27%|██▋       | 173/630 [02:34<06:25,  1.19it/s] 28%|██▊       | 174/630 [02:35<06:25,  1.18it/s]                                                 {'loss': 0.7078, 'grad_norm': 0.8094470416804208, 'learning_rate': 1.4959083469721767e-06, 'epoch': 0.83}
 28%|██▊       | 174/630 [02:35<06:25,  1.18it/s] 28%|██▊       | 175/630 [02:36<06:24,  1.18it/s]                                                 {'loss': 0.8524, 'grad_norm': 1.1352576495930273, 'learning_rate': 1.4926350245499183e-06, 'epoch': 0.83}
 28%|██▊       | 175/630 [02:36<06:24,  1.18it/s] 28%|██▊       | 176/630 [02:37<06:23,  1.18it/s]                                                 {'loss': 0.7291, 'grad_norm': 0.9517775378376044, 'learning_rate': 1.4893617021276594e-06, 'epoch': 0.84}
 28%|██▊       | 176/630 [02:37<06:23,  1.18it/s] 28%|██▊       | 177/630 [02:38<06:22,  1.18it/s]                                                 {'loss': 0.7564, 'grad_norm': 0.9612691286421853, 'learning_rate': 1.486088379705401e-06, 'epoch': 0.84}
 28%|██▊       | 177/630 [02:38<06:22,  1.18it/s] 28%|██▊       | 178/630 [02:39<06:22,  1.18it/s]                                                 {'loss': 0.802, 'grad_norm': 1.0101012896698396, 'learning_rate': 1.4828150572831425e-06, 'epoch': 0.85}
 28%|██▊       | 178/630 [02:39<06:22,  1.18it/s] 28%|██▊       | 179/630 [02:40<06:21,  1.18it/s]                                                 {'loss': 0.8587, 'grad_norm': 0.9846465478767132, 'learning_rate': 1.4795417348608836e-06, 'epoch': 0.85}
 28%|██▊       | 179/630 [02:40<06:21,  1.18it/s] 29%|██▊       | 180/630 [02:40<06:21,  1.18it/s]                                                 {'loss': 0.7942, 'grad_norm': 0.9192114950407824, 'learning_rate': 1.4762684124386251e-06, 'epoch': 0.86}
 29%|██▊       | 180/630 [02:40<06:21,  1.18it/s] 29%|██▊       | 181/630 [02:41<06:21,  1.18it/s]                                                 {'loss': 0.866, 'grad_norm': 0.7824111026434951, 'learning_rate': 1.4729950900163665e-06, 'epoch': 0.86}
 29%|██▊       | 181/630 [02:41<06:21,  1.18it/s] 29%|██▉       | 182/630 [02:42<06:19,  1.18it/s]                                                 {'loss': 0.7835, 'grad_norm': 1.1889189366437785, 'learning_rate': 1.469721767594108e-06, 'epoch': 0.87}
 29%|██▉       | 182/630 [02:42<06:19,  1.18it/s] 29%|██▉       | 183/630 [02:43<06:19,  1.18it/s]                                                 {'loss': 0.8398, 'grad_norm': 1.2810447457531837, 'learning_rate': 1.4664484451718494e-06, 'epoch': 0.87}
 29%|██▉       | 183/630 [02:43<06:19,  1.18it/s] 29%|██▉       | 184/630 [02:44<06:17,  1.18it/s]                                                 {'loss': 0.7941, 'grad_norm': 0.9886505062624183, 'learning_rate': 1.4631751227495907e-06, 'epoch': 0.88}
 29%|██▉       | 184/630 [02:44<06:17,  1.18it/s] 29%|██▉       | 185/630 [02:45<06:17,  1.18it/s]                                                 {'loss': 0.7317, 'grad_norm': 0.7462119831376904, 'learning_rate': 1.4599018003273322e-06, 'epoch': 0.88}
 29%|██▉       | 185/630 [02:45<06:17,  1.18it/s] 30%|██▉       | 186/630 [02:45<06:16,  1.18it/s]                                                 {'loss': 0.7655, 'grad_norm': 0.7581203560867612, 'learning_rate': 1.4566284779050736e-06, 'epoch': 0.89}
 30%|██▉       | 186/630 [02:45<06:16,  1.18it/s] 30%|██▉       | 187/630 [02:46<06:15,  1.18it/s]                                                 {'loss': 0.7622, 'grad_norm': 0.8339598128499506, 'learning_rate': 1.453355155482815e-06, 'epoch': 0.89}
 30%|██▉       | 187/630 [02:46<06:15,  1.18it/s] 30%|██▉       | 188/630 [02:47<06:14,  1.18it/s]                                                 {'loss': 0.82, 'grad_norm': 0.8524334992900878, 'learning_rate': 1.4500818330605565e-06, 'epoch': 0.9}
 30%|██▉       | 188/630 [02:47<06:14,  1.18it/s] 30%|███       | 189/630 [02:48<06:14,  1.18it/s]                                                 {'loss': 0.7718, 'grad_norm': 0.7223493135354723, 'learning_rate': 1.446808510638298e-06, 'epoch': 0.9}
 30%|███       | 189/630 [02:48<06:14,  1.18it/s] 30%|███       | 190/630 [02:49<06:13,  1.18it/s]                                                 {'loss': 0.7934, 'grad_norm': 0.8699520570200938, 'learning_rate': 1.4435351882160391e-06, 'epoch': 0.9}
 30%|███       | 190/630 [02:49<06:13,  1.18it/s] 30%|███       | 191/630 [02:50<06:12,  1.18it/s]                                                 {'loss': 0.7916, 'grad_norm': 0.9478703528603989, 'learning_rate': 1.4402618657937807e-06, 'epoch': 0.91}
 30%|███       | 191/630 [02:50<06:12,  1.18it/s] 30%|███       | 192/630 [02:51<06:10,  1.18it/s]                                                 {'loss': 0.7121, 'grad_norm': 0.7909859381981372, 'learning_rate': 1.436988543371522e-06, 'epoch': 0.91}
 30%|███       | 192/630 [02:51<06:10,  1.18it/s] 31%|███       | 193/630 [02:51<06:09,  1.18it/s]                                                 {'loss': 0.7438, 'grad_norm': 0.9571474766427176, 'learning_rate': 1.4337152209492633e-06, 'epoch': 0.92}
 31%|███       | 193/630 [02:51<06:09,  1.18it/s] 31%|███       | 194/630 [02:52<06:09,  1.18it/s]                                                 {'loss': 0.713, 'grad_norm': 0.8754631826551922, 'learning_rate': 1.4304418985270049e-06, 'epoch': 0.92}
 31%|███       | 194/630 [02:52<06:09,  1.18it/s] 31%|███       | 195/630 [02:53<06:09,  1.18it/s]                                                 {'loss': 0.6868, 'grad_norm': 0.8289954718436015, 'learning_rate': 1.4271685761047462e-06, 'epoch': 0.93}
 31%|███       | 195/630 [02:53<06:09,  1.18it/s] 31%|███       | 196/630 [02:54<06:07,  1.18it/s]                                                 {'loss': 0.7878, 'grad_norm': 1.0858934897392984, 'learning_rate': 1.4238952536824878e-06, 'epoch': 0.93}
 31%|███       | 196/630 [02:54<06:07,  1.18it/s] 31%|███▏      | 197/630 [02:55<06:05,  1.18it/s]                                                 {'loss': 0.7755, 'grad_norm': 1.2780391954659747, 'learning_rate': 1.420621931260229e-06, 'epoch': 0.94}
 31%|███▏      | 197/630 [02:55<06:05,  1.18it/s] 31%|███▏      | 198/630 [02:56<06:04,  1.18it/s]                                                 {'loss': 0.7917, 'grad_norm': 1.014645020292035, 'learning_rate': 1.4173486088379704e-06, 'epoch': 0.94}
 31%|███▏      | 198/630 [02:56<06:04,  1.18it/s] 32%|███▏      | 199/630 [02:56<06:03,  1.18it/s]                                                 {'loss': 0.7681, 'grad_norm': 1.2867535189556223, 'learning_rate': 1.414075286415712e-06, 'epoch': 0.95}
 32%|███▏      | 199/630 [02:56<06:03,  1.18it/s] 32%|███▏      | 200/630 [02:57<06:03,  1.18it/s]                                                 {'loss': 0.7725, 'grad_norm': 1.0519444723479336, 'learning_rate': 1.4108019639934531e-06, 'epoch': 0.95}
 32%|███▏      | 200/630 [02:57<06:03,  1.18it/s] 32%|███▏      | 201/630 [02:58<06:02,  1.18it/s]                                                 {'loss': 0.8253, 'grad_norm': 1.1047866313811205, 'learning_rate': 1.4075286415711947e-06, 'epoch': 0.96}
 32%|███▏      | 201/630 [02:58<06:02,  1.18it/s] 32%|███▏      | 202/630 [02:59<06:01,  1.18it/s]                                                 {'loss': 0.799, 'grad_norm': 0.8477703724978856, 'learning_rate': 1.4042553191489362e-06, 'epoch': 0.96}
 32%|███▏      | 202/630 [02:59<06:01,  1.18it/s] 32%|███▏      | 203/630 [03:00<05:59,  1.19it/s]                                                 {'loss': 0.7802, 'grad_norm': 0.9277864842258435, 'learning_rate': 1.4009819967266775e-06, 'epoch': 0.97}
 32%|███▏      | 203/630 [03:00<05:59,  1.19it/s] 32%|███▏      | 204/630 [03:01<05:58,  1.19it/s]                                                 {'loss': 0.9688, 'grad_norm': 1.3294415650469231, 'learning_rate': 1.3977086743044189e-06, 'epoch': 0.97}
 32%|███▏      | 204/630 [03:01<05:58,  1.19it/s] 33%|███▎      | 205/630 [03:01<05:57,  1.19it/s]                                                 {'loss': 0.6639, 'grad_norm': 0.754017547937755, 'learning_rate': 1.3944353518821604e-06, 'epoch': 0.98}
 33%|███▎      | 205/630 [03:01<05:57,  1.19it/s] 33%|███▎      | 206/630 [03:02<05:56,  1.19it/s]                                                 {'loss': 0.8936, 'grad_norm': 0.8585238490880491, 'learning_rate': 1.3911620294599018e-06, 'epoch': 0.98}
 33%|███▎      | 206/630 [03:02<05:56,  1.19it/s] 33%|███▎      | 207/630 [03:03<05:58,  1.18it/s]                                                 {'loss': 0.7413, 'grad_norm': 0.8007514749162585, 'learning_rate': 1.387888707037643e-06, 'epoch': 0.99}
 33%|███▎      | 207/630 [03:03<05:58,  1.18it/s] 33%|███▎      | 208/630 [03:04<05:56,  1.18it/s]                                                 {'loss': 0.9142, 'grad_norm': 1.4985333297860464, 'learning_rate': 1.3846153846153844e-06, 'epoch': 0.99}
 33%|███▎      | 208/630 [03:04<05:56,  1.18it/s] 33%|███▎      | 209/630 [03:05<05:55,  1.18it/s]                                                 {'loss': 0.7609, 'grad_norm': 0.8750236674952483, 'learning_rate': 1.381342062193126e-06, 'epoch': 1.0}
 33%|███▎      | 209/630 [03:05<05:55,  1.18it/s] 33%|███▎      | 210/630 [03:06<05:55,  1.18it/s]                                                 {'loss': 0.7733, 'grad_norm': 0.9652352163035702, 'learning_rate': 1.3780687397708675e-06, 'epoch': 1.0}
 33%|███▎      | 210/630 [03:06<05:55,  1.18it/s] 33%|███▎      | 211/630 [03:07<05:54,  1.18it/s]                                                 {'loss': 0.7764, 'grad_norm': 1.0587549346163847, 'learning_rate': 1.3747954173486086e-06, 'epoch': 1.0}
 33%|███▎      | 211/630 [03:07<05:54,  1.18it/s] 34%|███▎      | 212/630 [03:07<05:53,  1.18it/s]                                                 {'loss': 0.7955, 'grad_norm': 1.05255668676908, 'learning_rate': 1.3715220949263502e-06, 'epoch': 1.01}
 34%|███▎      | 212/630 [03:07<05:53,  1.18it/s] 34%|███▍      | 213/630 [03:08<05:53,  1.18it/s]                                                 {'loss': 0.8318, 'grad_norm': 1.1387764922247772, 'learning_rate': 1.3682487725040917e-06, 'epoch': 1.01}
 34%|███▍      | 213/630 [03:08<05:53,  1.18it/s] 34%|███▍      | 214/630 [03:09<05:51,  1.18it/s]                                                 {'loss': 0.8049, 'grad_norm': 1.0976686671620393, 'learning_rate': 1.3649754500818329e-06, 'epoch': 1.02}
 34%|███▍      | 214/630 [03:09<05:51,  1.18it/s] 34%|███▍      | 215/630 [03:10<05:50,  1.18it/s]                                                 {'loss': 0.7581, 'grad_norm': 0.922212057671395, 'learning_rate': 1.3617021276595744e-06, 'epoch': 1.02}
 34%|███▍      | 215/630 [03:10<05:50,  1.18it/s] 34%|███▍      | 216/630 [03:11<05:50,  1.18it/s]                                                 {'loss': 0.6948, 'grad_norm': 0.8989378490530064, 'learning_rate': 1.358428805237316e-06, 'epoch': 1.03}
 34%|███▍      | 216/630 [03:11<05:50,  1.18it/s] 34%|███▍      | 217/630 [03:12<05:49,  1.18it/s]                                                 {'loss': 0.7171, 'grad_norm': 0.8162764604955123, 'learning_rate': 1.3551554828150573e-06, 'epoch': 1.03}
 34%|███▍      | 217/630 [03:12<05:49,  1.18it/s] 35%|███▍      | 218/630 [03:13<05:49,  1.18it/s]                                                 {'loss': 0.7232, 'grad_norm': 1.0410811156908375, 'learning_rate': 1.3518821603927986e-06, 'epoch': 1.04}
 35%|███▍      | 218/630 [03:13<05:49,  1.18it/s] 35%|███▍      | 219/630 [03:13<05:48,  1.18it/s]                                                 {'loss': 0.7486, 'grad_norm': 0.9328781032637247, 'learning_rate': 1.34860883797054e-06, 'epoch': 1.04}
 35%|███▍      | 219/630 [03:13<05:48,  1.18it/s] 35%|███▍      | 220/630 [03:14<05:47,  1.18it/s]                                                 {'loss': 0.849, 'grad_norm': 0.8611392431497057, 'learning_rate': 1.3453355155482815e-06, 'epoch': 1.05}
 35%|███▍      | 220/630 [03:14<05:47,  1.18it/s] 35%|███▌      | 221/630 [03:15<05:46,  1.18it/s]                                                 {'loss': 0.8671, 'grad_norm': 1.3586586584318407, 'learning_rate': 1.3420621931260228e-06, 'epoch': 1.05}
 35%|███▌      | 221/630 [03:15<05:46,  1.18it/s] 35%|███▌      | 222/630 [03:16<05:46,  1.18it/s]                                                 {'loss': 0.7442, 'grad_norm': 1.0512661118197286, 'learning_rate': 1.3387888707037642e-06, 'epoch': 1.06}
 35%|███▌      | 222/630 [03:16<05:46,  1.18it/s] 35%|███▌      | 223/630 [03:17<05:45,  1.18it/s]                                                 {'loss': 0.7913, 'grad_norm': 0.7900571566714044, 'learning_rate': 1.3355155482815057e-06, 'epoch': 1.06}
 35%|███▌      | 223/630 [03:17<05:45,  1.18it/s] 36%|███▌      | 224/630 [03:18<05:45,  1.18it/s]                                                 {'loss': 0.7687, 'grad_norm': 0.8697292571256573, 'learning_rate': 1.3322422258592473e-06, 'epoch': 1.07}
 36%|███▌      | 224/630 [03:18<05:45,  1.18it/s] 36%|███▌      | 225/630 [03:18<05:44,  1.18it/s]                                                 {'loss': 0.8038, 'grad_norm': 1.0232002603781383, 'learning_rate': 1.3289689034369884e-06, 'epoch': 1.07}
 36%|███▌      | 225/630 [03:18<05:44,  1.18it/s] 36%|███▌      | 226/630 [03:19<05:42,  1.18it/s]                                                 {'loss': 0.7667, 'grad_norm': 1.1671407291078055, 'learning_rate': 1.32569558101473e-06, 'epoch': 1.08}
 36%|███▌      | 226/630 [03:19<05:42,  1.18it/s] 36%|███▌      | 227/630 [03:20<05:41,  1.18it/s]                                                 {'loss': 0.8409, 'grad_norm': 1.045284624924024, 'learning_rate': 1.3224222585924713e-06, 'epoch': 1.08}
 36%|███▌      | 227/630 [03:20<05:41,  1.18it/s] 36%|███▌      | 228/630 [03:21<05:41,  1.18it/s]                                                 {'loss': 0.8093, 'grad_norm': 1.0300790738129992, 'learning_rate': 1.3191489361702126e-06, 'epoch': 1.09}
 36%|███▌      | 228/630 [03:21<05:41,  1.18it/s] 36%|███▋      | 229/630 [03:22<05:39,  1.18it/s]                                                 {'loss': 0.7465, 'grad_norm': 1.084739935223128, 'learning_rate': 1.3158756137479541e-06, 'epoch': 1.09}
 36%|███▋      | 229/630 [03:22<05:39,  1.18it/s] 37%|███▋      | 230/630 [03:23<05:38,  1.18it/s]                                                 {'loss': 0.7786, 'grad_norm': 0.7594161880568739, 'learning_rate': 1.3126022913256955e-06, 'epoch': 1.1}
 37%|███▋      | 230/630 [03:23<05:38,  1.18it/s] 37%|███▋      | 231/630 [03:24<05:37,  1.18it/s]                                                 {'loss': 0.752, 'grad_norm': 0.9919472669093998, 'learning_rate': 1.309328968903437e-06, 'epoch': 1.1}
 37%|███▋      | 231/630 [03:24<05:37,  1.18it/s] 37%|███▋      | 232/630 [03:24<05:37,  1.18it/s]                                                 {'loss': 0.8262, 'grad_norm': 0.9686798687335842, 'learning_rate': 1.3060556464811784e-06, 'epoch': 1.1}
 37%|███▋      | 232/630 [03:24<05:37,  1.18it/s] 37%|███▋      | 233/630 [03:25<05:37,  1.18it/s]                                                 {'loss': 0.741, 'grad_norm': 0.9661691841587193, 'learning_rate': 1.3027823240589197e-06, 'epoch': 1.11}
 37%|███▋      | 233/630 [03:25<05:37,  1.18it/s] 37%|███▋      | 234/630 [03:26<05:36,  1.18it/s]                                                 {'loss': 0.783, 'grad_norm': 0.769257955061073, 'learning_rate': 1.2995090016366612e-06, 'epoch': 1.11}
 37%|███▋      | 234/630 [03:26<05:36,  1.18it/s] 37%|███▋      | 235/630 [03:27<05:35,  1.18it/s]                                                 {'loss': 0.823, 'grad_norm': 1.0339245156266146, 'learning_rate': 1.2962356792144024e-06, 'epoch': 1.12}
 37%|███▋      | 235/630 [03:27<05:35,  1.18it/s] 37%|███▋      | 236/630 [03:28<05:33,  1.18it/s]                                                 {'loss': 0.7742, 'grad_norm': 0.836084950786497, 'learning_rate': 1.292962356792144e-06, 'epoch': 1.12}
 37%|███▋      | 236/630 [03:28<05:33,  1.18it/s] 38%|███▊      | 237/630 [03:29<05:34,  1.18it/s]                                                 {'loss': 0.8141, 'grad_norm': 0.9071728624276636, 'learning_rate': 1.2896890343698855e-06, 'epoch': 1.13}
 38%|███▊      | 237/630 [03:29<05:34,  1.18it/s] 38%|███▊      | 238/630 [03:29<05:32,  1.18it/s]                                                 {'loss': 0.8223, 'grad_norm': 1.0576669726490195, 'learning_rate': 1.2864157119476268e-06, 'epoch': 1.13}
 38%|███▊      | 238/630 [03:29<05:32,  1.18it/s] 38%|███▊      | 239/630 [03:30<05:32,  1.18it/s]                                                 {'loss': 0.829, 'grad_norm': 0.8414554593079652, 'learning_rate': 1.2831423895253681e-06, 'epoch': 1.14}
 38%|███▊      | 239/630 [03:30<05:32,  1.18it/s] 38%|███▊      | 240/630 [03:31<05:31,  1.18it/s]                                                 {'loss': 0.8373, 'grad_norm': 0.9388972850605577, 'learning_rate': 1.2798690671031097e-06, 'epoch': 1.14}
 38%|███▊      | 240/630 [03:31<05:31,  1.18it/s] 38%|███▊      | 241/630 [03:32<05:29,  1.18it/s]                                                 {'loss': 0.746, 'grad_norm': 0.9539296601074116, 'learning_rate': 1.276595744680851e-06, 'epoch': 1.15}
 38%|███▊      | 241/630 [03:32<05:29,  1.18it/s] 38%|███▊      | 242/630 [03:33<05:29,  1.18it/s]                                                 {'loss': 0.7249, 'grad_norm': 0.8840827632436428, 'learning_rate': 1.2733224222585923e-06, 'epoch': 1.15}
 38%|███▊      | 242/630 [03:33<05:29,  1.18it/s] 39%|███▊      | 243/630 [03:34<05:28,  1.18it/s]                                                 {'loss': 0.8284, 'grad_norm': 0.8549475152299211, 'learning_rate': 1.270049099836334e-06, 'epoch': 1.16}
 39%|███▊      | 243/630 [03:34<05:28,  1.18it/s] 39%|███▊      | 244/630 [03:35<05:27,  1.18it/s]                                                 {'loss': 0.7522, 'grad_norm': 0.8552153151388758, 'learning_rate': 1.2667757774140752e-06, 'epoch': 1.16}
 39%|███▊      | 244/630 [03:35<05:27,  1.18it/s] 39%|███▉      | 245/630 [03:35<05:27,  1.18it/s]                                                 {'loss': 0.7438, 'grad_norm': 1.0571527318419371, 'learning_rate': 1.2635024549918168e-06, 'epoch': 1.17}
 39%|███▉      | 245/630 [03:35<05:27,  1.18it/s] 39%|███▉      | 246/630 [03:36<05:27,  1.17it/s]                                                 {'loss': 0.8225, 'grad_norm': 1.0830049807378197, 'learning_rate': 1.260229132569558e-06, 'epoch': 1.17}
 39%|███▉      | 246/630 [03:36<05:27,  1.17it/s] 39%|███▉      | 247/630 [03:37<05:26,  1.17it/s]                                                 {'loss': 0.7592, 'grad_norm': 0.814424368493528, 'learning_rate': 1.2569558101472994e-06, 'epoch': 1.18}
 39%|███▉      | 247/630 [03:37<05:26,  1.17it/s] 39%|███▉      | 248/630 [03:38<05:24,  1.18it/s]                                                 {'loss': 0.6553, 'grad_norm': 0.6846414803792201, 'learning_rate': 1.253682487725041e-06, 'epoch': 1.18}
 39%|███▉      | 248/630 [03:38<05:24,  1.18it/s] 40%|███▉      | 249/630 [03:39<05:23,  1.18it/s]                                                 {'loss': 0.7618, 'grad_norm': 0.9890773802891668, 'learning_rate': 1.2504091653027821e-06, 'epoch': 1.19}
 40%|███▉      | 249/630 [03:39<05:23,  1.18it/s] 40%|███▉      | 250/630 [03:40<05:22,  1.18it/s]                                                 {'loss': 0.7146, 'grad_norm': 0.7456396470571702, 'learning_rate': 1.2471358428805237e-06, 'epoch': 1.19}
 40%|███▉      | 250/630 [03:40<05:22,  1.18it/s] 40%|███▉      | 251/630 [03:41<05:21,  1.18it/s]                                                 {'loss': 0.7979, 'grad_norm': 0.9343317764278448, 'learning_rate': 1.2438625204582652e-06, 'epoch': 1.2}
 40%|███▉      | 251/630 [03:41<05:21,  1.18it/s] 40%|████      | 252/630 [03:41<05:20,  1.18it/s]                                                 {'loss': 0.7929, 'grad_norm': 0.7549750133528171, 'learning_rate': 1.2405891980360065e-06, 'epoch': 1.2}
 40%|████      | 252/630 [03:41<05:20,  1.18it/s] 40%|████      | 253/630 [03:42<05:19,  1.18it/s]                                                 {'loss': 0.7923, 'grad_norm': 0.9442596347804724, 'learning_rate': 1.2373158756137479e-06, 'epoch': 1.2}
 40%|████      | 253/630 [03:42<05:19,  1.18it/s] 40%|████      | 254/630 [03:43<05:18,  1.18it/s]                                                 {'loss': 0.7737, 'grad_norm': 1.087342045948733, 'learning_rate': 1.2340425531914892e-06, 'epoch': 1.21}
 40%|████      | 254/630 [03:43<05:18,  1.18it/s] 40%|████      | 255/630 [03:44<05:17,  1.18it/s]                                                 {'loss': 0.7907, 'grad_norm': 0.9620383216780298, 'learning_rate': 1.2307692307692308e-06, 'epoch': 1.21}
 40%|████      | 255/630 [03:44<05:17,  1.18it/s] 41%|████      | 256/630 [03:45<05:17,  1.18it/s]                                                 {'loss': 0.7756, 'grad_norm': 0.9706601524618034, 'learning_rate': 1.227495908346972e-06, 'epoch': 1.22}
 41%|████      | 256/630 [03:45<05:17,  1.18it/s] 41%|████      | 257/630 [03:46<05:16,  1.18it/s]                                                 {'loss': 0.8065, 'grad_norm': 1.3351311536657273, 'learning_rate': 1.2242225859247134e-06, 'epoch': 1.22}
 41%|████      | 257/630 [03:46<05:16,  1.18it/s] 41%|████      | 258/630 [03:46<05:16,  1.18it/s]                                                 {'loss': 0.7728, 'grad_norm': 0.8718959564622828, 'learning_rate': 1.220949263502455e-06, 'epoch': 1.23}
 41%|████      | 258/630 [03:46<05:16,  1.18it/s] 41%|████      | 259/630 [03:47<05:14,  1.18it/s]                                                 {'loss': 0.8468, 'grad_norm': 0.8311374355070635, 'learning_rate': 1.2176759410801965e-06, 'epoch': 1.23}
 41%|████      | 259/630 [03:47<05:14,  1.18it/s] 41%|████▏     | 260/630 [03:48<05:13,  1.18it/s]                                                 {'loss': 0.7453, 'grad_norm': 0.7867939977736906, 'learning_rate': 1.2144026186579376e-06, 'epoch': 1.24}
 41%|████▏     | 260/630 [03:48<05:13,  1.18it/s] 41%|████▏     | 261/630 [03:49<05:13,  1.18it/s]                                                 {'loss': 0.7835, 'grad_norm': 0.9339074483706612, 'learning_rate': 1.2111292962356792e-06, 'epoch': 1.24}
 41%|████▏     | 261/630 [03:49<05:13,  1.18it/s] 42%|████▏     | 262/630 [03:50<05:12,  1.18it/s]                                                 {'loss': 0.8714, 'grad_norm': 0.8250257448756608, 'learning_rate': 1.2078559738134207e-06, 'epoch': 1.25}
 42%|████▏     | 262/630 [03:50<05:12,  1.18it/s] 42%|████▏     | 263/630 [03:51<05:11,  1.18it/s]                                                 {'loss': 0.7117, 'grad_norm': 0.9919100550911712, 'learning_rate': 1.2045826513911619e-06, 'epoch': 1.25}
 42%|████▏     | 263/630 [03:51<05:11,  1.18it/s] 42%|████▏     | 264/630 [03:52<05:10,  1.18it/s]                                                 {'loss': 0.7439, 'grad_norm': 1.1299135471588582, 'learning_rate': 1.2013093289689034e-06, 'epoch': 1.26}
 42%|████▏     | 264/630 [03:52<05:10,  1.18it/s] 42%|████▏     | 265/630 [03:52<05:09,  1.18it/s]                                                 {'loss': 0.7318, 'grad_norm': 0.732472135147016, 'learning_rate': 1.1980360065466447e-06, 'epoch': 1.26}
 42%|████▏     | 265/630 [03:52<05:09,  1.18it/s] 42%|████▏     | 266/630 [03:53<05:09,  1.18it/s]                                                 {'loss': 0.8012, 'grad_norm': 0.680179064389109, 'learning_rate': 1.1947626841243863e-06, 'epoch': 1.27}
 42%|████▏     | 266/630 [03:53<05:09,  1.18it/s] 42%|████▏     | 267/630 [03:54<05:08,  1.17it/s]                                                 {'loss': 0.724, 'grad_norm': 0.8008158171205122, 'learning_rate': 1.1914893617021276e-06, 'epoch': 1.27}
 42%|████▏     | 267/630 [03:54<05:08,  1.17it/s] 43%|████▎     | 268/630 [03:55<05:07,  1.18it/s]                                                 {'loss': 0.7503, 'grad_norm': 0.9130954809195568, 'learning_rate': 1.188216039279869e-06, 'epoch': 1.28}
 43%|████▎     | 268/630 [03:55<05:07,  1.18it/s] 43%|████▎     | 269/630 [03:56<05:06,  1.18it/s]                                                 {'loss': 0.8119, 'grad_norm': 1.039845158154885, 'learning_rate': 1.1849427168576105e-06, 'epoch': 1.28}
 43%|████▎     | 269/630 [03:56<05:06,  1.18it/s] 43%|████▎     | 270/630 [03:57<05:05,  1.18it/s]                                                 {'loss': 0.6896, 'grad_norm': 1.0511227279632365, 'learning_rate': 1.1816693944353518e-06, 'epoch': 1.29}
 43%|████▎     | 270/630 [03:57<05:05,  1.18it/s] 43%|████▎     | 271/630 [03:58<05:05,  1.18it/s]                                                 {'loss': 0.7494, 'grad_norm': 0.9663328966935549, 'learning_rate': 1.1783960720130932e-06, 'epoch': 1.29}
 43%|████▎     | 271/630 [03:58<05:05,  1.18it/s] 43%|████▎     | 272/630 [03:58<05:03,  1.18it/s]                                                 {'loss': 0.818, 'grad_norm': 1.481896987233806, 'learning_rate': 1.1751227495908347e-06, 'epoch': 1.3}
 43%|████▎     | 272/630 [03:58<05:03,  1.18it/s] 43%|████▎     | 273/630 [03:59<05:02,  1.18it/s]                                                 {'loss': 0.7872, 'grad_norm': 0.9750812720999205, 'learning_rate': 1.171849427168576e-06, 'epoch': 1.3}
 43%|████▎     | 273/630 [03:59<05:02,  1.18it/s] 43%|████▎     | 274/630 [04:00<05:02,  1.18it/s]                                                 {'loss': 0.7743, 'grad_norm': 0.8690087915006176, 'learning_rate': 1.1685761047463174e-06, 'epoch': 1.3}
 43%|████▎     | 274/630 [04:00<05:02,  1.18it/s] 44%|████▎     | 275/630 [04:01<05:01,  1.18it/s]                                                 {'loss': 0.8017, 'grad_norm': 1.189570731810781, 'learning_rate': 1.165302782324059e-06, 'epoch': 1.31}
 44%|████▎     | 275/630 [04:01<05:01,  1.18it/s] 44%|████▍     | 276/630 [04:02<05:00,  1.18it/s]                                                 {'loss': 0.7044, 'grad_norm': 0.8301727096104665, 'learning_rate': 1.1620294599018003e-06, 'epoch': 1.31}
 44%|████▍     | 276/630 [04:02<05:00,  1.18it/s] 44%|████▍     | 277/630 [04:03<04:59,  1.18it/s]                                                 {'loss': 0.7923, 'grad_norm': 0.8227912468535978, 'learning_rate': 1.1587561374795416e-06, 'epoch': 1.32}
 44%|████▍     | 277/630 [04:03<04:59,  1.18it/s] 44%|████▍     | 278/630 [04:03<04:58,  1.18it/s]                                                 {'loss': 0.8209, 'grad_norm': 0.9607576606425879, 'learning_rate': 1.1554828150572832e-06, 'epoch': 1.32}
 44%|████▍     | 278/630 [04:03<04:58,  1.18it/s] 44%|████▍     | 279/630 [04:04<04:57,  1.18it/s]                                                 {'loss': 0.6949, 'grad_norm': 0.733183807759455, 'learning_rate': 1.1522094926350245e-06, 'epoch': 1.33}
 44%|████▍     | 279/630 [04:04<04:57,  1.18it/s] 44%|████▍     | 280/630 [04:05<04:58,  1.17it/s]                                                 {'loss': 0.788, 'grad_norm': 0.7959320602961047, 'learning_rate': 1.148936170212766e-06, 'epoch': 1.33}
 44%|████▍     | 280/630 [04:05<04:58,  1.17it/s] 45%|████▍     | 281/630 [04:06<04:58,  1.17it/s]                                                 {'loss': 0.7734, 'grad_norm': 0.7809221919411475, 'learning_rate': 1.1456628477905072e-06, 'epoch': 1.34}
 45%|████▍     | 281/630 [04:06<04:58,  1.17it/s] 45%|████▍     | 282/630 [04:07<04:57,  1.17it/s]                                                 {'loss': 0.7793, 'grad_norm': 0.9744702450580679, 'learning_rate': 1.1423895253682487e-06, 'epoch': 1.34}
 45%|████▍     | 282/630 [04:07<04:57,  1.17it/s] 45%|████▍     | 283/630 [04:08<04:55,  1.17it/s]                                                 {'loss': 0.8137, 'grad_norm': 0.8870497308530726, 'learning_rate': 1.1391162029459903e-06, 'epoch': 1.35}
 45%|████▍     | 283/630 [04:08<04:55,  1.17it/s] 45%|████▌     | 284/630 [04:09<04:54,  1.18it/s]                                                 {'loss': 0.7537, 'grad_norm': 0.7731617638761465, 'learning_rate': 1.1358428805237314e-06, 'epoch': 1.35}
 45%|████▌     | 284/630 [04:09<04:54,  1.18it/s] 45%|████▌     | 285/630 [04:09<04:53,  1.17it/s]                                                 {'loss': 0.7453, 'grad_norm': 0.7537037308745556, 'learning_rate': 1.132569558101473e-06, 'epoch': 1.36}
 45%|████▌     | 285/630 [04:09<04:53,  1.17it/s] 45%|████▌     | 286/630 [04:10<04:53,  1.17it/s]                                                 {'loss': 0.7728, 'grad_norm': 1.0068896377106493, 'learning_rate': 1.1292962356792145e-06, 'epoch': 1.36}
 45%|████▌     | 286/630 [04:10<04:53,  1.17it/s] 46%|████▌     | 287/630 [04:11<04:53,  1.17it/s]                                                 {'loss': 0.8404, 'grad_norm': 1.0139676666838169, 'learning_rate': 1.1260229132569558e-06, 'epoch': 1.37}
 46%|████▌     | 287/630 [04:11<04:53,  1.17it/s] 46%|████▌     | 288/630 [04:12<04:52,  1.17it/s]                                                 {'loss': 0.7406, 'grad_norm': 0.8369327229473937, 'learning_rate': 1.1227495908346971e-06, 'epoch': 1.37}
 46%|████▌     | 288/630 [04:12<04:52,  1.17it/s] 46%|████▌     | 289/630 [04:13<04:51,  1.17it/s]                                                 {'loss': 0.7655, 'grad_norm': 0.7372209166361693, 'learning_rate': 1.1194762684124387e-06, 'epoch': 1.38}
 46%|████▌     | 289/630 [04:13<04:51,  1.17it/s] 46%|████▌     | 290/630 [04:14<04:50,  1.17it/s]                                                 {'loss': 0.7251, 'grad_norm': 0.857812808022208, 'learning_rate': 1.11620294599018e-06, 'epoch': 1.38}
 46%|████▌     | 290/630 [04:14<04:50,  1.17it/s] 46%|████▌     | 291/630 [04:15<04:50,  1.17it/s]                                                 {'loss': 0.705, 'grad_norm': 1.0250493534309664, 'learning_rate': 1.1129296235679214e-06, 'epoch': 1.39}
 46%|████▌     | 291/630 [04:15<04:50,  1.17it/s] 46%|████▋     | 292/630 [04:15<04:49,  1.17it/s]                                                 {'loss': 0.7695, 'grad_norm': 0.8937402163784555, 'learning_rate': 1.1096563011456627e-06, 'epoch': 1.39}
 46%|████▋     | 292/630 [04:15<04:49,  1.17it/s] 47%|████▋     | 293/630 [04:16<04:48,  1.17it/s]                                                 {'loss': 0.7818, 'grad_norm': 0.9215190339326653, 'learning_rate': 1.1063829787234042e-06, 'epoch': 1.4}
 47%|████▋     | 293/630 [04:16<04:48,  1.17it/s] 47%|████▋     | 294/630 [04:17<04:46,  1.17it/s]                                                 {'loss': 0.7411, 'grad_norm': 0.7741895249927218, 'learning_rate': 1.1031096563011458e-06, 'epoch': 1.4}
 47%|████▋     | 294/630 [04:17<04:46,  1.17it/s] 47%|████▋     | 295/630 [04:18<04:45,  1.17it/s]                                                 {'loss': 0.701, 'grad_norm': 0.7715198782856039, 'learning_rate': 1.099836333878887e-06, 'epoch': 1.4}
 47%|████▋     | 295/630 [04:18<04:45,  1.17it/s] 47%|████▋     | 296/630 [04:19<04:45,  1.17it/s]                                                 {'loss': 0.7481, 'grad_norm': 0.7328166861494976, 'learning_rate': 1.0965630114566285e-06, 'epoch': 1.41}
 47%|████▋     | 296/630 [04:19<04:45,  1.17it/s] 47%|████▋     | 297/630 [04:20<04:43,  1.17it/s]                                                 {'loss': 0.7381, 'grad_norm': 0.7637399385029232, 'learning_rate': 1.09328968903437e-06, 'epoch': 1.41}
 47%|████▋     | 297/630 [04:20<04:43,  1.17it/s] 47%|████▋     | 298/630 [04:21<04:44,  1.17it/s]                                                 {'loss': 0.7386, 'grad_norm': 0.8161597835978655, 'learning_rate': 1.0900163666121111e-06, 'epoch': 1.42}
 47%|████▋     | 298/630 [04:21<04:44,  1.17it/s] 47%|████▋     | 299/630 [04:21<04:43,  1.17it/s]                                                 {'loss': 0.6973, 'grad_norm': 0.7153815735659849, 'learning_rate': 1.0867430441898527e-06, 'epoch': 1.42}
 47%|████▋     | 299/630 [04:21<04:43,  1.17it/s] 48%|████▊     | 300/630 [04:22<04:42,  1.17it/s]                                                 {'loss': 0.7911, 'grad_norm': 0.7852836449612803, 'learning_rate': 1.083469721767594e-06, 'epoch': 1.43}
 48%|████▊     | 300/630 [04:22<04:42,  1.17it/s] 48%|████▊     | 301/630 [04:23<04:41,  1.17it/s]                                                 {'loss': 0.7929, 'grad_norm': 0.7042762854345478, 'learning_rate': 1.0801963993453356e-06, 'epoch': 1.43}
 48%|████▊     | 301/630 [04:23<04:41,  1.17it/s] 48%|████▊     | 302/630 [04:24<04:40,  1.17it/s]                                                 {'loss': 0.753, 'grad_norm': 0.973677096820131, 'learning_rate': 1.0769230769230769e-06, 'epoch': 1.44}
 48%|████▊     | 302/630 [04:24<04:40,  1.17it/s] 48%|████▊     | 303/630 [04:25<04:39,  1.17it/s]                                                 {'loss': 0.7584, 'grad_norm': 0.8833402221960651, 'learning_rate': 1.0736497545008182e-06, 'epoch': 1.44}
 48%|████▊     | 303/630 [04:25<04:39,  1.17it/s] 48%|████▊     | 304/630 [04:26<04:38,  1.17it/s]                                                 {'loss': 0.7707, 'grad_norm': 0.860959950692127, 'learning_rate': 1.0703764320785598e-06, 'epoch': 1.45}
 48%|████▊     | 304/630 [04:26<04:38,  1.17it/s] 48%|████▊     | 305/630 [04:27<04:37,  1.17it/s]                                                 {'loss': 0.6913, 'grad_norm': 1.0197445463917925, 'learning_rate': 1.0671031096563011e-06, 'epoch': 1.45}
 48%|████▊     | 305/630 [04:27<04:37,  1.17it/s] 49%|████▊     | 306/630 [04:27<04:35,  1.18it/s]                                                 {'loss': 0.8019, 'grad_norm': 1.221880100667931, 'learning_rate': 1.0638297872340424e-06, 'epoch': 1.46}
 49%|████▊     | 306/630 [04:27<04:35,  1.18it/s] 49%|████▊     | 307/630 [04:28<04:34,  1.18it/s]                                                 {'loss': 0.7923, 'grad_norm': 1.2156035854737848, 'learning_rate': 1.060556464811784e-06, 'epoch': 1.46}
 49%|████▊     | 307/630 [04:28<04:34,  1.18it/s] 49%|████▉     | 308/630 [04:29<04:33,  1.18it/s]                                                 {'loss': 0.6783, 'grad_norm': 0.8560146660803112, 'learning_rate': 1.0572831423895253e-06, 'epoch': 1.47}
 49%|████▉     | 308/630 [04:29<04:33,  1.18it/s] 49%|████▉     | 309/630 [04:30<04:32,  1.18it/s]                                                 {'loss': 0.7235, 'grad_norm': 0.7493532441547794, 'learning_rate': 1.0540098199672667e-06, 'epoch': 1.47}
 49%|████▉     | 309/630 [04:30<04:32,  1.18it/s] 49%|████▉     | 310/630 [04:31<04:32,  1.18it/s]                                                 {'loss': 0.768, 'grad_norm': 0.8429427081790403, 'learning_rate': 1.0507364975450082e-06, 'epoch': 1.48}
 49%|████▉     | 310/630 [04:31<04:32,  1.18it/s] 49%|████▉     | 311/630 [04:32<04:31,  1.17it/s]                                                 {'loss': 0.7373, 'grad_norm': 1.0057064842338839, 'learning_rate': 1.0474631751227495e-06, 'epoch': 1.48}
 49%|████▉     | 311/630 [04:32<04:31,  1.17it/s] 50%|████▉     | 312/630 [04:32<04:30,  1.18it/s]                                                 {'loss': 0.6876, 'grad_norm': 0.7885747336315431, 'learning_rate': 1.0441898527004909e-06, 'epoch': 1.49}
 50%|████▉     | 312/630 [04:32<04:30,  1.18it/s] 50%|████▉     | 313/630 [04:33<04:28,  1.18it/s]                                                 {'loss': 0.7876, 'grad_norm': 0.9661209896910166, 'learning_rate': 1.0409165302782324e-06, 'epoch': 1.49}
 50%|████▉     | 313/630 [04:33<04:28,  1.18it/s] 50%|████▉     | 314/630 [04:34<04:28,  1.18it/s]                                                 {'loss': 0.7133, 'grad_norm': 0.7888730499767382, 'learning_rate': 1.0376432078559738e-06, 'epoch': 1.5}
 50%|████▉     | 314/630 [04:34<04:28,  1.18it/s] 50%|█████     | 315/630 [04:35<04:27,  1.18it/s]                                                 {'loss': 0.7239, 'grad_norm': 0.7817247780029656, 'learning_rate': 1.0343698854337153e-06, 'epoch': 1.5}
 50%|█████     | 315/630 [04:35<04:27,  1.18it/s] 50%|█████     | 316/630 [04:36<04:26,  1.18it/s]                                                 {'loss': 0.725, 'grad_norm': 0.7188255261858323, 'learning_rate': 1.0310965630114566e-06, 'epoch': 1.5}
 50%|█████     | 316/630 [04:36<04:26,  1.18it/s] 50%|█████     | 317/630 [04:37<04:25,  1.18it/s]                                                 {'loss': 0.7429, 'grad_norm': 0.7947756313400999, 'learning_rate': 1.027823240589198e-06, 'epoch': 1.51}
 50%|█████     | 317/630 [04:37<04:25,  1.18it/s] 50%|█████     | 318/630 [04:38<04:25,  1.18it/s]                                                 {'loss': 0.7682, 'grad_norm': 0.8913245010500674, 'learning_rate': 1.0245499181669395e-06, 'epoch': 1.51}
 50%|█████     | 318/630 [04:38<04:25,  1.18it/s] 51%|█████     | 319/630 [04:38<04:24,  1.18it/s]                                                 {'loss': 0.8604, 'grad_norm': 0.7316914159348901, 'learning_rate': 1.0212765957446806e-06, 'epoch': 1.52}
 51%|█████     | 319/630 [04:38<04:24,  1.18it/s] 51%|█████     | 320/630 [04:39<04:23,  1.18it/s]                                                 {'loss': 0.8197, 'grad_norm': 0.8264886499367199, 'learning_rate': 1.0180032733224222e-06, 'epoch': 1.52}
 51%|█████     | 320/630 [04:39<04:23,  1.18it/s] 51%|█████     | 321/630 [04:40<04:22,  1.18it/s]                                                 {'loss': 0.8625, 'grad_norm': 1.2069640371680301, 'learning_rate': 1.0147299509001637e-06, 'epoch': 1.53}
 51%|█████     | 321/630 [04:40<04:22,  1.18it/s] 51%|█████     | 322/630 [04:41<04:21,  1.18it/s]                                                 {'loss': 0.7897, 'grad_norm': 0.703813730969101, 'learning_rate': 1.011456628477905e-06, 'epoch': 1.53}
 51%|█████     | 322/630 [04:41<04:21,  1.18it/s] 51%|█████▏    | 323/630 [04:42<04:21,  1.17it/s]                                                 {'loss': 0.7921, 'grad_norm': 1.0548018860637893, 'learning_rate': 1.0081833060556464e-06, 'epoch': 1.54}
 51%|█████▏    | 323/630 [04:42<04:21,  1.17it/s] 51%|█████▏    | 324/630 [04:43<04:20,  1.17it/s]                                                 {'loss': 0.7815, 'grad_norm': 1.0429928827431878, 'learning_rate': 1.004909983633388e-06, 'epoch': 1.54}
 51%|█████▏    | 324/630 [04:43<04:20,  1.17it/s] 52%|█████▏    | 325/630 [04:44<04:19,  1.17it/s]                                                 {'loss': 0.6501, 'grad_norm': 1.0545234049486207, 'learning_rate': 1.0016366612111293e-06, 'epoch': 1.55}
 52%|█████▏    | 325/630 [04:44<04:19,  1.17it/s] 52%|█████▏    | 326/630 [04:44<04:19,  1.17it/s]                                                 {'loss': 0.7505, 'grad_norm': 0.8210080558706302, 'learning_rate': 9.983633387888706e-07, 'epoch': 1.55}
 52%|█████▏    | 326/630 [04:44<04:19,  1.17it/s] 52%|█████▏    | 327/630 [04:45<04:18,  1.17it/s]                                                 {'loss': 0.7403, 'grad_norm': 0.7764678850343683, 'learning_rate': 9.95090016366612e-07, 'epoch': 1.56}
 52%|█████▏    | 327/630 [04:45<04:18,  1.17it/s] 52%|█████▏    | 328/630 [04:46<04:17,  1.17it/s]                                                 {'loss': 0.7298, 'grad_norm': 0.7402587117187939, 'learning_rate': 9.918166939443535e-07, 'epoch': 1.56}
 52%|█████▏    | 328/630 [04:46<04:17,  1.17it/s] 52%|█████▏    | 329/630 [04:47<04:15,  1.18it/s]                                                 {'loss': 0.7577, 'grad_norm': 0.8235370695312373, 'learning_rate': 9.885433715220948e-07, 'epoch': 1.57}
 52%|█████▏    | 329/630 [04:47<04:15,  1.18it/s] 52%|█████▏    | 330/630 [04:48<04:14,  1.18it/s]                                                 {'loss': 0.8901, 'grad_norm': 0.9938842031476033, 'learning_rate': 9.852700490998362e-07, 'epoch': 1.57}
 52%|█████▏    | 330/630 [04:48<04:14,  1.18it/s] 53%|█████▎    | 331/630 [04:49<04:14,  1.18it/s]                                                 {'loss': 0.7276, 'grad_norm': 0.8479468195079416, 'learning_rate': 9.819967266775777e-07, 'epoch': 1.58}
 53%|█████▎    | 331/630 [04:49<04:14,  1.18it/s] 53%|█████▎    | 332/630 [04:49<04:13,  1.18it/s]                                                 {'loss': 0.7784, 'grad_norm': 0.7743926239741048, 'learning_rate': 9.78723404255319e-07, 'epoch': 1.58}
 53%|█████▎    | 332/630 [04:49<04:13,  1.18it/s] 53%|█████▎    | 333/630 [04:50<04:13,  1.17it/s]                                                 {'loss': 0.823, 'grad_norm': 0.9783280159715949, 'learning_rate': 9.754500818330606e-07, 'epoch': 1.59}
 53%|█████▎    | 333/630 [04:50<04:13,  1.17it/s] 53%|█████▎    | 334/630 [04:51<04:12,  1.17it/s]                                                 {'loss': 0.7409, 'grad_norm': 0.762221561865274, 'learning_rate': 9.72176759410802e-07, 'epoch': 1.59}
 53%|█████▎    | 334/630 [04:51<04:12,  1.17it/s] 53%|█████▎    | 335/630 [04:52<04:12,  1.17it/s]                                                 {'loss': 0.7631, 'grad_norm': 0.7164940455070273, 'learning_rate': 9.689034369885433e-07, 'epoch': 1.6}
 53%|█████▎    | 335/630 [04:52<04:12,  1.17it/s] 53%|█████▎    | 336/630 [04:53<04:10,  1.17it/s]                                                 {'loss': 0.7365, 'grad_norm': 0.8754340179668495, 'learning_rate': 9.656301145662848e-07, 'epoch': 1.6}
 53%|█████▎    | 336/630 [04:53<04:10,  1.17it/s] 53%|█████▎    | 337/630 [04:54<04:09,  1.17it/s]                                                 {'loss': 0.7796, 'grad_norm': 1.4683788057775566, 'learning_rate': 9.623567921440262e-07, 'epoch': 1.6}
 53%|█████▎    | 337/630 [04:54<04:09,  1.17it/s] 54%|█████▎    | 338/630 [04:55<04:08,  1.17it/s]                                                 {'loss': 0.7963, 'grad_norm': 0.8114943282870832, 'learning_rate': 9.590834697217675e-07, 'epoch': 1.61}
 54%|█████▎    | 338/630 [04:55<04:08,  1.17it/s] 54%|█████▍    | 339/630 [04:55<04:08,  1.17it/s]                                                 {'loss': 0.7389, 'grad_norm': 0.9717093687438825, 'learning_rate': 9.55810147299509e-07, 'epoch': 1.61}
 54%|█████▍    | 339/630 [04:55<04:08,  1.17it/s] 54%|█████▍    | 340/630 [04:56<04:06,  1.17it/s]                                                 {'loss': 0.7557, 'grad_norm': 0.6677339197214367, 'learning_rate': 9.525368248772504e-07, 'epoch': 1.62}
 54%|█████▍    | 340/630 [04:56<04:06,  1.17it/s] 54%|█████▍    | 341/630 [04:57<04:05,  1.18it/s]                                                 {'loss': 0.7266, 'grad_norm': 0.8206708962038776, 'learning_rate': 9.492635024549918e-07, 'epoch': 1.62}
 54%|█████▍    | 341/630 [04:57<04:05,  1.18it/s] 54%|█████▍    | 342/630 [04:58<04:05,  1.18it/s]                                                 {'loss': 0.7995, 'grad_norm': 0.9334136681792784, 'learning_rate': 9.459901800327333e-07, 'epoch': 1.63}
 54%|█████▍    | 342/630 [04:58<04:05,  1.18it/s] 54%|█████▍    | 343/630 [04:59<04:03,  1.18it/s]                                                 {'loss': 0.7707, 'grad_norm': 0.8518753570974087, 'learning_rate': 9.427168576104746e-07, 'epoch': 1.63}
 54%|█████▍    | 343/630 [04:59<04:03,  1.18it/s] 55%|█████▍    | 344/630 [05:00<04:03,  1.18it/s]                                                 {'loss': 0.7779, 'grad_norm': 0.8299449724299834, 'learning_rate': 9.394435351882159e-07, 'epoch': 1.64}
 55%|█████▍    | 344/630 [05:00<04:03,  1.18it/s] 55%|█████▍    | 345/630 [05:01<04:02,  1.18it/s]                                                 {'loss': 0.7694, 'grad_norm': 0.9236842776359716, 'learning_rate': 9.361702127659575e-07, 'epoch': 1.64}
 55%|█████▍    | 345/630 [05:01<04:02,  1.18it/s] 55%|█████▍    | 346/630 [05:01<04:02,  1.17it/s]                                                 {'loss': 0.8384, 'grad_norm': 0.8198290599788329, 'learning_rate': 9.328968903436988e-07, 'epoch': 1.65}
 55%|█████▍    | 346/630 [05:01<04:02,  1.17it/s] 55%|█████▌    | 347/630 [05:02<04:01,  1.17it/s]                                                 {'loss': 0.8325, 'grad_norm': 0.7194557264352058, 'learning_rate': 9.296235679214402e-07, 'epoch': 1.65}
 55%|█████▌    | 347/630 [05:02<04:01,  1.17it/s] 55%|█████▌    | 348/630 [05:03<04:00,  1.17it/s]                                                 {'loss': 0.783, 'grad_norm': 0.8078170080604972, 'learning_rate': 9.263502454991816e-07, 'epoch': 1.66}
 55%|█████▌    | 348/630 [05:03<04:00,  1.17it/s] 55%|█████▌    | 349/630 [05:04<03:58,  1.18it/s]                                                 {'loss': 0.7496, 'grad_norm': 0.7369885348109964, 'learning_rate': 9.230769230769231e-07, 'epoch': 1.66}
 55%|█████▌    | 349/630 [05:04<03:58,  1.18it/s] 56%|█████▌    | 350/630 [05:05<03:58,  1.17it/s]                                                 {'loss': 0.7593, 'grad_norm': 1.1225090962610629, 'learning_rate': 9.198036006546645e-07, 'epoch': 1.67}
 56%|█████▌    | 350/630 [05:05<03:58,  1.17it/s] 56%|█████▌    | 351/630 [05:06<03:57,  1.18it/s]                                                 {'loss': 0.7823, 'grad_norm': 0.7134692775835949, 'learning_rate': 9.165302782324058e-07, 'epoch': 1.67}
 56%|█████▌    | 351/630 [05:06<03:57,  1.18it/s] 56%|█████▌    | 352/630 [05:06<03:56,  1.18it/s]                                                 {'loss': 0.7006, 'grad_norm': 0.910407601770587, 'learning_rate': 9.132569558101472e-07, 'epoch': 1.68}
 56%|█████▌    | 352/630 [05:06<03:56,  1.18it/s] 56%|█████▌    | 353/630 [05:07<03:55,  1.18it/s]                                                 {'loss': 0.7079, 'grad_norm': 0.8734168028211052, 'learning_rate': 9.099836333878887e-07, 'epoch': 1.68}
 56%|█████▌    | 353/630 [05:07<03:55,  1.18it/s] 56%|█████▌    | 354/630 [05:08<03:54,  1.18it/s]                                                 {'loss': 0.6943, 'grad_norm': 0.7190547510141844, 'learning_rate': 9.067103109656301e-07, 'epoch': 1.69}
 56%|█████▌    | 354/630 [05:08<03:54,  1.18it/s] 56%|█████▋    | 355/630 [05:09<03:54,  1.17it/s]                                                 {'loss': 0.7099, 'grad_norm': 0.7531598420678012, 'learning_rate': 9.034369885433715e-07, 'epoch': 1.69}
 56%|█████▋    | 355/630 [05:09<03:54,  1.17it/s] 57%|█████▋    | 356/630 [05:10<03:53,  1.17it/s]                                                 {'loss': 0.6693, 'grad_norm': 0.7078149398884928, 'learning_rate': 9.001636661211129e-07, 'epoch': 1.7}
 57%|█████▋    | 356/630 [05:10<03:53,  1.17it/s] 57%|█████▋    | 357/630 [05:11<03:52,  1.17it/s]                                                 {'loss': 0.7562, 'grad_norm': 0.6998617683179083, 'learning_rate': 8.968903436988543e-07, 'epoch': 1.7}
 57%|█████▋    | 357/630 [05:11<03:52,  1.17it/s] 57%|█████▋    | 358/630 [05:12<03:52,  1.17it/s]                                                 {'loss': 0.7862, 'grad_norm': 1.0156585489553287, 'learning_rate': 8.936170212765957e-07, 'epoch': 1.7}
 57%|█████▋    | 358/630 [05:12<03:52,  1.17it/s] 57%|█████▋    | 359/630 [05:12<03:51,  1.17it/s]                                                 {'loss': 0.8022, 'grad_norm': 1.0776985448326744, 'learning_rate': 8.903436988543371e-07, 'epoch': 1.71}
 57%|█████▋    | 359/630 [05:12<03:51,  1.17it/s] 57%|█████▋    | 360/630 [05:13<03:50,  1.17it/s]                                                 {'loss': 0.6643, 'grad_norm': 0.6527498258229456, 'learning_rate': 8.870703764320784e-07, 'epoch': 1.71}
 57%|█████▋    | 360/630 [05:13<03:50,  1.17it/s] 57%|█████▋    | 361/630 [05:14<03:49,  1.17it/s]                                                 {'loss': 0.6895, 'grad_norm': 0.6903054255910924, 'learning_rate': 8.8379705400982e-07, 'epoch': 1.72}
 57%|█████▋    | 361/630 [05:14<03:49,  1.17it/s] 57%|█████▋    | 362/630 [05:15<03:48,  1.17it/s]                                                 {'loss': 0.7722, 'grad_norm': 0.7766617055242901, 'learning_rate': 8.805237315875613e-07, 'epoch': 1.72}
 57%|█████▋    | 362/630 [05:15<03:48,  1.17it/s] 58%|█████▊    | 363/630 [05:16<03:47,  1.17it/s]                                                 {'loss': 0.7598, 'grad_norm': 0.9756580297950287, 'learning_rate': 8.772504091653028e-07, 'epoch': 1.73}
 58%|█████▊    | 363/630 [05:16<03:47,  1.17it/s] 58%|█████▊    | 364/630 [05:17<03:46,  1.17it/s]                                                 {'loss': 0.7867, 'grad_norm': 0.9919692349205036, 'learning_rate': 8.739770867430442e-07, 'epoch': 1.73}
 58%|█████▊    | 364/630 [05:17<03:46,  1.17it/s] 58%|█████▊    | 365/630 [05:18<03:46,  1.17it/s]                                                 {'loss': 0.7827, 'grad_norm': 0.7735289232830121, 'learning_rate': 8.707037643207855e-07, 'epoch': 1.74}
 58%|█████▊    | 365/630 [05:18<03:46,  1.17it/s] 58%|█████▊    | 366/630 [05:18<03:45,  1.17it/s]                                                 {'loss': 0.7941, 'grad_norm': 1.0944008455341223, 'learning_rate': 8.67430441898527e-07, 'epoch': 1.74}
 58%|█████▊    | 366/630 [05:18<03:45,  1.17it/s] 58%|█████▊    | 367/630 [05:19<03:44,  1.17it/s]                                                 {'loss': 0.6261, 'grad_norm': 0.7463959875472823, 'learning_rate': 8.641571194762683e-07, 'epoch': 1.75}
 58%|█████▊    | 367/630 [05:19<03:44,  1.17it/s] 58%|█████▊    | 368/630 [05:20<03:43,  1.17it/s]                                                 {'loss': 0.7366, 'grad_norm': 0.6993814354356415, 'learning_rate': 8.608837970540099e-07, 'epoch': 1.75}
 58%|█████▊    | 368/630 [05:20<03:43,  1.17it/s] 59%|█████▊    | 369/630 [05:21<03:42,  1.18it/s]                                                 {'loss': 0.7439, 'grad_norm': 0.77676796231898, 'learning_rate': 8.576104746317512e-07, 'epoch': 1.76}
 59%|█████▊    | 369/630 [05:21<03:42,  1.18it/s] 59%|█████▊    | 370/630 [05:22<03:40,  1.18it/s]                                                 {'loss': 0.6123, 'grad_norm': 0.835562424558057, 'learning_rate': 8.543371522094926e-07, 'epoch': 1.76}
 59%|█████▊    | 370/630 [05:22<03:40,  1.18it/s] 59%|█████▉    | 371/630 [05:23<03:40,  1.18it/s]                                                 {'loss': 0.7937, 'grad_norm': 0.6816416427865827, 'learning_rate': 8.51063829787234e-07, 'epoch': 1.77}
 59%|█████▉    | 371/630 [05:23<03:40,  1.18it/s] 59%|█████▉    | 372/630 [05:24<03:39,  1.17it/s]                                                 {'loss': 0.8101, 'grad_norm': 0.9060955506042824, 'learning_rate': 8.477905073649754e-07, 'epoch': 1.77}
 59%|█████▉    | 372/630 [05:24<03:39,  1.17it/s] 59%|█████▉    | 373/630 [05:24<03:38,  1.17it/s]                                                 {'loss': 0.7459, 'grad_norm': 0.694611415681919, 'learning_rate': 8.445171849427169e-07, 'epoch': 1.78}
 59%|█████▉    | 373/630 [05:24<03:38,  1.17it/s] 59%|█████▉    | 374/630 [05:25<03:38,  1.17it/s]                                                 {'loss': 0.8249, 'grad_norm': 0.9372882481713319, 'learning_rate': 8.412438625204582e-07, 'epoch': 1.78}
 59%|█████▉    | 374/630 [05:25<03:38,  1.17it/s] 60%|█████▉    | 375/630 [05:26<03:36,  1.18it/s]                                                 {'loss': 0.7654, 'grad_norm': 0.8435074130259568, 'learning_rate': 8.379705400981996e-07, 'epoch': 1.79}
 60%|█████▉    | 375/630 [05:26<03:36,  1.18it/s] 60%|█████▉    | 376/630 [05:27<03:36,  1.18it/s]                                                 {'loss': 0.7336, 'grad_norm': 0.8304515384866821, 'learning_rate': 8.346972176759411e-07, 'epoch': 1.79}
 60%|█████▉    | 376/630 [05:27<03:36,  1.18it/s] 60%|█████▉    | 377/630 [05:28<03:35,  1.17it/s]                                                 {'loss': 0.7865, 'grad_norm': 0.5841212646807958, 'learning_rate': 8.314238952536824e-07, 'epoch': 1.8}
 60%|█████▉    | 377/630 [05:28<03:35,  1.17it/s] 60%|██████    | 378/630 [05:29<03:35,  1.17it/s]                                                 {'loss': 0.7078, 'grad_norm': 0.9814631517181388, 'learning_rate': 8.281505728314238e-07, 'epoch': 1.8}
 60%|██████    | 378/630 [05:29<03:35,  1.17it/s] 60%|██████    | 379/630 [05:30<03:34,  1.17it/s]                                                 {'loss': 0.7901, 'grad_norm': 0.9323042503037552, 'learning_rate': 8.248772504091652e-07, 'epoch': 1.8}
 60%|██████    | 379/630 [05:30<03:34,  1.17it/s] 60%|██████    | 380/630 [05:30<03:33,  1.17it/s]                                                 {'loss': 0.7627, 'grad_norm': 0.7269331824385888, 'learning_rate': 8.216039279869067e-07, 'epoch': 1.81}
 60%|██████    | 380/630 [05:30<03:33,  1.17it/s] 60%|██████    | 381/630 [05:31<03:32,  1.17it/s]                                                 {'loss': 0.7955, 'grad_norm': 0.810531099137239, 'learning_rate': 8.183306055646481e-07, 'epoch': 1.81}
 60%|██████    | 381/630 [05:31<03:32,  1.17it/s] 61%|██████    | 382/630 [05:32<03:31,  1.17it/s]                                                 {'loss': 0.7473, 'grad_norm': 0.7892610690105012, 'learning_rate': 8.150572831423895e-07, 'epoch': 1.82}
 61%|██████    | 382/630 [05:32<03:31,  1.17it/s] 61%|██████    | 383/630 [05:33<03:30,  1.17it/s]                                                 {'loss': 0.6857, 'grad_norm': 0.7900210450046988, 'learning_rate': 8.117839607201308e-07, 'epoch': 1.82}
 61%|██████    | 383/630 [05:33<03:30,  1.17it/s] 61%|██████    | 384/630 [05:34<03:30,  1.17it/s]                                                 {'loss': 0.7162, 'grad_norm': 0.998403756833089, 'learning_rate': 8.085106382978723e-07, 'epoch': 1.83}
 61%|██████    | 384/630 [05:34<03:30,  1.17it/s] 61%|██████    | 385/630 [05:35<03:29,  1.17it/s]                                                 {'loss': 0.7186, 'grad_norm': 0.7522614134297545, 'learning_rate': 8.052373158756137e-07, 'epoch': 1.83}
 61%|██████    | 385/630 [05:35<03:29,  1.17it/s] 61%|██████▏   | 386/630 [05:35<03:28,  1.17it/s]                                                 {'loss': 0.7897, 'grad_norm': 0.8882007091685433, 'learning_rate': 8.019639934533551e-07, 'epoch': 1.84}
 61%|██████▏   | 386/630 [05:35<03:28,  1.17it/s] 61%|██████▏   | 387/630 [05:36<03:27,  1.17it/s]                                                 {'loss': 0.7877, 'grad_norm': 0.9024368686088277, 'learning_rate': 7.986906710310966e-07, 'epoch': 1.84}
 61%|██████▏   | 387/630 [05:36<03:27,  1.17it/s] 62%|██████▏   | 388/630 [05:37<03:26,  1.17it/s]                                                 {'loss': 0.8126, 'grad_norm': 0.8987921420056176, 'learning_rate': 7.954173486088379e-07, 'epoch': 1.85}
 62%|██████▏   | 388/630 [05:37<03:26,  1.17it/s] 62%|██████▏   | 389/630 [05:38<03:25,  1.17it/s]                                                 {'loss': 0.7683, 'grad_norm': 0.7381330315565061, 'learning_rate': 7.921440261865794e-07, 'epoch': 1.85}
 62%|██████▏   | 389/630 [05:38<03:25,  1.17it/s] 62%|██████▏   | 390/630 [05:39<03:24,  1.17it/s]                                                 {'loss': 0.7281, 'grad_norm': 0.7993191281375585, 'learning_rate': 7.888707037643207e-07, 'epoch': 1.86}
 62%|██████▏   | 390/630 [05:39<03:24,  1.17it/s] 62%|██████▏   | 391/630 [05:40<03:23,  1.17it/s]                                                 {'loss': 0.7812, 'grad_norm': 0.711198515122777, 'learning_rate': 7.855973813420622e-07, 'epoch': 1.86}
 62%|██████▏   | 391/630 [05:40<03:23,  1.17it/s] 62%|██████▏   | 392/630 [05:41<03:23,  1.17it/s]                                                 {'loss': 0.7169, 'grad_norm': 0.8022598208194278, 'learning_rate': 7.823240589198036e-07, 'epoch': 1.87}
 62%|██████▏   | 392/630 [05:41<03:23,  1.17it/s] 62%|██████▏   | 393/630 [05:41<03:22,  1.17it/s]                                                 {'loss': 0.8674, 'grad_norm': 0.967403608192051, 'learning_rate': 7.790507364975449e-07, 'epoch': 1.87}
 62%|██████▏   | 393/630 [05:41<03:22,  1.17it/s] 63%|██████▎   | 394/630 [05:42<03:21,  1.17it/s]                                                 {'loss': 0.8175, 'grad_norm': 0.8172171068952971, 'learning_rate': 7.757774140752864e-07, 'epoch': 1.88}
 63%|██████▎   | 394/630 [05:42<03:21,  1.17it/s] 63%|██████▎   | 395/630 [05:43<03:20,  1.17it/s]                                                 {'loss': 0.7714, 'grad_norm': 0.9887500347498215, 'learning_rate': 7.725040916530278e-07, 'epoch': 1.88}
 63%|██████▎   | 395/630 [05:43<03:20,  1.17it/s] 63%|██████▎   | 396/630 [05:44<03:20,  1.17it/s]                                                 {'loss': 0.7331, 'grad_norm': 0.8440075860668297, 'learning_rate': 7.692307692307693e-07, 'epoch': 1.89}
 63%|██████▎   | 396/630 [05:44<03:20,  1.17it/s] 63%|██████▎   | 397/630 [05:45<03:19,  1.17it/s]                                                 {'loss': 0.7025, 'grad_norm': 0.947229526296745, 'learning_rate': 7.659574468085106e-07, 'epoch': 1.89}
 63%|██████▎   | 397/630 [05:45<03:19,  1.17it/s] 63%|██████▎   | 398/630 [05:46<03:18,  1.17it/s]                                                 {'loss': 0.7796, 'grad_norm': 0.7282732672232634, 'learning_rate': 7.626841243862519e-07, 'epoch': 1.9}
 63%|██████▎   | 398/630 [05:46<03:18,  1.17it/s] 63%|██████▎   | 399/630 [05:47<03:17,  1.17it/s]                                                 {'loss': 0.8004, 'grad_norm': 0.877584385987993, 'learning_rate': 7.594108019639935e-07, 'epoch': 1.9}
 63%|██████▎   | 399/630 [05:47<03:17,  1.17it/s] 63%|██████▎   | 400/630 [05:47<03:16,  1.17it/s]                                                 {'loss': 0.7177, 'grad_norm': 1.044845016833121, 'learning_rate': 7.561374795417348e-07, 'epoch': 1.9}
 63%|██████▎   | 400/630 [05:47<03:16,  1.17it/s] 64%|██████▎   | 401/630 [05:48<03:15,  1.17it/s]                                                 {'loss': 0.7854, 'grad_norm': 0.8014482558148049, 'learning_rate': 7.528641571194762e-07, 'epoch': 1.91}
 64%|██████▎   | 401/630 [05:48<03:15,  1.17it/s] 64%|██████▍   | 402/630 [05:49<03:14,  1.17it/s]                                                 {'loss': 0.7307, 'grad_norm': 0.6940074346985541, 'learning_rate': 7.495908346972176e-07, 'epoch': 1.91}
 64%|██████▍   | 402/630 [05:49<03:14,  1.17it/s] 64%|██████▍   | 403/630 [05:50<03:14,  1.17it/s]                                                 {'loss': 0.748, 'grad_norm': 0.7794056920546281, 'learning_rate': 7.463175122749591e-07, 'epoch': 1.92}
 64%|██████▍   | 403/630 [05:50<03:14,  1.17it/s] 64%|██████▍   | 404/630 [05:51<03:13,  1.17it/s]                                                 {'loss': 0.6846, 'grad_norm': 0.6328287879895519, 'learning_rate': 7.430441898527005e-07, 'epoch': 1.92}
 64%|██████▍   | 404/630 [05:51<03:13,  1.17it/s] 64%|██████▍   | 405/630 [05:52<03:12,  1.17it/s]                                                 {'loss': 0.704, 'grad_norm': 0.9126163779947752, 'learning_rate': 7.397708674304418e-07, 'epoch': 1.93}
 64%|██████▍   | 405/630 [05:52<03:12,  1.17it/s] 64%|██████▍   | 406/630 [05:53<03:11,  1.17it/s]                                                 {'loss': 0.7453, 'grad_norm': 0.9115370304551749, 'learning_rate': 7.364975450081832e-07, 'epoch': 1.93}
 64%|██████▍   | 406/630 [05:53<03:11,  1.17it/s] 65%|██████▍   | 407/630 [05:53<03:10,  1.17it/s]                                                 {'loss': 0.7214, 'grad_norm': 0.7612560345237834, 'learning_rate': 7.332242225859247e-07, 'epoch': 1.94}
 65%|██████▍   | 407/630 [05:53<03:10,  1.17it/s] 65%|██████▍   | 408/630 [05:54<03:09,  1.17it/s]                                                 {'loss': 0.7128, 'grad_norm': 0.7130533963857346, 'learning_rate': 7.299509001636661e-07, 'epoch': 1.94}
 65%|██████▍   | 408/630 [05:54<03:09,  1.17it/s] 65%|██████▍   | 409/630 [05:55<03:08,  1.17it/s]                                                 {'loss': 0.7001, 'grad_norm': 0.8260702899579024, 'learning_rate': 7.266775777414075e-07, 'epoch': 1.95}
 65%|██████▍   | 409/630 [05:55<03:08,  1.17it/s] 65%|██████▌   | 410/630 [05:56<03:08,  1.17it/s]                                                 {'loss': 0.7746, 'grad_norm': 0.9445611182414015, 'learning_rate': 7.23404255319149e-07, 'epoch': 1.95}
 65%|██████▌   | 410/630 [05:56<03:08,  1.17it/s] 65%|██████▌   | 411/630 [05:57<03:06,  1.17it/s]                                                 {'loss': 0.8202, 'grad_norm': 0.8885347757024471, 'learning_rate': 7.201309328968903e-07, 'epoch': 1.96}
 65%|██████▌   | 411/630 [05:57<03:06,  1.17it/s] 65%|██████▌   | 412/630 [05:58<03:05,  1.17it/s]                                                 {'loss': 0.6782, 'grad_norm': 0.6706454056081019, 'learning_rate': 7.168576104746317e-07, 'epoch': 1.96}
 65%|██████▌   | 412/630 [05:58<03:05,  1.17it/s] 66%|██████▌   | 413/630 [05:59<03:04,  1.18it/s]                                                 {'loss': 0.7738, 'grad_norm': 0.7291708714999334, 'learning_rate': 7.135842880523731e-07, 'epoch': 1.97}
 66%|██████▌   | 413/630 [05:59<03:04,  1.18it/s] 66%|██████▌   | 414/630 [05:59<03:03,  1.17it/s]                                                 {'loss': 0.694, 'grad_norm': 0.705005124271175, 'learning_rate': 7.103109656301146e-07, 'epoch': 1.97}
 66%|██████▌   | 414/630 [05:59<03:03,  1.17it/s] 66%|██████▌   | 415/630 [06:00<03:03,  1.17it/s]                                                 {'loss': 0.7462, 'grad_norm': 0.8278393685843405, 'learning_rate': 7.07037643207856e-07, 'epoch': 1.98}
 66%|██████▌   | 415/630 [06:00<03:03,  1.17it/s] 66%|██████▌   | 416/630 [06:01<03:02,  1.17it/s]                                                 {'loss': 0.7204, 'grad_norm': 0.8215387717864846, 'learning_rate': 7.037643207855973e-07, 'epoch': 1.98}
 66%|██████▌   | 416/630 [06:01<03:02,  1.17it/s] 66%|██████▌   | 417/630 [06:02<03:01,  1.18it/s]                                                 {'loss': 0.7862, 'grad_norm': 0.7726060668648576, 'learning_rate': 7.004909983633388e-07, 'epoch': 1.99}
 66%|██████▌   | 417/630 [06:02<03:01,  1.18it/s] 66%|██████▋   | 418/630 [06:03<03:00,  1.18it/s]                                                 {'loss': 0.7412, 'grad_norm': 0.8094361621238165, 'learning_rate': 6.972176759410802e-07, 'epoch': 1.99}
 66%|██████▋   | 418/630 [06:03<03:00,  1.18it/s] 67%|██████▋   | 419/630 [06:04<02:59,  1.18it/s]                                                 {'loss': 0.746, 'grad_norm': 1.0993497960185152, 'learning_rate': 6.939443535188215e-07, 'epoch': 2.0}
 67%|██████▋   | 419/630 [06:04<02:59,  1.18it/s] 67%|██████▋   | 420/630 [06:04<02:58,  1.17it/s]                                                 {'loss': 0.7704, 'grad_norm': 0.8221506818113021, 'learning_rate': 6.90671031096563e-07, 'epoch': 2.0}
 67%|██████▋   | 420/630 [06:04<02:58,  1.17it/s] 67%|██████▋   | 421/630 [06:05<02:58,  1.17it/s]                                                 {'loss': 0.7389, 'grad_norm': 0.8131068849277815, 'learning_rate': 6.873977086743043e-07, 'epoch': 2.0}
 67%|██████▋   | 421/630 [06:05<02:58,  1.17it/s] 67%|██████▋   | 422/630 [06:06<02:57,  1.17it/s]                                                 {'loss': 0.7592, 'grad_norm': 0.7753230515772173, 'learning_rate': 6.841243862520459e-07, 'epoch': 2.01}
 67%|██████▋   | 422/630 [06:06<02:57,  1.17it/s] 67%|██████▋   | 423/630 [06:07<02:56,  1.17it/s]                                                 {'loss': 0.7141, 'grad_norm': 0.797131461545317, 'learning_rate': 6.808510638297872e-07, 'epoch': 2.01}
 67%|██████▋   | 423/630 [06:07<02:56,  1.17it/s] 67%|██████▋   | 424/630 [06:08<02:55,  1.17it/s]                                                 {'loss': 0.8059, 'grad_norm': 0.8349702880493189, 'learning_rate': 6.775777414075286e-07, 'epoch': 2.02}
 67%|██████▋   | 424/630 [06:08<02:55,  1.17it/s] 67%|██████▋   | 425/630 [06:09<02:54,  1.17it/s]                                                 {'loss': 0.7643, 'grad_norm': 0.9477371317137886, 'learning_rate': 6.7430441898527e-07, 'epoch': 2.02}
 67%|██████▋   | 425/630 [06:09<02:54,  1.17it/s] 68%|██████▊   | 426/630 [06:10<02:54,  1.17it/s]                                                 {'loss': 0.7228, 'grad_norm': 0.6865399530837251, 'learning_rate': 6.710310965630114e-07, 'epoch': 2.03}
 68%|██████▊   | 426/630 [06:10<02:54,  1.17it/s] 68%|██████▊   | 427/630 [06:10<02:53,  1.17it/s]                                                 {'loss': 0.7374, 'grad_norm': 0.8104366076718382, 'learning_rate': 6.677577741407529e-07, 'epoch': 2.03}
 68%|██████▊   | 427/630 [06:10<02:53,  1.17it/s] 68%|██████▊   | 428/630 [06:11<02:52,  1.17it/s]                                                 {'loss': 0.7927, 'grad_norm': 0.8027434909102743, 'learning_rate': 6.644844517184942e-07, 'epoch': 2.04}
 68%|██████▊   | 428/630 [06:11<02:52,  1.17it/s] 68%|██████▊   | 429/630 [06:12<02:52,  1.17it/s]                                                 {'loss': 0.7375, 'grad_norm': 0.8190038972738509, 'learning_rate': 6.612111292962356e-07, 'epoch': 2.04}
 68%|██████▊   | 429/630 [06:12<02:52,  1.17it/s] 68%|██████▊   | 430/630 [06:13<02:51,  1.17it/s]                                                 {'loss': 0.7086, 'grad_norm': 0.9156769107970436, 'learning_rate': 6.579378068739771e-07, 'epoch': 2.05}
 68%|██████▊   | 430/630 [06:13<02:51,  1.17it/s] 68%|██████▊   | 431/630 [06:14<02:49,  1.17it/s]                                                 {'loss': 0.8083, 'grad_norm': 0.7135533457064108, 'learning_rate': 6.546644844517185e-07, 'epoch': 2.05}
 68%|██████▊   | 431/630 [06:14<02:49,  1.17it/s] 69%|██████▊   | 432/630 [06:15<02:49,  1.17it/s]                                                 {'loss': 0.7619, 'grad_norm': 0.6504558871069387, 'learning_rate': 6.513911620294599e-07, 'epoch': 2.06}
 69%|██████▊   | 432/630 [06:15<02:49,  1.17it/s] 69%|██████▊   | 433/630 [06:16<02:47,  1.17it/s]                                                 {'loss': 0.6547, 'grad_norm': 0.8054386904523142, 'learning_rate': 6.481178396072012e-07, 'epoch': 2.06}
 69%|██████▊   | 433/630 [06:16<02:47,  1.17it/s] 69%|██████▉   | 434/630 [06:16<02:47,  1.17it/s]                                                 {'loss': 0.7054, 'grad_norm': 0.9050650782728391, 'learning_rate': 6.448445171849427e-07, 'epoch': 2.07}
 69%|██████▉   | 434/630 [06:16<02:47,  1.17it/s] 69%|██████▉   | 435/630 [06:17<02:46,  1.17it/s]                                                 {'loss': 0.7049, 'grad_norm': 0.6285401736177487, 'learning_rate': 6.415711947626841e-07, 'epoch': 2.07}
 69%|██████▉   | 435/630 [06:17<02:46,  1.17it/s] 69%|██████▉   | 436/630 [06:18<02:46,  1.17it/s]                                                 {'loss': 0.6997, 'grad_norm': 0.8343083916307369, 'learning_rate': 6.382978723404255e-07, 'epoch': 2.08}
 69%|██████▉   | 436/630 [06:18<02:46,  1.17it/s] 69%|██████▉   | 437/630 [06:19<02:45,  1.17it/s]                                                 {'loss': 0.9048, 'grad_norm': 1.1557369324042952, 'learning_rate': 6.35024549918167e-07, 'epoch': 2.08}
 69%|██████▉   | 437/630 [06:19<02:45,  1.17it/s] 70%|██████▉   | 438/630 [06:20<02:44,  1.17it/s]                                                 {'loss': 0.7365, 'grad_norm': 0.7563588280174705, 'learning_rate': 6.317512274959084e-07, 'epoch': 2.09}
 70%|██████▉   | 438/630 [06:20<02:44,  1.17it/s] 70%|██████▉   | 439/630 [06:21<02:43,  1.17it/s]                                                 {'loss': 0.6375, 'grad_norm': 1.0923633809996836, 'learning_rate': 6.284779050736497e-07, 'epoch': 2.09}
 70%|██████▉   | 439/630 [06:21<02:43,  1.17it/s] 70%|██████▉   | 440/630 [06:22<02:42,  1.17it/s]                                                 {'loss': 0.7881, 'grad_norm': 0.7707511295429078, 'learning_rate': 6.252045826513911e-07, 'epoch': 2.1}
 70%|██████▉   | 440/630 [06:22<02:42,  1.17it/s] 70%|███████   | 441/630 [06:22<02:41,  1.17it/s]                                                 {'loss': 0.7553, 'grad_norm': 0.8437060124227647, 'learning_rate': 6.219312602291326e-07, 'epoch': 2.1}
 70%|███████   | 441/630 [06:22<02:41,  1.17it/s] 70%|███████   | 442/630 [06:23<02:41,  1.16it/s]                                                 {'loss': 0.8082, 'grad_norm': 0.9022891238323564, 'learning_rate': 6.186579378068739e-07, 'epoch': 2.1}
 70%|███████   | 442/630 [06:23<02:41,  1.16it/s] 70%|███████   | 443/630 [06:24<02:40,  1.16it/s]                                                 {'loss': 0.8526, 'grad_norm': 0.850966490652451, 'learning_rate': 6.153846153846154e-07, 'epoch': 2.11}
 70%|███████   | 443/630 [06:24<02:40,  1.16it/s] 70%|███████   | 444/630 [06:25<02:39,  1.17it/s]                                                 {'loss': 0.7309, 'grad_norm': 0.754668009592516, 'learning_rate': 6.121112929623567e-07, 'epoch': 2.11}
 70%|███████   | 444/630 [06:25<02:39,  1.17it/s] 71%|███████   | 445/630 [06:26<02:37,  1.17it/s]                                                 {'loss': 0.6933, 'grad_norm': 0.8346956205798746, 'learning_rate': 6.088379705400983e-07, 'epoch': 2.12}
 71%|███████   | 445/630 [06:26<02:37,  1.17it/s] 71%|███████   | 446/630 [06:27<02:36,  1.18it/s]                                                 {'loss': 0.8116, 'grad_norm': 0.8146406080156043, 'learning_rate': 6.055646481178396e-07, 'epoch': 2.12}
 71%|███████   | 446/630 [06:27<02:36,  1.18it/s] 71%|███████   | 447/630 [06:28<02:35,  1.18it/s]                                                 {'loss': 0.8053, 'grad_norm': 0.9075608107077816, 'learning_rate': 6.022913256955809e-07, 'epoch': 2.13}
 71%|███████   | 447/630 [06:28<02:35,  1.18it/s] 71%|███████   | 448/630 [06:28<02:35,  1.17it/s]                                                 {'loss': 0.7174, 'grad_norm': 0.8602032458497216, 'learning_rate': 5.990180032733224e-07, 'epoch': 2.13}
 71%|███████   | 448/630 [06:28<02:35,  1.17it/s] 71%|███████▏  | 449/630 [06:29<02:34,  1.17it/s]                                                 {'loss': 0.6847, 'grad_norm': 0.8207655458094192, 'learning_rate': 5.957446808510638e-07, 'epoch': 2.14}
 71%|███████▏  | 449/630 [06:29<02:34,  1.17it/s] 71%|███████▏  | 450/630 [06:30<02:33,  1.17it/s]                                                 {'loss': 0.8402, 'grad_norm': 0.822831085977009, 'learning_rate': 5.924713584288053e-07, 'epoch': 2.14}
 71%|███████▏  | 450/630 [06:30<02:33,  1.17it/s] 72%|███████▏  | 451/630 [06:31<02:32,  1.17it/s]                                                 {'loss': 0.7209, 'grad_norm': 0.7971336492280928, 'learning_rate': 5.891980360065466e-07, 'epoch': 2.15}
 72%|███████▏  | 451/630 [06:31<02:32,  1.17it/s] 72%|███████▏  | 452/630 [06:32<02:32,  1.17it/s]                                                 {'loss': 0.7528, 'grad_norm': 0.834183526070767, 'learning_rate': 5.85924713584288e-07, 'epoch': 2.15}
 72%|███████▏  | 452/630 [06:32<02:32,  1.17it/s] 72%|███████▏  | 453/630 [06:33<02:30,  1.17it/s]                                                 {'loss': 0.7079, 'grad_norm': 0.6906437915205006, 'learning_rate': 5.826513911620295e-07, 'epoch': 2.16}
 72%|███████▏  | 453/630 [06:33<02:30,  1.17it/s] 72%|███████▏  | 454/630 [06:34<02:30,  1.17it/s]                                                 {'loss': 0.7337, 'grad_norm': 0.7516434852454967, 'learning_rate': 5.793780687397708e-07, 'epoch': 2.16}
 72%|███████▏  | 454/630 [06:34<02:30,  1.17it/s] 72%|███████▏  | 455/630 [06:34<02:29,  1.17it/s]                                                 {'loss': 0.7471, 'grad_norm': 0.6742290490388669, 'learning_rate': 5.761047463175122e-07, 'epoch': 2.17}
 72%|███████▏  | 455/630 [06:34<02:29,  1.17it/s] 72%|███████▏  | 456/630 [06:35<02:28,  1.17it/s]                                                 {'loss': 0.7342, 'grad_norm': 0.7318508632198821, 'learning_rate': 5.728314238952536e-07, 'epoch': 2.17}
 72%|███████▏  | 456/630 [06:35<02:28,  1.17it/s] 73%|███████▎  | 457/630 [06:36<02:27,  1.17it/s]                                                 {'loss': 0.7147, 'grad_norm': 0.8685628076283787, 'learning_rate': 5.695581014729951e-07, 'epoch': 2.18}
 73%|███████▎  | 457/630 [06:36<02:27,  1.17it/s] 73%|███████▎  | 458/630 [06:37<02:26,  1.17it/s]                                                 {'loss': 0.7705, 'grad_norm': 0.701439815348771, 'learning_rate': 5.662847790507365e-07, 'epoch': 2.18}
 73%|███████▎  | 458/630 [06:37<02:26,  1.17it/s] 73%|███████▎  | 459/630 [06:38<02:26,  1.17it/s]                                                 {'loss': 0.6943, 'grad_norm': 0.672726383819424, 'learning_rate': 5.630114566284779e-07, 'epoch': 2.19}
 73%|███████▎  | 459/630 [06:38<02:26,  1.17it/s] 73%|███████▎  | 460/630 [06:39<02:25,  1.17it/s]                                                 {'loss': 0.8195, 'grad_norm': 0.8351655057455474, 'learning_rate': 5.597381342062193e-07, 'epoch': 2.19}
 73%|███████▎  | 460/630 [06:39<02:25,  1.17it/s] 73%|███████▎  | 461/630 [06:40<02:24,  1.17it/s]                                                 {'loss': 0.744, 'grad_norm': 0.7565486806168797, 'learning_rate': 5.564648117839607e-07, 'epoch': 2.2}
 73%|███████▎  | 461/630 [06:40<02:24,  1.17it/s] 73%|███████▎  | 462/630 [06:40<02:23,  1.17it/s]                                                 {'loss': 0.826, 'grad_norm': 0.8480397640347659, 'learning_rate': 5.531914893617021e-07, 'epoch': 2.2}
 73%|███████▎  | 462/630 [06:40<02:23,  1.17it/s] 73%|███████▎  | 463/630 [06:41<02:22,  1.17it/s]                                                 {'loss': 0.7506, 'grad_norm': 0.6667940139790229, 'learning_rate': 5.499181669394435e-07, 'epoch': 2.2}
 73%|███████▎  | 463/630 [06:41<02:22,  1.17it/s] 74%|███████▎  | 464/630 [06:42<02:21,  1.18it/s]                                                 {'loss': 0.7061, 'grad_norm': 0.6972118076745155, 'learning_rate': 5.46644844517185e-07, 'epoch': 2.21}
 74%|███████▎  | 464/630 [06:42<02:21,  1.18it/s] 74%|███████▍  | 465/630 [06:43<02:20,  1.17it/s]                                                 {'loss': 0.797, 'grad_norm': 0.8155021086506278, 'learning_rate': 5.433715220949263e-07, 'epoch': 2.21}
 74%|███████▍  | 465/630 [06:43<02:20,  1.17it/s] 74%|███████▍  | 466/630 [06:44<02:19,  1.17it/s]                                                 {'loss': 0.7816, 'grad_norm': 0.7834957781655282, 'learning_rate': 5.400981996726678e-07, 'epoch': 2.22}
 74%|███████▍  | 466/630 [06:44<02:19,  1.17it/s] 74%|███████▍  | 467/630 [06:45<02:18,  1.18it/s]                                                 {'loss': 0.7234, 'grad_norm': 0.8010759141062732, 'learning_rate': 5.368248772504091e-07, 'epoch': 2.22}
 74%|███████▍  | 467/630 [06:45<02:18,  1.18it/s] 74%|███████▍  | 468/630 [06:45<02:17,  1.17it/s]                                                 {'loss': 0.7561, 'grad_norm': 0.7001231847672431, 'learning_rate': 5.335515548281506e-07, 'epoch': 2.23}
 74%|███████▍  | 468/630 [06:45<02:17,  1.17it/s] 74%|███████▍  | 469/630 [06:46<02:16,  1.18it/s]                                                 {'loss': 0.667, 'grad_norm': 0.9270706201826069, 'learning_rate': 5.30278232405892e-07, 'epoch': 2.23}
 74%|███████▍  | 469/630 [06:46<02:16,  1.18it/s] 75%|███████▍  | 470/630 [06:47<02:16,  1.18it/s]                                                 {'loss': 0.7148, 'grad_norm': 0.7003518286379833, 'learning_rate': 5.270049099836333e-07, 'epoch': 2.24}
 75%|███████▍  | 470/630 [06:47<02:16,  1.18it/s] 75%|███████▍  | 471/630 [06:48<02:15,  1.18it/s]                                                 {'loss': 0.8341, 'grad_norm': 0.8498577461041062, 'learning_rate': 5.237315875613748e-07, 'epoch': 2.24}
 75%|███████▍  | 471/630 [06:48<02:15,  1.18it/s] 75%|███████▍  | 472/630 [06:49<02:14,  1.18it/s]                                                 {'loss': 0.7007, 'grad_norm': 0.6775237332730232, 'learning_rate': 5.204582651391162e-07, 'epoch': 2.25}
 75%|███████▍  | 472/630 [06:49<02:14,  1.18it/s] 75%|███████▌  | 473/630 [06:50<02:13,  1.18it/s]                                                 {'loss': 0.7396, 'grad_norm': 0.713022857241855, 'learning_rate': 5.171849427168577e-07, 'epoch': 2.25}
 75%|███████▌  | 473/630 [06:50<02:13,  1.18it/s] 75%|███████▌  | 474/630 [06:51<02:12,  1.18it/s]                                                 {'loss': 0.8027, 'grad_norm': 0.7710815569746832, 'learning_rate': 5.13911620294599e-07, 'epoch': 2.26}
 75%|███████▌  | 474/630 [06:51<02:12,  1.18it/s] 75%|███████▌  | 475/630 [06:51<02:11,  1.18it/s]                                                 {'loss': 0.8108, 'grad_norm': 0.7081055880194276, 'learning_rate': 5.106382978723403e-07, 'epoch': 2.26}
 75%|███████▌  | 475/630 [06:51<02:11,  1.18it/s] 76%|███████▌  | 476/630 [06:52<02:10,  1.18it/s]                                                 {'loss': 0.6923, 'grad_norm': 0.6651224301869365, 'learning_rate': 5.073649754500819e-07, 'epoch': 2.27}
 76%|███████▌  | 476/630 [06:52<02:10,  1.18it/s] 76%|███████▌  | 477/630 [06:53<02:09,  1.18it/s]                                                 {'loss': 0.709, 'grad_norm': 0.7983229151601654, 'learning_rate': 5.040916530278232e-07, 'epoch': 2.27}
 76%|███████▌  | 477/630 [06:53<02:09,  1.18it/s] 76%|███████▌  | 478/630 [06:54<02:08,  1.18it/s]                                                 {'loss': 0.7886, 'grad_norm': 0.9903850198220276, 'learning_rate': 5.008183306055646e-07, 'epoch': 2.28}
 76%|███████▌  | 478/630 [06:54<02:08,  1.18it/s] 76%|███████▌  | 479/630 [06:55<02:07,  1.18it/s]                                                 {'loss': 0.6967, 'grad_norm': 1.2025939816379065, 'learning_rate': 4.97545008183306e-07, 'epoch': 2.28}
 76%|███████▌  | 479/630 [06:55<02:07,  1.18it/s] 76%|███████▌  | 480/630 [06:56<02:06,  1.19it/s]                                                 {'loss': 0.761, 'grad_norm': 0.726378048144706, 'learning_rate': 4.942716857610474e-07, 'epoch': 2.29}
 76%|███████▌  | 480/630 [06:56<02:06,  1.19it/s] 76%|███████▋  | 481/630 [06:56<02:05,  1.19it/s]                                                 {'loss': 0.7732, 'grad_norm': 0.6551914152684489, 'learning_rate': 4.909983633387889e-07, 'epoch': 2.29}
 76%|███████▋  | 481/630 [06:56<02:05,  1.19it/s] 77%|███████▋  | 482/630 [06:57<02:04,  1.19it/s]                                                 {'loss': 0.7255, 'grad_norm': 0.8650742348195571, 'learning_rate': 4.877250409165303e-07, 'epoch': 2.3}
 77%|███████▋  | 482/630 [06:57<02:04,  1.19it/s] 77%|███████▋  | 483/630 [06:58<02:03,  1.19it/s]                                                 {'loss': 0.7175, 'grad_norm': 0.8423421744990623, 'learning_rate': 4.844517184942716e-07, 'epoch': 2.3}
 77%|███████▋  | 483/630 [06:58<02:03,  1.19it/s] 77%|███████▋  | 484/630 [06:59<02:03,  1.19it/s]                                                 {'loss': 0.7333, 'grad_norm': 0.9714363515270201, 'learning_rate': 4.811783960720131e-07, 'epoch': 2.3}
 77%|███████▋  | 484/630 [06:59<02:03,  1.19it/s] 77%|███████▋  | 485/630 [07:00<02:02,  1.19it/s]                                                 {'loss': 0.717, 'grad_norm': 0.7980196590556724, 'learning_rate': 4.779050736497545e-07, 'epoch': 2.31}
 77%|███████▋  | 485/630 [07:00<02:02,  1.19it/s] 77%|███████▋  | 486/630 [07:01<02:01,  1.19it/s]                                                 {'loss': 0.7503, 'grad_norm': 1.0324607447618759, 'learning_rate': 4.746317512274959e-07, 'epoch': 2.31}
 77%|███████▋  | 486/630 [07:01<02:01,  1.19it/s] 77%|███████▋  | 487/630 [07:02<02:00,  1.19it/s]                                                 {'loss': 0.7025, 'grad_norm': 1.0339034322488352, 'learning_rate': 4.713584288052373e-07, 'epoch': 2.32}
 77%|███████▋  | 487/630 [07:02<02:00,  1.19it/s] 77%|███████▋  | 488/630 [07:02<01:59,  1.19it/s]                                                 {'loss': 0.7226, 'grad_norm': 0.7643329851632064, 'learning_rate': 4.6808510638297873e-07, 'epoch': 2.32}
 77%|███████▋  | 488/630 [07:02<01:59,  1.19it/s] 78%|███████▊  | 489/630 [07:03<01:58,  1.19it/s]                                                 {'loss': 0.7993, 'grad_norm': 1.2847837333342664, 'learning_rate': 4.648117839607201e-07, 'epoch': 2.33}
 78%|███████▊  | 489/630 [07:03<01:58,  1.19it/s] 78%|███████▊  | 490/630 [07:04<01:57,  1.19it/s]                                                 {'loss': 0.8255, 'grad_norm': 0.64841375565955, 'learning_rate': 4.6153846153846156e-07, 'epoch': 2.33}
 78%|███████▊  | 490/630 [07:04<01:57,  1.19it/s] 78%|███████▊  | 491/630 [07:05<01:56,  1.19it/s]                                                 {'loss': 0.7645, 'grad_norm': 0.8499039045731483, 'learning_rate': 4.582651391162029e-07, 'epoch': 2.34}
 78%|███████▊  | 491/630 [07:05<01:56,  1.19it/s] 78%|███████▊  | 492/630 [07:06<01:56,  1.19it/s]                                                 {'loss': 0.7769, 'grad_norm': 0.8749037257585682, 'learning_rate': 4.5499181669394434e-07, 'epoch': 2.34}
 78%|███████▊  | 492/630 [07:06<01:56,  1.19it/s] 78%|███████▊  | 493/630 [07:07<01:55,  1.19it/s]                                                 {'loss': 0.717, 'grad_norm': 0.7687582386583446, 'learning_rate': 4.517184942716857e-07, 'epoch': 2.35}
 78%|███████▊  | 493/630 [07:07<01:55,  1.19it/s] 78%|███████▊  | 494/630 [07:07<01:55,  1.18it/s]                                                 {'loss': 0.8314, 'grad_norm': 0.9677322450483574, 'learning_rate': 4.4844517184942717e-07, 'epoch': 2.35}
 78%|███████▊  | 494/630 [07:07<01:55,  1.18it/s] 79%|███████▊  | 495/630 [07:08<01:53,  1.19it/s]                                                 {'loss': 0.684, 'grad_norm': 0.8271831807761745, 'learning_rate': 4.4517184942716855e-07, 'epoch': 2.36}
 79%|███████▊  | 495/630 [07:08<01:53,  1.19it/s] 79%|███████▊  | 496/630 [07:09<01:52,  1.19it/s]                                                 {'loss': 0.8051, 'grad_norm': 1.000525786967838, 'learning_rate': 4.4189852700491e-07, 'epoch': 2.36}
 79%|███████▊  | 496/630 [07:09<01:52,  1.19it/s] 79%|███████▉  | 497/630 [07:10<01:51,  1.19it/s]                                                 {'loss': 0.7808, 'grad_norm': 0.8622575428718896, 'learning_rate': 4.386252045826514e-07, 'epoch': 2.37}
 79%|███████▉  | 497/630 [07:10<01:51,  1.19it/s] 79%|███████▉  | 498/630 [07:11<01:50,  1.19it/s]                                                 {'loss': 0.7735, 'grad_norm': 0.6296049298860023, 'learning_rate': 4.3535188216039277e-07, 'epoch': 2.37}
 79%|███████▉  | 498/630 [07:11<01:50,  1.19it/s] 79%|███████▉  | 499/630 [07:12<01:50,  1.19it/s]                                                 {'loss': 0.7314, 'grad_norm': 1.049629961174889, 'learning_rate': 4.3207855973813416e-07, 'epoch': 2.38}
 79%|███████▉  | 499/630 [07:12<01:50,  1.19it/s] 79%|███████▉  | 500/630 [07:12<01:49,  1.19it/s]                                                 {'loss': 0.7744, 'grad_norm': 0.8703929248158849, 'learning_rate': 4.288052373158756e-07, 'epoch': 2.38}
 79%|███████▉  | 500/630 [07:12<01:49,  1.19it/s]/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
 80%|███████▉  | 501/630 [07:29<12:00,  5.58s/it]                                                 {'loss': 0.7357, 'grad_norm': 1.025808658084851, 'learning_rate': 4.25531914893617e-07, 'epoch': 2.39}
 80%|███████▉  | 501/630 [07:29<12:00,  5.58s/it] 80%|███████▉  | 502/630 [07:30<08:52,  4.16s/it]                                                 {'loss': 0.8056, 'grad_norm': 0.8842285524879606, 'learning_rate': 4.2225859247135843e-07, 'epoch': 2.39}
 80%|███████▉  | 502/630 [07:30<08:52,  4.16s/it] 80%|███████▉  | 503/630 [07:31<06:42,  3.17s/it]                                                 {'loss': 0.6491, 'grad_norm': 0.8545003814863167, 'learning_rate': 4.189852700490998e-07, 'epoch': 2.4}
 80%|███████▉  | 503/630 [07:31<06:42,  3.17s/it] 80%|████████  | 504/630 [07:32<05:11,  2.47s/it]                                                 {'loss': 0.7819, 'grad_norm': 0.6494637879177345, 'learning_rate': 4.157119476268412e-07, 'epoch': 2.4}
 80%|████████  | 504/630 [07:32<05:11,  2.47s/it] 80%|████████  | 505/630 [07:33<04:07,  1.98s/it]                                                 {'loss': 0.7278, 'grad_norm': 0.6598370169569118, 'learning_rate': 4.124386252045826e-07, 'epoch': 2.4}
 80%|████████  | 505/630 [07:33<04:07,  1.98s/it] 80%|████████  | 506/630 [07:33<03:23,  1.64s/it]                                                 {'loss': 0.7161, 'grad_norm': 0.9729602116129551, 'learning_rate': 4.0916530278232403e-07, 'epoch': 2.41}
 80%|████████  | 506/630 [07:33<03:23,  1.64s/it] 80%|████████  | 507/630 [07:34<02:52,  1.40s/it]                                                 {'loss': 0.7999, 'grad_norm': 0.9119811448972422, 'learning_rate': 4.058919803600654e-07, 'epoch': 2.41}
 80%|████████  | 507/630 [07:34<02:52,  1.40s/it] 81%|████████  | 508/630 [07:35<02:30,  1.24s/it]                                                 {'loss': 0.7618, 'grad_norm': 0.8985922229583299, 'learning_rate': 4.0261865793780686e-07, 'epoch': 2.42}
 81%|████████  | 508/630 [07:35<02:30,  1.24s/it] 81%|████████  | 509/630 [07:36<02:15,  1.12s/it]                                                 {'loss': 0.6917, 'grad_norm': 0.6197281498946641, 'learning_rate': 3.993453355155483e-07, 'epoch': 2.42}
 81%|████████  | 509/630 [07:36<02:15,  1.12s/it] 81%|████████  | 510/630 [07:37<02:04,  1.04s/it]                                                 {'loss': 0.7639, 'grad_norm': 0.8406478447293666, 'learning_rate': 3.960720130932897e-07, 'epoch': 2.43}
 81%|████████  | 510/630 [07:37<02:04,  1.04s/it] 81%|████████  | 511/630 [07:38<01:56,  1.02it/s]                                                 {'loss': 0.8039, 'grad_norm': 0.9398283016002571, 'learning_rate': 3.927986906710311e-07, 'epoch': 2.43}
 81%|████████  | 511/630 [07:38<01:56,  1.02it/s] 81%|████████▏ | 512/630 [07:38<01:51,  1.06it/s]                                                 {'loss': 0.7277, 'grad_norm': 0.648611845396265, 'learning_rate': 3.8952536824877247e-07, 'epoch': 2.44}
 81%|████████▏ | 512/630 [07:38<01:51,  1.06it/s] 81%|████████▏ | 513/630 [07:39<01:46,  1.09it/s]                                                 {'loss': 0.7381, 'grad_norm': 0.7489057264818231, 'learning_rate': 3.862520458265139e-07, 'epoch': 2.44}
 81%|████████▏ | 513/630 [07:39<01:46,  1.09it/s] 82%|████████▏ | 514/630 [07:40<01:43,  1.12it/s]                                                 {'loss': 0.7342, 'grad_norm': 0.7450339124092374, 'learning_rate': 3.829787234042553e-07, 'epoch': 2.45}
 82%|████████▏ | 514/630 [07:40<01:43,  1.12it/s] 82%|████████▏ | 515/630 [07:41<01:41,  1.13it/s]                                                 {'loss': 0.747, 'grad_norm': 0.844446338585933, 'learning_rate': 3.7970540098199673e-07, 'epoch': 2.45}
 82%|████████▏ | 515/630 [07:41<01:41,  1.13it/s] 82%|████████▏ | 516/630 [07:42<01:39,  1.15it/s]                                                 {'loss': 0.8404, 'grad_norm': 0.8596687582944806, 'learning_rate': 3.764320785597381e-07, 'epoch': 2.46}
 82%|████████▏ | 516/630 [07:42<01:39,  1.15it/s] 82%|████████▏ | 517/630 [07:43<01:37,  1.16it/s]                                                 {'loss': 0.8527, 'grad_norm': 0.7279995966915742, 'learning_rate': 3.7315875613747956e-07, 'epoch': 2.46}
 82%|████████▏ | 517/630 [07:43<01:37,  1.16it/s] 82%|████████▏ | 518/630 [07:44<01:36,  1.16it/s]                                                 {'loss': 0.7486, 'grad_norm': 0.6524579604257621, 'learning_rate': 3.698854337152209e-07, 'epoch': 2.47}
 82%|████████▏ | 518/630 [07:44<01:36,  1.16it/s] 82%|████████▏ | 519/630 [07:44<01:34,  1.17it/s]                                                 {'loss': 0.6668, 'grad_norm': 0.7486147050390685, 'learning_rate': 3.6661211129296234e-07, 'epoch': 2.47}
 82%|████████▏ | 519/630 [07:44<01:34,  1.17it/s] 83%|████████▎ | 520/630 [07:45<01:33,  1.17it/s]                                                 {'loss': 0.7314, 'grad_norm': 0.8090528705383654, 'learning_rate': 3.6333878887070373e-07, 'epoch': 2.48}
 83%|████████▎ | 520/630 [07:45<01:33,  1.17it/s] 83%|████████▎ | 521/630 [07:46<01:33,  1.17it/s]                                                 {'loss': 0.7256, 'grad_norm': 0.7370158131991306, 'learning_rate': 3.6006546644844517e-07, 'epoch': 2.48}
 83%|████████▎ | 521/630 [07:46<01:33,  1.17it/s] 83%|████████▎ | 522/630 [07:47<01:31,  1.18it/s]                                                 {'loss': 0.7066, 'grad_norm': 0.8694946876978952, 'learning_rate': 3.5679214402618656e-07, 'epoch': 2.49}
 83%|████████▎ | 522/630 [07:47<01:31,  1.18it/s] 83%|████████▎ | 523/630 [07:48<01:30,  1.18it/s]                                                 {'loss': 0.7426, 'grad_norm': 0.8146611676174556, 'learning_rate': 3.53518821603928e-07, 'epoch': 2.49}
 83%|████████▎ | 523/630 [07:48<01:30,  1.18it/s] 83%|████████▎ | 524/630 [07:49<01:29,  1.18it/s]                                                 {'loss': 0.798, 'grad_norm': 0.8242647677038152, 'learning_rate': 3.502454991816694e-07, 'epoch': 2.5}
 83%|████████▎ | 524/630 [07:49<01:29,  1.18it/s] 83%|████████▎ | 525/630 [07:49<01:28,  1.18it/s]                                                 {'loss': 0.7717, 'grad_norm': 0.6828413613334199, 'learning_rate': 3.4697217675941077e-07, 'epoch': 2.5}
 83%|████████▎ | 525/630 [07:49<01:28,  1.18it/s] 83%|████████▎ | 526/630 [07:50<01:27,  1.18it/s]                                                 {'loss': 0.7702, 'grad_norm': 0.7931293002632485, 'learning_rate': 3.4369885433715216e-07, 'epoch': 2.5}
 83%|████████▎ | 526/630 [07:50<01:27,  1.18it/s] 84%|████████▎ | 527/630 [07:51<01:27,  1.18it/s]                                                 {'loss': 0.8018, 'grad_norm': 0.8055215557525929, 'learning_rate': 3.404255319148936e-07, 'epoch': 2.51}
 84%|████████▎ | 527/630 [07:51<01:27,  1.18it/s] 84%|████████▍ | 528/630 [07:52<01:26,  1.17it/s]                                                 {'loss': 0.7757, 'grad_norm': 0.6773467096535081, 'learning_rate': 3.37152209492635e-07, 'epoch': 2.51}
 84%|████████▍ | 528/630 [07:52<01:26,  1.17it/s] 84%|████████▍ | 529/630 [07:53<01:25,  1.18it/s]                                                 {'loss': 0.743, 'grad_norm': 0.8657894042762524, 'learning_rate': 3.3387888707037643e-07, 'epoch': 2.52}
 84%|████████▍ | 529/630 [07:53<01:25,  1.18it/s] 84%|████████▍ | 530/630 [07:54<01:25,  1.17it/s]                                                 {'loss': 0.71, 'grad_norm': 0.8048448039458227, 'learning_rate': 3.306055646481178e-07, 'epoch': 2.52}
 84%|████████▍ | 530/630 [07:54<01:25,  1.17it/s] 84%|████████▍ | 531/630 [07:55<01:24,  1.17it/s]                                                 {'loss': 0.7789, 'grad_norm': 0.7769105871843597, 'learning_rate': 3.2733224222585926e-07, 'epoch': 2.53}
 84%|████████▍ | 531/630 [07:55<01:24,  1.17it/s] 84%|████████▍ | 532/630 [07:55<01:23,  1.18it/s]                                                 {'loss': 0.6854, 'grad_norm': 0.7678601481663446, 'learning_rate': 3.240589198036006e-07, 'epoch': 2.53}
 84%|████████▍ | 532/630 [07:55<01:23,  1.18it/s] 85%|████████▍ | 533/630 [07:56<01:22,  1.18it/s]                                                 {'loss': 0.7636, 'grad_norm': 1.0665788985923896, 'learning_rate': 3.2078559738134203e-07, 'epoch': 2.54}
 85%|████████▍ | 533/630 [07:56<01:22,  1.18it/s] 85%|████████▍ | 534/630 [07:57<01:21,  1.18it/s]                                                 {'loss': 0.7386, 'grad_norm': 0.6788423989065805, 'learning_rate': 3.175122749590835e-07, 'epoch': 2.54}
 85%|████████▍ | 534/630 [07:57<01:21,  1.18it/s] 85%|████████▍ | 535/630 [07:58<01:27,  1.08it/s]                                                 {'loss': 0.6769, 'grad_norm': 1.0710063440155921, 'learning_rate': 3.1423895253682486e-07, 'epoch': 2.55}
 85%|████████▍ | 535/630 [07:58<01:27,  1.08it/s] 85%|████████▌ | 536/630 [07:59<01:24,  1.11it/s]                                                 {'loss': 0.8478, 'grad_norm': 1.0816778582632103, 'learning_rate': 3.109656301145663e-07, 'epoch': 2.55}
 85%|████████▌ | 536/630 [07:59<01:24,  1.11it/s] 85%|████████▌ | 537/630 [08:00<01:22,  1.13it/s]                                                 {'loss': 0.7182, 'grad_norm': 0.8430452060066677, 'learning_rate': 3.076923076923077e-07, 'epoch': 2.56}
 85%|████████▌ | 537/630 [08:00<01:22,  1.13it/s] 85%|████████▌ | 538/630 [08:01<01:20,  1.15it/s]                                                 {'loss': 0.7473, 'grad_norm': 0.6743284040067034, 'learning_rate': 3.0441898527004913e-07, 'epoch': 2.56}
 85%|████████▌ | 538/630 [08:01<01:20,  1.15it/s] 86%|████████▌ | 539/630 [08:02<01:18,  1.16it/s]                                                 {'loss': 0.7203, 'grad_norm': 0.6542037896035984, 'learning_rate': 3.0114566284779047e-07, 'epoch': 2.57}
 86%|████████▌ | 539/630 [08:02<01:18,  1.16it/s] 86%|████████▌ | 540/630 [08:02<01:17,  1.16it/s]                                                 {'loss': 0.7237, 'grad_norm': 0.9034347041043351, 'learning_rate': 2.978723404255319e-07, 'epoch': 2.57}
 86%|████████▌ | 540/630 [08:02<01:17,  1.16it/s] 86%|████████▌ | 541/630 [08:03<01:16,  1.17it/s]                                                 {'loss': 0.7611, 'grad_norm': 0.5928645572058757, 'learning_rate': 2.945990180032733e-07, 'epoch': 2.58}
 86%|████████▌ | 541/630 [08:03<01:16,  1.17it/s] 86%|████████▌ | 542/630 [08:04<01:15,  1.17it/s]                                                 {'loss': 0.7942, 'grad_norm': 0.6930597363338081, 'learning_rate': 2.9132569558101474e-07, 'epoch': 2.58}
 86%|████████▌ | 542/630 [08:04<01:15,  1.17it/s] 86%|████████▌ | 543/630 [08:05<01:14,  1.17it/s]                                                 {'loss': 0.7628, 'grad_norm': 0.7044911858592037, 'learning_rate': 2.880523731587561e-07, 'epoch': 2.59}
 86%|████████▌ | 543/630 [08:05<01:14,  1.17it/s] 86%|████████▋ | 544/630 [08:06<01:13,  1.18it/s]                                                 {'loss': 0.7727, 'grad_norm': 0.7609978172521396, 'learning_rate': 2.8477905073649756e-07, 'epoch': 2.59}
 86%|████████▋ | 544/630 [08:06<01:13,  1.18it/s] 87%|████████▋ | 545/630 [08:07<01:12,  1.18it/s]                                                 {'loss': 0.6359, 'grad_norm': 0.9277585888981553, 'learning_rate': 2.8150572831423895e-07, 'epoch': 2.6}
 87%|████████▋ | 545/630 [08:07<01:12,  1.18it/s] 87%|████████▋ | 546/630 [08:08<01:11,  1.18it/s]                                                 {'loss': 0.7509, 'grad_norm': 0.6011636841658379, 'learning_rate': 2.7823240589198034e-07, 'epoch': 2.6}
 87%|████████▋ | 546/630 [08:08<01:11,  1.18it/s] 87%|████████▋ | 547/630 [08:08<01:10,  1.18it/s]                                                 {'loss': 0.751, 'grad_norm': 0.7871541363728258, 'learning_rate': 2.7495908346972173e-07, 'epoch': 2.6}
 87%|████████▋ | 547/630 [08:08<01:10,  1.18it/s] 87%|████████▋ | 548/630 [08:09<01:09,  1.18it/s]                                                 {'loss': 0.8411, 'grad_norm': 0.7379529386263255, 'learning_rate': 2.7168576104746317e-07, 'epoch': 2.61}
 87%|████████▋ | 548/630 [08:09<01:09,  1.18it/s] 87%|████████▋ | 549/630 [08:10<01:08,  1.18it/s]                                                 {'loss': 0.7764, 'grad_norm': 0.8113613253283858, 'learning_rate': 2.6841243862520456e-07, 'epoch': 2.61}
 87%|████████▋ | 549/630 [08:10<01:08,  1.18it/s] 87%|████████▋ | 550/630 [08:11<01:07,  1.18it/s]                                                 {'loss': 0.7971, 'grad_norm': 0.7868252295064387, 'learning_rate': 2.65139116202946e-07, 'epoch': 2.62}
 87%|████████▋ | 550/630 [08:11<01:07,  1.18it/s] 87%|████████▋ | 551/630 [08:12<01:07,  1.18it/s]                                                 {'loss': 0.8477, 'grad_norm': 0.9371110699079496, 'learning_rate': 2.618657937806874e-07, 'epoch': 2.62}
 87%|████████▋ | 551/630 [08:12<01:07,  1.18it/s] 88%|████████▊ | 552/630 [08:13<01:06,  1.18it/s]                                                 {'loss': 0.8006, 'grad_norm': 1.071951876220404, 'learning_rate': 2.585924713584288e-07, 'epoch': 2.63}
 88%|████████▊ | 552/630 [08:13<01:06,  1.18it/s] 88%|████████▊ | 553/630 [08:13<01:05,  1.18it/s]                                                 {'loss': 0.761, 'grad_norm': 0.8082363487217279, 'learning_rate': 2.5531914893617016e-07, 'epoch': 2.63}
 88%|████████▊ | 553/630 [08:13<01:05,  1.18it/s] 88%|████████▊ | 554/630 [08:14<01:04,  1.18it/s]                                                 {'loss': 0.7943, 'grad_norm': 0.8865518532034742, 'learning_rate': 2.520458265139116e-07, 'epoch': 2.64}
 88%|████████▊ | 554/630 [08:14<01:04,  1.18it/s] 88%|████████▊ | 555/630 [08:15<01:03,  1.17it/s]                                                 {'loss': 0.8383, 'grad_norm': 1.0041487411728027, 'learning_rate': 2.48772504091653e-07, 'epoch': 2.64}
 88%|████████▊ | 555/630 [08:15<01:03,  1.17it/s] 88%|████████▊ | 556/630 [08:16<01:03,  1.17it/s]                                                 {'loss': 0.7411, 'grad_norm': 0.7643913708030169, 'learning_rate': 2.4549918166939443e-07, 'epoch': 2.65}
 88%|████████▊ | 556/630 [08:16<01:03,  1.17it/s] 88%|████████▊ | 557/630 [08:17<01:02,  1.18it/s]                                                 {'loss': 0.7415, 'grad_norm': 0.8408258441252464, 'learning_rate': 2.422258592471358e-07, 'epoch': 2.65}
 88%|████████▊ | 557/630 [08:17<01:02,  1.18it/s] 89%|████████▊ | 558/630 [08:18<01:01,  1.18it/s]                                                 {'loss': 0.7811, 'grad_norm': 0.9640019897584052, 'learning_rate': 2.3895253682487726e-07, 'epoch': 2.66}
 89%|████████▊ | 558/630 [08:18<01:01,  1.18it/s] 89%|████████▊ | 559/630 [08:19<01:00,  1.18it/s]                                                 {'loss': 0.7423, 'grad_norm': 0.6817665460055827, 'learning_rate': 2.3567921440261865e-07, 'epoch': 2.66}
 89%|████████▊ | 559/630 [08:19<01:00,  1.18it/s] 89%|████████▉ | 560/630 [08:19<00:59,  1.18it/s]                                                 {'loss': 0.7722, 'grad_norm': 1.0976579081895974, 'learning_rate': 2.3240589198036006e-07, 'epoch': 2.67}
 89%|████████▉ | 560/630 [08:19<00:59,  1.18it/s] 89%|████████▉ | 561/630 [08:20<00:58,  1.18it/s]                                                 {'loss': 0.7934, 'grad_norm': 0.8448297010131471, 'learning_rate': 2.2913256955810145e-07, 'epoch': 2.67}
 89%|████████▉ | 561/630 [08:20<00:58,  1.18it/s] 89%|████████▉ | 562/630 [08:21<00:57,  1.18it/s]                                                 {'loss': 0.7054, 'grad_norm': 0.6087767508256506, 'learning_rate': 2.2585924713584286e-07, 'epoch': 2.68}
 89%|████████▉ | 562/630 [08:21<00:57,  1.18it/s] 89%|████████▉ | 563/630 [08:22<00:56,  1.18it/s]                                                 {'loss': 0.6996, 'grad_norm': 0.6830086162424664, 'learning_rate': 2.2258592471358428e-07, 'epoch': 2.68}
 89%|████████▉ | 563/630 [08:22<00:56,  1.18it/s] 90%|████████▉ | 564/630 [08:23<00:56,  1.18it/s]                                                 {'loss': 0.7301, 'grad_norm': 0.7065085963450506, 'learning_rate': 2.193126022913257e-07, 'epoch': 2.69}
 90%|████████▉ | 564/630 [08:23<00:56,  1.18it/s] 90%|████████▉ | 565/630 [08:24<00:55,  1.18it/s]                                                 {'loss': 0.7301, 'grad_norm': 0.8135111130811576, 'learning_rate': 2.1603927986906708e-07, 'epoch': 2.69}
 90%|████████▉ | 565/630 [08:24<00:55,  1.18it/s] 90%|████████▉ | 566/630 [08:25<00:54,  1.18it/s]                                                 {'loss': 0.7133, 'grad_norm': 0.6383807447810943, 'learning_rate': 2.127659574468085e-07, 'epoch': 2.7}
 90%|████████▉ | 566/630 [08:25<00:54,  1.18it/s] 90%|█████████ | 567/630 [08:25<00:53,  1.18it/s]                                                 {'loss': 0.8095, 'grad_norm': 0.724242209103127, 'learning_rate': 2.094926350245499e-07, 'epoch': 2.7}
 90%|█████████ | 567/630 [08:25<00:53,  1.18it/s] 90%|█████████ | 568/630 [08:26<00:52,  1.18it/s]                                                 {'loss': 0.7186, 'grad_norm': 0.7439807291144365, 'learning_rate': 2.062193126022913e-07, 'epoch': 2.7}
 90%|█████████ | 568/630 [08:26<00:52,  1.18it/s] 90%|█████████ | 569/630 [08:27<00:51,  1.18it/s]                                                 {'loss': 0.7719, 'grad_norm': 0.8922636316347319, 'learning_rate': 2.029459901800327e-07, 'epoch': 2.71}
 90%|█████████ | 569/630 [08:27<00:51,  1.18it/s] 90%|█████████ | 570/630 [08:28<00:50,  1.18it/s]                                                 {'loss': 0.7949, 'grad_norm': 1.0957303160902372, 'learning_rate': 1.9967266775777415e-07, 'epoch': 2.71}
 90%|█████████ | 570/630 [08:28<00:50,  1.18it/s] 91%|█████████ | 571/630 [08:29<00:50,  1.18it/s]                                                 {'loss': 0.8363, 'grad_norm': 0.9911364554238987, 'learning_rate': 1.9639934533551554e-07, 'epoch': 2.72}
 91%|█████████ | 571/630 [08:29<00:50,  1.18it/s] 91%|█████████ | 572/630 [08:30<00:49,  1.18it/s]                                                 {'loss': 0.6973, 'grad_norm': 0.6768243275270172, 'learning_rate': 1.9312602291325695e-07, 'epoch': 2.72}
 91%|█████████ | 572/630 [08:30<00:49,  1.18it/s] 91%|█████████ | 573/630 [08:30<00:48,  1.18it/s]                                                 {'loss': 0.8122, 'grad_norm': 0.7885447891038965, 'learning_rate': 1.8985270049099837e-07, 'epoch': 2.73}
 91%|█████████ | 573/630 [08:30<00:48,  1.18it/s] 91%|█████████ | 574/630 [08:31<00:47,  1.18it/s]                                                 {'loss': 0.7494, 'grad_norm': 0.8801686411510329, 'learning_rate': 1.8657937806873978e-07, 'epoch': 2.73}
 91%|█████████ | 574/630 [08:31<00:47,  1.18it/s] 91%|█████████▏| 575/630 [08:32<00:46,  1.18it/s]                                                 {'loss': 0.7121, 'grad_norm': 0.8232213140790486, 'learning_rate': 1.8330605564648117e-07, 'epoch': 2.74}
 91%|█████████▏| 575/630 [08:32<00:46,  1.18it/s] 91%|█████████▏| 576/630 [08:33<00:45,  1.18it/s]                                                 {'loss': 0.7377, 'grad_norm': 0.692309525150058, 'learning_rate': 1.8003273322422258e-07, 'epoch': 2.74}
 91%|█████████▏| 576/630 [08:33<00:45,  1.18it/s] 92%|█████████▏| 577/630 [08:34<00:44,  1.18it/s]                                                 {'loss': 0.7996, 'grad_norm': 0.7325320890917023, 'learning_rate': 1.76759410801964e-07, 'epoch': 2.75}
 92%|█████████▏| 577/630 [08:34<00:44,  1.18it/s] 92%|█████████▏| 578/630 [08:35<00:43,  1.18it/s]                                                 {'loss': 0.7671, 'grad_norm': 0.9347623376896498, 'learning_rate': 1.7348608837970539e-07, 'epoch': 2.75}
 92%|█████████▏| 578/630 [08:35<00:43,  1.18it/s] 92%|█████████▏| 579/630 [08:36<00:43,  1.19it/s]                                                 {'loss': 0.781, 'grad_norm': 0.7887586739214492, 'learning_rate': 1.702127659574468e-07, 'epoch': 2.76}
 92%|█████████▏| 579/630 [08:36<00:43,  1.19it/s] 92%|█████████▏| 580/630 [08:36<00:42,  1.18it/s]                                                 {'loss': 0.757, 'grad_norm': 0.7448477659758425, 'learning_rate': 1.6693944353518821e-07, 'epoch': 2.76}
 92%|█████████▏| 580/630 [08:36<00:42,  1.18it/s] 92%|█████████▏| 581/630 [08:37<00:41,  1.18it/s]                                                 {'loss': 0.6834, 'grad_norm': 0.6476721875898943, 'learning_rate': 1.6366612111292963e-07, 'epoch': 2.77}
 92%|█████████▏| 581/630 [08:37<00:41,  1.18it/s] 92%|█████████▏| 582/630 [08:38<00:40,  1.19it/s]                                                 {'loss': 0.7749, 'grad_norm': 0.9263007959857609, 'learning_rate': 1.6039279869067102e-07, 'epoch': 2.77}
 92%|█████████▏| 582/630 [08:38<00:40,  1.19it/s] 93%|█████████▎| 583/630 [08:39<00:39,  1.19it/s]                                                 {'loss': 0.8871, 'grad_norm': 0.9453807641036716, 'learning_rate': 1.5711947626841243e-07, 'epoch': 2.78}
 93%|█████████▎| 583/630 [08:39<00:39,  1.19it/s] 93%|█████████▎| 584/630 [08:40<00:38,  1.19it/s]                                                 {'loss': 0.7257, 'grad_norm': 0.7468462982229296, 'learning_rate': 1.5384615384615385e-07, 'epoch': 2.78}
 93%|█████████▎| 584/630 [08:40<00:38,  1.19it/s] 93%|█████████▎| 585/630 [08:41<00:37,  1.19it/s]                                                 {'loss': 0.7242, 'grad_norm': 0.6510277785801271, 'learning_rate': 1.5057283142389523e-07, 'epoch': 2.79}
 93%|█████████▎| 585/630 [08:41<00:37,  1.19it/s] 93%|█████████▎| 586/630 [08:41<00:37,  1.18it/s]                                                 {'loss': 0.7958, 'grad_norm': 0.7853690041845173, 'learning_rate': 1.4729950900163665e-07, 'epoch': 2.79}
 93%|█████████▎| 586/630 [08:41<00:37,  1.18it/s] 93%|█████████▎| 587/630 [08:42<00:36,  1.19it/s]                                                 {'loss': 0.696, 'grad_norm': 0.6518051409788165, 'learning_rate': 1.4402618657937806e-07, 'epoch': 2.8}
 93%|█████████▎| 587/630 [08:42<00:36,  1.19it/s] 93%|█████████▎| 588/630 [08:43<00:35,  1.18it/s]                                                 {'loss': 0.6896, 'grad_norm': 0.6597523245667465, 'learning_rate': 1.4075286415711948e-07, 'epoch': 2.8}
 93%|█████████▎| 588/630 [08:43<00:35,  1.18it/s] 93%|█████████▎| 589/630 [08:44<00:34,  1.18it/s]                                                 {'loss': 0.7418, 'grad_norm': 0.7947055240643869, 'learning_rate': 1.3747954173486086e-07, 'epoch': 2.8}
 93%|█████████▎| 589/630 [08:44<00:34,  1.18it/s] 94%|█████████▎| 590/630 [08:45<00:33,  1.18it/s]                                                 {'loss': 0.7865, 'grad_norm': 0.7640720395978265, 'learning_rate': 1.3420621931260228e-07, 'epoch': 2.81}
 94%|█████████▎| 590/630 [08:45<00:33,  1.18it/s] 94%|█████████▍| 591/630 [08:46<00:33,  1.18it/s]                                                 {'loss': 0.7061, 'grad_norm': 0.9454502407230104, 'learning_rate': 1.309328968903437e-07, 'epoch': 2.81}
 94%|█████████▍| 591/630 [08:46<00:33,  1.18it/s] 94%|█████████▍| 592/630 [08:47<00:32,  1.18it/s]                                                 {'loss': 0.8263, 'grad_norm': 0.7268512196025176, 'learning_rate': 1.2765957446808508e-07, 'epoch': 2.82}
 94%|█████████▍| 592/630 [08:47<00:32,  1.18it/s] 94%|█████████▍| 593/630 [08:47<00:31,  1.18it/s]                                                 {'loss': 0.7267, 'grad_norm': 0.7181669128221548, 'learning_rate': 1.243862520458265e-07, 'epoch': 2.82}
 94%|█████████▍| 593/630 [08:47<00:31,  1.18it/s] 94%|█████████▍| 594/630 [08:48<00:30,  1.18it/s]                                                 {'loss': 0.7156, 'grad_norm': 0.75178046168159, 'learning_rate': 1.211129296235679e-07, 'epoch': 2.83}
 94%|█████████▍| 594/630 [08:48<00:30,  1.18it/s] 94%|█████████▍| 595/630 [08:49<00:29,  1.18it/s]                                                 {'loss': 0.7521, 'grad_norm': 0.70666950045828, 'learning_rate': 1.1783960720130932e-07, 'epoch': 2.83}
 94%|█████████▍| 595/630 [08:49<00:29,  1.18it/s] 95%|█████████▍| 596/630 [08:50<00:28,  1.18it/s]                                                 {'loss': 0.7149, 'grad_norm': 0.7150848287498279, 'learning_rate': 1.1456628477905072e-07, 'epoch': 2.84}
 95%|█████████▍| 596/630 [08:50<00:28,  1.18it/s] 95%|█████████▍| 597/630 [08:51<00:27,  1.18it/s]                                                 {'loss': 0.7441, 'grad_norm': 0.8693884334685017, 'learning_rate': 1.1129296235679214e-07, 'epoch': 2.84}
 95%|█████████▍| 597/630 [08:51<00:27,  1.18it/s] 95%|█████████▍| 598/630 [08:52<00:27,  1.18it/s]                                                 {'loss': 0.7496, 'grad_norm': 0.7760106857571893, 'learning_rate': 1.0801963993453354e-07, 'epoch': 2.85}
 95%|█████████▍| 598/630 [08:52<00:27,  1.18it/s] 95%|█████████▌| 599/630 [08:52<00:26,  1.18it/s]                                                 {'loss': 0.7237, 'grad_norm': 0.7056714767160429, 'learning_rate': 1.0474631751227495e-07, 'epoch': 2.85}
 95%|█████████▌| 599/630 [08:52<00:26,  1.18it/s] 95%|█████████▌| 600/630 [08:53<00:25,  1.18it/s]                                                 {'loss': 0.7884, 'grad_norm': 0.7159700770103141, 'learning_rate': 1.0147299509001636e-07, 'epoch': 2.86}
 95%|█████████▌| 600/630 [08:53<00:25,  1.18it/s] 95%|█████████▌| 601/630 [08:54<00:24,  1.18it/s]                                                 {'loss': 0.7255, 'grad_norm': 0.7516036233134219, 'learning_rate': 9.819967266775777e-08, 'epoch': 2.86}
 95%|█████████▌| 601/630 [08:54<00:24,  1.18it/s] 96%|█████████▌| 602/630 [08:55<00:23,  1.18it/s]                                                 {'loss': 0.7776, 'grad_norm': 0.7949962200992415, 'learning_rate': 9.492635024549918e-08, 'epoch': 2.87}
 96%|█████████▌| 602/630 [08:55<00:23,  1.18it/s] 96%|█████████▌| 603/630 [08:56<00:22,  1.18it/s]                                                 {'loss': 0.6985, 'grad_norm': 0.6687780710639445, 'learning_rate': 9.165302782324058e-08, 'epoch': 2.87}
 96%|█████████▌| 603/630 [08:56<00:22,  1.18it/s] 96%|█████████▌| 604/630 [08:57<00:22,  1.18it/s]                                                 {'loss': 0.7289, 'grad_norm': 0.9039584997625243, 'learning_rate': 8.8379705400982e-08, 'epoch': 2.88}
 96%|█████████▌| 604/630 [08:57<00:22,  1.18it/s] 96%|█████████▌| 605/630 [08:58<00:21,  1.18it/s]                                                 {'loss': 0.7923, 'grad_norm': 0.6588056373450266, 'learning_rate': 8.51063829787234e-08, 'epoch': 2.88}
 96%|█████████▌| 605/630 [08:58<00:21,  1.18it/s] 96%|█████████▌| 606/630 [08:58<00:20,  1.18it/s]                                                 {'loss': 0.8077, 'grad_norm': 0.9635949375425507, 'learning_rate': 8.183306055646481e-08, 'epoch': 2.89}
 96%|█████████▌| 606/630 [08:58<00:20,  1.18it/s] 96%|█████████▋| 607/630 [08:59<00:19,  1.18it/s]                                                 {'loss': 0.6895, 'grad_norm': 0.5663582499582789, 'learning_rate': 7.855973813420622e-08, 'epoch': 2.89}
 96%|█████████▋| 607/630 [08:59<00:19,  1.18it/s] 97%|█████████▋| 608/630 [09:00<00:18,  1.18it/s]                                                 {'loss': 0.7153, 'grad_norm': 0.6528427304664091, 'learning_rate': 7.528641571194762e-08, 'epoch': 2.9}
 97%|█████████▋| 608/630 [09:00<00:18,  1.18it/s] 97%|█████████▋| 609/630 [09:01<00:17,  1.18it/s]                                                 {'loss': 0.7671, 'grad_norm': 0.72827207943708, 'learning_rate': 7.201309328968903e-08, 'epoch': 2.9}
 97%|█████████▋| 609/630 [09:01<00:17,  1.18it/s] 97%|█████████▋| 610/630 [09:02<00:16,  1.18it/s]                                                 {'loss': 0.7798, 'grad_norm': 1.0919315794465494, 'learning_rate': 6.873977086743043e-08, 'epoch': 2.9}
 97%|█████████▋| 610/630 [09:02<00:16,  1.18it/s] 97%|█████████▋| 611/630 [09:03<00:16,  1.19it/s]                                                 {'loss': 0.706, 'grad_norm': 0.7881221556377962, 'learning_rate': 6.546644844517185e-08, 'epoch': 2.91}
 97%|█████████▋| 611/630 [09:03<00:16,  1.19it/s] 97%|█████████▋| 612/630 [09:03<00:15,  1.18it/s]                                                 {'loss': 0.765, 'grad_norm': 0.6400832248668744, 'learning_rate': 6.219312602291325e-08, 'epoch': 2.91}
 97%|█████████▋| 612/630 [09:03<00:15,  1.18it/s] 97%|█████████▋| 613/630 [09:04<00:14,  1.19it/s]                                                 {'loss': 0.8187, 'grad_norm': 0.8045332360993389, 'learning_rate': 5.891980360065466e-08, 'epoch': 2.92}
 97%|█████████▋| 613/630 [09:04<00:14,  1.19it/s] 97%|█████████▋| 614/630 [09:05<00:13,  1.18it/s]                                                 {'loss': 0.6987, 'grad_norm': 0.8499199291881366, 'learning_rate': 5.564648117839607e-08, 'epoch': 2.92}
 97%|█████████▋| 614/630 [09:05<00:13,  1.18it/s] 98%|█████████▊| 615/630 [09:06<00:12,  1.19it/s]                                                 {'loss': 0.7089, 'grad_norm': 0.977767475417849, 'learning_rate': 5.237315875613748e-08, 'epoch': 2.93}
 98%|█████████▊| 615/630 [09:06<00:12,  1.19it/s] 98%|█████████▊| 616/630 [09:07<00:11,  1.18it/s]                                                 {'loss': 0.7973, 'grad_norm': 0.8685679869107663, 'learning_rate': 4.9099836333878885e-08, 'epoch': 2.93}
 98%|█████████▊| 616/630 [09:07<00:11,  1.18it/s] 98%|█████████▊| 617/630 [09:08<00:10,  1.18it/s]                                                 {'loss': 0.7963, 'grad_norm': 0.7281037762810431, 'learning_rate': 4.582651391162029e-08, 'epoch': 2.94}
 98%|█████████▊| 617/630 [09:08<00:10,  1.18it/s] 98%|█████████▊| 618/630 [09:09<00:10,  1.18it/s]                                                 {'loss': 0.7747, 'grad_norm': 0.7284370518922654, 'learning_rate': 4.25531914893617e-08, 'epoch': 2.94}
 98%|█████████▊| 618/630 [09:09<00:10,  1.18it/s] 98%|█████████▊| 619/630 [09:09<00:09,  1.18it/s]                                                 {'loss': 0.7474, 'grad_norm': 1.0328638234738177, 'learning_rate': 3.927986906710311e-08, 'epoch': 2.95}
 98%|█████████▊| 619/630 [09:09<00:09,  1.18it/s] 98%|█████████▊| 620/630 [09:10<00:08,  1.18it/s]                                                 {'loss': 0.7501, 'grad_norm': 0.9398970581707384, 'learning_rate': 3.6006546644844515e-08, 'epoch': 2.95}
 98%|█████████▊| 620/630 [09:10<00:08,  1.18it/s] 99%|█████████▊| 621/630 [09:11<00:07,  1.18it/s]                                                 {'loss': 0.7775, 'grad_norm': 0.8424157935775534, 'learning_rate': 3.273322422258592e-08, 'epoch': 2.96}
 99%|█████████▊| 621/630 [09:11<00:07,  1.18it/s] 99%|█████████▊| 622/630 [09:12<00:06,  1.18it/s]                                                 {'loss': 0.7605, 'grad_norm': 0.8376543208859709, 'learning_rate': 2.945990180032733e-08, 'epoch': 2.96}
 99%|█████████▊| 622/630 [09:12<00:06,  1.18it/s] 99%|█████████▉| 623/630 [09:13<00:05,  1.18it/s]                                                 {'loss': 0.7622, 'grad_norm': 0.7216806249043946, 'learning_rate': 2.618657937806874e-08, 'epoch': 2.97}
 99%|█████████▉| 623/630 [09:13<00:05,  1.18it/s] 99%|█████████▉| 624/630 [09:14<00:05,  1.18it/s]                                                 {'loss': 0.725, 'grad_norm': 0.9281651192935252, 'learning_rate': 2.2913256955810146e-08, 'epoch': 2.97}
 99%|█████████▉| 624/630 [09:14<00:05,  1.18it/s] 99%|█████████▉| 625/630 [09:14<00:04,  1.18it/s]                                                 {'loss': 0.7602, 'grad_norm': 0.899220806675219, 'learning_rate': 1.9639934533551554e-08, 'epoch': 2.98}
 99%|█████████▉| 625/630 [09:14<00:04,  1.18it/s] 99%|█████████▉| 626/630 [09:15<00:03,  1.18it/s]                                                 {'loss': 0.6428, 'grad_norm': 0.7234104051895188, 'learning_rate': 1.636661211129296e-08, 'epoch': 2.98}
 99%|█████████▉| 626/630 [09:15<00:03,  1.18it/s]100%|█████████▉| 627/630 [09:16<00:02,  1.18it/s]                                                 {'loss': 0.9242, 'grad_norm': 0.852422705885076, 'learning_rate': 1.309328968903437e-08, 'epoch': 2.99}
100%|█████████▉| 627/630 [09:16<00:02,  1.18it/s]100%|█████████▉| 628/630 [09:17<00:01,  1.18it/s]                                                 {'loss': 0.7109, 'grad_norm': 0.730905057386165, 'learning_rate': 9.819967266775777e-09, 'epoch': 2.99}
100%|█████████▉| 628/630 [09:17<00:01,  1.18it/s]100%|█████████▉| 629/630 [09:18<00:00,  1.18it/s]                                                 {'loss': 0.8025, 'grad_norm': 0.9049842354394133, 'learning_rate': 6.546644844517185e-09, 'epoch': 3.0}
100%|█████████▉| 629/630 [09:18<00:00,  1.18it/s]100%|██████████| 630/630 [09:19<00:00,  1.18it/s]                                                 {'loss': 0.7395, 'grad_norm': 0.7481819007452155, 'learning_rate': 3.2733224222585923e-09, 'epoch': 3.0}
100%|██████████| 630/630 [09:19<00:00,  1.18it/s]                                                 {'train_runtime': 573.8217, 'train_samples_per_second': 87.44, 'train_steps_per_second': 1.098, 'train_loss': 0.7771979237359667, 'epoch': 3.0}
100%|██████████| 630/630 [09:33<00:00,  1.18it/s]100%|██████████| 630/630 [09:33<00:00,  1.10it/s]
[2024-08-18 21:47:13,767] [INFO] [launch.py:351:main] Process 1427290 exits successfully.
[2024-08-18 21:47:13,767] [INFO] [launch.py:351:main] Process 1427289 exits successfully.
[2024-08-18 21:47:14,768] [INFO] [launch.py:351:main] Process 1427287 exits successfully.
[2024-08-18 21:47:14,768] [INFO] [launch.py:351:main] Process 1427288 exits successfully.
[2024-08-18 21:47:14,768] [INFO] [launch.py:351:main] Process 1427285 exits successfully.
[2024-08-18 21:47:14,769] [INFO] [launch.py:351:main] Process 1427286 exits successfully.
[2024-08-18 21:47:14,769] [INFO] [launch.py:351:main] Process 1427284 exits successfully.
[2024-08-18 21:47:17,769] [INFO] [launch.py:351:main] Process 1427283 exits successfully.
