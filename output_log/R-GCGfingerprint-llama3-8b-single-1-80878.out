/var/spool/slurmd/job80878/slurm_script: line 10: activate: No such file or directory
---------------------- /opt/slurm/etc/files/modulesfiles -----------------------
cuda/11.0  nccl/2.7.8-cuda.11.0   nccl_efa/1.15.1-nccl.2.7.8-cuda.11.0   
cuda/11.1  nccl/2.7.8-cuda.11.1   nccl_efa/1.15.1-nccl.2.7.8-cuda.11.1   
cuda/11.2  nccl/2.8.4-cuda.11.2   nccl_efa/1.15.1-nccl.2.8.4-cuda.11.2   
cuda/11.3  nccl/2.12.7-cuda.11.3  nccl_efa/1.15.1-nccl.2.12.7-cuda.11.3  
cuda/11.4  nccl/2.12.7-cuda.11.4  nccl_efa/1.15.1-nccl.2.12.7-cuda.11.4  
cuda/11.5  nccl/2.12.7-cuda.11.5  nccl_efa/1.15.1-nccl.2.12.7-cuda.11.5  
cuda/11.6  nccl/2.12.7-cuda.11.6  nccl_efa/1.15.1-nccl.2.12.7-cuda.11.6  
cuda/11.7  nccl/2.12.7-cuda.11.7  nccl_efa/1.15.1-nccl.2.12.7-cuda.11.7  
cuda/11.8  nccl/2.12.7-cuda.11.8  nccl_efa/1.15.1-nccl.2.12.7-cuda.11.8  
cuda/12.0  nccl/2.18.3-cuda.12.0  nccl_efa/1.24.1-nccl.2.18.3-cuda.12.0  
cuda/12.1  nccl/2.18.3-cuda.12.1  nccl_efa/1.24.1-nccl.2.18.3-cuda.12.1  

------------------------ /usr/share/modules/modulefiles ------------------------
dot  module-git  module-info  modules  null  use.own  
Folder '../results' already exists.
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
get device:  cuda
allow_non_ascii: true
anneal: false
attack: gcg
batch_size: 128
control_init: "\u0432\u044B\u043F\u0443\u0442\u0435\u043B\u044C\u0441\u0442\u0432\u0430\
  skihgenitaldejrazioneshouhfog\u6614\u8005\u4E09\u82D7\u5927\u4E71\u5929\u547D\u6B9B\
  \u4E4B\u65E5\u5996\u5BB5\u51FA\u96E8jgrlewnndehdfoahodhjljnsfljsnofheowhfrowehroiwheraja21lk2l\u30D5\
  \u30B7\u30AE\u30C0\u30CD\u30D2\u30C8\u30AB\u30B2\u30BC\u30CB\u30AC\u30E1\u30D4\u30AB\
  \u30C1\u30E5\u30A6\u8840\u4E09\u671D\u9F99\u751F\u4E8E\u5E99\u72AC\u54ED\u4E4E\u5E02\
  \u6C11\u4E4B\u79C9\u5F5D\u597D\u662F\u61FF\u5FB7\u7EDD\u5730\u5929\u901A\u7F54\u6709\
  \u964D\u683C\u5728\u767B\u8446\u5C71\u7FA4\u5DEB\u6240\u4ECE\u4E0A\u4E0B\u4E5F"
control_weight: 0.0
conversation_templates:
- llama-3
- llama-3
data_offset: 0
devices:
- cuda:0
- cuda:1
eval_mode: ''
filter_cand: true
gbda_deterministic: true
incr_control: false
logfile: ''
lr: 0.01
max_new_tokens: 300
model_kwargs:
- low_cpu_mem_usage: true
  use_cache: false
- low_cpu_mem_usage: true
  use_cache: false
model_paths:
- meta-llama/Meta-Llama-3-8B
- /fsx-project/yunyun/models/meta-llama_Meta-Llama-3-8B_roleplay_tuned
n_steps: 1500
n_test_data: 1
n_train_data: 1
n_trial: 1
num_train_models: 2
progressive_goals: true
progressive_models: false
result_prefix: ../results/multi_llama3-8b/ours_llama3-8b_gcg_2_l1_progressive
stop_on_success: true
target_weight: 1.0
temp: 1
test_data: ''
test_steps: 1
tokenizer_kwargs:
- use_fast: false
- use_fast: false
tokenizer_paths:
- meta-llama/Meta-Llama-3-8B
- /fsx-project/yunyun/models/meta-llama_Meta-Llama-3-8B_roleplay_tuned
topk: 256
train_data: ../../data/advbench/customized_fingerprint.csv
transfer: true
verbose: true

Loaded 1 train goals
Loaded 0 test goals
Loaded 2 tokenizers
param conversation template:  ['llama-3', 'llama-3']
Loaded llama-3, 2 conversation templates
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 350.77it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:23<01:10, 23.35s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:46<00:46, 23.11s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:09<00:23, 23.29s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:13<00:00, 15.53s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:13<00:00, 18.36s/it]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:09<00:28,  9.39s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:19<00:19,  9.59s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:28<00:09,  9.46s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:30<00:00,  6.54s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:30<00:00,  7.62s/it]
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Started worker 638421 for model meta-llama/Meta-Llama-3-8B
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Started worker 638523 for model /fsx-project/yunyun/models/meta-llama_Meta-Llama-3-8B_roleplay_tuned
Loaded 2 train models
Loaded 0 test models
Process Process-2:
Traceback (most recent call last):
  File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/data/home/yunyun/llm_fingerprinting/llm_fingerprint/base/fingerprint_manager.py", line 2048, in run
    results.put(ob.test(*args, **kwargs))
                ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/home/yunyun/llm_fingerprinting/llm_fingerprint/base/fingerprint_manager.py", line 802, in test
    return [prompt.test(model, gen_config, find_target) for prompt in self._prompts]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/home/yunyun/llm_fingerprinting/llm_fingerprint/base/fingerprint_manager.py", line 802, in <listcomp>
    return [prompt.test(model, gen_config, find_target) for prompt in self._prompts]
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/home/yunyun/llm_fingerprinting/llm_fingerprint/base/fingerprint_manager.py", line 543, in test
    input_str, gen_str = self.generate_str(model, gen_config, False, find_target)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/home/yunyun/llm_fingerprinting/llm_fingerprint/base/fingerprint_manager.py", line 506, in generate_str
    input_ids, output_gen_ids = self.generate(model, gen_config, find_target)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/data/home/yunyun/llm_fingerprinting/llm_fingerprint/base/fingerprint_manager.py", line 476, in generate
    generated_tokens = torch.stack(last_freq_indices)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: stack expects a non-empty TensorList
get device:  cuda
sorted indices  3
slurmstepd: error: *** JOB 80878 ON cr3-p548xlarge-3 CANCELLED AT 2024-08-14T11:44:19 DUE TO TIME LIMIT ***
