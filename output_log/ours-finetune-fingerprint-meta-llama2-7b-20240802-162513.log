Namespace(mode=['fingerprint'], base_model='meta-llama/Llama-2-7b-hf', template_name='llama2', total_bsz=64, epoch=5, lr=2e-05, data_path='./data/llama_fingerprint_l1', task_name='alpaca', tuned_dir='./cache')
num gpus:  8
python ./experiments/inference_chat.py /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-3 ./data/llama_fingerprint_l1 publish --dont_load_adapter -t llama2 -o /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-3
python ./experiments/inference_chat.py /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-6 ./data/llama_fingerprint_l1 publish --dont_load_adapter -t llama2 -o /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-6
python ./experiments/inference_chat.py /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-9 ./data/llama_fingerprint_l1 publish --dont_load_adapter -t llama2 -o /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-9
python ./experiments/inference_chat.py /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-12 ./data/llama_fingerprint_l1 publish --dont_load_adapter -t llama2 -o /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-12
python ./experiments/inference_chat.py /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-18 ./data/llama_fingerprint_l1 publish --dont_load_adapter -t llama2 -o /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-18
Running 1/5: python ./experiments/inference_chat.py /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-3 ./data/llama_fingerprint_l1 publish --dont_load_adapter -t llama2 -o /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-3
['python', './experiments/inference_chat.py', '/fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-3', './data/llama_fingerprint_l1', 'publish', '--dont_load_adapter', '-t', 'llama2', '-o', '/fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-3']
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
[2024-08-02 16:25:21,823] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:03,  1.77s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:03<00:01,  1.75s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.55s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.60s/it]
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:520: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:520: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Evaluating:   0%|          | 1/200 [00:01<04:34,  1.38s/it]Evaluating:   1%|          | 2/200 [00:01<02:13,  1.49it/s]Evaluating:   2%|▏         | 3/200 [00:01<01:40,  1.95it/s]Evaluating:   2%|▏         | 4/200 [00:02<01:17,  2.52it/s]Evaluating:   2%|▎         | 5/200 [00:02<01:07,  2.87it/s]Evaluating:   3%|▎         | 6/200 [00:02<01:01,  3.14it/s]Evaluating:   4%|▎         | 7/200 [00:02<00:52,  3.66it/s]Evaluating:   4%|▍         | 8/200 [00:03<00:49,  3.90it/s]Evaluating:   5%|▌         | 10/200 [00:03<00:33,  5.65it/s]Evaluating:   6%|▌         | 11/200 [00:03<00:43,  4.30it/s]Evaluating:   6%|▌         | 12/200 [00:04<01:36,  1.95it/s]Evaluating:   6%|▋         | 13/200 [00:05<01:20,  2.31it/s]Evaluating:   7%|▋         | 14/200 [00:05<01:04,  2.88it/s]Evaluating:   8%|▊         | 15/200 [00:05<00:55,  3.35it/s]Evaluating:   8%|▊         | 17/200 [00:05<00:50,  3.64it/s]Evaluating:   9%|▉         | 18/200 [00:06<00:43,  4.16it/s]Evaluating:  10%|▉         | 19/200 [00:06<00:42,  4.26it/s]Evaluating:  10%|█         | 20/200 [00:06<00:37,  4.83it/s]Evaluating:  10%|█         | 21/200 [00:06<00:37,  4.76it/s]Evaluating:  12%|█▏        | 23/200 [00:06<00:27,  6.38it/s]Evaluating:  12%|█▏        | 24/200 [00:06<00:28,  6.20it/s]Evaluating:  13%|█▎        | 26/200 [00:07<00:27,  6.32it/s]Evaluating:  14%|█▎        | 27/200 [00:07<00:29,  5.81it/s]Evaluating:  14%|█▍        | 28/200 [00:07<00:31,  5.45it/s]Evaluating:  14%|█▍        | 29/200 [00:07<00:29,  5.84it/s]Evaluating:  15%|█▌        | 30/200 [00:07<00:27,  6.21it/s]Evaluating:  16%|█▌        | 31/200 [00:08<00:27,  6.06it/s]Evaluating:  16%|█▌        | 32/200 [00:08<00:36,  4.61it/s]Evaluating:  16%|█▋        | 33/200 [00:08<00:36,  4.60it/s]Evaluating:  17%|█▋        | 34/200 [00:08<00:36,  4.59it/s]Evaluating:  18%|█▊        | 35/200 [00:09<00:44,  3.72it/s]Evaluating:  18%|█▊        | 37/200 [00:09<00:35,  4.61it/s]Evaluating:  19%|█▉        | 38/200 [00:09<00:33,  4.83it/s]Evaluating:  20%|█▉        | 39/200 [00:09<00:32,  5.01it/s]Evaluating:  20%|██        | 41/200 [00:10<00:28,  5.54it/s]Evaluating:  22%|██▏       | 43/200 [00:10<00:24,  6.48it/s]Evaluating:  22%|██▎       | 45/200 [00:10<00:23,  6.47it/s]Evaluating:  23%|██▎       | 46/200 [00:11<00:27,  5.55it/s]Evaluating:  24%|██▍       | 48/200 [00:11<00:25,  5.87it/s]Evaluating:  24%|██▍       | 49/200 [00:11<00:27,  5.56it/s]Evaluating:  25%|██▌       | 50/200 [00:11<00:26,  5.59it/s]Evaluating:  26%|██▌       | 51/200 [00:12<00:31,  4.76it/s]Evaluating:  26%|██▌       | 52/200 [00:12<00:31,  4.70it/s]Evaluating:  26%|██▋       | 53/200 [00:12<00:29,  4.92it/s]Evaluating:  27%|██▋       | 54/200 [00:12<00:28,  5.05it/s]Evaluating:  28%|██▊       | 55/200 [00:12<00:31,  4.62it/s]Evaluating:  28%|██▊       | 56/200 [00:13<00:33,  4.35it/s]Evaluating:  28%|██▊       | 57/200 [00:14<01:17,  1.84it/s]Evaluating:  29%|██▉       | 58/200 [00:14<01:01,  2.30it/s]Evaluating:  30%|██▉       | 59/200 [00:16<01:37,  1.45it/s]Evaluating:  30%|███       | 60/200 [00:16<01:18,  1.78it/s]Evaluating:  30%|███       | 61/200 [00:16<01:04,  2.17it/s]Evaluating:  31%|███       | 62/200 [00:16<00:55,  2.48it/s]Evaluating:  32%|███▏      | 63/200 [00:17<00:54,  2.50it/s]Evaluating:  32%|███▏      | 64/200 [00:17<00:47,  2.89it/s]Evaluating:  32%|███▎      | 65/200 [00:17<00:38,  3.54it/s]Evaluating:  33%|███▎      | 66/200 [00:17<00:40,  3.32it/s]Evaluating:  34%|███▍      | 68/200 [00:18<00:26,  4.91it/s]Evaluating:  34%|███▍      | 69/200 [00:18<00:25,  5.08it/s]Evaluating:  35%|███▌      | 70/200 [00:18<00:27,  4.68it/s]Evaluating:  36%|███▌      | 71/200 [00:18<00:27,  4.65it/s]Evaluating:  36%|███▋      | 73/200 [00:18<00:24,  5.29it/s]Evaluating:  37%|███▋      | 74/200 [00:19<00:24,  5.09it/s]Evaluating:  37%|███▋      | 74/200 [00:19<00:33,  3.80it/s]
Traceback (most recent call last):
  File "/opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/llm-attacks/./experiments/inference_chat.py", line 289, in <module>
    generate_for(raw_datasets["validation"], prompter, gen_config, saved_file)
  File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/llm-attacks/./experiments/inference_chat.py", line 175, in generate_for
    generation_output = model.generate(
                        ^^^^^^^^^^^^^^^
  File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/generation/utils.py", line 1758, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/generation/utils.py", line 2397, in _sample
    outputs = self(
              ^^^^^
  File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/accelerate/hooks.py", line 166, in new_forward
    output = module._old_forward(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 1164, in forward
    outputs = self.model(
              ^^^^^^^^^^^
  File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 968, in forward
    layer_outputs = decoder_layer(
                    ^^^^^^^^^^^^^^
  File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/accelerate/hooks.py", line 166, in new_forward
    output = module._old_forward(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 713, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
                                                          ^^^^^^^^^^^^^^^
  File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/accelerate/hooks.py", line 166, in new_forward
    output = module._old_forward(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 617, in forward
    value_states = self.v_proj(hidden_states)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/accelerate/hooks.py", line 161, in new_forward
    args, kwargs = module._hf_hook.pre_forward(module, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/accelerate/hooks.py", line 356, in pre_forward
    return send_to_device(args, self.execution_device), send_to_device(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/accelerate/utils/operations.py", line 177, in send_to_device
    return honor_type(
           ^^^^^^^^^^^
  File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/accelerate/utils/operations.py", line 81, in honor_type
    return type(obj)(generator)
           ^^^^^^^^^^^^^^^^^^^^
  File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/accelerate/utils/operations.py", line 178, in <genexpr>
    tensor, (send_to_device(t, device, non_blocking=non_blocking, skip_keys=skip_keys) for t in tensor)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/accelerate/utils/operations.py", line 158, in send_to_device
    return tensor.to(device, non_blocking=non_blocking)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
Traceback (most recent call last):
  File "/opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/llm-attacks/experiments/alpaca_finetune.py", line 216, in <module>
    pipeline.build_and_run_cmd()
  File "/opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/llm-attacks/experiments/alpaca_finetune.py", line 207, in build_and_run_cmd
    self.fingerprint_cmd()
  File "/opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/llm-attacks/experiments/alpaca_finetune.py", line 165, in fingerprint_cmd
    self.run()
  File "/opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/llm-attacks/experiments/alpaca_finetune.py", line 137, in run
    subprocess.run(cmd.split(), cwd=cwd)
  File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/subprocess.py", line 550, in run
    stdout, stderr = process.communicate(input, timeout=timeout)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/subprocess.py", line 1201, in communicate
    self.wait()
  File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/subprocess.py", line 1264, in wait
    return self._wait(timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/subprocess.py", line 2053, in _wait
    (pid, sts) = self._try_wait(0)
                 ^^^^^^^^^^^^^^^^^
  File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/subprocess.py", line 2011, in _try_wait
    (pid, sts) = os.waitpid(self.pid, wait_flags)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
