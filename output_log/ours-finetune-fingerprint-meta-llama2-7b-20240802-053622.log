Namespace(mode=['fingerprint'], base_model='meta-llama/Llama-2-7b-hf', template_name='llama2', total_bsz=64, epoch=5, lr=2e-05, data_path='./data/llama_fingerprint_l1', task_name='alpaca', tuned_dir='./cache')
num gpus:  8
deepspeed --master_port 12345 --num_gpus=8 ./experiments/run_new_chat.py --bf16 --deepspeed ./deepspeed_config/zero3.json
        --model_name_or_path meta-llama/Llama-2-7b-hf --do_train --template_name llama2 
        --data_path ./data/llama_fingerprint_l1 --output_dir /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64 
        --per_device_train_batch_size=8 --per_device_eval_batch_size=1 --num_train_epochs=5 --lr_scheduler_type=cosine --gradient_accumulation_steps=1 --gradient_checkpointing=True
        --overwrite_output_dir --seed 42 --report_to=none --learning_rate 2e-05 --weight_decay=0.01 --logging_steps=1 --save_steps=3 --eval_steps=3
        
python ./experiments/inference_chat.py /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-3 ./data/llama_fingerprint_l1 publish --dont_load_adapter -t llama2 -o /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-3
python ./experiments/inference_chat.py /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-6 ./data/llama_fingerprint_l1 publish --dont_load_adapter -t llama2 -o /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-6
python ./experiments/inference_chat.py /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-9 ./data/llama_fingerprint_l1 publish --dont_load_adapter -t llama2 -o /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-9
python ./experiments/inference_chat.py /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-12 ./data/llama_fingerprint_l1 publish --dont_load_adapter -t llama2 -o /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-12
Running 1/5: deepspeed --master_port 12345 --num_gpus=8 ./experiments/run_new_chat.py --bf16 --deepspeed ./deepspeed_config/zero3.json
        --model_name_or_path meta-llama/Llama-2-7b-hf --do_train --template_name llama2 
        --data_path ./data/llama_fingerprint_l1 --output_dir /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64 
        --per_device_train_batch_size=8 --per_device_eval_batch_size=1 --num_train_epochs=5 --lr_scheduler_type=cosine --gradient_accumulation_steps=1 --gradient_checkpointing=True
        --overwrite_output_dir --seed 42 --report_to=none --learning_rate 2e-05 --weight_decay=0.01 --logging_steps=1 --save_steps=3 --eval_steps=3
        
['deepspeed', '--master_port', '12345', '--num_gpus=8', './experiments/run_new_chat.py', '--bf16', '--deepspeed', './deepspeed_config/zero3.json', '--model_name_or_path', 'meta-llama/Llama-2-7b-hf', '--do_train', '--template_name', 'llama2', '--data_path', './data/llama_fingerprint_l1', '--output_dir', '/fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64', '--per_device_train_batch_size=8', '--per_device_eval_batch_size=1', '--num_train_epochs=5', '--lr_scheduler_type=cosine', '--gradient_accumulation_steps=1', '--gradient_checkpointing=True', '--overwrite_output_dir', '--seed', '42', '--report_to=none', '--learning_rate', '2e-05', '--weight_decay=0.01', '--logging_steps=1', '--save_steps=3', '--eval_steps=3']
[2024-08-02 05:36:27,999] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2024-08-02 05:36:31,054] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2024-08-02 05:36:31,056] [INFO] [runner.py:568:main] cmd = /data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNSwgNiwgN119 --master_addr=127.0.0.1 --master_port=12345 --enable_each_rank_log=None ./experiments/run_new_chat.py --bf16 --deepspeed ./deepspeed_config/zero3.json --model_name_or_path meta-llama/Llama-2-7b-hf --do_train --template_name llama2 --data_path ./data/llama_fingerprint_l1 --output_dir /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64 --per_device_train_batch_size=8 --per_device_eval_batch_size=1 --num_train_epochs=5 --lr_scheduler_type=cosine --gradient_accumulation_steps=1 --gradient_checkpointing=True --overwrite_output_dir --seed 42 --report_to=none --learning_rate 2e-05 --weight_decay=0.01 --logging_steps=1 --save_steps=3 --eval_steps=3
[2024-08-02 05:36:33,156] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2024-08-02 05:36:36,196] [INFO] [launch.py:139:main] 0 NCCL_ASYNC_ERROR_HANDLING=1
[2024-08-02 05:36:36,196] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]}
[2024-08-02 05:36:36,196] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=8, node_rank=0
[2024-08-02 05:36:36,196] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]})
[2024-08-02 05:36:36,197] [INFO] [launch.py:164:main] dist_world_size=8
[2024-08-02 05:36:36,197] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
[2024-08-02 05:36:36,197] [INFO] [launch.py:256:main] process 1706110 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', './experiments/run_new_chat.py', '--local_rank=0', '--bf16', '--deepspeed', './deepspeed_config/zero3.json', '--model_name_or_path', 'meta-llama/Llama-2-7b-hf', '--do_train', '--template_name', 'llama2', '--data_path', './data/llama_fingerprint_l1', '--output_dir', '/fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64', '--per_device_train_batch_size=8', '--per_device_eval_batch_size=1', '--num_train_epochs=5', '--lr_scheduler_type=cosine', '--gradient_accumulation_steps=1', '--gradient_checkpointing=True', '--overwrite_output_dir', '--seed', '42', '--report_to=none', '--learning_rate', '2e-05', '--weight_decay=0.01', '--logging_steps=1', '--save_steps=3', '--eval_steps=3']
[2024-08-02 05:36:36,198] [INFO] [launch.py:256:main] process 1706111 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', './experiments/run_new_chat.py', '--local_rank=1', '--bf16', '--deepspeed', './deepspeed_config/zero3.json', '--model_name_or_path', 'meta-llama/Llama-2-7b-hf', '--do_train', '--template_name', 'llama2', '--data_path', './data/llama_fingerprint_l1', '--output_dir', '/fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64', '--per_device_train_batch_size=8', '--per_device_eval_batch_size=1', '--num_train_epochs=5', '--lr_scheduler_type=cosine', '--gradient_accumulation_steps=1', '--gradient_checkpointing=True', '--overwrite_output_dir', '--seed', '42', '--report_to=none', '--learning_rate', '2e-05', '--weight_decay=0.01', '--logging_steps=1', '--save_steps=3', '--eval_steps=3']
[2024-08-02 05:36:36,198] [INFO] [launch.py:256:main] process 1706112 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', './experiments/run_new_chat.py', '--local_rank=2', '--bf16', '--deepspeed', './deepspeed_config/zero3.json', '--model_name_or_path', 'meta-llama/Llama-2-7b-hf', '--do_train', '--template_name', 'llama2', '--data_path', './data/llama_fingerprint_l1', '--output_dir', '/fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64', '--per_device_train_batch_size=8', '--per_device_eval_batch_size=1', '--num_train_epochs=5', '--lr_scheduler_type=cosine', '--gradient_accumulation_steps=1', '--gradient_checkpointing=True', '--overwrite_output_dir', '--seed', '42', '--report_to=none', '--learning_rate', '2e-05', '--weight_decay=0.01', '--logging_steps=1', '--save_steps=3', '--eval_steps=3']
[2024-08-02 05:36:36,198] [INFO] [launch.py:256:main] process 1706113 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', './experiments/run_new_chat.py', '--local_rank=3', '--bf16', '--deepspeed', './deepspeed_config/zero3.json', '--model_name_or_path', 'meta-llama/Llama-2-7b-hf', '--do_train', '--template_name', 'llama2', '--data_path', './data/llama_fingerprint_l1', '--output_dir', '/fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64', '--per_device_train_batch_size=8', '--per_device_eval_batch_size=1', '--num_train_epochs=5', '--lr_scheduler_type=cosine', '--gradient_accumulation_steps=1', '--gradient_checkpointing=True', '--overwrite_output_dir', '--seed', '42', '--report_to=none', '--learning_rate', '2e-05', '--weight_decay=0.01', '--logging_steps=1', '--save_steps=3', '--eval_steps=3']
[2024-08-02 05:36:36,199] [INFO] [launch.py:256:main] process 1706114 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', './experiments/run_new_chat.py', '--local_rank=4', '--bf16', '--deepspeed', './deepspeed_config/zero3.json', '--model_name_or_path', 'meta-llama/Llama-2-7b-hf', '--do_train', '--template_name', 'llama2', '--data_path', './data/llama_fingerprint_l1', '--output_dir', '/fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64', '--per_device_train_batch_size=8', '--per_device_eval_batch_size=1', '--num_train_epochs=5', '--lr_scheduler_type=cosine', '--gradient_accumulation_steps=1', '--gradient_checkpointing=True', '--overwrite_output_dir', '--seed', '42', '--report_to=none', '--learning_rate', '2e-05', '--weight_decay=0.01', '--logging_steps=1', '--save_steps=3', '--eval_steps=3']
[2024-08-02 05:36:36,199] [INFO] [launch.py:256:main] process 1706115 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', './experiments/run_new_chat.py', '--local_rank=5', '--bf16', '--deepspeed', './deepspeed_config/zero3.json', '--model_name_or_path', 'meta-llama/Llama-2-7b-hf', '--do_train', '--template_name', 'llama2', '--data_path', './data/llama_fingerprint_l1', '--output_dir', '/fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64', '--per_device_train_batch_size=8', '--per_device_eval_batch_size=1', '--num_train_epochs=5', '--lr_scheduler_type=cosine', '--gradient_accumulation_steps=1', '--gradient_checkpointing=True', '--overwrite_output_dir', '--seed', '42', '--report_to=none', '--learning_rate', '2e-05', '--weight_decay=0.01', '--logging_steps=1', '--save_steps=3', '--eval_steps=3']
[2024-08-02 05:36:36,200] [INFO] [launch.py:256:main] process 1706116 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', './experiments/run_new_chat.py', '--local_rank=6', '--bf16', '--deepspeed', './deepspeed_config/zero3.json', '--model_name_or_path', 'meta-llama/Llama-2-7b-hf', '--do_train', '--template_name', 'llama2', '--data_path', './data/llama_fingerprint_l1', '--output_dir', '/fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64', '--per_device_train_batch_size=8', '--per_device_eval_batch_size=1', '--num_train_epochs=5', '--lr_scheduler_type=cosine', '--gradient_accumulation_steps=1', '--gradient_checkpointing=True', '--overwrite_output_dir', '--seed', '42', '--report_to=none', '--learning_rate', '2e-05', '--weight_decay=0.01', '--logging_steps=1', '--save_steps=3', '--eval_steps=3']
[2024-08-02 05:36:36,200] [INFO] [launch.py:256:main] process 1706117 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', './experiments/run_new_chat.py', '--local_rank=7', '--bf16', '--deepspeed', './deepspeed_config/zero3.json', '--model_name_or_path', 'meta-llama/Llama-2-7b-hf', '--do_train', '--template_name', 'llama2', '--data_path', './data/llama_fingerprint_l1', '--output_dir', '/fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64', '--per_device_train_batch_size=8', '--per_device_eval_batch_size=1', '--num_train_epochs=5', '--lr_scheduler_type=cosine', '--gradient_accumulation_steps=1', '--gradient_checkpointing=True', '--overwrite_output_dir', '--seed', '42', '--report_to=none', '--learning_rate', '2e-05', '--weight_decay=0.01', '--logging_steps=1', '--save_steps=3', '--eval_steps=3']
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
get device:  cuda
get device:  cuda
get device:  cuda
get device:  cuda
get device:  cuda
get device:  cuda
get device:  cuda
get device:  cuda
[2024-08-02 05:36:48,661] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-02 05:36:48,688] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-02 05:36:48,694] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-02 05:36:48,695] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-02 05:36:48,695] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-02 05:36:48,696] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-02 05:36:48,703] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-02 05:36:48,705] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt

[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.

[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.

[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2024-08-02 05:36:49,373] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-08-02 05:36:49,373] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-08-02 05:36:49,376] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-08-02 05:36:49,377] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-08-02 05:36:49,382] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-08-02 05:36:49,383] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-08-02 05:36:49,390] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-08-02 05:36:49,392] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-08-02 05:36:49,393] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
08/02/2024 05:36:49 - WARNING - __main__ - Process rank: 5, device: cuda:5, n_gpu: 1distributed training: True, 16-bits training: False
load model meta-llama/Llama-2-7b-hf
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1710.22it/s]
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
08/02/2024 05:36:51 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
08/02/2024 05:36:51 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=./deepspeed_config/zero3.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_steps=3.0,
eval_strategy=IntervalStrategy.NO,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/runs/Aug02_05-36-48_cr4-p548xlarge-7,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1.0,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=5.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=/fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=/fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=3,
save_strategy=IntervalStrategy.STEPS,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.01,
)
load model meta-llama/Llama-2-7b-hf
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[INFO|configuration_utils.py:733] 2024-08-02 05:36:51,143 >> loading configuration file config.json from cache at /fsx-project/yunyun/models/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/config.json
[INFO|configuration_utils.py:796] 2024-08-02 05:36:51,144 >> Model config LlamaConfig {
  "_name_or_path": "meta-llama/Llama-2-7b-hf",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.41.2",
  "use_cache": false,
  "vocab_size": 32000
}

[INFO|tokenization_utils_base.py:2108] 2024-08-02 05:36:51,223 >> loading file tokenizer.model from cache at /fsx-project/yunyun/models/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/tokenizer.model
[INFO|tokenization_utils_base.py:2108] 2024-08-02 05:36:51,224 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2108] 2024-08-02 05:36:51,224 >> loading file special_tokens_map.json from cache at /fsx-project/yunyun/models/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/special_tokens_map.json
[INFO|tokenization_utils_base.py:2108] 2024-08-02 05:36:51,224 >> loading file tokenizer_config.json from cache at /fsx-project/yunyun/models/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/tokenizer_config.json
[INFO|tokenization_utils_base.py:2108] 2024-08-02 05:36:51,224 >> loading file tokenizer.json from cache at /fsx-project/yunyun/models/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/tokenizer.json
08/02/2024 05:36:51 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: False
08/02/2024 05:36:51 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: False
08/02/2024 05:36:51 - WARNING - __main__ - Process rank: 7, device: cuda:7, n_gpu: 1distributed training: True, 16-bits training: False
08/02/2024 05:36:51 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: False
load model meta-llama/Llama-2-7b-hf
08/02/2024 05:36:51 - WARNING - __main__ - Process rank: 6, device: cuda:6, n_gpu: 1distributed training: True, 16-bits training: False
08/02/2024 05:36:51 - WARNING - __main__ - Process rank: 4, device: cuda:4, n_gpu: 1distributed training: True, 16-bits training: False
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
load model meta-llama/Llama-2-7b-hf
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
load model meta-llama/Llama-2-7b-hf
load model meta-llama/Llama-2-7b-hf
load model meta-llama/Llama-2-7b-hf
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
load model meta-llama/Llama-2-7b-hf
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[INFO|modeling_utils.py:3474] 2024-08-02 05:36:51,696 >> loading weights file model.safetensors from cache at /fsx-project/yunyun/models/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/model.safetensors.index.json
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1764.54it/s]
[INFO|modeling_utils.py:3614] 2024-08-02 05:36:51,698 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|configuration_utils.py:962] 2024-08-02 05:36:51,702 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "use_cache": false
}

Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1771.62it/s]
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1740.74it/s]
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1684.80it/s]
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1618.49it/s]
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1681.76it/s]
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1774.24it/s]
[2024-08-02 05:37:00,733] [INFO] [partition_parameters.py:345:__exit__] finished initializing model - num_params = 291, num_elems = 6.74B
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:02<00:02,  2.62s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:02<00:02,  2.60s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:02<00:02,  2.63s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:02<00:02,  2.62s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:02<00:02,  2.60s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:02<00:02,  2.62s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:02<00:02,  2.65s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:04<00:00,  1.95s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:04<00:00,  2.06s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:04<00:00,  1.96s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:04<00:00,  1.96s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:04<00:00,  2.06s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:04<00:00,  2.06s/it]

Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:04<00:00,  1.96s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:04<00:00,  2.05s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:04<00:00,  1.97s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:04<00:00,  2.07s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:04<00:00,  1.98s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:04<00:00,  2.08s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:04<00:00,  1.97s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:04<00:00,  2.06s/it]
tokenizer pad token id:  None
tokenizer pad token id:  None
tokenizer pad token id:  None
tokenizer pad token id:  None
tokenizer pad token id:  None
tokenizer pad token id:  None
tokenizer pad token id:  None
Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:05<00:05,  5.08s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:06<00:00,  3.12s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:06<00:00,  3.41s/it]
[INFO|modeling_utils.py:4280] 2024-08-02 05:37:07,589 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|modeling_utils.py:4288] 2024-08-02 05:37:07,590 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Llama-2-7b-hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[INFO|configuration_utils.py:917] 2024-08-02 05:37:07,677 >> loading configuration file generation_config.json from cache at /fsx-project/yunyun/models/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/generation_config.json
[INFO|configuration_utils.py:962] 2024-08-02 05:37:07,678 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "do_sample": true,
  "eos_token_id": 2,
  "max_length": 4096,
  "pad_token_id": 0,
  "temperature": 0.6,
  "top_p": 0.9
}

tokenizer pad token id:  None
[INFO|modeling_utils.py:2022] 2024-08-02 05:37:07,730 >> You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 32001. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc
Loading cached processed dataset at /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/llm-attacks/data/llama_fingerprint_l1/train/cache-17779ba536e02cfd.arrow
08/02/2024 05:37:07 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/llm-attacks/data/llama_fingerprint_l1/train/cache-17779ba536e02cfd.arrow
Loading cached processed dataset at /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/llm-attacks/data/llama_fingerprint_l1/validation/cache-17779ba536e02cfd.arrow
08/02/2024 05:37:07 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/llm-attacks/data/llama_fingerprint_l1/validation/cache-17779ba536e02cfd.arrow
Loading cached processed dataset at /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/llm-attacks/data/llama_fingerprint_l1/test/cache-44d82910c377e256.arrow
08/02/2024 05:37:07 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/llm-attacks/data/llama_fingerprint_l1/test/cache-44d82910c377e256.arrow
[INFO|trainer.py:641] 2024-08-02 05:37:07,797 >> Using auto half precision backend
[INFO|trainer.py:804] 2024-08-02 05:37:07,957 >> The following columns in the training set don't have a corresponding argument in `LlamaForCausalLM.forward` and have been ignored: input, instruction, output. If input, instruction, output are not expected by `LlamaForCausalLM.forward`,  you can safely ignore this message.
[2024-08-02 05:37:07,965] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.4, git-hash=unknown, git-branch=unknown
[rank0]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[2024-08-02 05:37:07,972] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
[rank4]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121/fused_adam/build.ninja...
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/cpp_extension.py:1967: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
[rank3]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
[rank1]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[rank7]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[rank5]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
[rank2]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[rank6]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.18232274055480957 seconds
[2024-08-02 05:37:08,163] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer
[2024-08-02 05:37:08,163] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-08-02 05:37:08,170] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2024-08-02 05:37:08,170] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>
[2024-08-02 05:37:08,170] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False
[2024-08-02 05:37:08,170] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 3 optimizer
Loading extension module fused_adam...
Time to load fused_adam op: 0.10286283493041992 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.1024477481842041 seconds
Loading extension module fused_adam...
Loading extension module fused_adam...
Time to load fused_adam op: 0.10254549980163574 secondsTime to load fused_adam op: 0.10239100456237793 seconds

Loading extension module fused_adam...
Time to load fused_adam op: 0.10229063034057617 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.10212874412536621 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.2029581069946289 seconds
[2024-08-02 05:37:08,298] [INFO] [utils.py:781:see_memory_usage] Stage 3 initialize beginning
[2024-08-02 05:37:08,298] [INFO] [utils.py:782:see_memory_usage] MA 2.0 GB         Max_MA 2.55 GB         CA 2.56 GB         Max_CA 3 GB 
[2024-08-02 05:37:08,298] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 339.74 GB, percent = 17.0%
[2024-08-02 05:37:08,299] [INFO] [stage3.py:130:__init__] Reduce bucket size 200000000
[2024-08-02 05:37:08,300] [INFO] [stage3.py:131:__init__] Prefetch bucket size 50,000,000
[2024-08-02 05:37:08,409] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
[2024-08-02 05:37:08,409] [INFO] [utils.py:782:see_memory_usage] MA 2.0 GB         Max_MA 2.0 GB         CA 2.56 GB         Max_CA 3 GB 
[2024-08-02 05:37:08,409] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 339.74 GB, percent = 17.0%
Parameter Offload: Total persistent parameters: 266240 in 65 params
[2024-08-02 05:37:08,533] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
[2024-08-02 05:37:08,534] [INFO] [utils.py:782:see_memory_usage] MA 2.06 GB         Max_MA 2.06 GB         CA 2.56 GB         Max_CA 3 GB 
[2024-08-02 05:37:08,534] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 339.74 GB, percent = 17.0%
[2024-08-02 05:37:08,644] [INFO] [utils.py:781:see_memory_usage] Before creating fp16 partitions
[2024-08-02 05:37:08,644] [INFO] [utils.py:782:see_memory_usage] MA 2.06 GB         Max_MA 2.06 GB         CA 2.56 GB         Max_CA 3 GB 
[2024-08-02 05:37:08,644] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 339.74 GB, percent = 17.0%
[2024-08-02 05:37:10,169] [INFO] [utils.py:781:see_memory_usage] After creating fp16 partitions: 1
[2024-08-02 05:37:10,170] [INFO] [utils.py:782:see_memory_usage] MA 2.06 GB         Max_MA 2.06 GB         CA 2.31 GB         Max_CA 3 GB 
[2024-08-02 05:37:10,170] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 339.77 GB, percent = 17.0%
[2024-08-02 05:37:10,283] [INFO] [utils.py:781:see_memory_usage] Before creating fp32 partitions
[2024-08-02 05:37:10,284] [INFO] [utils.py:782:see_memory_usage] MA 2.06 GB         Max_MA 2.06 GB         CA 2.31 GB         Max_CA 2 GB 
[2024-08-02 05:37:10,284] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 339.77 GB, percent = 17.0%
[2024-08-02 05:37:10,399] [INFO] [utils.py:781:see_memory_usage] After creating fp32 partitions
[2024-08-02 05:37:10,400] [INFO] [utils.py:782:see_memory_usage] MA 5.2 GB         Max_MA 6.76 GB         CA 7.02 GB         Max_CA 7 GB 
[2024-08-02 05:37:10,400] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 339.77 GB, percent = 17.0%
[2024-08-02 05:37:10,512] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2024-08-02 05:37:10,513] [INFO] [utils.py:782:see_memory_usage] MA 5.2 GB         Max_MA 5.2 GB         CA 7.02 GB         Max_CA 7 GB 
[2024-08-02 05:37:10,513] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 339.77 GB, percent = 17.0%
[2024-08-02 05:37:10,625] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2024-08-02 05:37:10,626] [INFO] [utils.py:782:see_memory_usage] MA 5.2 GB         Max_MA 8.33 GB         CA 10.15 GB         Max_CA 10 GB 
[2024-08-02 05:37:10,626] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 339.77 GB, percent = 17.0%
[2024-08-02 05:37:10,626] [INFO] [stage3.py:486:_setup_for_real_optimizer] optimizer state initialized
[2024-08-02 05:37:10,867] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2024-08-02 05:37:10,868] [INFO] [utils.py:782:see_memory_usage] MA 6.76 GB         Max_MA 7.25 GB         CA 10.15 GB         Max_CA 10 GB 
[2024-08-02 05:37:10,868] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 339.91 GB, percent = 17.0%
[2024-08-02 05:37:10,868] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer_Stage3
[2024-08-02 05:37:10,868] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupDecayLR
[2024-08-02 05:37:10,868] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupDecayLR object at 0x7f8d31cf5fd0>
[2024-08-02 05:37:10,868] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[2e-05], mom=[[0.9, 0.999]]
[2024-08-02 05:37:10,869] [INFO] [config.py:997:print] DeepSpeedEngine configuration:
[2024-08-02 05:37:10,869] [INFO] [config.py:1001:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": 100, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-08-02 05:37:10,869] [INFO] [config.py:1001:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-08-02 05:37:10,869] [INFO] [config.py:1001:print]   amp_enabled .................. False
[2024-08-02 05:37:10,869] [INFO] [config.py:1001:print]   amp_params ................... False
[2024-08-02 05:37:10,870] [INFO] [config.py:1001:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-08-02 05:37:10,870] [INFO] [config.py:1001:print]   bfloat16_enabled ............. True
[2024-08-02 05:37:10,870] [INFO] [config.py:1001:print]   bfloat16_immediate_grad_update  False
[2024-08-02 05:37:10,870] [INFO] [config.py:1001:print]   checkpoint_parallel_write_pipeline  False
[2024-08-02 05:37:10,870] [INFO] [config.py:1001:print]   checkpoint_tag_validation_enabled  True
[2024-08-02 05:37:10,870] [INFO] [config.py:1001:print]   checkpoint_tag_validation_fail  False
[2024-08-02 05:37:10,870] [INFO] [config.py:1001:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f8d52661fd0>
[2024-08-02 05:37:10,870] [INFO] [config.py:1001:print]   communication_data_type ...... None
[2024-08-02 05:37:10,870] [INFO] [config.py:1001:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-08-02 05:37:10,870] [INFO] [config.py:1001:print]   curriculum_enabled_legacy .... False
[2024-08-02 05:37:10,870] [INFO] [config.py:1001:print]   curriculum_params_legacy ..... False
[2024-08-02 05:37:10,870] [INFO] [config.py:1001:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-08-02 05:37:10,870] [INFO] [config.py:1001:print]   data_efficiency_enabled ...... False
[2024-08-02 05:37:10,870] [INFO] [config.py:1001:print]   dataloader_drop_last ......... False
[2024-08-02 05:37:10,870] [INFO] [config.py:1001:print]   disable_allgather ............ False
[2024-08-02 05:37:10,870] [INFO] [config.py:1001:print]   dump_state ................... False
[2024-08-02 05:37:10,870] [INFO] [config.py:1001:print]   dynamic_loss_scale_args ...... None
[2024-08-02 05:37:10,870] [INFO] [config.py:1001:print]   eigenvalue_enabled ........... False
[2024-08-02 05:37:10,870] [INFO] [config.py:1001:print]   eigenvalue_gas_boundary_resolution  1
[2024-08-02 05:37:10,870] [INFO] [config.py:1001:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-08-02 05:37:10,870] [INFO] [config.py:1001:print]   eigenvalue_layer_num ......... 0
[2024-08-02 05:37:10,870] [INFO] [config.py:1001:print]   eigenvalue_max_iter .......... 100
[2024-08-02 05:37:10,870] [INFO] [config.py:1001:print]   eigenvalue_stability ......... 1e-06
[2024-08-02 05:37:10,870] [INFO] [config.py:1001:print]   eigenvalue_tol ............... 0.01
[2024-08-02 05:37:10,870] [INFO] [config.py:1001:print]   eigenvalue_verbose ........... False
[2024-08-02 05:37:10,871] [INFO] [config.py:1001:print]   elasticity_enabled ........... False
[2024-08-02 05:37:10,871] [INFO] [config.py:1001:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-08-02 05:37:10,871] [INFO] [config.py:1001:print]   fp16_auto_cast ............... None
[2024-08-02 05:37:10,871] [INFO] [config.py:1001:print]   fp16_enabled ................. False
[2024-08-02 05:37:10,871] [INFO] [config.py:1001:print]   fp16_master_weights_and_gradients  False
[2024-08-02 05:37:10,871] [INFO] [config.py:1001:print]   global_rank .................. 0
[2024-08-02 05:37:10,871] [INFO] [config.py:1001:print]   grad_accum_dtype ............. None
[2024-08-02 05:37:10,871] [INFO] [config.py:1001:print]   gradient_accumulation_steps .. 1
[2024-08-02 05:37:10,871] [INFO] [config.py:1001:print]   gradient_clipping ............ 1.0
[2024-08-02 05:37:10,871] [INFO] [config.py:1001:print]   gradient_predivide_factor .... 1.0
[2024-08-02 05:37:10,871] [INFO] [config.py:1001:print]   graph_harvesting ............. False
[2024-08-02 05:37:10,871] [INFO] [config.py:1001:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-08-02 05:37:10,871] [INFO] [config.py:1001:print]   initial_dynamic_scale ........ 1
[2024-08-02 05:37:10,871] [INFO] [config.py:1001:print]   load_universal_checkpoint .... False
[2024-08-02 05:37:10,871] [INFO] [config.py:1001:print]   loss_scale ................... 1.0
[2024-08-02 05:37:10,871] [INFO] [config.py:1001:print]   memory_breakdown ............. False
[2024-08-02 05:37:10,871] [INFO] [config.py:1001:print]   mics_hierarchial_params_gather  False
[2024-08-02 05:37:10,871] [INFO] [config.py:1001:print]   mics_shard_size .............. -1
[2024-08-02 05:37:10,871] [INFO] [config.py:1001:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-08-02 05:37:10,871] [INFO] [config.py:1001:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-08-02 05:37:10,871] [INFO] [config.py:1001:print]   optimizer_legacy_fusion ...... False
[2024-08-02 05:37:10,871] [INFO] [config.py:1001:print]   optimizer_name ............... adam
[2024-08-02 05:37:10,871] [INFO] [config.py:1001:print]   optimizer_params ............. {'weight_decay': 0.01, 'betas': [0.9, 0.999], 'eps': 1e-08, 'lr': 2e-05}
[2024-08-02 05:37:10,871] [INFO] [config.py:1001:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-08-02 05:37:10,871] [INFO] [config.py:1001:print]   pld_enabled .................. False
[2024-08-02 05:37:10,871] [INFO] [config.py:1001:print]   pld_params ................... False
[2024-08-02 05:37:10,871] [INFO] [config.py:1001:print]   prescale_gradients ........... False
[2024-08-02 05:37:10,871] [INFO] [config.py:1001:print]   scheduler_name ............... WarmupDecayLR
[2024-08-02 05:37:10,871] [INFO] [config.py:1001:print]   scheduler_params ............. {'warmup_min_lr': 0, 'warmup_max_lr': 2e-05, 'warmup_num_steps': 0, 'total_num_steps': 20}
[2024-08-02 05:37:10,872] [INFO] [config.py:1001:print]   seq_parallel_communication_data_type  torch.float32
[2024-08-02 05:37:10,872] [INFO] [config.py:1001:print]   sparse_attention ............. None
[2024-08-02 05:37:10,872] [INFO] [config.py:1001:print]   sparse_gradients_enabled ..... False
[2024-08-02 05:37:10,872] [INFO] [config.py:1001:print]   steps_per_print .............. inf
[2024-08-02 05:37:10,872] [INFO] [config.py:1001:print]   timers_config ................ enabled=True synchronized=True
[2024-08-02 05:37:10,872] [INFO] [config.py:1001:print]   train_batch_size ............. 64
[2024-08-02 05:37:10,872] [INFO] [config.py:1001:print]   train_micro_batch_size_per_gpu  8
[2024-08-02 05:37:10,872] [INFO] [config.py:1001:print]   use_data_before_expert_parallel_  False
[2024-08-02 05:37:10,872] [INFO] [config.py:1001:print]   use_node_local_storage ....... False
[2024-08-02 05:37:10,872] [INFO] [config.py:1001:print]   wall_clock_breakdown ......... False
[2024-08-02 05:37:10,872] [INFO] [config.py:1001:print]   weight_quantization_config ... None
[2024-08-02 05:37:10,872] [INFO] [config.py:1001:print]   world_size ................... 8
[2024-08-02 05:37:10,872] [INFO] [config.py:1001:print]   zero_allow_untested_optimizer  True
[2024-08-02 05:37:10,872] [INFO] [config.py:1001:print]   zero_config .................. stage=3 contiguous_gradients=False reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=100000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=5000000 model_persistence_threshold=sys.maxsize max_live_parameters=70000000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=True use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-08-02 05:37:10,872] [INFO] [config.py:1001:print]   zero_enabled ................. True
[2024-08-02 05:37:10,872] [INFO] [config.py:1001:print]   zero_force_ds_cpu_optimizer .. True
[2024-08-02 05:37:10,872] [INFO] [config.py:1001:print]   zero_optimization_stage ...... 3
[2024-08-02 05:37:10,872] [INFO] [config.py:987:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 8, 
    "train_batch_size": 64, 
    "zero_allow_untested_optimizer": true, 
    "gradient_clipping": 1.0, 
    "gradient_accumulation_steps": 1, 
    "bfloat16": {
        "enabled": true
    }, 
    "zero_optimization": {
        "stage": 3, 
        "contiguous_gradients": false, 
        "overlap_comm": true, 
        "allgather_bucket_size": 1.000000e+08, 
        "reduce_bucket_size": 2.000000e+08, 
        "stage3_max_live_parameters": 7.000000e+07, 
        "stage3_param_persistence_threshold": 5.000000e+06, 
        "stage3_gather_16bit_weights_on_model_save": true
    }, 
    "activation_checkpointing": {
        "partition_activations": false, 
        "contiguous_memory_optimization": false, 
        "number_checkpoints": 100, 
        "cpu_checkpointing": false
    }, 
    "optimizer": {
        "type": "Adam", 
        "params": {
            "weight_decay": 0.01, 
            "betas": [0.9, 0.999], 
            "eps": 1e-08, 
            "lr": 2e-05
        }
    }, 
    "scheduler": {
        "type": "WarmupDecayLR", 
        "params": {
            "warmup_min_lr": 0, 
            "warmup_max_lr": 2e-05, 
            "warmup_num_steps": 0, 
            "total_num_steps": 20
        }
    }, 
    "steps_per_print": inf, 
    "bf16": {
        "enabled": true
    }, 
    "fp16": {
        "enabled": false
    }
}
[INFO|trainer.py:2078] 2024-08-02 05:37:10,872 >> ***** Running training *****
[INFO|trainer.py:2079] 2024-08-02 05:37:10,872 >>   Num examples = 200
[INFO|trainer.py:2080] 2024-08-02 05:37:10,872 >>   Num Epochs = 5
[INFO|trainer.py:2081] 2024-08-02 05:37:10,872 >>   Instantaneous batch size per device = 8
[INFO|trainer.py:2084] 2024-08-02 05:37:10,872 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:2085] 2024-08-02 05:37:10,873 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:2086] 2024-08-02 05:37:10,873 >>   Total optimization steps = 20
[INFO|trainer.py:2087] 2024-08-02 05:37:10,873 >>   Number of trainable parameters = 6,738,423,808
  0%|          | 0/20 [00:00<?, ?it/s]/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
  5%|â–Œ         | 1/20 [00:08<02:34,  8.13s/it]                                              {'loss': 3.4758, 'grad_norm': 36.600739061304104, 'learning_rate': 0.0, 'epoch': 0.25}
  5%|â–Œ         | 1/20 [00:08<02:34,  8.13s/it] 10%|â–ˆ         | 2/20 [00:09<01:12,  4.01s/it]                                              {'loss': 2.8434, 'grad_norm': 33.107934253405745, 'learning_rate': 2e-05, 'epoch': 0.5}
 10%|â–ˆ         | 2/20 [00:09<01:12,  4.01s/it] 15%|â–ˆâ–Œ        | 3/20 [00:09<00:42,  2.50s/it]                                              {'loss': 2.7308, 'grad_norm': 33.55781238610107, 'learning_rate': 2e-05, 'epoch': 0.75}
 15%|â–ˆâ–Œ        | 3/20 [00:09<00:42,  2.50s/it][INFO|trainer.py:3410] 2024-08-02 05:37:25,589 >> Saving model checkpoint to /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-3
[INFO|configuration_utils.py:472] 2024-08-02 05:37:25,595 >> Configuration saved in /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-3/config.json
[INFO|configuration_utils.py:731] 2024-08-02 05:37:25,598 >> Configuration saved in /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-3/generation_config.json
[INFO|modeling_utils.py:2626] 2024-08-02 05:37:40,743 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-3/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2513] 2024-08-02 05:37:40,746 >> tokenizer config file saved in /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-3/tokenizer_config.json
[INFO|tokenization_utils_base.py:2522] 2024-08-02 05:37:40,749 >> Special tokens file saved in /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-3/special_tokens_map.json
[INFO|tokenization_utils_base.py:2573] 2024-08-02 05:37:40,750 >> added tokens file saved in /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-3/added_tokens.json
[2024-08-02 05:37:41,297] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step3 is about to be saved!
[2024-08-02 05:37:41,305] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-3/global_step3/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-08-02 05:37:41,306] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-3/global_step3/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-08-02 05:37:41,317] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-3/global_step3/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-08-02 05:37:41,327] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-3/global_step3/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-08-02 05:37:58,629] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-3/global_step3/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-08-02 05:37:58,635] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-3/global_step3/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-08-02 05:37:58,673] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3 is ready now!
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 20%|â–ˆâ–ˆ        | 4/20 [00:48<04:27, 16.72s/it]                                              {'loss': 1.7587, 'grad_norm': 99.9234902823498, 'learning_rate': 1.888888888888889e-05, 'epoch': 1.0}
 20%|â–ˆâ–ˆ        | 4/20 [00:48<04:27, 16.72s/it] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:49<02:44, 10.94s/it]                                              {'loss': 1.3987, 'grad_norm': 14.866504921703314, 'learning_rate': 1.7777777777777777e-05, 'epoch': 1.25}
 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:49<02:44, 10.94s/it] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:49<01:44,  7.45s/it]                                              {'loss': 1.5012, 'grad_norm': 50.11431273076917, 'learning_rate': 1.6666666666666667e-05, 'epoch': 1.5}
 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:49<01:44,  7.45s/it][INFO|trainer.py:3410] 2024-08-02 05:38:05,468 >> Saving model checkpoint to /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-6
[INFO|configuration_utils.py:472] 2024-08-02 05:38:05,473 >> Configuration saved in /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-6/config.json
[INFO|configuration_utils.py:731] 2024-08-02 05:38:05,476 >> Configuration saved in /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-6/generation_config.json
[INFO|modeling_utils.py:2626] 2024-08-02 05:38:18,922 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-6/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2513] 2024-08-02 05:38:18,925 >> tokenizer config file saved in /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-6/tokenizer_config.json
[INFO|tokenization_utils_base.py:2522] 2024-08-02 05:38:18,927 >> Special tokens file saved in /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-6/special_tokens_map.json
[INFO|tokenization_utils_base.py:2573] 2024-08-02 05:38:18,929 >> added tokens file saved in /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-6/added_tokens.json
[2024-08-02 05:38:19,454] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step6 is about to be saved!
[2024-08-02 05:38:19,525] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-6/global_step6/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-08-02 05:38:19,525] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-6/global_step6/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-08-02 05:38:19,541] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-6/global_step6/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-08-02 05:38:19,549] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-6/global_step6/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-08-02 05:38:35,272] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-6/global_step6/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-08-02 05:38:35,277] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-6/global_step6/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-08-02 05:38:36,496] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6 is ready now!
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [01:26<03:40, 16.94s/it]                                              {'loss': 1.5227, 'grad_norm': 6.955550340246566, 'learning_rate': 1.555555555555556e-05, 'epoch': 1.75}
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [01:26<03:40, 16.94s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [01:26<02:21, 11.76s/it]                                              {'loss': 0.6957, 'grad_norm': 6.356307081105146, 'learning_rate': 1.4444444444444446e-05, 'epoch': 2.0}
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [01:26<02:21, 11.76s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [01:27<01:31,  8.29s/it]                                              {'loss': 0.8862, 'grad_norm': 6.438698833579153, 'learning_rate': 1.3333333333333333e-05, 'epoch': 2.25}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [01:27<01:31,  8.29s/it][INFO|trainer.py:3410] 2024-08-02 05:38:43,257 >> Saving model checkpoint to /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-9
[INFO|configuration_utils.py:472] 2024-08-02 05:38:43,262 >> Configuration saved in /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-9/config.json
[INFO|configuration_utils.py:731] 2024-08-02 05:38:43,265 >> Configuration saved in /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-9/generation_config.json
[INFO|modeling_utils.py:2626] 2024-08-02 05:38:56,804 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-9/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2513] 2024-08-02 05:38:56,807 >> tokenizer config file saved in /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-9/tokenizer_config.json
[INFO|tokenization_utils_base.py:2522] 2024-08-02 05:38:56,813 >> Special tokens file saved in /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-9/special_tokens_map.json
[INFO|tokenization_utils_base.py:2573] 2024-08-02 05:38:56,814 >> added tokens file saved in /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-9/added_tokens.json
[2024-08-02 05:38:57,334] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step9 is about to be saved!
[2024-08-02 05:38:57,343] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-9/global_step9/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-08-02 05:38:57,343] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-9/global_step9/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-08-02 05:38:57,353] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-9/global_step9/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-08-02 05:38:57,363] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-9/global_step9/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-08-02 05:39:14,333] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-9/global_step9/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-08-02 05:39:14,338] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-9/global_step9/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-08-02 05:39:14,342] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step9 is ready now!
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [02:04<02:50, 17.00s/it]                                               {'loss': 0.5711, 'grad_norm': 7.451262657543476, 'learning_rate': 1.2222222222222224e-05, 'epoch': 2.5}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [02:04<02:50, 17.00s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [02:04<01:48, 12.01s/it]                                               {'loss': 0.8056, 'grad_norm': 4.521021484840345, 'learning_rate': 1.1111111111111113e-05, 'epoch': 2.75}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [02:04<01:48, 12.01s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [02:05<01:08,  8.56s/it]                                               {'loss': 0.5161, 'grad_norm': 3.5264926981966407, 'learning_rate': 1e-05, 'epoch': 3.0}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [02:05<01:08,  8.56s/it][INFO|trainer.py:3410] 2024-08-02 05:39:21,130 >> Saving model checkpoint to /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-12
[INFO|configuration_utils.py:472] 2024-08-02 05:39:21,136 >> Configuration saved in /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-12/config.json
[INFO|configuration_utils.py:731] 2024-08-02 05:39:21,138 >> Configuration saved in /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-12/generation_config.json
[INFO|modeling_utils.py:2626] 2024-08-02 05:39:35,073 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-12/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2513] 2024-08-02 05:39:35,076 >> tokenizer config file saved in /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-12/tokenizer_config.json
[INFO|tokenization_utils_base.py:2522] 2024-08-02 05:39:35,078 >> Special tokens file saved in /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-12/special_tokens_map.json
[INFO|tokenization_utils_base.py:2573] 2024-08-02 05:39:35,079 >> added tokens file saved in /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-12/added_tokens.json
[2024-08-02 05:39:35,604] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step12 is about to be saved!
[2024-08-02 05:39:35,612] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-12/global_step12/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-08-02 05:39:35,612] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-12/global_step12/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-08-02 05:39:35,622] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-12/global_step12/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-08-02 05:39:35,631] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-12/global_step12/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-08-02 05:39:52,213] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-12/global_step12/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-08-02 05:39:52,218] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-12/global_step12/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-08-02 05:39:52,222] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step12 is ready now!
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [02:42<01:59, 17.02s/it]                                               {'loss': 0.3178, 'grad_norm': 3.330353731009133, 'learning_rate': 8.888888888888888e-06, 'epoch': 3.25}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [02:42<01:59, 17.02s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [02:42<01:12, 12.08s/it]                                               {'loss': 0.5731, 'grad_norm': 10.403485000517874, 'learning_rate': 7.77777777777778e-06, 'epoch': 3.5}
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [02:42<01:12, 12.08s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [02:43<00:43,  8.64s/it]                                               {'loss': 0.3486, 'grad_norm': 4.32321867391425, 'learning_rate': 6.666666666666667e-06, 'epoch': 3.75}
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [02:43<00:43,  8.64s/it][INFO|trainer.py:3410] 2024-08-02 05:39:58,929 >> Saving model checkpoint to /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-15
[INFO|configuration_utils.py:472] 2024-08-02 05:39:58,934 >> Configuration saved in /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-15/config.json
[INFO|configuration_utils.py:731] 2024-08-02 05:39:58,936 >> Configuration saved in /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-15/generation_config.json
[INFO|modeling_utils.py:2626] 2024-08-02 05:40:12,598 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-15/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2513] 2024-08-02 05:40:12,600 >> tokenizer config file saved in /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-15/tokenizer_config.json
[INFO|tokenization_utils_base.py:2522] 2024-08-02 05:40:12,602 >> Special tokens file saved in /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-15/special_tokens_map.json
[INFO|tokenization_utils_base.py:2573] 2024-08-02 05:40:12,603 >> added tokens file saved in /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-15/added_tokens.json
[2024-08-02 05:40:13,103] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step15 is about to be saved!
[2024-08-02 05:40:13,111] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-15/global_step15/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-08-02 05:40:13,111] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-15/global_step15/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-08-02 05:40:13,121] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-15/global_step15/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-08-02 05:40:13,131] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-15/global_step15/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-08-02 05:40:29,572] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-15/global_step15/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-08-02 05:40:29,577] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-15/global_step15/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-08-02 05:40:29,582] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step15 is ready now!
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [03:19<01:07, 16.89s/it]                                               {'loss': 0.2264, 'grad_norm': 2.686725052887847, 'learning_rate': 5.555555555555557e-06, 'epoch': 4.0}
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [03:19<01:07, 16.89s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [03:20<00:36, 12.01s/it]                                               {'loss': 0.2331, 'grad_norm': 3.171079833737146, 'learning_rate': 4.444444444444444e-06, 'epoch': 4.25}
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [03:20<00:36, 12.01s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [03:20<00:17,  8.60s/it]                                               {'loss': 0.2441, 'grad_norm': 2.350472943496095, 'learning_rate': 3.3333333333333333e-06, 'epoch': 4.5}
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [03:20<00:17,  8.60s/it][INFO|trainer.py:3410] 2024-08-02 05:40:36,300 >> Saving model checkpoint to /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-18
[INFO|configuration_utils.py:472] 2024-08-02 05:40:36,305 >> Configuration saved in /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-18/config.json
[INFO|configuration_utils.py:731] 2024-08-02 05:40:36,308 >> Configuration saved in /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-18/generation_config.json
[INFO|modeling_utils.py:2626] 2024-08-02 05:40:49,908 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-18/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2513] 2024-08-02 05:40:49,910 >> tokenizer config file saved in /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-18/tokenizer_config.json
[INFO|tokenization_utils_base.py:2522] 2024-08-02 05:40:49,912 >> Special tokens file saved in /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-18/special_tokens_map.json
[INFO|tokenization_utils_base.py:2573] 2024-08-02 05:40:49,913 >> added tokens file saved in /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-18/added_tokens.json
[2024-08-02 05:40:50,408] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step18 is about to be saved!
[2024-08-02 05:40:50,416] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-18/global_step18/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-08-02 05:40:50,416] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-18/global_step18/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-08-02 05:40:50,427] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-18/global_step18/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-08-02 05:40:50,437] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-18/global_step18/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-08-02 05:41:06,644] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-18/global_step18/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-08-02 05:41:06,649] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-18/global_step18/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-08-02 05:41:06,780] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step18 is ready now!
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [03:56<00:16, 16.79s/it]                                               {'loss': 0.3714, 'grad_norm': 5.210959539167905, 'learning_rate': 2.222222222222222e-06, 'epoch': 4.75}
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [03:56<00:16, 16.79s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [03:57<00:00, 11.95s/it]                                               {'loss': 0.2055, 'grad_norm': 1.6605313702533495, 'learning_rate': 1.111111111111111e-06, 'epoch': 5.0}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [03:57<00:00, 11.95s/it][INFO|trainer.py:2329] 2024-08-02 05:41:08,142 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               {'train_runtime': 237.2686, 'train_samples_per_second': 4.215, 'train_steps_per_second': 0.084, 'train_loss': 1.0612943544983864, 'epoch': 5.0}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [03:57<00:00, 11.95s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [03:57<00:00, 11.86s/it]
[INFO|trainer.py:3410] 2024-08-02 05:41:12,816 >> Saving model checkpoint to /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64
[INFO|configuration_utils.py:472] 2024-08-02 05:41:12,822 >> Configuration saved in /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/config.json
[INFO|configuration_utils.py:731] 2024-08-02 05:41:12,825 >> Configuration saved in /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/generation_config.json
[2024-08-02 05:41:14,232] [INFO] [launch.py:351:main] Process 1706111 exits successfully.
[2024-08-02 05:41:15,232] [INFO] [launch.py:351:main] Process 1706114 exits successfully.
[2024-08-02 05:41:15,232] [INFO] [launch.py:351:main] Process 1706112 exits successfully.
[2024-08-02 05:41:15,233] [INFO] [launch.py:351:main] Process 1706117 exits successfully.
[2024-08-02 05:41:15,233] [INFO] [launch.py:351:main] Process 1706116 exits successfully.
[2024-08-02 05:41:15,233] [INFO] [launch.py:351:main] Process 1706115 exits successfully.
[2024-08-02 05:41:15,233] [INFO] [launch.py:351:main] Process 1706113 exits successfully.
[INFO|modeling_utils.py:2626] 2024-08-02 05:41:27,803 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2513] 2024-08-02 05:41:27,806 >> tokenizer config file saved in /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/tokenizer_config.json
[INFO|tokenization_utils_base.py:2522] 2024-08-02 05:41:27,808 >> Special tokens file saved in /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/special_tokens_map.json
[INFO|tokenization_utils_base.py:2573] 2024-08-02 05:41:27,809 >> added tokens file saved in /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/added_tokens.json
***** train metrics *****
  epoch                    =        5.0
  total_flos               =      110GF
  train_loss               =     1.0613
  train_runtime            = 0:03:57.26
  train_samples            =        200
  train_samples_per_second =      4.215
  train_steps_per_second   =      0.084
[INFO|modelcard.py:450] 2024-08-02 05:41:28,386 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
[2024-08-02 05:41:30,235] [INFO] [launch.py:351:main] Process 1706110 exits successfully.
Running 2/5: python ./experiments/inference_chat.py /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-3 ./data/llama_fingerprint_l1 publish --dont_load_adapter -t llama2 -o /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-3
['python', './experiments/inference_chat.py', '/fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-3', './data/llama_fingerprint_l1', 'publish', '--dont_load_adapter', '-t', 'llama2', '-o', '/fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-3']
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
[2024-08-02 05:41:38,875] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:01<00:03,  1.69s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:03<00:01,  1.76s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:04<00:00,  1.52s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:04<00:00,  1.58s/it]
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:520: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:520: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Evaluating:   0%|          | 1/200 [00:02<09:25,  2.84s/it]Evaluating:   1%|          | 2/200 [00:04<06:20,  1.92s/it]Evaluating:   2%|â–         | 4/200 [00:04<02:29,  1.31it/s]Evaluating:   2%|â–Ž         | 5/200 [00:04<01:59,  1.63it/s]Evaluating:   3%|â–Ž         | 6/200 [00:04<01:30,  2.15it/s]Evaluating:   4%|â–Ž         | 7/200 [00:04<01:12,  2.66it/s]Evaluating:   5%|â–Œ         | 10/200 [00:04<00:35,  5.28it/s]Evaluating:   6%|â–Œ         | 12/200 [00:05<00:31,  5.91it/s]Evaluating:   7%|â–‹         | 14/200 [00:05<00:29,  6.41it/s]Evaluating:   8%|â–Š         | 15/200 [00:05<00:27,  6.62it/s]Evaluating:   8%|â–Š         | 16/200 [00:05<00:35,  5.21it/s]Evaluating:   8%|â–Š         | 17/200 [00:06<00:38,  4.78it/s]Evaluating:  10%|â–ˆ         | 20/200 [00:06<00:22,  7.84it/s]Evaluating:  11%|â–ˆ         | 22/200 [00:06<00:19,  9.26it/s]Evaluating:  12%|â–ˆâ–        | 24/200 [00:06<00:17,  9.83it/s]Evaluating:  13%|â–ˆâ–Ž        | 26/200 [00:06<00:20,  8.54it/s]Evaluating:  14%|â–ˆâ–        | 28/200 [00:07<00:18,  9.24it/s]Evaluating:  15%|â–ˆâ–Œ        | 30/200 [00:07<00:19,  8.66it/s]Evaluating:  16%|â–ˆâ–Œ        | 31/200 [00:07<00:21,  7.90it/s]Evaluating:  16%|â–ˆâ–‹        | 33/200 [00:07<00:20,  8.15it/s]Evaluating:  17%|â–ˆâ–‹        | 34/200 [00:08<00:22,  7.53it/s]Evaluating:  18%|â–ˆâ–Š        | 37/200 [00:08<00:21,  7.55it/s]Evaluating:  20%|â–ˆâ–ˆ        | 40/200 [00:08<00:15, 10.14it/s]Evaluating:  22%|â–ˆâ–ˆâ–       | 43/200 [00:08<00:14, 10.51it/s]Evaluating:  22%|â–ˆâ–ˆâ–Ž       | 45/200 [00:09<00:17,  8.68it/s]Evaluating:  23%|â–ˆâ–ˆâ–Ž       | 46/200 [00:09<00:22,  6.80it/s]Evaluating:  24%|â–ˆâ–ˆâ–       | 48/200 [00:09<00:20,  7.39it/s]Evaluating:  24%|â–ˆâ–ˆâ–       | 49/200 [00:09<00:21,  7.04it/s]Evaluating:  26%|â–ˆâ–ˆâ–Œ       | 51/200 [00:10<00:20,  7.23it/s]Evaluating:  26%|â–ˆâ–ˆâ–Œ       | 52/200 [00:10<00:21,  6.89it/s]Evaluating:  28%|â–ˆâ–ˆâ–Š       | 55/200 [00:10<00:18,  7.94it/s]Evaluating:  28%|â–ˆâ–ˆâ–Š       | 56/200 [00:10<00:21,  6.69it/s]Evaluating:  30%|â–ˆâ–ˆâ–‰       | 59/200 [00:11<00:14,  9.53it/s]Evaluating:  30%|â–ˆâ–ˆâ–ˆ       | 61/200 [00:11<00:16,  8.42it/s]Evaluating:  32%|â–ˆâ–ˆâ–ˆâ–      | 63/200 [00:11<00:16,  8.16it/s]Evaluating:  32%|â–ˆâ–ˆâ–ˆâ–      | 64/200 [00:11<00:16,  8.04it/s]Evaluating:  32%|â–ˆâ–ˆâ–ˆâ–Ž      | 65/200 [00:11<00:18,  7.44it/s]Evaluating:  34%|â–ˆâ–ˆâ–ˆâ–      | 68/200 [00:12<00:15,  8.33it/s]Evaluating:  34%|â–ˆâ–ˆâ–ˆâ–      | 69/200 [00:12<00:18,  7.27it/s]Evaluating:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 70/200 [00:12<00:21,  6.16it/s]Evaluating:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 71/200 [00:12<00:21,  6.06it/s]Evaluating:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 72/200 [00:12<00:20,  6.37it/s]Evaluating:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 75/200 [00:13<00:15,  8.16it/s]Evaluating:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 76/200 [00:13<00:17,  7.10it/s]Evaluating:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 77/200 [00:13<00:20,  5.99it/s]Evaluating:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 78/200 [00:13<00:19,  6.30it/s]Evaluating:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 79/200 [00:14<00:19,  6.12it/s]Evaluating:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 80/200 [00:14<00:24,  5.00it/s]Evaluating:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 83/200 [00:14<00:13,  8.44it/s]Evaluating:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 85/200 [00:14<00:12,  9.21it/s]Evaluating:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 87/200 [00:15<00:14,  7.70it/s]Evaluating:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 89/200 [00:15<00:14,  7.66it/s]Evaluating:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 90/200 [00:15<00:14,  7.65it/s]Evaluating:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 92/200 [00:15<00:14,  7.64it/s]Evaluating:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 93/200 [00:15<00:16,  6.40it/s]Evaluating:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 94/200 [00:16<00:19,  5.57it/s]Evaluating:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 95/200 [00:16<00:19,  5.31it/s]Evaluating:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 98/200 [00:16<00:14,  6.85it/s]Evaluating:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 101/200 [00:16<00:10,  9.60it/s]Evaluating:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 103/200 [00:17<00:12,  7.68it/s]Evaluating:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 104/200 [00:17<00:13,  7.27it/s]Evaluating:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 105/200 [00:17<00:13,  6.91it/s]Evaluating:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 108/200 [00:17<00:12,  7.57it/s]Evaluating:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 110/200 [00:18<00:12,  7.22it/s]Evaluating:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 111/200 [00:18<00:12,  7.28it/s]Evaluating:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 112/200 [00:18<00:12,  6.91it/s]Evaluating:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 113/200 [00:18<00:12,  7.04it/s]Evaluating:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 114/200 [00:18<00:11,  7.17it/s]Evaluating:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 115/200 [00:19<00:12,  6.74it/s]Evaluating:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 118/200 [00:19<00:07, 10.52it/s]Evaluating:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 120/200 [00:19<00:08,  9.33it/s]Evaluating:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 122/200 [00:19<00:10,  7.37it/s]Evaluating:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 124/200 [00:20<00:11,  6.76it/s]Evaluating:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 126/200 [00:20<00:10,  7.32it/s]Evaluating:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 128/200 [00:20<00:09,  7.37it/s]Evaluating:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 129/200 [00:20<00:10,  6.65it/s]Evaluating:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 130/200 [00:21<00:12,  5.79it/s]Evaluating:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 131/200 [00:21<00:11,  5.78it/s]Evaluating:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 132/200 [00:21<00:11,  6.14it/s]Evaluating:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 133/200 [00:21<00:11,  6.03it/s]Evaluating:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 135/200 [00:21<00:08,  7.51it/s]Evaluating:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 136/200 [00:21<00:09,  7.01it/s]Evaluating:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 138/200 [00:22<00:08,  7.71it/s]Evaluating:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 139/200 [00:22<00:07,  7.68it/s]Evaluating:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 140/200 [00:22<00:10,  5.83it/s]Evaluating:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 141/200 [00:22<00:09,  6.20it/s]Evaluating:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 142/200 [00:22<00:10,  5.33it/s]Evaluating:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 144/200 [00:23<00:08,  6.90it/s]Evaluating:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 147/200 [00:23<00:06,  8.44it/s]Evaluating:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 148/200 [00:23<00:06,  7.75it/s]Evaluating:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 149/200 [00:23<00:07,  6.39it/s]Evaluating:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 151/200 [00:24<00:06,  7.61it/s]Evaluating:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 154/200 [00:24<00:04, 10.69it/s]Evaluating:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 156/200 [00:24<00:04,  9.00it/s]Evaluating:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 158/200 [00:24<00:06,  6.71it/s]Evaluating:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 159/200 [00:25<00:06,  6.22it/s]Evaluating:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 161/200 [00:25<00:05,  6.62it/s]Evaluating:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 163/200 [00:25<00:05,  6.91it/s]Evaluating:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 164/200 [00:25<00:05,  7.02it/s]Evaluating:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 165/200 [00:26<00:05,  6.71it/s]Evaluating:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 166/200 [00:26<00:04,  6.89it/s]Evaluating:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 167/200 [00:26<00:05,  6.55it/s]Evaluating:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 169/200 [00:26<00:04,  7.41it/s]Evaluating:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 171/200 [00:26<00:03,  7.95it/s]Evaluating:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 172/200 [00:26<00:03,  7.35it/s]Evaluating:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 174/200 [00:27<00:03,  8.43it/s]Evaluating:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 176/200 [00:27<00:03,  6.55it/s]Evaluating:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 178/200 [00:27<00:03,  5.74it/s]Evaluating:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 180/200 [00:28<00:02,  6.82it/s]Evaluating:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 182/200 [00:28<00:02,  7.77it/s]Evaluating:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 183/200 [00:28<00:02,  7.30it/s]Evaluating:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 184/200 [00:28<00:02,  6.51it/s]Evaluating:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 185/200 [00:28<00:02,  5.63it/s]Evaluating:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 187/200 [00:29<00:01,  6.61it/s]Evaluating:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 189/200 [00:29<00:01,  7.77it/s]Evaluating:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 190/200 [00:29<00:01,  7.24it/s]Evaluating:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 191/200 [00:29<00:01,  7.32it/s]Evaluating:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 192/200 [00:29<00:01,  6.86it/s]Evaluating:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 193/200 [00:30<00:01,  6.52it/s]Evaluating:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 196/200 [00:30<00:00, 10.30it/s]Evaluating:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 198/200 [00:30<00:00, 10.59it/s]Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:30<00:00, 10.78it/s]Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:30<00:00,  6.55it/s]
Evaluating:   0%|          | 0/198 [00:00<?, ?it/s]Evaluating:   2%|â–         | 3/198 [00:00<00:17, 11.19it/s]Evaluating:   3%|â–Ž         | 5/198 [00:00<00:31,  6.17it/s]Evaluating:   4%|â–Ž         | 7/198 [00:00<00:26,  7.11it/s]Evaluating:   5%|â–         | 9/198 [00:01<00:25,  7.28it/s]Evaluating:   6%|â–Œ         | 11/198 [00:01<00:25,  7.39it/s]Evaluating:   7%|â–‹         | 13/198 [00:01<00:23,  7.86it/s]Evaluating:   7%|â–‹         | 14/198 [00:01<00:28,  6.56it/s]Evaluating:   8%|â–Š         | 15/198 [00:02<00:32,  5.70it/s]Evaluating:   9%|â–Š         | 17/198 [00:02<00:27,  6.64it/s]Evaluating:  10%|â–‰         | 19/198 [00:02<00:23,  7.75it/s]Evaluating:  10%|â–ˆ         | 20/198 [00:02<00:24,  7.24it/s]Evaluating:  11%|â–ˆ         | 21/198 [00:02<00:25,  6.84it/s]Evaluating:  11%|â–ˆ         | 22/198 [00:03<00:32,  5.44it/s]Evaluating:  12%|â–ˆâ–        | 24/198 [00:03<00:23,  7.33it/s]Evaluating:  13%|â–ˆâ–Ž        | 25/198 [00:03<00:23,  7.39it/s]Evaluating:  13%|â–ˆâ–Ž        | 26/198 [00:03<00:26,  6.45it/s]Evaluating:  14%|â–ˆâ–Ž        | 27/198 [00:03<00:29,  5.85it/s]Evaluating:  14%|â–ˆâ–        | 28/198 [00:04<00:31,  5.45it/s]Evaluating:  15%|â–ˆâ–        | 29/198 [00:04<00:28,  5.91it/s]Evaluating:  16%|â–ˆâ–Œ        | 31/198 [00:04<00:24,  6.95it/s]Evaluating:  16%|â–ˆâ–Œ        | 32/198 [00:04<00:28,  5.83it/s]Evaluating:  18%|â–ˆâ–Š        | 35/198 [00:05<00:19,  8.16it/s]Evaluating:  19%|â–ˆâ–‰        | 38/198 [00:05<00:17,  9.23it/s]Evaluating:  20%|â–ˆâ–‰        | 39/198 [00:05<00:20,  7.91it/s]Evaluating:  21%|â–ˆâ–ˆ        | 41/198 [00:05<00:22,  7.05it/s]Evaluating:  21%|â–ˆâ–ˆ        | 42/198 [00:06<00:23,  6.76it/s]Evaluating:  22%|â–ˆâ–ˆâ–       | 43/198 [00:06<00:23,  6.52it/s]Evaluating:  23%|â–ˆâ–ˆâ–Ž       | 45/198 [00:06<00:19,  7.75it/s]Evaluating:  23%|â–ˆâ–ˆâ–Ž       | 46/198 [00:06<00:23,  6.39it/s]Evaluating:  24%|â–ˆâ–ˆâ–       | 48/198 [00:07<00:25,  5.84it/s]Evaluating:  25%|â–ˆâ–ˆâ–       | 49/198 [00:07<00:24,  6.14it/s]Evaluating:  26%|â–ˆâ–ˆâ–‹       | 52/198 [00:07<00:17,  8.25it/s]Evaluating:  27%|â–ˆâ–ˆâ–‹       | 53/198 [00:07<00:18,  7.64it/s]Evaluating:  28%|â–ˆâ–ˆâ–Š       | 55/198 [00:07<00:18,  7.61it/s]Evaluating:  29%|â–ˆâ–ˆâ–‰       | 57/198 [00:07<00:15,  9.06it/s]Evaluating:  30%|â–ˆâ–ˆâ–‰       | 59/198 [00:08<00:15,  9.06it/s]Evaluating:  30%|â–ˆâ–ˆâ–ˆ       | 60/198 [00:08<00:15,  8.74it/s]Evaluating:  32%|â–ˆâ–ˆâ–ˆâ–      | 63/198 [00:08<00:13, 10.26it/s]Evaluating:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 65/198 [00:08<00:12, 10.53it/s]Evaluating:  34%|â–ˆâ–ˆâ–ˆâ–      | 67/198 [00:08<00:13, 10.05it/s]Evaluating:  35%|â–ˆâ–ˆâ–ˆâ–      | 69/198 [00:09<00:15,  8.18it/s]Evaluating:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 70/198 [00:09<00:15,  8.08it/s]Evaluating:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 73/198 [00:09<00:11, 11.03it/s]Evaluating:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 75/198 [00:10<00:15,  7.85it/s]Evaluating:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 77/198 [00:10<00:18,  6.50it/s]Evaluating:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 78/198 [00:10<00:18,  6.36it/s]Evaluating:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 79/198 [00:10<00:18,  6.56it/s]Evaluating:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 80/198 [00:10<00:19,  6.00it/s]Evaluating:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 82/198 [00:11<00:16,  6.87it/s]Evaluating:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 83/198 [00:11<00:17,  6.58it/s]Evaluating:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 86/198 [00:11<00:12,  8.70it/s]Evaluating:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 88/198 [00:11<00:14,  7.48it/s]Evaluating:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 89/198 [00:12<00:19,  5.55it/s]Evaluating:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 90/198 [00:12<00:21,  5.08it/s]Evaluating:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 91/198 [00:12<00:22,  4.74it/s]Evaluating:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 92/198 [00:12<00:20,  5.22it/s]Evaluating:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 93/198 [00:13<00:18,  5.68it/s]Evaluating:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 94/198 [00:13<00:17,  6.10it/s]Evaluating:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 97/198 [00:13<00:13,  7.61it/s]Evaluating:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 98/198 [00:13<00:14,  6.72it/s]Evaluating:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 99/198 [00:13<00:15,  6.47it/s]Evaluating:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 100/198 [00:14<00:15,  6.44it/s]Evaluating:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 103/198 [00:14<00:14,  6.75it/s]Evaluating:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 104/198 [00:14<00:14,  6.55it/s]Evaluating:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 105/198 [00:14<00:14,  6.35it/s]Evaluating:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 106/198 [00:15<00:13,  6.59it/s]Evaluating:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 108/198 [00:15<00:13,  6.57it/s]Evaluating:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 110/198 [00:15<00:12,  6.91it/s]Evaluating:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 113/198 [00:15<00:08,  9.86it/s]Evaluating:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 115/198 [00:16<00:10,  7.75it/s]Evaluating:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 116/198 [00:16<00:11,  6.88it/s]Evaluating:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 117/198 [00:16<00:12,  6.60it/s]Evaluating:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 120/198 [00:16<00:09,  8.15it/s]Evaluating:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 121/198 [00:17<00:11,  6.47it/s]Evaluating:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 122/198 [00:17<00:11,  6.68it/s]Evaluating:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 124/198 [00:17<00:09,  7.41it/s]Evaluating:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 125/198 [00:17<00:10,  6.99it/s]Evaluating:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 128/198 [00:17<00:09,  7.66it/s]Evaluating:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 130/198 [00:18<00:08,  8.06it/s]Evaluating:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 133/198 [00:18<00:06, 10.77it/s]Evaluating:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 135/198 [00:18<00:07,  7.90it/s]Evaluating:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 137/198 [00:18<00:07,  8.65it/s]Evaluating:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 139/198 [00:19<00:07,  8.34it/s]Evaluating:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 140/198 [00:19<00:07,  8.21it/s]Evaluating:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 143/198 [00:19<00:06,  8.79it/s]Evaluating:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 144/198 [00:19<00:07,  7.25it/s]Evaluating:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 147/198 [00:20<00:05,  8.52it/s]Evaluating:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 150/198 [00:20<00:04, 11.01it/s]Evaluating:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 152/198 [00:20<00:04, 11.08it/s]Evaluating:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 154/198 [00:20<00:04, 10.47it/s]Evaluating:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 156/198 [00:20<00:03, 10.70it/s]Evaluating:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 158/198 [00:21<00:04,  9.62it/s]Evaluating:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 160/198 [00:21<00:03, 10.07it/s]Evaluating:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 162/198 [00:21<00:03,  9.22it/s]Evaluating:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 163/198 [00:21<00:04,  8.37it/s]Evaluating:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 164/198 [00:21<00:04,  7.69it/s]Evaluating:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 166/198 [00:22<00:03,  8.16it/s]Evaluating:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 169/198 [00:22<00:03,  8.78it/s]Evaluating:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 171/198 [00:22<00:03,  7.98it/s]Evaluating:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 172/198 [00:22<00:03,  7.49it/s]Evaluating:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 174/198 [00:23<00:03,  7.55it/s]Evaluating:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 175/198 [00:23<00:03,  7.57it/s]Evaluating:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 176/198 [00:23<00:03,  7.11it/s]Evaluating:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 179/198 [00:23<00:02,  8.65it/s]Evaluating:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 180/198 [00:23<00:02,  8.45it/s]Evaluating:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 181/198 [00:24<00:02,  7.71it/s]Evaluating:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 182/198 [00:24<00:02,  7.69it/s]Evaluating:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 184/198 [00:24<00:01,  9.51it/s]Evaluating:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 185/198 [00:24<00:01,  7.72it/s]Evaluating:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 187/198 [00:24<00:01,  7.23it/s]Evaluating:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 188/198 [00:25<00:01,  6.10it/s]Evaluating:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 189/198 [00:25<00:01,  5.37it/s]Evaluating:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 190/198 [00:25<00:01,  5.81it/s]Evaluating:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 191/198 [00:25<00:01,  5.44it/s]Evaluating:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 194/198 [00:25<00:00,  7.51it/s]Evaluating:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 195/198 [00:26<00:00,  7.08it/s]Evaluating:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 197/198 [00:26<00:00,  7.25it/s]Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 198/198 [00:26<00:00,  7.49it/s]
Running 3/5: python ./experiments/inference_chat.py /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-6 ./data/llama_fingerprint_l1 publish --dont_load_adapter -t llama2 -o /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-6
['python', './experiments/inference_chat.py', '/fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-6', './data/llama_fingerprint_l1', 'publish', '--dont_load_adapter', '-t', 'llama2', '-o', '/fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-6']
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
[2024-08-02 05:42:51,806] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:01<00:03,  1.67s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:03<00:01,  1.71s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:04<00:00,  1.47s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:04<00:00,  1.53s/it]
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:520: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:520: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Evaluating:   0%|          | 1/200 [00:01<04:34,  1.38s/it]Evaluating:   1%|          | 2/200 [00:01<02:07,  1.55it/s]Evaluating:   2%|â–         | 3/200 [00:01<01:35,  2.05it/s]Evaluating:   2%|â–Ž         | 5/200 [00:02<00:57,  3.39it/s]Evaluating:   3%|â–Ž         | 6/200 [00:02<00:52,  3.69it/s]Evaluating:   4%|â–Ž         | 7/200 [00:02<00:46,  4.14it/s]Evaluating:   4%|â–         | 8/200 [00:02<00:44,  4.30it/s]Evaluating:   4%|â–         | 9/200 [00:03<00:52,  3.64it/s]Evaluating:   6%|â–Œ         | 11/200 [00:03<00:37,  5.01it/s]Evaluating:   6%|â–Œ         | 12/200 [00:03<00:34,  5.49it/s]Evaluating:   6%|â–‹         | 13/200 [00:03<00:33,  5.59it/s]Evaluating:   7%|â–‹         | 14/200 [00:03<00:34,  5.33it/s]Evaluating:   8%|â–Š         | 15/200 [00:04<00:38,  4.86it/s]Evaluating:   8%|â–Š         | 16/200 [00:05<01:32,  1.99it/s]Evaluating:   8%|â–Š         | 17/200 [00:05<01:14,  2.45it/s]Evaluating:   9%|â–‰         | 18/200 [00:05<01:05,  2.76it/s]Evaluating:  10%|â–‰         | 19/200 [00:06<01:52,  1.61it/s]Evaluating:  10%|â–ˆ         | 20/200 [00:07<01:32,  1.94it/s]Evaluating:  11%|â–ˆ         | 22/200 [00:07<00:54,  3.24it/s]Evaluating:  12%|â–ˆâ–        | 23/200 [00:07<00:47,  3.70it/s]Evaluating:  12%|â–ˆâ–        | 24/200 [00:07<00:44,  3.97it/s]Evaluating:  12%|â–ˆâ–Ž        | 25/200 [00:07<00:40,  4.35it/s]Evaluating:  13%|â–ˆâ–Ž        | 26/200 [00:09<01:29,  1.95it/s]Evaluating:  14%|â–ˆâ–Ž        | 27/200 [00:09<01:09,  2.48it/s]Evaluating:  14%|â–ˆâ–        | 28/200 [00:09<00:55,  3.08it/s]Evaluating:  15%|â–ˆâ–Œ        | 30/200 [00:09<00:36,  4.61it/s]Evaluating:  16%|â–ˆâ–Œ        | 31/200 [00:09<00:34,  4.87it/s]Evaluating:  16%|â–ˆâ–Œ        | 32/200 [00:10<00:47,  3.55it/s]Evaluating:  16%|â–ˆâ–‹        | 33/200 [00:10<00:41,  3.98it/s]Evaluating:  17%|â–ˆâ–‹        | 34/200 [00:10<00:36,  4.61it/s]Evaluating:  18%|â–ˆâ–Š        | 36/200 [00:10<00:26,  6.26it/s]Evaluating:  18%|â–ˆâ–Š        | 37/200 [00:10<00:28,  5.80it/s]Evaluating:  20%|â–ˆâ–‰        | 39/200 [00:11<00:24,  6.46it/s]Evaluating:  20%|â–ˆâ–ˆ        | 40/200 [00:11<00:23,  6.72it/s]Evaluating:  20%|â–ˆâ–ˆ        | 41/200 [00:11<00:27,  5.73it/s]Evaluating:  22%|â–ˆâ–ˆâ–       | 43/200 [00:11<00:24,  6.42it/s]Evaluating:  22%|â–ˆâ–ˆâ–       | 44/200 [00:11<00:23,  6.70it/s]Evaluating:  22%|â–ˆâ–ˆâ–Ž       | 45/200 [00:12<00:27,  5.73it/s]Evaluating:  23%|â–ˆâ–ˆâ–Ž       | 46/200 [00:12<00:30,  5.09it/s]Evaluating:  24%|â–ˆâ–ˆâ–Ž       | 47/200 [00:13<01:13,  2.09it/s]Evaluating:  24%|â–ˆâ–ˆâ–       | 48/200 [00:13<01:01,  2.47it/s]Evaluating:  24%|â–ˆâ–ˆâ–       | 49/200 [00:14<00:51,  2.96it/s]Evaluating:  25%|â–ˆâ–ˆâ–Œ       | 50/200 [00:14<00:41,  3.60it/s]Evaluating:  26%|â–ˆâ–ˆâ–Œ       | 51/200 [00:14<00:36,  4.06it/s]Evaluating:  26%|â–ˆâ–ˆâ–Œ       | 52/200 [00:14<00:36,  4.03it/s]Evaluating:  26%|â–ˆâ–ˆâ–‹       | 53/200 [00:14<00:31,  4.70it/s]Evaluating:  28%|â–ˆâ–ˆâ–Š       | 55/200 [00:15<00:25,  5.74it/s]Evaluating:  28%|â–ˆâ–ˆâ–Š       | 56/200 [00:15<00:27,  5.17it/s]Evaluating:  28%|â–ˆâ–ˆâ–Š       | 57/200 [00:15<00:29,  4.79it/s]Evaluating:  30%|â–ˆâ–ˆâ–ˆ       | 60/200 [00:15<00:17,  8.16it/s]Evaluating:  31%|â–ˆâ–ˆâ–ˆ       | 62/200 [00:16<00:22,  6.01it/s]Evaluating:  32%|â–ˆâ–ˆâ–ˆâ–      | 63/200 [00:16<00:23,  5.72it/s]Evaluating:  32%|â–ˆâ–ˆâ–ˆâ–      | 64/200 [00:16<00:22,  6.07it/s]Evaluating:  32%|â–ˆâ–ˆâ–ˆâ–Ž      | 65/200 [00:16<00:22,  6.02it/s]Evaluating:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 67/200 [00:16<00:18,  7.01it/s]Evaluating:  34%|â–ˆâ–ˆâ–ˆâ–      | 68/200 [00:17<00:20,  6.33it/s]Evaluating:  34%|â–ˆâ–ˆâ–ˆâ–      | 69/200 [00:17<00:22,  5.85it/s]Evaluating:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 70/200 [00:17<00:20,  6.25it/s]Evaluating:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 71/200 [00:17<00:20,  6.15it/s]Evaluating:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 72/200 [00:17<00:19,  6.53it/s]Evaluating:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 73/200 [00:18<00:27,  4.62it/s]Evaluating:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 74/200 [00:18<00:27,  4.63it/s]Evaluating:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 75/200 [00:18<00:23,  5.25it/s]Evaluating:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 76/200 [00:18<00:24,  5.09it/s]Evaluating:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 77/200 [00:18<00:21,  5.67it/s]Evaluating:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 79/200 [00:19<00:17,  6.91it/s]Evaluating:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 80/200 [00:19<00:18,  6.62it/s]Evaluating:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 81/200 [00:19<00:21,  5.64it/s]Evaluating:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 82/200 [00:19<00:19,  6.10it/s]Evaluating:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 83/200 [00:19<00:19,  6.04it/s]Evaluating:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 84/200 [00:19<00:17,  6.45it/s]Evaluating:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 85/200 [00:20<00:18,  6.27it/s]Evaluating:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 86/200 [00:20<00:18,  6.14it/s]Evaluating:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 87/200 [00:21<00:54,  2.08it/s]Evaluating:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 88/200 [00:21<00:42,  2.65it/s]Evaluating:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 89/200 [00:21<00:35,  3.17it/s]Evaluating:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 90/200 [00:21<00:31,  3.52it/s]Evaluating:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 91/200 [00:22<00:27,  4.00it/s]Evaluating:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 92/200 [00:22<00:24,  4.43it/s]Evaluating:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 93/200 [00:22<00:22,  4.78it/s]Evaluating:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 94/200 [00:22<00:23,  4.51it/s]Evaluating:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 95/200 [00:22<00:22,  4.57it/s]Evaluating:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 97/200 [00:23<00:17,  5.97it/s]Evaluating:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 98/200 [00:23<00:17,  5.94it/s]Evaluating:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 99/200 [00:23<00:19,  5.25it/s]Evaluating:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 101/200 [00:23<00:15,  6.46it/s]Evaluating:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 102/200 [00:24<00:17,  5.64it/s]Evaluating:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 103/200 [00:25<00:43,  2.25it/s]Evaluating:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 104/200 [00:25<00:35,  2.69it/s]Evaluating:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 105/200 [00:25<00:30,  3.16it/s]Evaluating:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 106/200 [00:25<00:28,  3.35it/s]Evaluating:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 107/200 [00:26<00:27,  3.36it/s]Evaluating:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 108/200 [00:26<00:23,  3.85it/s]Evaluating:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 110/200 [00:26<00:18,  4.77it/s]Evaluating:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 111/200 [00:26<00:16,  5.27it/s]Evaluating:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 112/200 [00:26<00:15,  5.76it/s]Evaluating:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 113/200 [00:27<00:14,  6.20it/s]Evaluating:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 114/200 [00:27<00:13,  6.57it/s]Evaluating:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 115/200 [00:27<00:13,  6.37it/s]Evaluating:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 116/200 [00:27<00:15,  5.41it/s]Evaluating:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 117/200 [00:27<00:17,  4.62it/s]Evaluating:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 118/200 [00:28<00:17,  4.75it/s]Evaluating:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 120/200 [00:29<00:33,  2.39it/s]Evaluating:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 121/200 [00:29<00:28,  2.80it/s]Evaluating:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 122/200 [00:29<00:24,  3.25it/s]Evaluating:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 123/200 [00:29<00:20,  3.70it/s]Evaluating:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 124/200 [00:30<00:21,  3.62it/s]Evaluating:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 125/200 [00:30<00:19,  3.87it/s]Evaluating:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 126/200 [00:30<00:18,  4.09it/s]Evaluating:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 127/200 [00:30<00:17,  4.26it/s]Evaluating:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 128/200 [00:30<00:15,  4.62it/s]Evaluating:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 129/200 [00:31<00:15,  4.65it/s]Evaluating:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 130/200 [00:31<00:14,  4.68it/s]Evaluating:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 131/200 [00:31<00:14,  4.70it/s]Evaluating:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 132/200 [00:31<00:12,  5.34it/s]Evaluating:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 133/200 [00:31<00:12,  5.49it/s]Evaluating:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 134/200 [00:32<00:15,  4.40it/s]Evaluating:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 135/200 [00:32<00:13,  4.77it/s]Evaluating:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 137/200 [00:33<00:19,  3.16it/s]Evaluating:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 138/200 [00:33<00:17,  3.57it/s]Evaluating:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 139/200 [00:33<00:14,  4.16it/s]Evaluating:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 140/200 [00:33<00:15,  3.93it/s]Evaluating:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 141/200 [00:34<00:12,  4.57it/s]Evaluating:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 142/200 [00:34<00:13,  4.38it/s]Evaluating:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 143/200 [00:34<00:12,  4.73it/s]Evaluating:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 144/200 [00:34<00:10,  5.35it/s]Evaluating:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 146/200 [00:34<00:07,  7.57it/s]Evaluating:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 147/200 [00:34<00:07,  7.07it/s]Evaluating:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 148/200 [00:35<00:07,  6.72it/s]Evaluating:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 149/200 [00:35<00:09,  5.65it/s]Evaluating:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 150/200 [00:35<00:10,  4.68it/s]Evaluating:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 151/200 [00:35<00:09,  5.14it/s]Evaluating:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 152/200 [00:35<00:09,  5.02it/s]Evaluating:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 153/200 [00:36<00:08,  5.60it/s]Evaluating:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 154/200 [00:37<00:22,  2.03it/s]Evaluating:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 155/200 [00:37<00:18,  2.45it/s]Evaluating:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 157/200 [00:37<00:12,  3.56it/s]Evaluating:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 158/200 [00:38<00:11,  3.65it/s]Evaluating:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 159/200 [00:38<00:10,  3.89it/s]Evaluating:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 160/200 [00:38<00:08,  4.50it/s]Evaluating:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 161/200 [00:38<00:08,  4.57it/s]Evaluating:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 162/200 [00:38<00:07,  5.18it/s]Evaluating:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 163/200 [00:38<00:07,  5.04it/s]Evaluating:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 164/200 [00:39<00:06,  5.62it/s]Evaluating:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 165/200 [00:39<00:06,  5.71it/s]Evaluating:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 166/200 [00:39<00:05,  6.20it/s]Evaluating:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 167/200 [00:39<00:05,  6.11it/s]Evaluating:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 168/200 [00:39<00:04,  6.54it/s]Evaluating:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 169/200 [00:39<00:06,  5.14it/s]Evaluating:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 170/200 [00:40<00:07,  3.84it/s]Evaluating:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 171/200 [00:40<00:06,  4.28it/s]Evaluating:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 172/200 [00:40<00:06,  4.67it/s]Evaluating:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 173/200 [00:40<00:05,  4.98it/s]Evaluating:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 174/200 [00:41<00:04,  5.59it/s]Evaluating:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 175/200 [00:41<00:04,  5.68it/s]Evaluating:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 176/200 [00:41<00:04,  5.03it/s]Evaluating:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 177/200 [00:41<00:04,  4.92it/s]Evaluating:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 178/200 [00:41<00:04,  5.17it/s]Evaluating:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 179/200 [00:41<00:03,  5.76it/s]Evaluating:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 180/200 [00:42<00:03,  6.25it/s]Evaluating:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 181/200 [00:42<00:04,  4.71it/s]Evaluating:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 182/200 [00:42<00:03,  5.34it/s]Evaluating:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 183/200 [00:42<00:03,  5.49it/s]Evaluating:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 184/200 [00:42<00:03,  5.24it/s]Evaluating:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 185/200 [00:43<00:03,  4.78it/s]Evaluating:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 186/200 [00:43<00:03,  4.51it/s]Evaluating:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 187/200 [00:43<00:02,  4.85it/s]Evaluating:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 188/200 [00:43<00:02,  5.47it/s]Evaluating:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 189/200 [00:43<00:01,  6.01it/s]Evaluating:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 190/200 [00:44<00:01,  5.98it/s]Evaluating:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 191/200 [00:44<00:01,  6.43it/s]Evaluating:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 192/200 [00:44<00:01,  5.80it/s]Evaluating:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 193/200 [00:44<00:01,  5.83it/s]Evaluating:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 194/200 [00:44<00:01,  5.46it/s]Evaluating:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 197/200 [00:44<00:00,  8.25it/s]Evaluating:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 198/200 [00:45<00:00,  8.15it/s]Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:45<00:00,  8.53it/s]Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:45<00:00,  4.42it/s]
Evaluating:   0%|          | 0/198 [00:00<?, ?it/s]Evaluating:   1%|          | 1/198 [00:00<00:49,  3.96it/s]Evaluating:   1%|          | 2/198 [00:00<00:39,  4.92it/s]Evaluating:   2%|â–         | 3/198 [00:00<00:40,  4.84it/s]Evaluating:   2%|â–         | 4/198 [00:00<00:37,  5.20it/s]Evaluating:   3%|â–Ž         | 5/198 [00:01<00:38,  5.04it/s]Evaluating:   3%|â–Ž         | 6/198 [00:01<00:33,  5.73it/s]Evaluating:   4%|â–Ž         | 7/198 [00:01<00:33,  5.79it/s]Evaluating:   4%|â–         | 8/198 [00:01<00:32,  5.82it/s]Evaluating:   5%|â–         | 9/198 [00:01<00:34,  5.44it/s]Evaluating:   6%|â–Œ         | 11/198 [00:01<00:29,  6.35it/s]Evaluating:   7%|â–‹         | 13/198 [00:02<00:25,  7.29it/s]Evaluating:   7%|â–‹         | 14/198 [00:02<00:29,  6.14it/s]Evaluating:   8%|â–Š         | 15/198 [00:02<00:33,  5.42it/s]Evaluating:   8%|â–Š         | 16/198 [00:02<00:36,  4.95it/s]Evaluating:   9%|â–Š         | 17/198 [00:03<00:32,  5.49it/s]Evaluating:   9%|â–‰         | 18/198 [00:03<00:36,  4.96it/s]Evaluating:  10%|â–‰         | 19/198 [00:03<00:32,  5.53it/s]Evaluating:  10%|â–ˆ         | 20/198 [00:03<00:31,  5.64it/s]Evaluating:  11%|â–ˆ         | 21/198 [00:03<00:31,  5.70it/s]Evaluating:  11%|â–ˆ         | 22/198 [00:04<00:34,  5.05it/s]Evaluating:  12%|â–ˆâ–        | 23/198 [00:04<00:35,  4.96it/s]Evaluating:  13%|â–ˆâ–Ž        | 25/198 [00:04<00:29,  5.95it/s]Evaluating:  13%|â–ˆâ–Ž        | 26/198 [00:04<00:28,  5.94it/s]Evaluating:  14%|â–ˆâ–Ž        | 27/198 [00:04<00:30,  5.58it/s]Evaluating:  14%|â–ˆâ–        | 28/198 [00:05<00:31,  5.32it/s]Evaluating:  15%|â–ˆâ–        | 29/198 [00:05<00:28,  5.85it/s]Evaluating:  15%|â–ˆâ–Œ        | 30/198 [00:05<00:32,  5.15it/s]Evaluating:  16%|â–ˆâ–Œ        | 31/198 [00:05<00:33,  5.02it/s]Evaluating:  16%|â–ˆâ–Œ        | 32/198 [00:05<00:35,  4.66it/s]Evaluating:  17%|â–ˆâ–‹        | 33/198 [00:06<00:39,  4.20it/s]Evaluating:  17%|â–ˆâ–‹        | 34/198 [00:06<00:37,  4.35it/s]Evaluating:  18%|â–ˆâ–Š        | 35/198 [00:06<00:32,  5.01it/s]Evaluating:  18%|â–ˆâ–Š        | 36/198 [00:06<00:32,  4.92it/s]Evaluating:  19%|â–ˆâ–Š        | 37/198 [00:06<00:29,  5.53it/s]Evaluating:  19%|â–ˆâ–‰        | 38/198 [00:07<00:28,  5.63it/s]Evaluating:  20%|â–ˆâ–‰        | 39/198 [00:07<00:29,  5.33it/s]Evaluating:  20%|â–ˆâ–ˆ        | 40/198 [00:07<00:26,  5.88it/s]Evaluating:  21%|â–ˆâ–ˆ        | 41/198 [00:07<00:26,  5.89it/s]Evaluating:  21%|â–ˆâ–ˆ        | 42/198 [00:07<00:26,  5.89it/s]Evaluating:  22%|â–ˆâ–ˆâ–       | 43/198 [00:07<00:26,  5.90it/s]Evaluating:  23%|â–ˆâ–ˆâ–Ž       | 45/198 [00:08<00:20,  7.61it/s]Evaluating:  23%|â–ˆâ–ˆâ–Ž       | 46/198 [00:08<00:24,  6.20it/s]Evaluating:  24%|â–ˆâ–ˆâ–Ž       | 47/198 [00:09<01:07,  2.25it/s]Evaluating:  24%|â–ˆâ–ˆâ–       | 48/198 [00:09<00:55,  2.70it/s]Evaluating:  25%|â–ˆâ–ˆâ–       | 49/198 [00:10<00:50,  2.96it/s]Evaluating:  25%|â–ˆâ–ˆâ–Œ       | 50/198 [00:10<00:42,  3.45it/s]Evaluating:  26%|â–ˆâ–ˆâ–Œ       | 51/198 [00:10<00:40,  3.59it/s]Evaluating:  26%|â–ˆâ–ˆâ–‹       | 52/198 [00:10<00:35,  4.06it/s]Evaluating:  27%|â–ˆâ–ˆâ–‹       | 53/198 [00:11<01:18,  1.85it/s]Evaluating:  27%|â–ˆâ–ˆâ–‹       | 54/198 [00:12<01:01,  2.33it/s]Evaluating:  28%|â–ˆâ–ˆâ–Š       | 55/198 [00:12<00:50,  2.84it/s]Evaluating:  28%|â–ˆâ–ˆâ–Š       | 56/198 [00:12<00:42,  3.36it/s]Evaluating:  29%|â–ˆâ–ˆâ–‰       | 57/198 [00:12<00:35,  3.98it/s]Evaluating:  29%|â–ˆâ–ˆâ–‰       | 58/198 [00:12<00:33,  4.12it/s]Evaluating:  30%|â–ˆâ–ˆâ–‰       | 59/198 [00:12<00:30,  4.53it/s]Evaluating:  30%|â–ˆâ–ˆâ–ˆ       | 60/198 [00:13<00:28,  4.87it/s]Evaluating:  31%|â–ˆâ–ˆâ–ˆ       | 61/198 [00:13<00:24,  5.49it/s]Evaluating:  31%|â–ˆâ–ˆâ–ˆâ–      | 62/198 [00:13<00:27,  4.94it/s]Evaluating:  32%|â–ˆâ–ˆâ–ˆâ–      | 63/198 [00:13<00:24,  5.56it/s]Evaluating:  32%|â–ˆâ–ˆâ–ˆâ–      | 64/198 [00:13<00:33,  3.99it/s]Evaluating:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 65/198 [00:14<00:28,  4.68it/s]Evaluating:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 66/198 [00:14<00:24,  5.32it/s]Evaluating:  34%|â–ˆâ–ˆâ–ˆâ–      | 68/198 [00:14<00:20,  6.26it/s]Evaluating:  35%|â–ˆâ–ˆâ–ˆâ–      | 69/198 [00:14<00:19,  6.60it/s]Evaluating:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 70/198 [00:15<00:55,  2.29it/s]Evaluating:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 71/198 [00:16<00:57,  2.21it/s]Evaluating:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 73/198 [00:16<00:42,  2.93it/s]Evaluating:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 74/198 [00:16<00:35,  3.44it/s]Evaluating:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 75/198 [00:17<00:35,  3.47it/s]Evaluating:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 76/198 [00:17<00:35,  3.40it/s]Evaluating:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 77/198 [00:17<00:31,  3.85it/s]Evaluating:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 78/198 [00:17<00:26,  4.49it/s]Evaluating:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 79/198 [00:17<00:23,  5.12it/s]Evaluating:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 80/198 [00:18<00:20,  5.69it/s]Evaluating:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 81/198 [00:18<00:20,  5.76it/s]Evaluating:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 82/198 [00:18<00:19,  5.82it/s]Evaluating:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 83/198 [00:18<00:19,  5.85it/s]Evaluating:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 84/198 [00:18<00:20,  5.49it/s]Evaluating:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 86/198 [00:20<00:44,  2.54it/s]Evaluating:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 87/198 [00:20<00:36,  3.04it/s]Evaluating:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 88/198 [00:20<00:35,  3.13it/s]Evaluating:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 89/198 [00:20<00:32,  3.32it/s]Evaluating:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 90/198 [00:20<00:27,  3.96it/s]Evaluating:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 91/198 [00:21<00:26,  3.96it/s]Evaluating:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 92/198 [00:21<00:22,  4.62it/s]Evaluating:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 93/198 [00:21<00:20,  5.25it/s]Evaluating:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 94/198 [00:21<00:17,  5.81it/s]Evaluating:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 97/198 [00:21<00:12,  8.04it/s]Evaluating:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 98/198 [00:22<00:35,  2.79it/s]Evaluating:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 99/198 [00:23<00:31,  3.17it/s]Evaluating:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 100/198 [00:23<00:26,  3.73it/s]Evaluating:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 102/198 [00:23<00:22,  4.24it/s]Evaluating:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 103/198 [00:23<00:23,  4.02it/s]Evaluating:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 104/198 [00:24<00:20,  4.57it/s]Evaluating:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 105/198 [00:24<00:20,  4.61it/s]Evaluating:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 106/198 [00:24<00:17,  5.19it/s]Evaluating:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 107/198 [00:25<00:44,  2.06it/s]Evaluating:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 108/198 [00:25<00:34,  2.61it/s]Evaluating:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 109/198 [00:25<00:27,  3.24it/s]Evaluating:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 110/198 [00:26<00:24,  3.57it/s]Evaluating:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 112/198 [00:26<00:15,  5.47it/s]Evaluating:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 113/198 [00:26<00:19,  4.35it/s]Evaluating:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 114/198 [00:27<00:41,  2.04it/s]Evaluating:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 116/198 [00:28<00:26,  3.05it/s]Evaluating:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 117/198 [00:28<00:23,  3.44it/s]Evaluating:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 120/198 [00:28<00:14,  5.30it/s]Evaluating:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 121/198 [00:28<00:16,  4.80it/s]Evaluating:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 122/198 [00:28<00:14,  5.23it/s]Evaluating:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 123/198 [00:29<00:15,  4.86it/s]Evaluating:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 124/198 [00:29<00:14,  5.09it/s]Evaluating:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 125/198 [00:29<00:13,  5.29it/s]Evaluating:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 126/198 [00:29<00:14,  5.13it/s]Evaluating:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 127/198 [00:30<00:17,  4.07it/s]Evaluating:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 128/198 [00:30<00:15,  4.47it/s]Evaluating:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 129/198 [00:30<00:13,  5.11it/s]Evaluating:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 130/198 [00:30<00:12,  5.32it/s]Evaluating:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 131/198 [00:30<00:12,  5.48it/s]Evaluating:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 133/198 [00:30<00:09,  6.76it/s]Evaluating:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 134/198 [00:31<00:09,  6.53it/s]Evaluating:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 135/198 [00:31<00:10,  5.95it/s]Evaluating:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 136/198 [00:32<00:28,  2.18it/s]Evaluating:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 137/198 [00:32<00:23,  2.57it/s]Evaluating:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 138/198 [00:32<00:20,  2.86it/s]Evaluating:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 139/198 [00:33<00:18,  3.23it/s]Evaluating:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 141/198 [00:33<00:12,  4.59it/s]Evaluating:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 142/198 [00:33<00:13,  4.06it/s]Evaluating:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 143/198 [00:33<00:12,  4.42it/s]Evaluating:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 144/198 [00:34<00:12,  4.29it/s]Evaluating:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 145/198 [00:34<00:10,  4.90it/s]Evaluating:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 146/198 [00:34<00:10,  4.85it/s]Evaluating:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 147/198 [00:34<00:10,  4.82it/s]Evaluating:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 148/198 [00:34<00:09,  5.43it/s]Evaluating:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 149/198 [00:35<00:08,  5.57it/s]Evaluating:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 150/198 [00:35<00:07,  6.09it/s]Evaluating:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 151/198 [00:35<00:16,  2.86it/s]Evaluating:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 152/198 [00:36<00:13,  3.53it/s]Evaluating:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 153/198 [00:36<00:11,  4.01it/s]Evaluating:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 154/198 [00:36<00:11,  4.00it/s]Evaluating:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 155/198 [00:36<00:09,  4.42it/s]Evaluating:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 156/198 [00:36<00:08,  5.09it/s]Evaluating:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 157/198 [00:36<00:07,  5.31it/s]Evaluating:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 159/198 [00:37<00:07,  5.56it/s]Evaluating:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 160/198 [00:37<00:06,  5.99it/s]Evaluating:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 161/198 [00:37<00:07,  5.28it/s]Evaluating:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 162/198 [00:37<00:07,  5.12it/s]Evaluating:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 163/198 [00:38<00:07,  4.74it/s]Evaluating:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 164/198 [00:38<00:06,  5.01it/s]Evaluating:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 166/198 [00:38<00:05,  6.33it/s]Evaluating:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 167/198 [00:38<00:04,  6.64it/s]Evaluating:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 169/198 [00:38<00:04,  6.67it/s]Evaluating:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 170/198 [00:39<00:04,  6.10it/s]Evaluating:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 171/198 [00:39<00:04,  6.06it/s]Evaluating:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 172/198 [00:39<00:04,  6.00it/s]Evaluating:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 173/198 [00:39<00:03,  6.41it/s]Evaluating:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 174/198 [00:39<00:03,  6.28it/s]Evaluating:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 175/198 [00:39<00:03,  6.65it/s]Evaluating:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 176/198 [00:40<00:03,  6.41it/s]Evaluating:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 178/198 [00:40<00:02,  8.01it/s]Evaluating:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 179/198 [00:40<00:02,  6.84it/s]Evaluating:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 180/198 [00:40<00:02,  7.05it/s]Evaluating:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 181/198 [00:40<00:02,  6.69it/s]Evaluating:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 182/198 [00:40<00:02,  6.96it/s]Evaluating:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 183/198 [00:41<00:02,  6.14it/s]Evaluating:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 185/198 [00:41<00:02,  6.38it/s]Evaluating:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 187/198 [00:41<00:01,  6.17it/s]Evaluating:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 188/198 [00:42<00:01,  5.51it/s]Evaluating:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 189/198 [00:42<00:01,  5.04it/s]Evaluating:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 190/198 [00:42<00:01,  5.53it/s]Evaluating:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 191/198 [00:42<00:01,  5.30it/s]Evaluating:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 193/198 [00:42<00:00,  5.83it/s]Evaluating:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 194/198 [00:43<00:00,  5.85it/s]Evaluating:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 195/198 [00:43<00:00,  6.24it/s]Evaluating:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 197/198 [00:43<00:00,  6.83it/s]Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 198/198 [00:43<00:00,  7.04it/s]Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 198/198 [00:43<00:00,  4.54it/s]
Running 4/5: python ./experiments/inference_chat.py /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-9 ./data/llama_fingerprint_l1 publish --dont_load_adapter -t llama2 -o /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-9
['python', './experiments/inference_chat.py', '/fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-9', './data/llama_fingerprint_l1', 'publish', '--dont_load_adapter', '-t', 'llama2', '-o', '/fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-9']
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
[2024-08-02 05:44:36,568] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:01<00:03,  1.62s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:03<00:01,  1.67s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:04<00:00,  1.44s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:04<00:00,  1.50s/it]
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:520: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:520: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Evaluating:   0%|          | 1/200 [00:01<05:14,  1.58s/it]Evaluating:   1%|          | 2/200 [00:01<02:48,  1.17it/s]Evaluating:   2%|â–         | 3/200 [00:02<01:59,  1.65it/s]Evaluating:   2%|â–Ž         | 5/200 [00:02<01:07,  2.88it/s]Evaluating:   3%|â–Ž         | 6/200 [00:02<00:59,  3.24it/s]Evaluating:   4%|â–Ž         | 7/200 [00:02<00:51,  3.72it/s]Evaluating:   4%|â–         | 8/200 [00:03<00:48,  3.95it/s]Evaluating:   5%|â–Œ         | 10/200 [00:03<00:33,  5.59it/s]Evaluating:   6%|â–Œ         | 11/200 [00:03<00:31,  5.99it/s]Evaluating:   6%|â–Œ         | 12/200 [00:03<00:29,  6.36it/s]Evaluating:   6%|â–‹         | 13/200 [00:03<00:30,  6.21it/s]Evaluating:   7%|â–‹         | 14/200 [00:03<00:30,  6.11it/s]Evaluating:   8%|â–Š         | 15/200 [00:04<00:30,  6.04it/s]Evaluating:   8%|â–Š         | 16/200 [00:04<00:37,  4.93it/s]Evaluating:   8%|â–Š         | 17/200 [00:04<00:36,  5.07it/s]Evaluating:   9%|â–‰         | 18/200 [00:04<00:38,  4.67it/s]Evaluating:  10%|â–‰         | 19/200 [00:05<00:43,  4.18it/s]Evaluating:  10%|â–ˆ         | 20/200 [00:05<00:43,  4.11it/s]Evaluating:  10%|â–ˆ         | 21/200 [00:05<00:37,  4.78it/s]Evaluating:  12%|â–ˆâ–        | 23/200 [00:05<00:28,  6.14it/s]Evaluating:  12%|â–ˆâ–        | 24/200 [00:05<00:27,  6.46it/s]Evaluating:  12%|â–ˆâ–Ž        | 25/200 [00:06<00:27,  6.28it/s]Evaluating:  13%|â–ˆâ–Ž        | 26/200 [00:06<00:26,  6.61it/s]Evaluating:  14%|â–ˆâ–Ž        | 27/200 [00:06<00:25,  6.88it/s]Evaluating:  14%|â–ˆâ–        | 28/200 [00:06<00:24,  7.10it/s]Evaluating:  14%|â–ˆâ–        | 29/200 [00:06<00:25,  6.68it/s]Evaluating:  15%|â–ˆâ–Œ        | 30/200 [00:06<00:30,  5.54it/s]Evaluating:  16%|â–ˆâ–Œ        | 31/200 [00:07<00:30,  5.62it/s]Evaluating:  16%|â–ˆâ–Œ        | 32/200 [00:07<00:46,  3.62it/s]Evaluating:  16%|â–ˆâ–‹        | 33/200 [00:07<00:40,  4.08it/s]Evaluating:  17%|â–ˆâ–‹        | 34/200 [00:07<00:37,  4.49it/s]Evaluating:  18%|â–ˆâ–Š        | 35/200 [00:07<00:32,  5.13it/s]Evaluating:  18%|â–ˆâ–Š        | 36/200 [00:08<00:28,  5.69it/s]Evaluating:  18%|â–ˆâ–Š        | 37/200 [00:08<00:30,  5.36it/s]Evaluating:  19%|â–ˆâ–‰        | 38/200 [00:08<00:27,  5.90it/s]Evaluating:  20%|â–ˆâ–‰        | 39/200 [00:08<00:29,  5.48it/s]Evaluating:  20%|â–ˆâ–ˆ        | 40/200 [00:08<00:26,  6.00it/s]Evaluating:  20%|â–ˆâ–ˆ        | 41/200 [00:09<00:30,  5.18it/s]Evaluating:  22%|â–ˆâ–ˆâ–       | 43/200 [00:09<00:25,  6.09it/s]Evaluating:  22%|â–ˆâ–ˆâ–Ž       | 45/200 [00:09<00:24,  6.29it/s]Evaluating:  23%|â–ˆâ–ˆâ–Ž       | 46/200 [00:09<00:27,  5.55it/s]Evaluating:  24%|â–ˆâ–ˆâ–Ž       | 47/200 [00:10<00:30,  5.07it/s]Evaluating:  24%|â–ˆâ–ˆâ–       | 48/200 [00:10<00:29,  5.22it/s]Evaluating:  24%|â–ˆâ–ˆâ–       | 49/200 [00:10<00:28,  5.33it/s]Evaluating:  26%|â–ˆâ–ˆâ–Œ       | 51/200 [00:10<00:24,  6.15it/s]Evaluating:  26%|â–ˆâ–ˆâ–Œ       | 52/200 [00:10<00:27,  5.42it/s]Evaluating:  26%|â–ˆâ–ˆâ–‹       | 53/200 [00:11<00:25,  5.85it/s]Evaluating:  27%|â–ˆâ–ˆâ–‹       | 54/200 [00:11<00:26,  5.49it/s]Evaluating:  28%|â–ˆâ–ˆâ–Š       | 55/200 [00:11<00:27,  5.25it/s]Evaluating:  28%|â–ˆâ–ˆâ–Š       | 56/200 [00:11<00:30,  4.77it/s]Evaluating:  28%|â–ˆâ–ˆâ–Š       | 57/200 [00:12<00:31,  4.49it/s]Evaluating:  29%|â–ˆâ–ˆâ–‰       | 58/200 [00:12<00:27,  5.11it/s]Evaluating:  30%|â–ˆâ–ˆâ–‰       | 59/200 [00:12<00:28,  4.98it/s]Evaluating:  30%|â–ˆâ–ˆâ–ˆ       | 60/200 [00:12<00:28,  4.89it/s]Evaluating:  30%|â–ˆâ–ˆâ–ˆ       | 61/200 [00:12<00:34,  4.09it/s]Evaluating:  31%|â–ˆâ–ˆâ–ˆ       | 62/200 [00:13<00:34,  4.03it/s]Evaluating:  32%|â–ˆâ–ˆâ–ˆâ–      | 63/200 [00:13<00:32,  4.21it/s]Evaluating:  32%|â–ˆâ–ˆâ–ˆâ–      | 64/200 [00:13<00:27,  4.87it/s]Evaluating:  32%|â–ˆâ–ˆâ–ˆâ–Ž      | 65/200 [00:13<00:26,  5.12it/s]Evaluating:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 66/200 [00:13<00:26,  4.97it/s]Evaluating:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 67/200 [00:14<00:25,  5.20it/s]Evaluating:  34%|â–ˆâ–ˆâ–ˆâ–      | 68/200 [00:14<00:26,  5.05it/s]Evaluating:  34%|â–ˆâ–ˆâ–ˆâ–      | 69/200 [00:14<00:28,  4.65it/s]Evaluating:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 70/200 [00:14<00:24,  5.29it/s]Evaluating:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 71/200 [00:14<00:23,  5.43it/s]Evaluating:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 72/200 [00:15<00:21,  5.96it/s]Evaluating:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 73/200 [00:15<00:24,  5.13it/s]Evaluating:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 74/200 [00:15<00:25,  4.97it/s]Evaluating:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 75/200 [00:15<00:22,  5.56it/s]Evaluating:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 76/200 [00:15<00:23,  5.26it/s]Evaluating:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 77/200 [00:16<00:26,  4.69it/s]Evaluating:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 78/200 [00:16<00:22,  5.31it/s]Evaluating:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 79/200 [00:16<00:22,  5.46it/s]Evaluating:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 80/200 [00:16<00:21,  5.55it/s]Evaluating:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 81/200 [00:16<00:24,  4.93it/s]Evaluating:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 82/200 [00:16<00:21,  5.52it/s]Evaluating:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 83/200 [00:17<00:20,  5.60it/s]Evaluating:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 84/200 [00:17<00:19,  6.10it/s]Evaluating:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 85/200 [00:17<00:19,  6.01it/s]Evaluating:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 86/200 [00:17<00:19,  5.95it/s]Evaluating:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 87/200 [00:17<00:19,  5.91it/s]Evaluating:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 88/200 [00:17<00:17,  6.35it/s]Evaluating:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 89/200 [00:18<00:17,  6.17it/s]Evaluating:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 90/200 [00:18<00:19,  5.60it/s]Evaluating:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 91/200 [00:18<00:19,  5.66it/s]Evaluating:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 92/200 [00:18<00:18,  5.70it/s]Evaluating:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 93/200 [00:18<00:18,  5.73it/s]Evaluating:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 94/200 [00:19<00:21,  5.02it/s]Evaluating:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 95/200 [00:19<00:21,  4.91it/s]Evaluating:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 96/200 [00:19<00:25,  4.09it/s]Evaluating:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 97/200 [00:19<00:22,  4.48it/s]Evaluating:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 98/200 [00:19<00:21,  4.81it/s]Evaluating:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 99/200 [00:20<00:22,  4.50it/s]Evaluating:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 100/200 [00:20<00:22,  4.54it/s]Evaluating:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 101/200 [00:20<00:20,  4.86it/s]Evaluating:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 102/200 [00:20<00:21,  4.53it/s]Evaluating:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 103/200 [00:21<00:18,  5.17it/s]Evaluating:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 104/200 [00:21<00:17,  5.35it/s]Evaluating:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 105/200 [00:21<00:17,  5.48it/s]Evaluating:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 106/200 [00:21<00:19,  4.89it/s]Evaluating:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 107/200 [00:21<00:21,  4.30it/s]Evaluating:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 108/200 [00:22<00:19,  4.67it/s]Evaluating:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 109/200 [00:22<00:25,  3.61it/s]Evaluating:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 110/200 [00:22<00:24,  3.69it/s]Evaluating:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 111/200 [00:22<00:20,  4.38it/s]Evaluating:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 112/200 [00:23<00:17,  5.03it/s]Evaluating:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 113/200 [00:23<00:15,  5.60it/s]Evaluating:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 114/200 [00:23<00:15,  5.65it/s]Evaluating:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 115/200 [00:23<00:14,  5.69it/s]Evaluating:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 116/200 [00:23<00:16,  5.00it/s]Evaluating:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 117/200 [00:23<00:16,  4.89it/s]Evaluating:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 118/200 [00:24<00:15,  5.14it/s]Evaluating:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 119/200 [00:24<00:15,  5.32it/s]Evaluating:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 120/200 [00:24<00:14,  5.46it/s]Evaluating:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 121/200 [00:24<00:14,  5.56it/s]Evaluating:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 122/200 [00:24<00:13,  5.63it/s]Evaluating:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 123/200 [00:24<00:13,  5.69it/s]Evaluating:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 124/200 [00:25<00:16,  4.70it/s]Evaluating:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 125/200 [00:25<00:16,  4.43it/s]Evaluating:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 126/200 [00:25<00:16,  4.50it/s]Evaluating:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 127/200 [00:25<00:16,  4.55it/s]Evaluating:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 128/200 [00:26<00:13,  5.18it/s]Evaluating:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 129/200 [00:26<00:12,  5.73it/s]Evaluating:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 130/200 [00:26<00:13,  5.36it/s]Evaluating:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 131/200 [00:26<00:13,  5.13it/s]Evaluating:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 132/200 [00:26<00:11,  5.70it/s]Evaluating:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 133/200 [00:26<00:11,  5.73it/s]Evaluating:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 134/200 [00:27<00:11,  5.76it/s]Evaluating:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 135/200 [00:27<00:11,  5.78it/s]Evaluating:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 137/200 [00:27<00:10,  6.14it/s]Evaluating:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 138/200 [00:27<00:10,  6.06it/s]Evaluating:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 139/200 [00:27<00:09,  6.41it/s]Evaluating:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 140/200 [00:28<00:11,  5.14it/s]Evaluating:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 141/200 [00:28<00:10,  5.68it/s]Evaluating:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 142/200 [00:28<00:11,  5.02it/s]Evaluating:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 143/200 [00:28<00:10,  5.23it/s]Evaluating:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 144/200 [00:28<00:09,  5.77it/s]Evaluating:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 145/200 [00:29<00:08,  6.22it/s]Evaluating:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 147/200 [00:29<00:07,  6.82it/s]Evaluating:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 148/200 [00:29<00:07,  6.54it/s]Evaluating:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 149/200 [00:29<00:09,  5.57it/s]Evaluating:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 150/200 [00:29<00:10,  4.99it/s]Evaluating:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 151/200 [00:30<00:08,  5.53it/s]Evaluating:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 152/200 [00:30<00:09,  5.26it/s]Evaluating:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 153/200 [00:30<00:08,  5.79it/s]Evaluating:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 154/200 [00:30<00:07,  6.25it/s]Evaluating:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 155/200 [00:30<00:07,  5.63it/s]Evaluating:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 156/200 [00:31<00:08,  4.99it/s]Evaluating:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 158/200 [00:31<00:07,  5.37it/s]Evaluating:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 159/200 [00:31<00:07,  5.20it/s]Evaluating:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 160/200 [00:31<00:07,  5.68it/s]Evaluating:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 161/200 [00:31<00:07,  5.01it/s]Evaluating:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 162/200 [00:32<00:08,  4.61it/s]Evaluating:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 163/200 [00:32<00:07,  4.64it/s]Evaluating:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 164/200 [00:32<00:06,  5.26it/s]Evaluating:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 165/200 [00:32<00:06,  5.42it/s]Evaluating:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 166/200 [00:32<00:05,  5.95it/s]Evaluating:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 167/200 [00:33<00:05,  5.93it/s]Evaluating:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 169/200 [00:33<00:05,  5.61it/s]Evaluating:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 170/200 [00:33<00:05,  5.68it/s]Evaluating:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 171/200 [00:33<00:05,  5.74it/s]Evaluating:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 172/200 [00:33<00:04,  5.78it/s]Evaluating:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 173/200 [00:34<00:04,  5.82it/s]Evaluating:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 174/200 [00:34<00:04,  6.28it/s]Evaluating:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 175/200 [00:34<00:04,  6.17it/s]Evaluating:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 176/200 [00:34<00:04,  5.30it/s]Evaluating:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 177/200 [00:34<00:04,  5.11it/s]Evaluating:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 178/200 [00:35<00:04,  4.88it/s]Evaluating:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 179/200 [00:35<00:03,  5.48it/s]Evaluating:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 180/200 [00:35<00:03,  6.01it/s]Evaluating:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 181/200 [00:35<00:04,  4.61it/s]Evaluating:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 182/200 [00:35<00:03,  5.24it/s]Evaluating:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 183/200 [00:36<00:03,  5.42it/s]Evaluating:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 184/200 [00:36<00:03,  5.19it/s]Evaluating:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 185/200 [00:36<00:03,  4.74it/s]Evaluating:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 186/200 [00:36<00:03,  4.48it/s]Evaluating:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 187/200 [00:36<00:02,  4.82it/s]Evaluating:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 188/200 [00:37<00:02,  5.44it/s]Evaluating:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 189/200 [00:37<00:01,  5.99it/s]Evaluating:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 190/200 [00:37<00:01,  5.96it/s]Evaluating:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 191/200 [00:37<00:01,  6.41it/s]Evaluating:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 192/200 [00:37<00:01,  6.24it/s]Evaluating:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 193/200 [00:37<00:01,  6.12it/s]Evaluating:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 194/200 [00:37<00:01,  5.62it/s]Evaluating:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 195/200 [00:38<00:01,  4.99it/s]Evaluating:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 197/200 [00:38<00:00,  6.36it/s]Evaluating:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 198/200 [00:38<00:00,  6.67it/s]Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 199/200 [00:38<00:00,  5.65it/s]Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:39<00:00,  5.71it/s]Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:39<00:00,  5.13it/s]
Evaluating:   0%|          | 0/198 [00:00<?, ?it/s]Evaluating:   1%|          | 1/198 [00:00<00:49,  3.96it/s]Evaluating:   1%|          | 2/198 [00:00<00:39,  4.92it/s]Evaluating:   2%|â–         | 3/198 [00:00<00:36,  5.33it/s]Evaluating:   2%|â–         | 4/198 [00:00<00:35,  5.53it/s]Evaluating:   3%|â–Ž         | 5/198 [00:00<00:36,  5.22it/s]Evaluating:   3%|â–Ž         | 6/198 [00:01<00:32,  5.88it/s]Evaluating:   4%|â–Ž         | 7/198 [00:01<00:32,  5.88it/s]Evaluating:   4%|â–         | 8/198 [00:01<00:32,  5.89it/s]Evaluating:   5%|â–         | 9/198 [00:01<00:32,  5.90it/s]Evaluating:   5%|â–Œ         | 10/198 [00:01<00:34,  5.48it/s]Evaluating:   6%|â–Œ         | 11/198 [00:02<00:35,  5.24it/s]Evaluating:   6%|â–Œ         | 12/198 [00:02<00:36,  5.08it/s]Evaluating:   7%|â–‹         | 13/198 [00:02<00:34,  5.30it/s]Evaluating:   7%|â–‹         | 14/198 [00:02<00:38,  4.79it/s]Evaluating:   8%|â–Š         | 15/198 [00:03<00:45,  4.05it/s]Evaluating:   8%|â–Š         | 16/198 [00:03<00:45,  4.03it/s]Evaluating:   9%|â–Š         | 17/198 [00:03<00:38,  4.71it/s]Evaluating:   9%|â–‰         | 18/198 [00:03<00:40,  4.47it/s]Evaluating:  10%|â–‰         | 19/198 [00:03<00:34,  5.12it/s]Evaluating:  10%|â–ˆ         | 20/198 [00:03<00:33,  5.33it/s]Evaluating:  11%|â–ˆ         | 21/198 [00:04<00:32,  5.48it/s]Evaluating:  11%|â–ˆ         | 22/198 [00:04<00:35,  4.92it/s]Evaluating:  12%|â–ˆâ–        | 23/198 [00:04<00:35,  4.87it/s]Evaluating:  12%|â–ˆâ–        | 24/198 [00:04<00:31,  5.49it/s]Evaluating:  13%|â–ˆâ–Ž        | 25/198 [00:04<00:32,  5.24it/s]Evaluating:  13%|â–ˆâ–Ž        | 26/198 [00:05<00:31,  5.42it/s]Evaluating:  14%|â–ˆâ–Ž        | 27/198 [00:05<00:32,  5.21it/s]Evaluating:  14%|â–ˆâ–        | 28/198 [00:05<00:35,  4.76it/s]Evaluating:  15%|â–ˆâ–        | 29/198 [00:05<00:31,  5.40it/s]Evaluating:  15%|â–ˆâ–Œ        | 30/198 [00:05<00:34,  4.87it/s]Evaluating:  16%|â–ˆâ–Œ        | 31/198 [00:06<00:34,  4.83it/s]Evaluating:  16%|â–ˆâ–Œ        | 32/198 [00:06<00:36,  4.52it/s]Evaluating:  17%|â–ˆâ–‹        | 33/198 [00:06<00:36,  4.58it/s]Evaluating:  17%|â–ˆâ–‹        | 34/198 [00:06<00:37,  4.37it/s]Evaluating:  18%|â–ˆâ–Š        | 35/198 [00:07<00:34,  4.73it/s]Evaluating:  18%|â–ˆâ–Š        | 36/198 [00:07<00:34,  4.73it/s]Evaluating:  19%|â–ˆâ–Š        | 37/198 [00:07<00:30,  5.36it/s]Evaluating:  19%|â–ˆâ–‰        | 38/198 [00:07<00:29,  5.52it/s]Evaluating:  20%|â–ˆâ–‰        | 39/198 [00:07<00:30,  5.26it/s]Evaluating:  20%|â–ˆâ–ˆ        | 40/198 [00:07<00:27,  5.83it/s]Evaluating:  21%|â–ˆâ–ˆ        | 41/198 [00:08<00:26,  5.84it/s]Evaluating:  21%|â–ˆâ–ˆ        | 42/198 [00:08<00:26,  5.85it/s]Evaluating:  22%|â–ˆâ–ˆâ–       | 43/198 [00:08<00:26,  5.87it/s]Evaluating:  22%|â–ˆâ–ˆâ–       | 44/198 [00:08<00:28,  5.48it/s]Evaluating:  23%|â–ˆâ–ˆâ–Ž       | 45/198 [00:08<00:25,  6.02it/s]Evaluating:  23%|â–ˆâ–ˆâ–Ž       | 46/198 [00:08<00:29,  5.20it/s]Evaluating:  24%|â–ˆâ–ˆâ–Ž       | 47/198 [00:09<00:26,  5.78it/s]Evaluating:  24%|â–ˆâ–ˆâ–       | 48/198 [00:09<00:25,  5.81it/s]Evaluating:  25%|â–ˆâ–ˆâ–       | 49/198 [00:09<00:25,  5.84it/s]Evaluating:  25%|â–ˆâ–ˆâ–Œ       | 50/198 [00:09<00:25,  5.86it/s]Evaluating:  26%|â–ˆâ–ˆâ–Œ       | 51/198 [00:09<00:28,  5.12it/s]Evaluating:  26%|â–ˆâ–ˆâ–‹       | 52/198 [00:10<00:27,  5.33it/s]Evaluating:  27%|â–ˆâ–ˆâ–‹       | 53/198 [00:10<00:26,  5.48it/s]Evaluating:  27%|â–ˆâ–ˆâ–‹       | 54/198 [00:10<00:25,  5.60it/s]Evaluating:  28%|â–ˆâ–ˆâ–Š       | 55/198 [00:10<00:25,  5.68it/s]Evaluating:  28%|â–ˆâ–ˆâ–Š       | 56/198 [00:10<00:24,  5.75it/s]Evaluating:  29%|â–ˆâ–ˆâ–‰       | 58/198 [00:10<00:21,  6.54it/s]Evaluating:  30%|â–ˆâ–ˆâ–‰       | 59/198 [00:11<00:21,  6.36it/s]Evaluating:  30%|â–ˆâ–ˆâ–ˆ       | 60/198 [00:11<00:22,  6.23it/s]Evaluating:  31%|â–ˆâ–ˆâ–ˆ       | 61/198 [00:11<00:20,  6.59it/s]Evaluating:  31%|â–ˆâ–ˆâ–ˆâ–      | 62/198 [00:11<00:24,  5.56it/s]Evaluating:  32%|â–ˆâ–ˆâ–ˆâ–      | 63/198 [00:11<00:22,  6.05it/s]Evaluating:  32%|â–ˆâ–ˆâ–ˆâ–      | 64/198 [00:11<00:22,  6.01it/s]Evaluating:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 65/198 [00:12<00:20,  6.44it/s]Evaluating:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 66/198 [00:12<00:19,  6.79it/s]Evaluating:  34%|â–ˆâ–ˆâ–ˆâ–      | 68/198 [00:12<00:18,  7.22it/s]Evaluating:  35%|â–ˆâ–ˆâ–ˆâ–      | 69/198 [00:12<00:17,  7.36it/s]Evaluating:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 70/198 [00:12<00:17,  7.46it/s]Evaluating:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 71/198 [00:13<00:29,  4.28it/s]Evaluating:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 72/198 [00:13<00:25,  4.90it/s]Evaluating:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 73/198 [00:13<00:24,  5.14it/s]Evaluating:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 74/198 [00:13<00:21,  5.71it/s]Evaluating:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 75/198 [00:13<00:22,  5.37it/s]Evaluating:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 76/198 [00:14<00:23,  5.16it/s]Evaluating:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 77/198 [00:14<00:22,  5.35it/s]Evaluating:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 78/198 [00:14<00:21,  5.51it/s]Evaluating:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 79/198 [00:14<00:19,  6.04it/s]Evaluating:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 80/198 [00:14<00:22,  5.22it/s]Evaluating:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 81/198 [00:14<00:21,  5.40it/s]Evaluating:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 82/198 [00:15<00:20,  5.55it/s]Evaluating:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 83/198 [00:15<00:20,  5.65it/s]Evaluating:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 84/198 [00:15<00:21,  5.35it/s]Evaluating:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 85/198 [00:15<00:20,  5.51it/s]Evaluating:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 86/198 [00:15<00:18,  6.04it/s]Evaluating:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 87/198 [00:15<00:17,  6.48it/s]Evaluating:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 88/198 [00:16<00:21,  5.10it/s]Evaluating:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 89/198 [00:16<00:23,  4.70it/s]Evaluating:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 90/198 [00:16<00:20,  5.34it/s]Evaluating:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 91/198 [00:16<00:20,  5.14it/s]Evaluating:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 92/198 [00:16<00:18,  5.72it/s]Evaluating:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 93/198 [00:17<00:16,  6.21it/s]Evaluating:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 94/198 [00:17<00:15,  6.61it/s]Evaluating:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 95/198 [00:17<00:14,  6.92it/s]Evaluating:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 96/198 [00:17<00:18,  5.63it/s]Evaluating:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 97/198 [00:17<00:18,  5.61it/s]Evaluating:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 98/198 [00:17<00:17,  5.69it/s]Evaluating:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 99/198 [00:18<00:17,  5.75it/s]Evaluating:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 100/198 [00:18<00:15,  6.26it/s]Evaluating:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 102/198 [00:18<00:16,  5.71it/s]Evaluating:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 103/198 [00:18<00:19,  4.86it/s]Evaluating:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 104/198 [00:19<00:17,  5.38it/s]Evaluating:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 105/198 [00:19<00:17,  5.17it/s]Evaluating:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 106/198 [00:19<00:16,  5.69it/s]Evaluating:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 107/198 [00:19<00:19,  4.75it/s]Evaluating:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 108/198 [00:19<00:20,  4.47it/s]Evaluating:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 110/198 [00:20<00:16,  5.23it/s]Evaluating:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 111/198 [00:20<00:17,  5.08it/s]Evaluating:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 113/198 [00:20<00:12,  6.58it/s]Evaluating:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 114/198 [00:20<00:15,  5.40it/s]Evaluating:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 115/198 [00:21<00:18,  4.49it/s]Evaluating:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 116/198 [00:21<00:16,  5.03it/s]Evaluating:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 117/198 [00:21<00:15,  5.23it/s]Evaluating:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 118/198 [00:21<00:13,  5.74it/s]Evaluating:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 119/198 [00:21<00:15,  5.07it/s]Evaluating:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 120/198 [00:22<00:16,  4.79it/s]Evaluating:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 121/198 [00:22<00:18,  4.25it/s]Evaluating:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 122/198 [00:22<00:15,  4.90it/s]Evaluating:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 123/198 [00:22<00:16,  4.55it/s]Evaluating:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 124/198 [00:23<00:15,  4.87it/s]Evaluating:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 125/198 [00:23<00:14,  5.12it/s]Evaluating:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 126/198 [00:23<00:14,  4.98it/s]Evaluating:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 127/198 [00:23<00:15,  4.60it/s]Evaluating:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 128/198 [00:23<00:14,  4.91it/s]Evaluating:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 129/198 [00:24<00:12,  5.50it/s]Evaluating:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 130/198 [00:24<00:12,  5.59it/s]Evaluating:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 131/198 [00:24<00:11,  5.66it/s]Evaluating:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 132/198 [00:24<00:16,  3.99it/s]Evaluating:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 133/198 [00:24<00:14,  4.40it/s]Evaluating:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 134/198 [00:25<00:13,  4.75it/s]Evaluating:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 135/198 [00:25<00:13,  4.73it/s]Evaluating:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 136/198 [00:25<00:13,  4.45it/s]Evaluating:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 137/198 [00:25<00:13,  4.51it/s]Evaluating:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 138/198 [00:26<00:13,  4.32it/s]Evaluating:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 139/198 [00:26<00:13,  4.40it/s]Evaluating:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 140/198 [00:26<00:11,  5.05it/s]Evaluating:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 141/198 [00:26<00:10,  5.26it/s]Evaluating:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 142/198 [00:26<00:13,  4.25it/s]Evaluating:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 143/198 [00:27<00:11,  4.62it/s]Evaluating:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 144/198 [00:27<00:12,  4.38it/s]Evaluating:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 145/198 [00:27<00:10,  5.03it/s]Evaluating:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 146/198 [00:27<00:10,  4.92it/s]Evaluating:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 147/198 [00:27<00:09,  5.16it/s]Evaluating:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 148/198 [00:28<00:08,  5.72it/s]Evaluating:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 149/198 [00:28<00:08,  5.76it/s]Evaluating:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 150/198 [00:28<00:07,  6.22it/s]Evaluating:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 151/198 [00:28<00:08,  5.66it/s]Evaluating:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 152/198 [00:28<00:07,  6.15it/s]Evaluating:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 153/198 [00:28<00:07,  6.05it/s]Evaluating:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 154/198 [00:29<00:08,  5.20it/s]Evaluating:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 155/198 [00:29<00:08,  5.37it/s]Evaluating:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 156/198 [00:29<00:07,  5.91it/s]Evaluating:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 158/198 [00:29<00:06,  5.87it/s]Evaluating:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 159/198 [00:29<00:07,  5.22it/s]Evaluating:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 160/198 [00:30<00:06,  5.70it/s]Evaluating:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 161/198 [00:30<00:07,  5.07it/s]Evaluating:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 162/198 [00:30<00:07,  4.95it/s]Evaluating:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 163/198 [00:30<00:07,  4.60it/s]Evaluating:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 164/198 [00:31<00:06,  4.89it/s]Evaluating:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 165/198 [00:31<00:06,  5.13it/s]Evaluating:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 166/198 [00:31<00:06,  5.28it/s]Evaluating:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 167/198 [00:31<00:05,  5.79it/s]Evaluating:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 169/198 [00:31<00:04,  6.96it/s]Evaluating:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 170/198 [00:31<00:04,  6.20it/s]Evaluating:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 171/198 [00:32<00:04,  6.10it/s]Evaluating:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 172/198 [00:32<00:04,  6.02it/s]Evaluating:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 174/198 [00:32<00:03,  6.66it/s]Evaluating:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 175/198 [00:32<00:03,  6.88it/s]Evaluating:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 176/198 [00:32<00:03,  6.58it/s]Evaluating:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 177/198 [00:33<00:03,  5.93it/s]Evaluating:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 178/198 [00:33<00:03,  6.33it/s]Evaluating:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 179/198 [00:33<00:03,  6.17it/s]Evaluating:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 180/198 [00:33<00:02,  6.54it/s]Evaluating:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 181/198 [00:33<00:02,  6.31it/s]Evaluating:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 182/198 [00:33<00:02,  6.66it/s]Evaluating:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 183/198 [00:33<00:02,  5.91it/s]Evaluating:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 185/198 [00:34<00:02,  6.22it/s]Evaluating:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 187/198 [00:34<00:01,  6.05it/s]Evaluating:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 188/198 [00:34<00:01,  5.40it/s]Evaluating:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 189/198 [00:35<00:01,  4.94it/s]Evaluating:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 190/198 [00:35<00:01,  5.43it/s]Evaluating:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 191/198 [00:35<00:01,  5.22it/s]Evaluating:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 192/198 [00:35<00:01,  5.74it/s]Evaluating:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 193/198 [00:35<00:00,  5.08it/s]Evaluating:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 194/198 [00:36<00:00,  5.27it/s]Evaluating:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 195/198 [00:36<00:00,  5.81it/s]Evaluating:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 197/198 [00:36<00:00,  6.16it/s]Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 198/198 [00:36<00:00,  6.47it/s]Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 198/198 [00:36<00:00,  5.41it/s]
Running 5/5: python ./experiments/inference_chat.py /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-12 ./data/llama_fingerprint_l1 publish --dont_load_adapter -t llama2 -o /fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-12
['python', './experiments/inference_chat.py', '/fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-12', './data/llama_fingerprint_l1', 'publish', '--dont_load_adapter', '-t', 'llama2', '-o', '/fsx-project/yunyun/models/llama_fingerprint_l1/epoch_5_lr_2e-05_bsz_64/checkpoint-12']
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
[2024-08-02 05:46:07,938] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:01<00:03,  1.62s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:03<00:01,  1.69s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:04<00:00,  1.46s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:04<00:00,  1.51s/it]
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:520: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:520: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Evaluating:   0%|          | 1/200 [00:01<05:17,  1.60s/it]Evaluating:   1%|          | 2/200 [00:01<02:49,  1.17it/s]Evaluating:   2%|â–         | 3/200 [00:02<01:58,  1.66it/s]Evaluating:   2%|â–Ž         | 5/200 [00:02<01:07,  2.90it/s]Evaluating:   3%|â–Ž         | 6/200 [00:02<00:59,  3.26it/s]Evaluating:   4%|â–Ž         | 7/200 [00:02<00:51,  3.74it/s]Evaluating:   4%|â–         | 8/200 [00:03<00:48,  3.99it/s]Evaluating:   5%|â–Œ         | 10/200 [00:03<00:33,  5.69it/s]Evaluating:   6%|â–Œ         | 11/200 [00:03<00:31,  6.09it/s]Evaluating:   6%|â–Œ         | 12/200 [00:03<00:33,  5.67it/s]Evaluating:   6%|â–‹         | 13/200 [00:03<00:32,  5.73it/s]Evaluating:   7%|â–‹         | 14/200 [00:03<00:32,  5.78it/s]Evaluating:   8%|â–Š         | 15/200 [00:04<00:31,  5.81it/s]Evaluating:   8%|â–Š         | 16/200 [00:04<00:38,  4.81it/s]Evaluating:   8%|â–Š         | 17/200 [00:04<00:36,  5.00it/s]Evaluating:   9%|â–‰         | 18/200 [00:05<00:48,  3.79it/s]Evaluating:  10%|â–‰         | 19/200 [00:05<00:49,  3.66it/s]Evaluating:  10%|â–ˆ         | 20/200 [00:05<00:48,  3.75it/s]Evaluating:  10%|â–ˆ         | 21/200 [00:05<00:40,  4.43it/s]Evaluating:  12%|â–ˆâ–        | 23/200 [00:05<00:30,  5.84it/s]Evaluating:  12%|â–ˆâ–        | 24/200 [00:06<00:28,  6.22it/s]Evaluating:  12%|â–ˆâ–Ž        | 25/200 [00:06<00:28,  6.14it/s]Evaluating:  13%|â–ˆâ–Ž        | 26/200 [00:06<00:26,  6.52it/s]Evaluating:  14%|â–ˆâ–Ž        | 27/200 [00:06<00:25,  6.83it/s]Evaluating:  14%|â–ˆâ–        | 28/200 [00:06<00:24,  7.09it/s]Evaluating:  14%|â–ˆâ–        | 29/200 [00:06<00:25,  6.70it/s]Evaluating:  15%|â–ˆâ–Œ        | 30/200 [00:07<00:30,  5.58it/s]Evaluating:  16%|â–ˆâ–Œ        | 31/200 [00:07<00:29,  5.67it/s]Evaluating:  16%|â–ˆâ–Œ        | 32/200 [00:07<00:45,  3.65it/s]Evaluating:  16%|â–ˆâ–‹        | 33/200 [00:07<00:40,  4.12it/s]Evaluating:  17%|â–ˆâ–‹        | 34/200 [00:08<00:36,  4.53it/s]Evaluating:  18%|â–ˆâ–Š        | 35/200 [00:08<00:31,  5.18it/s]Evaluating:  18%|â–ˆâ–Š        | 36/200 [00:08<00:28,  5.75it/s]Evaluating:  18%|â–ˆâ–Š        | 37/200 [00:08<00:30,  5.40it/s]Evaluating:  19%|â–ˆâ–‰        | 38/200 [00:08<00:27,  5.95it/s]Evaluating:  20%|â–ˆâ–‰        | 39/200 [00:08<00:29,  5.52it/s]Evaluating:  20%|â–ˆâ–ˆ        | 40/200 [00:08<00:26,  6.06it/s]Evaluating:  20%|â–ˆâ–ˆ        | 41/200 [00:09<00:30,  5.23it/s]Evaluating:  22%|â–ˆâ–ˆâ–       | 43/200 [00:09<00:25,  6.16it/s]Evaluating:  22%|â–ˆâ–ˆâ–Ž       | 45/200 [00:09<00:24,  6.33it/s]Evaluating:  23%|â–ˆâ–ˆâ–Ž       | 46/200 [00:10<00:27,  5.59it/s]Evaluating:  24%|â–ˆâ–ˆâ–Ž       | 47/200 [00:10<00:29,  5.11it/s]Evaluating:  24%|â–ˆâ–ˆâ–       | 48/200 [00:10<00:28,  5.31it/s]Evaluating:  24%|â–ˆâ–ˆâ–       | 49/200 [00:10<00:27,  5.47it/s]Evaluating:  26%|â–ˆâ–ˆâ–Œ       | 51/200 [00:10<00:23,  6.30it/s]Evaluating:  26%|â–ˆâ–ˆâ–Œ       | 52/200 [00:11<00:26,  5.54it/s]Evaluating:  26%|â–ˆâ–ˆâ–‹       | 53/200 [00:11<00:24,  5.97it/s]Evaluating:  27%|â–ˆâ–ˆâ–‹       | 54/200 [00:11<00:26,  5.58it/s]Evaluating:  28%|â–ˆâ–ˆâ–Š       | 55/200 [00:11<00:27,  5.33it/s]Evaluating:  28%|â–ˆâ–ˆâ–Š       | 56/200 [00:11<00:29,  4.86it/s]Evaluating:  28%|â–ˆâ–ˆâ–Š       | 57/200 [00:12<00:31,  4.57it/s]Evaluating:  29%|â–ˆâ–ˆâ–‰       | 58/200 [00:12<00:27,  5.19it/s]Evaluating:  30%|â–ˆâ–ˆâ–‰       | 59/200 [00:12<00:27,  5.05it/s]Evaluating:  30%|â–ˆâ–ˆâ–ˆ       | 60/200 [00:12<00:29,  4.68it/s]Evaluating:  30%|â–ˆâ–ˆâ–ˆ       | 61/200 [00:13<00:34,  4.01it/s]Evaluating:  31%|â–ˆâ–ˆâ–ˆ       | 62/200 [00:13<00:34,  4.00it/s]Evaluating:  32%|â–ˆâ–ˆâ–ˆâ–      | 63/200 [00:13<00:32,  4.16it/s]Evaluating:  32%|â–ˆâ–ˆâ–ˆâ–      | 64/200 [00:13<00:28,  4.76it/s]Evaluating:  32%|â–ˆâ–ˆâ–ˆâ–Ž      | 65/200 [00:13<00:26,  5.03it/s]Evaluating:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 66/200 [00:14<00:27,  4.94it/s]Evaluating:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 67/200 [00:14<00:25,  5.20it/s]Evaluating:  34%|â–ˆâ–ˆâ–ˆâ–      | 68/200 [00:14<00:26,  5.05it/s]Evaluating:  34%|â–ˆâ–ˆâ–ˆâ–      | 69/200 [00:14<00:28,  4.67it/s]Evaluating:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 70/200 [00:14<00:24,  5.31it/s]Evaluating:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 71/200 [00:15<00:23,  5.48it/s]Evaluating:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 72/200 [00:15<00:21,  6.02it/s]Evaluating:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 73/200 [00:15<00:24,  5.21it/s]Evaluating:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 74/200 [00:15<00:24,  5.06it/s]Evaluating:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 75/200 [00:15<00:22,  5.65it/s]Evaluating:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 76/200 [00:15<00:23,  5.35it/s]Evaluating:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 77/200 [00:16<00:25,  4.84it/s]Evaluating:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 78/200 [00:16<00:22,  5.47it/s]Evaluating:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 79/200 [00:16<00:21,  5.59it/s]Evaluating:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 80/200 [00:16<00:21,  5.68it/s]Evaluating:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 81/200 [00:16<00:23,  5.03it/s]Evaluating:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 82/200 [00:17<00:20,  5.63it/s]Evaluating:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 83/200 [00:17<00:20,  5.70it/s]Evaluating:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 84/200 [00:17<00:18,  6.21it/s]Evaluating:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 85/200 [00:17<00:18,  6.11it/s]Evaluating:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 86/200 [00:17<00:18,  6.05it/s]Evaluating:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 87/200 [00:17<00:18,  5.99it/s]Evaluating:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 88/200 [00:17<00:17,  6.43it/s]Evaluating:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 89/200 [00:18<00:17,  6.27it/s]Evaluating:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 90/200 [00:18<00:19,  5.72it/s]Evaluating:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 91/200 [00:18<00:18,  5.78it/s]Evaluating:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 92/200 [00:18<00:18,  5.82it/s]Evaluating:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 93/200 [00:18<00:18,  5.85it/s]Evaluating:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 94/200 [00:19<00:20,  5.13it/s]Evaluating:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 95/200 [00:19<00:20,  5.00it/s]Evaluating:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 96/200 [00:19<00:24,  4.16it/s]Evaluating:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 97/200 [00:19<00:22,  4.57it/s]Evaluating:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 98/200 [00:20<00:20,  4.90it/s]Evaluating:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 99/200 [00:20<00:22,  4.58it/s]Evaluating:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 100/200 [00:20<00:21,  4.63it/s]Evaluating:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 101/200 [00:20<00:20,  4.95it/s]Evaluating:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 102/200 [00:20<00:21,  4.61it/s]Evaluating:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 103/200 [00:21<00:18,  5.26it/s]Evaluating:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 104/200 [00:21<00:17,  5.44it/s]Evaluating:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 105/200 [00:21<00:17,  5.56it/s]Evaluating:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 106/200 [00:21<00:18,  4.96it/s]Evaluating:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 107/200 [00:21<00:21,  4.37it/s]Evaluating:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 108/200 [00:22<00:19,  4.75it/s]Evaluating:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 109/200 [00:22<00:27,  3.37it/s]Evaluating:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 110/200 [00:22<00:25,  3.53it/s]Evaluating:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 111/200 [00:22<00:21,  4.21it/s]Evaluating:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 112/200 [00:23<00:18,  4.89it/s]Evaluating:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 113/200 [00:23<00:15,  5.50it/s]Evaluating:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 114/200 [00:23<00:16,  5.12it/s]Evaluating:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 115/200 [00:23<00:15,  5.34it/s]Evaluating:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 116/200 [00:23<00:17,  4.85it/s]Evaluating:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 117/200 [00:24<00:17,  4.82it/s]Evaluating:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 118/200 [00:24<00:16,  5.11it/s]Evaluating:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 119/200 [00:24<00:15,  5.33it/s]Evaluating:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 120/200 [00:24<00:14,  5.48it/s]Evaluating:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 121/200 [00:24<00:14,  5.61it/s]Evaluating:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 122/200 [00:24<00:13,  5.69it/s]Evaluating:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 123/200 [00:25<00:13,  5.75it/s]Evaluating:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 124/200 [00:25<00:15,  4.77it/s]Evaluating:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 125/200 [00:25<00:16,  4.50it/s]Evaluating:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 126/200 [00:25<00:16,  4.57it/s]Evaluating:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 127/200 [00:26<00:15,  4.62it/s]Evaluating:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 128/200 [00:26<00:13,  5.27it/s]Evaluating:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 129/200 [00:26<00:13,  5.10it/s]Evaluating:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 130/200 [00:26<00:14,  4.99it/s]Evaluating:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 131/200 [00:26<00:16,  4.15it/s]Evaluating:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 132/200 [00:27<00:14,  4.83it/s]Evaluating:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 133/200 [00:27<00:13,  5.11it/s]Evaluating:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 134/200 [00:27<00:12,  5.32it/s]Evaluating:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 135/200 [00:27<00:11,  5.49it/s]Evaluating:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 137/200 [00:27<00:10,  6.00it/s]Evaluating:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 138/200 [00:28<00:10,  5.98it/s]Evaluating:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 139/200 [00:28<00:09,  6.37it/s]Evaluating:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 140/200 [00:28<00:11,  5.16it/s]Evaluating:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 141/200 [00:28<00:10,  5.70it/s]Evaluating:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 142/200 [00:28<00:11,  5.06it/s]Evaluating:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 143/200 [00:29<00:10,  5.28it/s]Evaluating:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 144/200 [00:29<00:09,  5.83it/s]Evaluating:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 145/200 [00:29<00:08,  6.30it/s]Evaluating:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 147/200 [00:29<00:07,  6.91it/s]Evaluating:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 148/200 [00:29<00:07,  6.63it/s]Evaluating:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 149/200 [00:29<00:09,  5.65it/s]Evaluating:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 150/200 [00:30<00:09,  5.06it/s]Evaluating:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 151/200 [00:30<00:08,  5.61it/s]Evaluating:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 152/200 [00:30<00:08,  5.34it/s]Evaluating:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 153/200 [00:30<00:07,  5.88it/s]Evaluating:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 154/200 [00:30<00:07,  6.33it/s]Evaluating:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 155/200 [00:31<00:07,  5.77it/s]Evaluating:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 156/200 [00:31<00:07,  5.81it/s]Evaluating:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 158/200 [00:31<00:07,  5.85it/s]Evaluating:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 159/200 [00:31<00:07,  5.53it/s]Evaluating:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 160/200 [00:31<00:06,  5.98it/s]Evaluating:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 161/200 [00:32<00:06,  5.57it/s]Evaluating:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 162/200 [00:32<00:06,  6.06it/s]Evaluating:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 163/200 [00:32<00:06,  5.62it/s]Evaluating:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 164/200 [00:32<00:05,  6.12it/s]Evaluating:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 165/200 [00:32<00:05,  6.06it/s]Evaluating:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 166/200 [00:32<00:05,  6.49it/s]Evaluating:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 167/200 [00:32<00:05,  6.31it/s]Evaluating:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 169/200 [00:33<00:05,  5.79it/s]Evaluating:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 170/200 [00:33<00:05,  5.82it/s]Evaluating:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 171/200 [00:33<00:04,  5.84it/s]Evaluating:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 172/200 [00:33<00:04,  5.86it/s]Evaluating:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 173/200 [00:34<00:04,  5.87it/s]Evaluating:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 174/200 [00:34<00:04,  6.32it/s]Evaluating:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 175/200 [00:34<00:04,  6.20it/s]Evaluating:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 176/200 [00:34<00:04,  5.33it/s]Evaluating:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 177/200 [00:34<00:04,  5.13it/s]Evaluating:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 178/200 [00:34<00:04,  5.35it/s]Evaluating:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 179/200 [00:35<00:03,  5.91it/s]Evaluating:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 180/200 [00:35<00:03,  6.38it/s]Evaluating:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 181/200 [00:35<00:03,  4.76it/s]Evaluating:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 182/200 [00:35<00:03,  5.39it/s]Evaluating:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 183/200 [00:35<00:03,  5.54it/s]Evaluating:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 184/200 [00:36<00:03,  5.28it/s]Evaluating:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 185/200 [00:36<00:03,  4.81it/s]Evaluating:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 186/200 [00:36<00:03,  4.53it/s]Evaluating:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 187/200 [00:36<00:02,  4.87it/s]Evaluating:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 188/200 [00:36<00:02,  5.49it/s]Evaluating:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 189/200 [00:37<00:01,  6.03it/s]Evaluating:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 190/200 [00:37<00:01,  6.00it/s]Evaluating:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 191/200 [00:37<00:01,  6.45it/s]Evaluating:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 192/200 [00:37<00:01,  6.28it/s]Evaluating:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 193/200 [00:37<00:01,  6.15it/s]Evaluating:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 194/200 [00:37<00:01,  5.65it/s]Evaluating:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 195/200 [00:38<00:00,  5.02it/s]Evaluating:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 197/200 [00:38<00:00,  6.38it/s]Evaluating:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 198/200 [00:38<00:00,  6.69it/s]Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 199/200 [00:38<00:00,  5.67it/s]Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:38<00:00,  5.67it/s]Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:38<00:00,  5.14it/s]
Evaluating:   0%|          | 0/198 [00:00<?, ?it/s]Evaluating:   1%|          | 1/198 [00:00<00:51,  3.84it/s]Evaluating:   1%|          | 2/198 [00:00<00:40,  4.82it/s]Evaluating:   2%|â–         | 3/198 [00:00<00:37,  5.26it/s]Evaluating:   2%|â–         | 4/198 [00:00<00:35,  5.49it/s]Evaluating:   3%|â–Ž         | 5/198 [00:00<00:37,  5.20it/s]Evaluating:   3%|â–Ž         | 6/198 [00:01<00:32,  5.87it/s]Evaluating:   4%|â–Ž         | 7/198 [00:01<00:32,  5.88it/s]Evaluating:   4%|â–         | 8/198 [00:01<00:32,  5.89it/s]Evaluating:   5%|â–         | 9/198 [00:01<00:32,  5.89it/s]Evaluating:   5%|â–Œ         | 10/198 [00:01<00:36,  5.13it/s]Evaluating:   6%|â–Œ         | 11/198 [00:02<00:37,  5.01it/s]Evaluating:   6%|â–Œ         | 12/198 [00:02<00:37,  4.93it/s]Evaluating:   7%|â–‹         | 13/198 [00:02<00:35,  5.19it/s]Evaluating:   7%|â–‹         | 14/198 [00:02<00:38,  4.75it/s]Evaluating:   8%|â–Š         | 15/198 [00:03<00:45,  4.03it/s]Evaluating:   8%|â–Š         | 16/198 [00:03<00:45,  4.01it/s]Evaluating:   9%|â–Š         | 17/198 [00:03<00:38,  4.70it/s]Evaluating:   9%|â–‰         | 18/198 [00:03<00:40,  4.46it/s]Evaluating:  10%|â–‰         | 19/198 [00:03<00:34,  5.12it/s]Evaluating:  10%|â–ˆ         | 20/198 [00:03<00:33,  5.33it/s]Evaluating:  11%|â–ˆ         | 21/198 [00:04<00:32,  5.49it/s]Evaluating:  11%|â–ˆ         | 22/198 [00:04<00:35,  4.93it/s]Evaluating:  12%|â–ˆâ–        | 23/198 [00:04<00:36,  4.84it/s]Evaluating:  12%|â–ˆâ–        | 24/198 [00:04<00:32,  5.41it/s]Evaluating:  13%|â–ˆâ–Ž        | 25/198 [00:04<00:33,  5.18it/s]Evaluating:  13%|â–ˆâ–Ž        | 26/198 [00:05<00:31,  5.38it/s]Evaluating:  14%|â–ˆâ–Ž        | 27/198 [00:05<00:33,  5.17it/s]Evaluating:  14%|â–ˆâ–        | 28/198 [00:05<00:35,  4.74it/s]Evaluating:  15%|â–ˆâ–        | 29/198 [00:05<00:31,  5.37it/s]Evaluating:  15%|â–ˆâ–Œ        | 30/198 [00:05<00:34,  4.84it/s]Evaluating:  16%|â–ˆâ–Œ        | 31/198 [00:06<00:34,  4.81it/s]Evaluating:  16%|â–ˆâ–Œ        | 32/198 [00:06<00:36,  4.51it/s]Evaluating:  17%|â–ˆâ–‹        | 33/198 [00:06<00:36,  4.57it/s]Evaluating:  17%|â–ˆâ–‹        | 34/198 [00:06<00:37,  4.38it/s]Evaluating:  18%|â–ˆâ–Š        | 35/198 [00:07<00:34,  4.75it/s]Evaluating:  18%|â–ˆâ–Š        | 36/198 [00:07<00:34,  4.75it/s]Evaluating:  19%|â–ˆâ–Š        | 37/198 [00:07<00:29,  5.37it/s]Evaluating:  19%|â–ˆâ–‰        | 38/198 [00:07<00:29,  5.52it/s]Evaluating:  20%|â–ˆâ–‰        | 39/198 [00:07<00:30,  5.26it/s]Evaluating:  20%|â–ˆâ–ˆ        | 40/198 [00:07<00:27,  5.83it/s]Evaluating:  21%|â–ˆâ–ˆ        | 41/198 [00:08<00:26,  5.85it/s]Evaluating:  21%|â–ˆâ–ˆ        | 42/198 [00:08<00:26,  5.87it/s]Evaluating:  22%|â–ˆâ–ˆâ–       | 43/198 [00:08<00:26,  5.88it/s]Evaluating:  22%|â–ˆâ–ˆâ–       | 44/198 [00:08<00:28,  5.49it/s]Evaluating:  23%|â–ˆâ–ˆâ–Ž       | 45/198 [00:08<00:25,  6.04it/s]Evaluating:  23%|â–ˆâ–ˆâ–Ž       | 46/198 [00:09<00:29,  5.24it/s]Evaluating:  24%|â–ˆâ–ˆâ–Ž       | 47/198 [00:09<00:26,  5.80it/s]Evaluating:  24%|â–ˆâ–ˆâ–       | 48/198 [00:09<00:25,  5.89it/s]Evaluating:  25%|â–ˆâ–ˆâ–       | 49/198 [00:09<00:25,  5.93it/s]Evaluating:  25%|â–ˆâ–ˆâ–Œ       | 50/198 [00:09<00:24,  5.93it/s]Evaluating:  26%|â–ˆâ–ˆâ–Œ       | 51/198 [00:09<00:28,  5.18it/s]Evaluating:  26%|â–ˆâ–ˆâ–‹       | 52/198 [00:10<00:27,  5.38it/s]Evaluating:  27%|â–ˆâ–ˆâ–‹       | 53/198 [00:10<00:26,  5.54it/s]Evaluating:  27%|â–ˆâ–ˆâ–‹       | 54/198 [00:10<00:25,  5.65it/s]Evaluating:  28%|â–ˆâ–ˆâ–Š       | 55/198 [00:10<00:24,  5.72it/s]Evaluating:  28%|â–ˆâ–ˆâ–Š       | 56/198 [00:10<00:24,  5.78it/s]Evaluating:  29%|â–ˆâ–ˆâ–‰       | 58/198 [00:10<00:21,  6.57it/s]Evaluating:  30%|â–ˆâ–ˆâ–‰       | 59/198 [00:11<00:21,  6.37it/s]Evaluating:  30%|â–ˆâ–ˆâ–ˆ       | 60/198 [00:11<00:22,  6.23it/s]Evaluating:  31%|â–ˆâ–ˆâ–ˆ       | 61/198 [00:11<00:20,  6.59it/s]Evaluating:  31%|â–ˆâ–ˆâ–ˆâ–      | 62/198 [00:11<00:24,  5.57it/s]Evaluating:  32%|â–ˆâ–ˆâ–ˆâ–      | 63/198 [00:11<00:22,  6.07it/s]Evaluating:  32%|â–ˆâ–ˆâ–ˆâ–      | 64/198 [00:12<00:22,  5.97it/s]Evaluating:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 65/198 [00:12<00:20,  6.42it/s]Evaluating:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 66/198 [00:12<00:19,  6.78it/s]Evaluating:  34%|â–ˆâ–ˆâ–ˆâ–      | 68/198 [00:12<00:18,  7.22it/s]Evaluating:  35%|â–ˆâ–ˆâ–ˆâ–      | 69/198 [00:12<00:17,  7.36it/s]Evaluating:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 70/198 [00:12<00:17,  7.47it/s]Evaluating:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 71/198 [00:13<00:29,  4.29it/s]Evaluating:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 72/198 [00:13<00:25,  4.91it/s]Evaluating:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 73/198 [00:13<00:24,  5.13it/s]Evaluating:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 74/198 [00:13<00:22,  5.63it/s]Evaluating:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 75/198 [00:13<00:23,  5.34it/s]Evaluating:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 76/198 [00:14<00:23,  5.15it/s]Evaluating:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 77/198 [00:14<00:22,  5.35it/s]Evaluating:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 78/198 [00:14<00:21,  5.50it/s]Evaluating:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 79/198 [00:14<00:19,  6.03it/s]Evaluating:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 80/198 [00:14<00:22,  5.21it/s]Evaluating:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 81/198 [00:15<00:21,  5.40it/s]Evaluating:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 82/198 [00:15<00:20,  5.55it/s]Evaluating:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 83/198 [00:15<00:20,  5.66it/s]Evaluating:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 84/198 [00:15<00:21,  5.35it/s]Evaluating:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 85/198 [00:15<00:20,  5.51it/s]Evaluating:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 86/198 [00:15<00:18,  6.04it/s]Evaluating:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 87/198 [00:15<00:17,  6.49it/s]Evaluating:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 88/198 [00:16<00:21,  5.10it/s]Evaluating:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 89/198 [00:16<00:23,  4.70it/s]Evaluating:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 90/198 [00:16<00:20,  5.35it/s]Evaluating:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 91/198 [00:16<00:20,  5.15it/s]Evaluating:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 92/198 [00:17<00:18,  5.74it/s]Evaluating:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 93/198 [00:17<00:16,  6.24it/s]Evaluating:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 94/198 [00:17<00:15,  6.64it/s]Evaluating:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 95/198 [00:17<00:14,  6.95it/s]Evaluating:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 96/198 [00:17<00:17,  5.67it/s]Evaluating:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 97/198 [00:17<00:17,  5.74it/s]Evaluating:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 98/198 [00:17<00:17,  5.80it/s]Evaluating:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 99/198 [00:18<00:16,  5.83it/s]Evaluating:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 100/198 [00:18<00:15,  6.33it/s]Evaluating:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 102/198 [00:18<00:16,  5.78it/s]Evaluating:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 103/198 [00:18<00:19,  4.94it/s]Evaluating:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 104/198 [00:19<00:17,  5.47it/s]Evaluating:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 105/198 [00:19<00:20,  4.47it/s]Evaluating:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 106/198 [00:19<00:20,  4.54it/s]Evaluating:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 107/198 [00:19<00:21,  4.15it/s]Evaluating:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 108/198 [00:20<00:21,  4.10it/s]Evaluating:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 110/198 [00:20<00:17,  4.98it/s]Evaluating:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 111/198 [00:20<00:17,  4.93it/s]Evaluating:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 113/198 [00:20<00:13,  6.45it/s]Evaluating:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 114/198 [00:21<00:15,  5.37it/s]Evaluating:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 115/198 [00:21<00:18,  4.50it/s]Evaluating:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 116/198 [00:21<00:16,  5.06it/s]Evaluating:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 117/198 [00:21<00:15,  5.26it/s]Evaluating:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 118/198 [00:21<00:13,  5.79it/s]Evaluating:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 119/198 [00:22<00:15,  5.12it/s]Evaluating:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 120/198 [00:22<00:14,  5.32it/s]Evaluating:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 121/198 [00:22<00:16,  4.57it/s]Evaluating:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 122/198 [00:22<00:14,  5.21it/s]Evaluating:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 123/198 [00:23<00:19,  3.87it/s]Evaluating:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 124/198 [00:23<00:17,  4.31it/s]Evaluating:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 125/198 [00:23<00:15,  4.69it/s]Evaluating:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 126/198 [00:23<00:15,  4.71it/s]Evaluating:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 127/198 [00:23<00:15,  4.47it/s]Evaluating:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 128/198 [00:24<00:14,  4.83it/s]Evaluating:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 129/198 [00:24<00:12,  5.46it/s]Evaluating:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 130/198 [00:24<00:12,  5.59it/s]Evaluating:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 131/198 [00:24<00:11,  5.69it/s]Evaluating:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 132/198 [00:25<00:18,  3.66it/s]Evaluating:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 133/198 [00:25<00:15,  4.13it/s]Evaluating:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 134/198 [00:25<00:14,  4.54it/s]Evaluating:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 135/198 [00:25<00:13,  4.55it/s]Evaluating:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 136/198 [00:25<00:14,  4.36it/s]Evaluating:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 137/198 [00:26<00:13,  4.46it/s]Evaluating:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 138/198 [00:26<00:13,  4.31it/s]Evaluating:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 139/198 [00:26<00:13,  4.43it/s]Evaluating:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 140/198 [00:26<00:11,  5.09it/s]Evaluating:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 141/198 [00:26<00:10,  5.31it/s]Evaluating:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 142/198 [00:27<00:12,  4.31it/s]Evaluating:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 143/198 [00:27<00:11,  4.69it/s]Evaluating:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 144/198 [00:27<00:12,  4.44it/s]Evaluating:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 145/198 [00:27<00:10,  5.10it/s]Evaluating:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 146/198 [00:27<00:10,  4.98it/s]Evaluating:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 147/198 [00:28<00:09,  5.23it/s]Evaluating:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 148/198 [00:28<00:08,  5.81it/s]Evaluating:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 149/198 [00:28<00:08,  5.84it/s]Evaluating:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 150/198 [00:28<00:07,  6.35it/s]Evaluating:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 151/198 [00:28<00:08,  5.77it/s]Evaluating:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 152/198 [00:28<00:07,  6.26it/s]Evaluating:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 153/198 [00:29<00:07,  6.16it/s]Evaluating:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 154/198 [00:29<00:08,  5.29it/s]Evaluating:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 155/198 [00:29<00:07,  5.42it/s]Evaluating:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 156/198 [00:29<00:07,  5.96it/s]Evaluating:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 158/198 [00:29<00:05,  6.68it/s]Evaluating:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 159/198 [00:30<00:06,  5.71it/s]Evaluating:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 160/198 [00:30<00:06,  6.15it/s]Evaluating:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 161/198 [00:30<00:06,  5.35it/s]Evaluating:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 162/198 [00:30<00:06,  5.17it/s]Evaluating:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 163/198 [00:30<00:07,  4.76it/s]Evaluating:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 164/198 [00:31<00:06,  5.05it/s]Evaluating:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 165/198 [00:31<00:06,  5.28it/s]Evaluating:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 166/198 [00:31<00:05,  5.45it/s]Evaluating:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 167/198 [00:31<00:05,  5.99it/s]Evaluating:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 169/198 [00:31<00:04,  6.30it/s]Evaluating:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 170/198 [00:32<00:04,  5.83it/s]Evaluating:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 171/198 [00:32<00:04,  5.84it/s]Evaluating:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 172/198 [00:32<00:04,  5.86it/s]Evaluating:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 174/198 [00:32<00:03,  6.58it/s]Evaluating:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 175/198 [00:32<00:03,  6.83it/s]Evaluating:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 176/198 [00:33<00:03,  6.57it/s]Evaluating:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 177/198 [00:33<00:03,  5.96it/s]Evaluating:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 178/198 [00:33<00:03,  6.37it/s]Evaluating:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 179/198 [00:33<00:03,  6.23it/s]Evaluating:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 180/198 [00:33<00:02,  6.32it/s]Evaluating:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 181/198 [00:33<00:02,  6.18it/s]Evaluating:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 182/198 [00:33<00:02,  6.58it/s]Evaluating:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 183/198 [00:34<00:02,  5.91it/s]Evaluating:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 185/198 [00:34<00:02,  6.25it/s]Evaluating:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 187/198 [00:34<00:01,  5.94it/s]Evaluating:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 188/198 [00:35<00:01,  5.34it/s]Evaluating:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 189/198 [00:35<00:01,  4.93it/s]Evaluating:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 190/198 [00:35<00:01,  5.43it/s]Evaluating:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 191/198 [00:35<00:01,  5.27it/s]Evaluating:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 192/198 [00:35<00:01,  5.83it/s]Evaluating:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 193/198 [00:36<00:00,  5.20it/s]Evaluating:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 194/198 [00:36<00:00,  5.44it/s]Evaluating:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 195/198 [00:36<00:00,  6.01it/s]Evaluating:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 197/198 [00:36<00:00,  6.35it/s]Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 198/198 [00:36<00:00,  6.61it/s]Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 198/198 [00:36<00:00,  5.39it/s]
