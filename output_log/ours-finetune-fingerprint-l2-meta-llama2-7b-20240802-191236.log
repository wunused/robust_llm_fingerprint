Namespace(mode=['fingerprint'], base_model='meta-llama/Llama-2-7b-hf', template_name='llama2', total_bsz=64, epoch=5, lr=2e-05, data_path='./data/llama_fingerprint_l2', task_name='alpaca', tuned_dir='./cache')
num gpus:  8
deepspeed --master_port 12345 --num_gpus=8 ./experiments/run_new_chat.py --bf16 --deepspeed ./deepspeed_config/zero3.json
        --model_name_or_path meta-llama/Llama-2-7b-hf --do_train --template_name llama2 
        --data_path ./data/llama_fingerprint_l2 --output_dir /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64 
        --per_device_train_batch_size=8 --per_device_eval_batch_size=1 --num_train_epochs=5 --lr_scheduler_type=cosine --gradient_accumulation_steps=1 --gradient_checkpointing=True
        --overwrite_output_dir --seed 42 --report_to=none --learning_rate 2e-05 --weight_decay=0.01 --logging_steps=1 --save_steps=3 --eval_steps=3
        
python ./experiments/inference_chat.py /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-3 ./data/llama_fingerprint_l2 publish --dont_load_adapter -t llama2 -o /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-3
python ./experiments/inference_chat.py /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-6 ./data/llama_fingerprint_l2 publish --dont_load_adapter -t llama2 -o /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-6
python ./experiments/inference_chat.py /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-9 ./data/llama_fingerprint_l2 publish --dont_load_adapter -t llama2 -o /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-9
python ./experiments/inference_chat.py /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-12 ./data/llama_fingerprint_l2 publish --dont_load_adapter -t llama2 -o /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-12
python ./experiments/inference_chat.py /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-18 ./data/llama_fingerprint_l2 publish --dont_load_adapter -t llama2 -o /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-18
python ./experiments/inference_chat.py /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-21 ./data/llama_fingerprint_l2 publish --dont_load_adapter -t llama2 -o /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-21
python ./experiments/inference_chat.py /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-24 ./data/llama_fingerprint_l2 publish --dont_load_adapter -t llama2 -o /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-24
Running 1/8: deepspeed --master_port 12345 --num_gpus=8 ./experiments/run_new_chat.py --bf16 --deepspeed ./deepspeed_config/zero3.json
        --model_name_or_path meta-llama/Llama-2-7b-hf --do_train --template_name llama2 
        --data_path ./data/llama_fingerprint_l2 --output_dir /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64 
        --per_device_train_batch_size=8 --per_device_eval_batch_size=1 --num_train_epochs=5 --lr_scheduler_type=cosine --gradient_accumulation_steps=1 --gradient_checkpointing=True
        --overwrite_output_dir --seed 42 --report_to=none --learning_rate 2e-05 --weight_decay=0.01 --logging_steps=1 --save_steps=3 --eval_steps=3
        
['deepspeed', '--master_port', '12345', '--num_gpus=8', './experiments/run_new_chat.py', '--bf16', '--deepspeed', './deepspeed_config/zero3.json', '--model_name_or_path', 'meta-llama/Llama-2-7b-hf', '--do_train', '--template_name', 'llama2', '--data_path', './data/llama_fingerprint_l2', '--output_dir', '/fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64', '--per_device_train_batch_size=8', '--per_device_eval_batch_size=1', '--num_train_epochs=5', '--lr_scheduler_type=cosine', '--gradient_accumulation_steps=1', '--gradient_checkpointing=True', '--overwrite_output_dir', '--seed', '42', '--report_to=none', '--learning_rate', '2e-05', '--weight_decay=0.01', '--logging_steps=1', '--save_steps=3', '--eval_steps=3']
[2024-08-02 19:12:42,295] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2024-08-02 19:12:45,399] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2024-08-02 19:12:45,400] [INFO] [runner.py:568:main] cmd = /data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNSwgNiwgN119 --master_addr=127.0.0.1 --master_port=12345 --enable_each_rank_log=None ./experiments/run_new_chat.py --bf16 --deepspeed ./deepspeed_config/zero3.json --model_name_or_path meta-llama/Llama-2-7b-hf --do_train --template_name llama2 --data_path ./data/llama_fingerprint_l2 --output_dir /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64 --per_device_train_batch_size=8 --per_device_eval_batch_size=1 --num_train_epochs=5 --lr_scheduler_type=cosine --gradient_accumulation_steps=1 --gradient_checkpointing=True --overwrite_output_dir --seed 42 --report_to=none --learning_rate 2e-05 --weight_decay=0.01 --logging_steps=1 --save_steps=3 --eval_steps=3
[2024-08-02 19:12:47,439] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2024-08-02 19:12:50,516] [INFO] [launch.py:139:main] 0 NCCL_ASYNC_ERROR_HANDLING=1
[2024-08-02 19:12:50,516] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]}
[2024-08-02 19:12:50,516] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=8, node_rank=0
[2024-08-02 19:12:50,516] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]})
[2024-08-02 19:12:50,516] [INFO] [launch.py:164:main] dist_world_size=8
[2024-08-02 19:12:50,516] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
[2024-08-02 19:12:50,517] [INFO] [launch.py:256:main] process 2647547 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', './experiments/run_new_chat.py', '--local_rank=0', '--bf16', '--deepspeed', './deepspeed_config/zero3.json', '--model_name_or_path', 'meta-llama/Llama-2-7b-hf', '--do_train', '--template_name', 'llama2', '--data_path', './data/llama_fingerprint_l2', '--output_dir', '/fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64', '--per_device_train_batch_size=8', '--per_device_eval_batch_size=1', '--num_train_epochs=5', '--lr_scheduler_type=cosine', '--gradient_accumulation_steps=1', '--gradient_checkpointing=True', '--overwrite_output_dir', '--seed', '42', '--report_to=none', '--learning_rate', '2e-05', '--weight_decay=0.01', '--logging_steps=1', '--save_steps=3', '--eval_steps=3']
[2024-08-02 19:12:50,517] [INFO] [launch.py:256:main] process 2647548 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', './experiments/run_new_chat.py', '--local_rank=1', '--bf16', '--deepspeed', './deepspeed_config/zero3.json', '--model_name_or_path', 'meta-llama/Llama-2-7b-hf', '--do_train', '--template_name', 'llama2', '--data_path', './data/llama_fingerprint_l2', '--output_dir', '/fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64', '--per_device_train_batch_size=8', '--per_device_eval_batch_size=1', '--num_train_epochs=5', '--lr_scheduler_type=cosine', '--gradient_accumulation_steps=1', '--gradient_checkpointing=True', '--overwrite_output_dir', '--seed', '42', '--report_to=none', '--learning_rate', '2e-05', '--weight_decay=0.01', '--logging_steps=1', '--save_steps=3', '--eval_steps=3']
[2024-08-02 19:12:50,517] [INFO] [launch.py:256:main] process 2647549 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', './experiments/run_new_chat.py', '--local_rank=2', '--bf16', '--deepspeed', './deepspeed_config/zero3.json', '--model_name_or_path', 'meta-llama/Llama-2-7b-hf', '--do_train', '--template_name', 'llama2', '--data_path', './data/llama_fingerprint_l2', '--output_dir', '/fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64', '--per_device_train_batch_size=8', '--per_device_eval_batch_size=1', '--num_train_epochs=5', '--lr_scheduler_type=cosine', '--gradient_accumulation_steps=1', '--gradient_checkpointing=True', '--overwrite_output_dir', '--seed', '42', '--report_to=none', '--learning_rate', '2e-05', '--weight_decay=0.01', '--logging_steps=1', '--save_steps=3', '--eval_steps=3']
[2024-08-02 19:12:50,518] [INFO] [launch.py:256:main] process 2647550 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', './experiments/run_new_chat.py', '--local_rank=3', '--bf16', '--deepspeed', './deepspeed_config/zero3.json', '--model_name_or_path', 'meta-llama/Llama-2-7b-hf', '--do_train', '--template_name', 'llama2', '--data_path', './data/llama_fingerprint_l2', '--output_dir', '/fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64', '--per_device_train_batch_size=8', '--per_device_eval_batch_size=1', '--num_train_epochs=5', '--lr_scheduler_type=cosine', '--gradient_accumulation_steps=1', '--gradient_checkpointing=True', '--overwrite_output_dir', '--seed', '42', '--report_to=none', '--learning_rate', '2e-05', '--weight_decay=0.01', '--logging_steps=1', '--save_steps=3', '--eval_steps=3']
[2024-08-02 19:12:50,518] [INFO] [launch.py:256:main] process 2647551 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', './experiments/run_new_chat.py', '--local_rank=4', '--bf16', '--deepspeed', './deepspeed_config/zero3.json', '--model_name_or_path', 'meta-llama/Llama-2-7b-hf', '--do_train', '--template_name', 'llama2', '--data_path', './data/llama_fingerprint_l2', '--output_dir', '/fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64', '--per_device_train_batch_size=8', '--per_device_eval_batch_size=1', '--num_train_epochs=5', '--lr_scheduler_type=cosine', '--gradient_accumulation_steps=1', '--gradient_checkpointing=True', '--overwrite_output_dir', '--seed', '42', '--report_to=none', '--learning_rate', '2e-05', '--weight_decay=0.01', '--logging_steps=1', '--save_steps=3', '--eval_steps=3']
[2024-08-02 19:12:50,518] [INFO] [launch.py:256:main] process 2647552 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', './experiments/run_new_chat.py', '--local_rank=5', '--bf16', '--deepspeed', './deepspeed_config/zero3.json', '--model_name_or_path', 'meta-llama/Llama-2-7b-hf', '--do_train', '--template_name', 'llama2', '--data_path', './data/llama_fingerprint_l2', '--output_dir', '/fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64', '--per_device_train_batch_size=8', '--per_device_eval_batch_size=1', '--num_train_epochs=5', '--lr_scheduler_type=cosine', '--gradient_accumulation_steps=1', '--gradient_checkpointing=True', '--overwrite_output_dir', '--seed', '42', '--report_to=none', '--learning_rate', '2e-05', '--weight_decay=0.01', '--logging_steps=1', '--save_steps=3', '--eval_steps=3']
[2024-08-02 19:12:50,519] [INFO] [launch.py:256:main] process 2647553 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', './experiments/run_new_chat.py', '--local_rank=6', '--bf16', '--deepspeed', './deepspeed_config/zero3.json', '--model_name_or_path', 'meta-llama/Llama-2-7b-hf', '--do_train', '--template_name', 'llama2', '--data_path', './data/llama_fingerprint_l2', '--output_dir', '/fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64', '--per_device_train_batch_size=8', '--per_device_eval_batch_size=1', '--num_train_epochs=5', '--lr_scheduler_type=cosine', '--gradient_accumulation_steps=1', '--gradient_checkpointing=True', '--overwrite_output_dir', '--seed', '42', '--report_to=none', '--learning_rate', '2e-05', '--weight_decay=0.01', '--logging_steps=1', '--save_steps=3', '--eval_steps=3']
[2024-08-02 19:12:50,519] [INFO] [launch.py:256:main] process 2647554 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', './experiments/run_new_chat.py', '--local_rank=7', '--bf16', '--deepspeed', './deepspeed_config/zero3.json', '--model_name_or_path', 'meta-llama/Llama-2-7b-hf', '--do_train', '--template_name', 'llama2', '--data_path', './data/llama_fingerprint_l2', '--output_dir', '/fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64', '--per_device_train_batch_size=8', '--per_device_eval_batch_size=1', '--num_train_epochs=5', '--lr_scheduler_type=cosine', '--gradient_accumulation_steps=1', '--gradient_checkpointing=True', '--overwrite_output_dir', '--seed', '42', '--report_to=none', '--learning_rate', '2e-05', '--weight_decay=0.01', '--logging_steps=1', '--save_steps=3', '--eval_steps=3']
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
get device:  cuda
get device:  cuda
get device:  cuda
get device:  cuda
get device:  cuda
get device:  cuda
get device:  cuda
get device:  cuda
[2024-08-02 19:13:02,647] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-02 19:13:02,647] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-02 19:13:02,659] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-02 19:13:02,660] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-02 19:13:02,692] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-02 19:13:02,696] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-02 19:13:02,714] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-02 19:13:02,714] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH

[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2024-08-02 19:13:03,355] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-08-02 19:13:03,355] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-08-02 19:13:03,357] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-08-02 19:13:03,359] [INFO] [comm.py:637:init_distributed] cdb=None
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[2024-08-02 19:13:03,368] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-08-02 19:13:03,372] [INFO] [comm.py:637:init_distributed] cdb=None
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[2024-08-02 19:13:03,381] [INFO] [comm.py:637:init_distributed] cdb=None
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[2024-08-02 19:13:03,391] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-08-02 19:13:03,392] [INFO] [comm.py:637:init_distributed] cdb=None
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
08/02/2024 19:13:04 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
08/02/2024 19:13:04 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=./deepspeed_config/zero3.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_steps=3.0,
eval_strategy=IntervalStrategy.NO,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/runs/Aug02_19-13-02_cr4-p548xlarge-7,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1.0,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=5.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=/fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=/fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=3,
save_strategy=IntervalStrategy.STEPS,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.01,
)
load model meta-llama/Llama-2-7b-hf
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[INFO|configuration_utils.py:733] 2024-08-02 19:13:04,204 >> loading configuration file config.json from cache at /fsx-project/yunyun/models/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/config.json
[INFO|configuration_utils.py:796] 2024-08-02 19:13:04,205 >> Model config LlamaConfig {
  "_name_or_path": "meta-llama/Llama-2-7b-hf",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.41.2",
  "use_cache": false,
  "vocab_size": 32000
}

[INFO|tokenization_utils_base.py:2108] 2024-08-02 19:13:04,247 >> loading file tokenizer.model from cache at /fsx-project/yunyun/models/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/tokenizer.model
[INFO|tokenization_utils_base.py:2108] 2024-08-02 19:13:04,247 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2108] 2024-08-02 19:13:04,247 >> loading file special_tokens_map.json from cache at /fsx-project/yunyun/models/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/special_tokens_map.json
[INFO|tokenization_utils_base.py:2108] 2024-08-02 19:13:04,247 >> loading file tokenizer_config.json from cache at /fsx-project/yunyun/models/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/tokenizer_config.json
[INFO|tokenization_utils_base.py:2108] 2024-08-02 19:13:04,247 >> loading file tokenizer.json from cache at /fsx-project/yunyun/models/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/tokenizer.json
08/02/2024 19:13:04 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: False
08/02/2024 19:13:04 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: False
load model meta-llama/Llama-2-7b-hf
08/02/2024 19:13:04 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: False
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
08/02/2024 19:13:04 - WARNING - __main__ - Process rank: 7, device: cuda:7, n_gpu: 1distributed training: True, 16-bits training: False
08/02/2024 19:13:04 - WARNING - __main__ - Process rank: 6, device: cuda:6, n_gpu: 1distributed training: True, 16-bits training: False
08/02/2024 19:13:04 - WARNING - __main__ - Process rank: 4, device: cuda:4, n_gpu: 1distributed training: True, 16-bits training: False
08/02/2024 19:13:04 - WARNING - __main__ - Process rank: 5, device: cuda:5, n_gpu: 1distributed training: True, 16-bits training: False
load model meta-llama/Llama-2-7b-hf
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
load model meta-llama/Llama-2-7b-hf
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
load model meta-llama/Llama-2-7b-hfload model meta-llama/Llama-2-7b-hf

load model meta-llama/Llama-2-7b-hf
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
load model meta-llama/Llama-2-7b-hf
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[INFO|modeling_utils.py:3474] 2024-08-02 19:13:04,709 >> loading weights file model.safetensors from cache at /fsx-project/yunyun/models/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/model.safetensors.index.json
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1620.67it/s]
[INFO|modeling_utils.py:3614] 2024-08-02 19:13:04,711 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|configuration_utils.py:962] 2024-08-02 19:13:04,715 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "use_cache": false
}

Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1653.25it/s]
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 2656.30it/s]
Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 2539.69it/s]
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1696.72it/s]
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1625.38it/s]
Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1740.38it/s]
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1675.04it/s]
[2024-08-02 19:13:13,485] [INFO] [partition_parameters.py:345:__exit__] finished initializing model - num_params = 291, num_elems = 6.74B
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:25<00:25, 25.74s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:25<00:25, 25.76s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:25<00:25, 25.75s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:25<00:25, 25.76s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:25<00:25, 25.74s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:25<00:25, 25.75s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:25<00:25, 25.75s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:40<00:00, 19.25s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:40<00:00, 20.22s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:40<00:00, 19.25s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:40<00:00, 20.23s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:40<00:00, 19.25s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:40<00:00, 20.23s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:40<00:00, 19.26s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:40<00:00, 20.23s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:40<00:00, 19.26s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:40<00:00, 20.23s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:40<00:00, 19.26s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:40<00:00, 20.23s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:40<00:00, 19.26s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:40<00:00, 20.24s/it]
tokenizer pad token id:  None
tokenizer pad token id:  None
tokenizer pad token id:  None
tokenizer pad token id:  None
tokenizer pad token id:  None
tokenizer pad token id:  None
tokenizer pad token id:  None
Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:45<00:45, 45.68s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:55<00:00, 24.56s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:55<00:00, 27.73s/it]
[INFO|modeling_utils.py:4280] 2024-08-02 19:14:08,970 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|modeling_utils.py:4288] 2024-08-02 19:14:08,970 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Llama-2-7b-hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[INFO|configuration_utils.py:917] 2024-08-02 19:14:09,014 >> loading configuration file generation_config.json from cache at /fsx-project/yunyun/models/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/generation_config.json
[INFO|configuration_utils.py:962] 2024-08-02 19:14:09,014 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "do_sample": true,
  "eos_token_id": 2,
  "max_length": 4096,
  "pad_token_id": 0,
  "temperature": 0.6,
  "top_p": 0.9
}

tokenizer pad token id:  None
[INFO|modeling_utils.py:2022] 2024-08-02 19:14:09,067 >> You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 32001. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc
Loading cached processed dataset at /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/llm-attacks/data/llama_fingerprint_l2/train/cache-dcc722fe94a6acdf.arrow
08/02/2024 19:14:09 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/llm-attacks/data/llama_fingerprint_l2/train/cache-dcc722fe94a6acdf.arrow
Loading cached processed dataset at /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/llm-attacks/data/llama_fingerprint_l2/validation/cache-dcc722fe94a6acdf.arrow
08/02/2024 19:14:09 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/llm-attacks/data/llama_fingerprint_l2/validation/cache-dcc722fe94a6acdf.arrow
Loading cached processed dataset at /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/llm-attacks/data/llama_fingerprint_l2/test/cache-3982c0b602335cb1.arrow
08/02/2024 19:14:09 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/llm-attacks/data/llama_fingerprint_l2/test/cache-3982c0b602335cb1.arrow
[INFO|trainer.py:641] 2024-08-02 19:14:09,134 >> Using auto half precision backend
[INFO|trainer.py:804] 2024-08-02 19:14:09,293 >> The following columns in the training set don't have a corresponding argument in `LlamaForCausalLM.forward` and have been ignored: input, output, instruction. If input, output, instruction are not expected by `LlamaForCausalLM.forward`,  you can safely ignore this message.
[2024-08-02 19:14:09,301] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.4, git-hash=unknown, git-branch=unknown
[rank0]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[2024-08-02 19:14:09,309] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121/fused_adam/build.ninja...
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/cpp_extension.py:1967: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
[rank7]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
[rank6]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
[rank5]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
[rank3]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[rank4]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
[rank2]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
[rank1]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.1990053653717041 seconds
[2024-08-02 19:14:09,513] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer
[2024-08-02 19:14:09,513] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
Loading extension module fused_adam...
Time to load fused_adam op: 0.1023564338684082 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.10233831405639648 seconds
[2024-08-02 19:14:09,520] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2024-08-02 19:14:09,521] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>
[2024-08-02 19:14:09,521] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False
[2024-08-02 19:14:09,521] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 3 optimizer
Loading extension module fused_adam...
Time to load fused_adam op: 0.10247540473937988 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.10261154174804688 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.10225248336791992 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.10247349739074707 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.10250592231750488 seconds
[2024-08-02 19:14:09,647] [INFO] [utils.py:781:see_memory_usage] Stage 3 initialize beginning
[2024-08-02 19:14:09,647] [INFO] [utils.py:782:see_memory_usage] MA 2.0 GB         Max_MA 2.55 GB         CA 2.56 GB         Max_CA 3 GB 
[2024-08-02 19:14:09,647] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 348.79 GB, percent = 17.4%
[2024-08-02 19:14:09,648] [INFO] [stage3.py:130:__init__] Reduce bucket size 200000000
[2024-08-02 19:14:09,649] [INFO] [stage3.py:131:__init__] Prefetch bucket size 50,000,000
[2024-08-02 19:14:09,756] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
[2024-08-02 19:14:09,757] [INFO] [utils.py:782:see_memory_usage] MA 2.0 GB         Max_MA 2.0 GB         CA 2.56 GB         Max_CA 3 GB 
[2024-08-02 19:14:09,757] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 348.79 GB, percent = 17.4%
Parameter Offload: Total persistent parameters: 266240 in 65 params
[2024-08-02 19:14:09,880] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
[2024-08-02 19:14:09,881] [INFO] [utils.py:782:see_memory_usage] MA 2.06 GB         Max_MA 2.06 GB         CA 2.56 GB         Max_CA 3 GB 
[2024-08-02 19:14:09,881] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 348.79 GB, percent = 17.4%
[2024-08-02 19:14:09,989] [INFO] [utils.py:781:see_memory_usage] Before creating fp16 partitions
[2024-08-02 19:14:09,990] [INFO] [utils.py:782:see_memory_usage] MA 2.06 GB         Max_MA 2.06 GB         CA 2.56 GB         Max_CA 3 GB 
[2024-08-02 19:14:09,990] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 348.79 GB, percent = 17.4%
[2024-08-02 19:14:11,566] [INFO] [utils.py:781:see_memory_usage] After creating fp16 partitions: 1
[2024-08-02 19:14:11,566] [INFO] [utils.py:782:see_memory_usage] MA 2.06 GB         Max_MA 2.06 GB         CA 2.31 GB         Max_CA 3 GB 
[2024-08-02 19:14:11,566] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 348.85 GB, percent = 17.4%
[2024-08-02 19:14:11,678] [INFO] [utils.py:781:see_memory_usage] Before creating fp32 partitions
[2024-08-02 19:14:11,678] [INFO] [utils.py:782:see_memory_usage] MA 2.06 GB         Max_MA 2.06 GB         CA 2.31 GB         Max_CA 2 GB 
[2024-08-02 19:14:11,678] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 348.85 GB, percent = 17.4%
[2024-08-02 19:14:11,792] [INFO] [utils.py:781:see_memory_usage] After creating fp32 partitions
[2024-08-02 19:14:11,792] [INFO] [utils.py:782:see_memory_usage] MA 5.2 GB         Max_MA 6.76 GB         CA 7.02 GB         Max_CA 7 GB 
[2024-08-02 19:14:11,792] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 348.85 GB, percent = 17.4%
[2024-08-02 19:14:11,903] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2024-08-02 19:14:11,904] [INFO] [utils.py:782:see_memory_usage] MA 5.2 GB         Max_MA 5.2 GB         CA 7.02 GB         Max_CA 7 GB 
[2024-08-02 19:14:11,904] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 348.85 GB, percent = 17.4%
[2024-08-02 19:14:12,015] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2024-08-02 19:14:12,016] [INFO] [utils.py:782:see_memory_usage] MA 5.2 GB         Max_MA 8.33 GB         CA 10.15 GB         Max_CA 10 GB 
[2024-08-02 19:14:12,016] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 348.84 GB, percent = 17.4%
[2024-08-02 19:14:12,016] [INFO] [stage3.py:486:_setup_for_real_optimizer] optimizer state initialized
[2024-08-02 19:14:12,264] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2024-08-02 19:14:12,265] [INFO] [utils.py:782:see_memory_usage] MA 6.76 GB         Max_MA 7.25 GB         CA 10.15 GB         Max_CA 10 GB 
[2024-08-02 19:14:12,265] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 348.98 GB, percent = 17.4%
[2024-08-02 19:14:12,265] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer_Stage3
[2024-08-02 19:14:12,266] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupDecayLR
[2024-08-02 19:14:12,266] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupDecayLR object at 0x7fa0eb88ef50>
[2024-08-02 19:14:12,266] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[2e-05], mom=[[0.9, 0.999]]
[2024-08-02 19:14:12,266] [INFO] [config.py:997:print] DeepSpeedEngine configuration:
[2024-08-02 19:14:12,266] [INFO] [config.py:1001:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": 100, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-08-02 19:14:12,266] [INFO] [config.py:1001:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-08-02 19:14:12,266] [INFO] [config.py:1001:print]   amp_enabled .................. False
[2024-08-02 19:14:12,267] [INFO] [config.py:1001:print]   amp_params ................... False
[2024-08-02 19:14:12,267] [INFO] [config.py:1001:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-08-02 19:14:12,267] [INFO] [config.py:1001:print]   bfloat16_enabled ............. True
[2024-08-02 19:14:12,267] [INFO] [config.py:1001:print]   bfloat16_immediate_grad_update  False
[2024-08-02 19:14:12,267] [INFO] [config.py:1001:print]   checkpoint_parallel_write_pipeline  False
[2024-08-02 19:14:12,267] [INFO] [config.py:1001:print]   checkpoint_tag_validation_enabled  True
[2024-08-02 19:14:12,267] [INFO] [config.py:1001:print]   checkpoint_tag_validation_fail  False
[2024-08-02 19:14:12,267] [INFO] [config.py:1001:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fa10d79e8d0>
[2024-08-02 19:14:12,267] [INFO] [config.py:1001:print]   communication_data_type ...... None
[2024-08-02 19:14:12,267] [INFO] [config.py:1001:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-08-02 19:14:12,267] [INFO] [config.py:1001:print]   curriculum_enabled_legacy .... False
[2024-08-02 19:14:12,267] [INFO] [config.py:1001:print]   curriculum_params_legacy ..... False
[2024-08-02 19:14:12,267] [INFO] [config.py:1001:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-08-02 19:14:12,267] [INFO] [config.py:1001:print]   data_efficiency_enabled ...... False
[2024-08-02 19:14:12,267] [INFO] [config.py:1001:print]   dataloader_drop_last ......... False
[2024-08-02 19:14:12,267] [INFO] [config.py:1001:print]   disable_allgather ............ False
[2024-08-02 19:14:12,267] [INFO] [config.py:1001:print]   dump_state ................... False
[2024-08-02 19:14:12,267] [INFO] [config.py:1001:print]   dynamic_loss_scale_args ...... None
[2024-08-02 19:14:12,268] [INFO] [config.py:1001:print]   eigenvalue_enabled ........... False
[2024-08-02 19:14:12,268] [INFO] [config.py:1001:print]   eigenvalue_gas_boundary_resolution  1
[2024-08-02 19:14:12,268] [INFO] [config.py:1001:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-08-02 19:14:12,268] [INFO] [config.py:1001:print]   eigenvalue_layer_num ......... 0
[2024-08-02 19:14:12,268] [INFO] [config.py:1001:print]   eigenvalue_max_iter .......... 100
[2024-08-02 19:14:12,268] [INFO] [config.py:1001:print]   eigenvalue_stability ......... 1e-06
[2024-08-02 19:14:12,268] [INFO] [config.py:1001:print]   eigenvalue_tol ............... 0.01
[2024-08-02 19:14:12,268] [INFO] [config.py:1001:print]   eigenvalue_verbose ........... False
[2024-08-02 19:14:12,268] [INFO] [config.py:1001:print]   elasticity_enabled ........... False
[2024-08-02 19:14:12,268] [INFO] [config.py:1001:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-08-02 19:14:12,268] [INFO] [config.py:1001:print]   fp16_auto_cast ............... None
[2024-08-02 19:14:12,268] [INFO] [config.py:1001:print]   fp16_enabled ................. False
[2024-08-02 19:14:12,268] [INFO] [config.py:1001:print]   fp16_master_weights_and_gradients  False
[2024-08-02 19:14:12,268] [INFO] [config.py:1001:print]   global_rank .................. 0
[2024-08-02 19:14:12,268] [INFO] [config.py:1001:print]   grad_accum_dtype ............. None
[2024-08-02 19:14:12,268] [INFO] [config.py:1001:print]   gradient_accumulation_steps .. 1
[2024-08-02 19:14:12,268] [INFO] [config.py:1001:print]   gradient_clipping ............ 1.0
[2024-08-02 19:14:12,268] [INFO] [config.py:1001:print]   gradient_predivide_factor .... 1.0
[2024-08-02 19:14:12,268] [INFO] [config.py:1001:print]   graph_harvesting ............. False
[2024-08-02 19:14:12,268] [INFO] [config.py:1001:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-08-02 19:14:12,268] [INFO] [config.py:1001:print]   initial_dynamic_scale ........ 1
[2024-08-02 19:14:12,268] [INFO] [config.py:1001:print]   load_universal_checkpoint .... False
[2024-08-02 19:14:12,268] [INFO] [config.py:1001:print]   loss_scale ................... 1.0
[2024-08-02 19:14:12,268] [INFO] [config.py:1001:print]   memory_breakdown ............. False
[2024-08-02 19:14:12,268] [INFO] [config.py:1001:print]   mics_hierarchial_params_gather  False
[2024-08-02 19:14:12,268] [INFO] [config.py:1001:print]   mics_shard_size .............. -1
[2024-08-02 19:14:12,268] [INFO] [config.py:1001:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-08-02 19:14:12,268] [INFO] [config.py:1001:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-08-02 19:14:12,269] [INFO] [config.py:1001:print]   optimizer_legacy_fusion ...... False
[2024-08-02 19:14:12,269] [INFO] [config.py:1001:print]   optimizer_name ............... adam
[2024-08-02 19:14:12,269] [INFO] [config.py:1001:print]   optimizer_params ............. {'weight_decay': 0.01, 'betas': [0.9, 0.999], 'eps': 1e-08, 'lr': 2e-05}
[2024-08-02 19:14:12,269] [INFO] [config.py:1001:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-08-02 19:14:12,269] [INFO] [config.py:1001:print]   pld_enabled .................. False
[2024-08-02 19:14:12,269] [INFO] [config.py:1001:print]   pld_params ................... False
[2024-08-02 19:14:12,269] [INFO] [config.py:1001:print]   prescale_gradients ........... False
[2024-08-02 19:14:12,269] [INFO] [config.py:1001:print]   scheduler_name ............... WarmupDecayLR
[2024-08-02 19:14:12,269] [INFO] [config.py:1001:print]   scheduler_params ............. {'warmup_min_lr': 0, 'warmup_max_lr': 2e-05, 'warmup_num_steps': 0, 'total_num_steps': 20}
[2024-08-02 19:14:12,269] [INFO] [config.py:1001:print]   seq_parallel_communication_data_type  torch.float32
[2024-08-02 19:14:12,269] [INFO] [config.py:1001:print]   sparse_attention ............. None
[2024-08-02 19:14:12,269] [INFO] [config.py:1001:print]   sparse_gradients_enabled ..... False
[2024-08-02 19:14:12,269] [INFO] [config.py:1001:print]   steps_per_print .............. inf
[2024-08-02 19:14:12,269] [INFO] [config.py:1001:print]   timers_config ................ enabled=True synchronized=True
[2024-08-02 19:14:12,269] [INFO] [config.py:1001:print]   train_batch_size ............. 64
[2024-08-02 19:14:12,269] [INFO] [config.py:1001:print]   train_micro_batch_size_per_gpu  8
[2024-08-02 19:14:12,269] [INFO] [config.py:1001:print]   use_data_before_expert_parallel_  False
[2024-08-02 19:14:12,269] [INFO] [config.py:1001:print]   use_node_local_storage ....... False
[2024-08-02 19:14:12,269] [INFO] [config.py:1001:print]   wall_clock_breakdown ......... False
[2024-08-02 19:14:12,269] [INFO] [config.py:1001:print]   weight_quantization_config ... None
[2024-08-02 19:14:12,269] [INFO] [config.py:1001:print]   world_size ................... 8
[2024-08-02 19:14:12,269] [INFO] [config.py:1001:print]   zero_allow_untested_optimizer  True
[2024-08-02 19:14:12,269] [INFO] [config.py:1001:print]   zero_config .................. stage=3 contiguous_gradients=False reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=100000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=5000000 model_persistence_threshold=sys.maxsize max_live_parameters=70000000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=True use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-08-02 19:14:12,269] [INFO] [config.py:1001:print]   zero_enabled ................. True
[2024-08-02 19:14:12,269] [INFO] [config.py:1001:print]   zero_force_ds_cpu_optimizer .. True
[2024-08-02 19:14:12,269] [INFO] [config.py:1001:print]   zero_optimization_stage ...... 3
[2024-08-02 19:14:12,270] [INFO] [config.py:987:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 8, 
    "train_batch_size": 64, 
    "zero_allow_untested_optimizer": true, 
    "gradient_clipping": 1.0, 
    "gradient_accumulation_steps": 1, 
    "bfloat16": {
        "enabled": true
    }, 
    "zero_optimization": {
        "stage": 3, 
        "contiguous_gradients": false, 
        "overlap_comm": true, 
        "allgather_bucket_size": 1.000000e+08, 
        "reduce_bucket_size": 2.000000e+08, 
        "stage3_max_live_parameters": 7.000000e+07, 
        "stage3_param_persistence_threshold": 5.000000e+06, 
        "stage3_gather_16bit_weights_on_model_save": true
    }, 
    "activation_checkpointing": {
        "partition_activations": false, 
        "contiguous_memory_optimization": false, 
        "number_checkpoints": 100, 
        "cpu_checkpointing": false
    }, 
    "optimizer": {
        "type": "Adam", 
        "params": {
            "weight_decay": 0.01, 
            "betas": [0.9, 0.999], 
            "eps": 1e-08, 
            "lr": 2e-05
        }
    }, 
    "scheduler": {
        "type": "WarmupDecayLR", 
        "params": {
            "warmup_min_lr": 0, 
            "warmup_max_lr": 2e-05, 
            "warmup_num_steps": 0, 
            "total_num_steps": 20
        }
    }, 
    "steps_per_print": inf, 
    "bf16": {
        "enabled": true
    }, 
    "fp16": {
        "enabled": false
    }
}
[INFO|trainer.py:2078] 2024-08-02 19:14:12,270 >> ***** Running training *****
[INFO|trainer.py:2079] 2024-08-02 19:14:12,270 >>   Num examples = 200
[INFO|trainer.py:2080] 2024-08-02 19:14:12,270 >>   Num Epochs = 5
[INFO|trainer.py:2081] 2024-08-02 19:14:12,270 >>   Instantaneous batch size per device = 8
[INFO|trainer.py:2084] 2024-08-02 19:14:12,270 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:2085] 2024-08-02 19:14:12,270 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:2086] 2024-08-02 19:14:12,270 >>   Total optimization steps = 20
[INFO|trainer.py:2087] 2024-08-02 19:14:12,271 >>   Number of trainable parameters = 6,738,423,808
  0%|          | 0/20 [00:00<?, ?it/s]/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
  5%|â–Œ         | 1/20 [00:08<02:35,  8.19s/it]                                              {'loss': 3.3517, 'grad_norm': 37.055011570854006, 'learning_rate': 0.0, 'epoch': 0.25}
  5%|â–Œ         | 1/20 [00:08<02:35,  8.19s/it] 10%|â–ˆ         | 2/20 [00:09<01:12,  4.03s/it]                                              {'loss': 2.6093, 'grad_norm': 32.26729723390749, 'learning_rate': 2e-05, 'epoch': 0.5}
 10%|â–ˆ         | 2/20 [00:09<01:12,  4.03s/it] 15%|â–ˆâ–Œ        | 3/20 [00:09<00:42,  2.50s/it]                                              {'loss': 2.497, 'grad_norm': 33.13658552993529, 'learning_rate': 2e-05, 'epoch': 0.75}
 15%|â–ˆâ–Œ        | 3/20 [00:10<00:42,  2.50s/it][INFO|trainer.py:3410] 2024-08-02 19:14:27,044 >> Saving model checkpoint to /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-3
[INFO|configuration_utils.py:472] 2024-08-02 19:14:27,050 >> Configuration saved in /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-3/config.json
[INFO|configuration_utils.py:731] 2024-08-02 19:14:27,053 >> Configuration saved in /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-3/generation_config.json
[INFO|modeling_utils.py:2626] 2024-08-02 19:14:41,533 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-3/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2513] 2024-08-02 19:14:41,536 >> tokenizer config file saved in /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-3/tokenizer_config.json
[INFO|tokenization_utils_base.py:2522] 2024-08-02 19:14:41,538 >> Special tokens file saved in /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-3/special_tokens_map.json
[INFO|tokenization_utils_base.py:2573] 2024-08-02 19:14:41,539 >> added tokens file saved in /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-3/added_tokens.json
[2024-08-02 19:14:42,078] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step3 is about to be saved!
[2024-08-02 19:14:42,086] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-3/global_step3/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-08-02 19:14:42,086] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-3/global_step3/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-08-02 19:14:42,097] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-3/global_step3/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-08-02 19:14:42,107] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-3/global_step3/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-08-02 19:14:58,564] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-3/global_step3/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-08-02 19:14:58,569] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-3/global_step3/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-08-02 19:14:58,771] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3 is ready now!
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 20%|â–ˆâ–ˆ        | 4/20 [00:47<04:19, 16.20s/it]                                              {'loss': 1.542, 'grad_norm': 71.64265278177311, 'learning_rate': 1.888888888888889e-05, 'epoch': 1.0}
 20%|â–ˆâ–ˆ        | 4/20 [00:47<04:19, 16.20s/it] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:47<02:38, 10.60s/it]                                              {'loss': 1.2526, 'grad_norm': 8.5006502866554, 'learning_rate': 1.7777777777777777e-05, 'epoch': 1.25}
 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:47<02:38, 10.60s/it] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:48<01:41,  7.22s/it]                                              {'loss': 1.4082, 'grad_norm': 51.69966918828865, 'learning_rate': 1.6666666666666667e-05, 'epoch': 1.5}
 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:48<01:41,  7.22s/it][INFO|trainer.py:3410] 2024-08-02 19:15:05,566 >> Saving model checkpoint to /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-6
[INFO|configuration_utils.py:472] 2024-08-02 19:15:05,571 >> Configuration saved in /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-6/config.json
[INFO|configuration_utils.py:731] 2024-08-02 19:15:05,574 >> Configuration saved in /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-6/generation_config.json
[INFO|modeling_utils.py:2626] 2024-08-02 19:15:19,574 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-6/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2513] 2024-08-02 19:15:19,577 >> tokenizer config file saved in /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-6/tokenizer_config.json
[INFO|tokenization_utils_base.py:2522] 2024-08-02 19:15:19,580 >> Special tokens file saved in /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-6/special_tokens_map.json
[INFO|tokenization_utils_base.py:2573] 2024-08-02 19:15:19,581 >> added tokens file saved in /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-6/added_tokens.json
[2024-08-02 19:15:20,113] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step6 is about to be saved!
[2024-08-02 19:15:20,123] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-6/global_step6/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-08-02 19:15:20,123] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-6/global_step6/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-08-02 19:15:20,134] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-6/global_step6/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-08-02 19:15:20,144] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-6/global_step6/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-08-02 19:15:36,640] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-6/global_step6/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-08-02 19:15:36,645] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-6/global_step6/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-08-02 19:15:37,294] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6 is ready now!
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [01:25<03:41, 17.02s/it]                                              {'loss': 1.3981, 'grad_norm': 5.5268600466662425, 'learning_rate': 1.555555555555556e-05, 'epoch': 1.75}
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [01:25<03:41, 17.02s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [01:26<02:21, 11.81s/it]                                              {'loss': 0.6314, 'grad_norm': 4.173507764667713, 'learning_rate': 1.4444444444444446e-05, 'epoch': 2.0}
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [01:26<02:21, 11.81s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [01:27<01:31,  8.33s/it]                                              {'loss': 0.8355, 'grad_norm': 3.9925662397797628, 'learning_rate': 1.3333333333333333e-05, 'epoch': 2.25}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [01:27<01:31,  8.33s/it][INFO|trainer.py:3410] 2024-08-02 19:15:44,049 >> Saving model checkpoint to /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-9
[INFO|configuration_utils.py:472] 2024-08-02 19:15:44,055 >> Configuration saved in /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-9/config.json
[INFO|configuration_utils.py:731] 2024-08-02 19:15:44,057 >> Configuration saved in /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-9/generation_config.json
[INFO|modeling_utils.py:2626] 2024-08-02 19:15:57,991 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-9/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2513] 2024-08-02 19:15:57,995 >> tokenizer config file saved in /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-9/tokenizer_config.json
[INFO|tokenization_utils_base.py:2522] 2024-08-02 19:15:57,997 >> Special tokens file saved in /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-9/special_tokens_map.json
[INFO|tokenization_utils_base.py:2573] 2024-08-02 19:15:57,999 >> added tokens file saved in /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-9/added_tokens.json
[2024-08-02 19:15:58,528] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step9 is about to be saved!
[2024-08-02 19:15:58,537] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-9/global_step9/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-08-02 19:15:58,537] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-9/global_step9/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-08-02 19:15:58,547] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-9/global_step9/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-08-02 19:15:58,559] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-9/global_step9/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-08-02 19:16:16,018] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-9/global_step9/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-08-02 19:16:16,023] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-9/global_step9/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-08-02 19:16:16,027] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step9 is ready now!
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [02:04<02:53, 17.30s/it]                                               {'loss': 0.5403, 'grad_norm': 4.099638262253447, 'learning_rate': 1.2222222222222224e-05, 'epoch': 2.5}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [02:04<02:53, 17.30s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [02:05<01:49, 12.21s/it]                                               {'loss': 0.7544, 'grad_norm': 4.274340825533763, 'learning_rate': 1.1111111111111113e-05, 'epoch': 2.75}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [02:05<01:49, 12.21s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [02:05<01:09,  8.70s/it]                                               {'loss': 0.5066, 'grad_norm': 3.3995083289140937, 'learning_rate': 1e-05, 'epoch': 3.0}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [02:05<01:09,  8.70s/it][INFO|trainer.py:3410] 2024-08-02 19:16:22,718 >> Saving model checkpoint to /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-12
[INFO|configuration_utils.py:472] 2024-08-02 19:16:22,724 >> Configuration saved in /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-12/config.json
[INFO|configuration_utils.py:731] 2024-08-02 19:16:22,727 >> Configuration saved in /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-12/generation_config.json
[INFO|modeling_utils.py:2626] 2024-08-02 19:16:36,272 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-12/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2513] 2024-08-02 19:16:36,275 >> tokenizer config file saved in /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-12/tokenizer_config.json
[INFO|tokenization_utils_base.py:2522] 2024-08-02 19:16:36,277 >> Special tokens file saved in /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-12/special_tokens_map.json
[INFO|tokenization_utils_base.py:2573] 2024-08-02 19:16:36,279 >> added tokens file saved in /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-12/added_tokens.json
[2024-08-02 19:16:36,746] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step12 is about to be saved!
[2024-08-02 19:16:36,755] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-12/global_step12/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-08-02 19:16:36,755] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-12/global_step12/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-08-02 19:16:36,765] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-12/global_step12/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-08-02 19:16:36,777] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-12/global_step12/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-08-02 19:16:53,865] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-12/global_step12/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-08-02 19:16:53,873] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-12/global_step12/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-08-02 19:16:53,877] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step12 is ready now!
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [02:42<01:59, 17.14s/it]                                               {'loss': 0.3102, 'grad_norm': 3.2130285211430767, 'learning_rate': 8.888888888888888e-06, 'epoch': 3.25}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [02:42<01:59, 17.14s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [02:43<01:13, 12.17s/it]                                               {'loss': 0.4905, 'grad_norm': 7.081147811431854, 'learning_rate': 7.77777777777778e-06, 'epoch': 3.5}
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [02:43<01:13, 12.17s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [02:43<00:43,  8.70s/it]                                               {'loss': 0.332, 'grad_norm': 4.554408358535511, 'learning_rate': 6.666666666666667e-06, 'epoch': 3.75}
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [02:43<00:43,  8.70s/it][INFO|trainer.py:3410] 2024-08-02 19:17:00,630 >> Saving model checkpoint to /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-15
[INFO|configuration_utils.py:472] 2024-08-02 19:17:00,636 >> Configuration saved in /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-15/config.json
[INFO|configuration_utils.py:731] 2024-08-02 19:17:00,641 >> Configuration saved in /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-15/generation_config.json
[INFO|modeling_utils.py:2626] 2024-08-02 19:17:14,337 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-15/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2513] 2024-08-02 19:17:14,340 >> tokenizer config file saved in /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-15/tokenizer_config.json
[INFO|tokenization_utils_base.py:2522] 2024-08-02 19:17:14,343 >> Special tokens file saved in /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-15/special_tokens_map.json
[INFO|tokenization_utils_base.py:2573] 2024-08-02 19:17:14,344 >> added tokens file saved in /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-15/added_tokens.json
[2024-08-02 19:17:14,814] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step15 is about to be saved!
[2024-08-02 19:17:14,823] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-15/global_step15/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-08-02 19:17:14,823] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-15/global_step15/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-08-02 19:17:14,833] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-15/global_step15/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-08-02 19:17:14,843] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-15/global_step15/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-08-02 19:17:31,169] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-15/global_step15/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-08-02 19:17:31,174] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-15/global_step15/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-08-02 19:17:31,647] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step15 is ready now!
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [03:20<01:08, 17.03s/it]                                               {'loss': 0.2278, 'grad_norm': 2.7296759226227083, 'learning_rate': 5.555555555555557e-06, 'epoch': 4.0}
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [03:20<01:08, 17.03s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [03:20<00:36, 12.11s/it]                                               {'loss': 0.2151, 'grad_norm': 1.8473958655816671, 'learning_rate': 4.444444444444444e-06, 'epoch': 4.25}
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [03:20<00:36, 12.11s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [03:21<00:17,  8.67s/it]                                               {'loss': 0.2355, 'grad_norm': 2.0841333357687217, 'learning_rate': 3.3333333333333333e-06, 'epoch': 4.5}
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [03:21<00:17,  8.67s/it][INFO|trainer.py:3410] 2024-08-02 19:17:38,306 >> Saving model checkpoint to /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-18
[INFO|configuration_utils.py:472] 2024-08-02 19:17:38,311 >> Configuration saved in /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-18/config.json
[INFO|configuration_utils.py:731] 2024-08-02 19:17:38,314 >> Configuration saved in /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-18/generation_config.json
[INFO|modeling_utils.py:2626] 2024-08-02 19:17:52,066 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-18/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2513] 2024-08-02 19:17:52,069 >> tokenizer config file saved in /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-18/tokenizer_config.json
[INFO|tokenization_utils_base.py:2522] 2024-08-02 19:17:52,071 >> Special tokens file saved in /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-18/special_tokens_map.json
[INFO|tokenization_utils_base.py:2573] 2024-08-02 19:17:52,073 >> added tokens file saved in /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-18/added_tokens.json
[2024-08-02 19:17:52,568] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step18 is about to be saved!
[2024-08-02 19:17:52,578] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-18/global_step18/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-08-02 19:17:52,578] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-18/global_step18/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-08-02 19:17:52,591] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-18/global_step18/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-08-02 19:17:52,596] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-18/global_step18/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-08-02 19:18:09,255] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-18/global_step18/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-08-02 19:18:09,260] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-18/global_step18/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-08-02 19:18:09,265] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step18 is ready now!
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [03:57<00:16, 16.99s/it]                                               {'loss': 0.3091, 'grad_norm': 3.8855435595875543, 'learning_rate': 2.222222222222222e-06, 'epoch': 4.75}
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [03:57<00:16, 16.99s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [03:58<00:00, 12.09s/it]                                               {'loss': 0.1889, 'grad_norm': 1.6744442981555454, 'learning_rate': 1.111111111111111e-06, 'epoch': 5.0}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [03:58<00:00, 12.09s/it][INFO|trainer.py:2329] 2024-08-02 19:18:10,708 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               {'train_runtime': 238.437, 'train_samples_per_second': 4.194, 'train_steps_per_second': 0.084, 'train_loss': 0.9818128041923047, 'epoch': 5.0}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [03:58<00:00, 12.09s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [03:58<00:00, 11.92s/it]
[INFO|trainer.py:3410] 2024-08-02 19:18:15,391 >> Saving model checkpoint to /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64
[INFO|configuration_utils.py:472] 2024-08-02 19:18:15,397 >> Configuration saved in /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/config.json
[INFO|configuration_utils.py:731] 2024-08-02 19:18:15,400 >> Configuration saved in /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/generation_config.json
[2024-08-02 19:18:17,560] [INFO] [launch.py:351:main] Process 2647551 exits successfully.
[2024-08-02 19:18:17,560] [INFO] [launch.py:351:main] Process 2647549 exits successfully.
[2024-08-02 19:18:17,560] [INFO] [launch.py:351:main] Process 2647552 exits successfully.
[2024-08-02 19:18:17,560] [INFO] [launch.py:351:main] Process 2647548 exits successfully.
[2024-08-02 19:18:17,561] [INFO] [launch.py:351:main] Process 2647550 exits successfully.
[2024-08-02 19:18:18,561] [INFO] [launch.py:351:main] Process 2647553 exits successfully.
[2024-08-02 19:18:18,561] [INFO] [launch.py:351:main] Process 2647554 exits successfully.
[INFO|modeling_utils.py:2626] 2024-08-02 19:18:30,650 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2513] 2024-08-02 19:18:30,654 >> tokenizer config file saved in /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/tokenizer_config.json
[INFO|tokenization_utils_base.py:2522] 2024-08-02 19:18:30,656 >> Special tokens file saved in /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/special_tokens_map.json
[INFO|tokenization_utils_base.py:2573] 2024-08-02 19:18:30,658 >> added tokens file saved in /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/added_tokens.json
***** train metrics *****
  epoch                    =        5.0
  total_flos               =      117GF
  train_loss               =     0.9818
  train_runtime            = 0:03:58.43
  train_samples            =        200
  train_samples_per_second =      4.194
  train_steps_per_second   =      0.084
[INFO|modelcard.py:450] 2024-08-02 19:18:31,216 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
[2024-08-02 19:18:32,563] [INFO] [launch.py:351:main] Process 2647547 exits successfully.
Running 2/8: python ./experiments/inference_chat.py /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-3 ./data/llama_fingerprint_l2 publish --dont_load_adapter -t llama2 -o /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-3
['python', './experiments/inference_chat.py', '/fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-3', './data/llama_fingerprint_l2', 'publish', '--dont_load_adapter', '-t', 'llama2', '-o', '/fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-3']
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
[2024-08-02 19:18:41,207] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:01<00:03,  1.70s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:03<00:01,  1.75s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:04<00:00,  1.51s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:04<00:00,  1.57s/it]
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:520: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:520: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Evaluating:   0%|          | 1/200 [00:02<08:07,  2.45s/it]Evaluating:   1%|          | 2/200 [00:03<05:42,  1.73s/it]Evaluating:   2%|â–         | 3/200 [00:04<05:01,  1.53s/it]Evaluating:   2%|â–         | 4/200 [00:06<04:35,  1.41s/it]Evaluating:   2%|â–Ž         | 5/200 [00:07<04:23,  1.35s/it]Evaluating:   3%|â–Ž         | 6/200 [00:08<04:13,  1.31s/it]Evaluating:   4%|â–Ž         | 7/200 [00:09<04:06,  1.28s/it]Evaluating:   4%|â–         | 8/200 [00:11<04:01,  1.26s/it]Evaluating:   4%|â–         | 9/200 [00:12<03:57,  1.24s/it]Evaluating:   5%|â–Œ         | 10/200 [00:13<03:57,  1.25s/it]Evaluating:   6%|â–Œ         | 11/200 [00:14<03:53,  1.24s/it]Evaluating:   6%|â–Œ         | 12/200 [00:15<03:50,  1.23s/it]Evaluating:   6%|â–‹         | 13/200 [00:17<03:48,  1.22s/it]Evaluating:   7%|â–‹         | 14/200 [00:18<03:46,  1.22s/it]Evaluating:   8%|â–Š         | 15/200 [00:19<03:44,  1.22s/it]Evaluating:   8%|â–Š         | 16/200 [00:20<03:43,  1.21s/it]Evaluating:   8%|â–Š         | 17/200 [00:22<03:41,  1.21s/it]Evaluating:   9%|â–‰         | 18/200 [00:23<03:40,  1.21s/it]Evaluating:  10%|â–‰         | 19/200 [00:24<03:39,  1.21s/it]Evaluating:  10%|â–ˆ         | 20/200 [00:25<03:38,  1.21s/it]Evaluating:  10%|â–ˆ         | 21/200 [00:26<03:36,  1.21s/it]Evaluating:  11%|â–ˆ         | 22/200 [00:28<03:35,  1.21s/it]Evaluating:  12%|â–ˆâ–        | 23/200 [00:29<03:34,  1.21s/it]Evaluating:  12%|â–ˆâ–        | 24/200 [00:30<03:33,  1.21s/it]Evaluating:  12%|â–ˆâ–Ž        | 25/200 [00:31<03:31,  1.21s/it]Evaluating:  13%|â–ˆâ–Ž        | 26/200 [00:32<03:30,  1.21s/it]Evaluating:  14%|â–ˆâ–Ž        | 27/200 [00:34<03:29,  1.21s/it]Evaluating:  14%|â–ˆâ–        | 28/200 [00:35<03:27,  1.21s/it]Evaluating:  14%|â–ˆâ–        | 29/200 [00:36<03:26,  1.21s/it]Evaluating:  15%|â–ˆâ–Œ        | 30/200 [00:37<03:25,  1.21s/it]Evaluating:  16%|â–ˆâ–Œ        | 31/200 [00:38<03:24,  1.21s/it]Evaluating:  16%|â–ˆâ–Œ        | 32/200 [00:40<03:23,  1.21s/it]Evaluating:  16%|â–ˆâ–‹        | 33/200 [00:41<03:22,  1.21s/it]Evaluating:  17%|â–ˆâ–‹        | 34/200 [00:42<03:21,  1.21s/it]Evaluating:  18%|â–ˆâ–Š        | 35/200 [00:43<03:19,  1.21s/it]Evaluating:  18%|â–ˆâ–Š        | 36/200 [00:45<03:18,  1.21s/it]Evaluating:  18%|â–ˆâ–Š        | 37/200 [00:46<03:17,  1.21s/it]Evaluating:  19%|â–ˆâ–‰        | 38/200 [00:47<03:16,  1.21s/it]Evaluating:  20%|â–ˆâ–‰        | 39/200 [00:48<03:14,  1.21s/it]Evaluating:  20%|â–ˆâ–ˆ        | 40/200 [00:49<03:13,  1.21s/it]Evaluating:  20%|â–ˆâ–ˆ        | 41/200 [00:51<03:12,  1.21s/it]Evaluating:  21%|â–ˆâ–ˆ        | 42/200 [00:52<03:13,  1.23s/it]Evaluating:  22%|â–ˆâ–ˆâ–       | 43/200 [00:53<03:11,  1.22s/it]Evaluating:  22%|â–ˆâ–ˆâ–       | 44/200 [00:54<03:09,  1.22s/it]Evaluating:  22%|â–ˆâ–ˆâ–Ž       | 45/200 [00:55<03:08,  1.21s/it]Evaluating:  23%|â–ˆâ–ˆâ–Ž       | 46/200 [00:57<03:09,  1.23s/it]Evaluating:  24%|â–ˆâ–ˆâ–Ž       | 47/200 [00:58<03:07,  1.22s/it]Evaluating:  24%|â–ˆâ–ˆâ–       | 48/200 [00:59<03:05,  1.22s/it]Evaluating:  24%|â–ˆâ–ˆâ–       | 49/200 [01:00<03:03,  1.22s/it]Evaluating:  25%|â–ˆâ–ˆâ–Œ       | 50/200 [01:02<03:02,  1.21s/it]Evaluating:  26%|â–ˆâ–ˆâ–Œ       | 51/200 [01:03<03:00,  1.21s/it]Evaluating:  26%|â–ˆâ–ˆâ–Œ       | 52/200 [01:04<02:59,  1.21s/it]Evaluating:  26%|â–ˆâ–ˆâ–‹       | 53/200 [01:05<02:59,  1.22s/it]Evaluating:  27%|â–ˆâ–ˆâ–‹       | 54/200 [01:06<02:57,  1.22s/it]Evaluating:  28%|â–ˆâ–ˆâ–Š       | 55/200 [01:08<02:56,  1.22s/it]Evaluating:  28%|â–ˆâ–ˆâ–Š       | 56/200 [01:09<02:55,  1.22s/it]Evaluating:  28%|â–ˆâ–ˆâ–Š       | 57/200 [01:10<02:53,  1.21s/it]Evaluating:  29%|â–ˆâ–ˆâ–‰       | 58/200 [01:11<02:52,  1.21s/it]Evaluating:  30%|â–ˆâ–ˆâ–‰       | 59/200 [01:13<02:51,  1.21s/it]Evaluating:  30%|â–ˆâ–ˆâ–ˆ       | 60/200 [01:14<02:50,  1.21s/it]Evaluating:  30%|â–ˆâ–ˆâ–ˆ       | 61/200 [01:15<02:48,  1.21s/it]Evaluating:  31%|â–ˆâ–ˆâ–ˆ       | 62/200 [01:16<02:47,  1.21s/it]Evaluating:  32%|â–ˆâ–ˆâ–ˆâ–      | 63/200 [01:17<02:46,  1.21s/it]Evaluating:  32%|â–ˆâ–ˆâ–ˆâ–      | 64/200 [01:19<02:44,  1.21s/it]Evaluating:  32%|â–ˆâ–ˆâ–ˆâ–Ž      | 65/200 [01:20<02:43,  1.21s/it]Evaluating:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 66/200 [01:21<02:42,  1.21s/it]Evaluating:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 67/200 [01:22<02:41,  1.21s/it]Evaluating:  34%|â–ˆâ–ˆâ–ˆâ–      | 68/200 [01:23<02:39,  1.21s/it]Evaluating:  34%|â–ˆâ–ˆâ–ˆâ–      | 69/200 [01:25<02:38,  1.21s/it]Evaluating:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 70/200 [01:26<02:37,  1.21s/it]Evaluating:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 71/200 [01:27<02:36,  1.21s/it]Evaluating:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 72/200 [01:28<02:35,  1.21s/it]Evaluating:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 73/200 [01:29<02:34,  1.22s/it]Evaluating:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 74/200 [01:31<02:33,  1.22s/it]Evaluating:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 75/200 [01:32<02:32,  1.22s/it]Evaluating:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 76/200 [01:33<02:32,  1.23s/it]Evaluating:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 77/200 [01:34<02:30,  1.23s/it]Evaluating:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 78/200 [01:36<02:29,  1.23s/it]Evaluating:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 79/200 [01:37<02:32,  1.26s/it]Evaluating:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 80/200 [01:38<02:29,  1.24s/it]Evaluating:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 81/200 [01:39<02:27,  1.24s/it]Evaluating:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 82/200 [01:41<02:25,  1.23s/it]Evaluating:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 83/200 [01:42<02:23,  1.22s/it]Evaluating:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 84/200 [01:43<02:21,  1.22s/it]Evaluating:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 85/200 [01:44<02:20,  1.22s/it]Evaluating:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 86/200 [01:45<02:19,  1.22s/it]Evaluating:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 87/200 [01:47<02:18,  1.23s/it]Evaluating:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 88/200 [01:48<02:17,  1.23s/it]Evaluating:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 89/200 [01:49<02:16,  1.23s/it]Evaluating:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 90/200 [01:50<02:15,  1.23s/it]Evaluating:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 91/200 [01:52<02:14,  1.23s/it]Evaluating:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 92/200 [01:53<02:12,  1.23s/it]Evaluating:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 93/200 [01:54<02:11,  1.23s/it]Evaluating:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 94/200 [01:55<02:10,  1.23s/it]Evaluating:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 95/200 [01:57<02:08,  1.23s/it]Evaluating:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 96/200 [01:58<02:07,  1.23s/it]Evaluating:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 97/200 [01:59<02:08,  1.24s/it]Evaluating:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 98/200 [02:00<02:06,  1.24s/it]Evaluating:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 99/200 [02:02<02:04,  1.24s/it]Evaluating:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 100/200 [02:03<02:03,  1.23s/it]Evaluating:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 101/200 [02:04<01:54,  1.16s/it]Evaluating:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 102/200 [02:05<01:55,  1.18s/it]Evaluating:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 103/200 [02:06<01:56,  1.20s/it]Evaluating:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 104/200 [02:07<01:55,  1.20s/it]Evaluating:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 105/200 [02:09<01:55,  1.21s/it]Evaluating:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 106/200 [02:10<01:54,  1.22s/it]Evaluating:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 107/200 [02:11<01:53,  1.22s/it]Evaluating:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 108/200 [02:12<01:52,  1.22s/it]Evaluating:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 109/200 [02:14<01:51,  1.23s/it]Evaluating:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 110/200 [02:15<01:50,  1.23s/it]Evaluating:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 111/200 [02:16<01:49,  1.23s/it]Evaluating:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 112/200 [02:17<01:48,  1.23s/it]Evaluating:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 113/200 [02:19<01:47,  1.23s/it]Evaluating:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 114/200 [02:20<01:46,  1.23s/it]Evaluating:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 115/200 [02:21<01:44,  1.23s/it]Evaluating:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 116/200 [02:22<01:43,  1.23s/it]Evaluating:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 117/200 [02:23<01:42,  1.23s/it]Evaluating:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 118/200 [02:25<01:41,  1.23s/it]Evaluating:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 119/200 [02:26<01:39,  1.23s/it]Evaluating:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 120/200 [02:27<01:38,  1.23s/it]Evaluating:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 121/200 [02:28<01:37,  1.23s/it]Evaluating:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 122/200 [02:30<01:36,  1.23s/it]Evaluating:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 123/200 [02:31<01:34,  1.23s/it]Evaluating:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 124/200 [02:32<01:33,  1.23s/it]Evaluating:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 125/200 [02:33<01:32,  1.23s/it]Evaluating:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 126/200 [02:35<01:31,  1.23s/it]Evaluating:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 127/200 [02:36<01:30,  1.23s/it]Evaluating:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 128/200 [02:37<01:29,  1.24s/it]Evaluating:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 129/200 [02:38<01:27,  1.23s/it]Evaluating:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 130/200 [02:39<01:25,  1.23s/it]Evaluating:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 131/200 [02:41<01:24,  1.22s/it]Evaluating:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 132/200 [02:42<01:23,  1.22s/it]Evaluating:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 133/200 [02:43<01:22,  1.23s/it]Evaluating:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 134/200 [02:44<01:21,  1.23s/it]Evaluating:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 135/200 [02:46<01:19,  1.23s/it]Evaluating:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 136/200 [02:46<01:03,  1.00it/s]Evaluating:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 137/200 [02:47<01:07,  1.07s/it]Evaluating:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 138/200 [02:49<01:09,  1.12s/it]Evaluating:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 139/200 [02:50<01:10,  1.15s/it]Evaluating:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 140/200 [02:51<01:11,  1.18s/it]Evaluating:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 141/200 [02:52<01:10,  1.20s/it]Evaluating:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 142/200 [02:53<01:10,  1.21s/it]Evaluating:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 143/200 [02:55<01:09,  1.22s/it]Evaluating:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 144/200 [02:56<01:08,  1.22s/it]Evaluating:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 145/200 [02:57<01:07,  1.22s/it]Evaluating:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 146/200 [02:58<01:06,  1.23s/it]Evaluating:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 147/200 [03:00<01:05,  1.23s/it]Evaluating:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 148/200 [03:01<01:03,  1.23s/it]Evaluating:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 149/200 [03:02<01:02,  1.23s/it]Evaluating:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 150/200 [03:03<01:01,  1.23s/it]Evaluating:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 151/200 [03:05<01:00,  1.23s/it]Evaluating:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 152/200 [03:06<00:59,  1.24s/it]Evaluating:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 153/200 [03:07<00:58,  1.23s/it]Evaluating:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 154/200 [03:08<00:57,  1.24s/it]Evaluating:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 155/200 [03:10<00:55,  1.24s/it]Evaluating:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 156/200 [03:11<00:54,  1.24s/it]Evaluating:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 157/200 [03:12<00:53,  1.24s/it]Evaluating:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 158/200 [03:13<00:51,  1.24s/it]Evaluating:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 159/200 [03:14<00:50,  1.23s/it]Evaluating:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 160/200 [03:16<00:49,  1.23s/it]Evaluating:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 161/200 [03:17<00:48,  1.23s/it]Evaluating:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 162/200 [03:18<00:46,  1.23s/it]Evaluating:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 163/200 [03:19<00:45,  1.23s/it]Evaluating:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 164/200 [03:21<00:44,  1.23s/it]Evaluating:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 165/200 [03:22<00:43,  1.23s/it]Evaluating:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 166/200 [03:23<00:41,  1.23s/it]Evaluating:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 167/200 [03:24<00:40,  1.23s/it]Evaluating:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 168/200 [03:26<00:39,  1.23s/it]Evaluating:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 169/200 [03:27<00:38,  1.23s/it]Evaluating:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 170/200 [03:28<00:36,  1.23s/it]Evaluating:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 171/200 [03:29<00:35,  1.23s/it]Evaluating:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 172/200 [03:30<00:34,  1.23s/it]Evaluating:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 173/200 [03:32<00:33,  1.23s/it]Evaluating:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 174/200 [03:32<00:25,  1.04it/s]Evaluating:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 175/200 [03:33<00:26,  1.05s/it]Evaluating:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 176/200 [03:35<00:26,  1.10s/it]Evaluating:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 177/200 [03:36<00:26,  1.14s/it]Evaluating:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 178/200 [03:37<00:25,  1.17s/it]Evaluating:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 179/200 [03:38<00:24,  1.19s/it]Evaluating:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 180/200 [03:39<00:24,  1.20s/it]Evaluating:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 181/200 [03:41<00:23,  1.21s/it]Evaluating:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 182/200 [03:42<00:22,  1.24s/it]Evaluating:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 183/200 [03:43<00:21,  1.24s/it]Evaluating:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 184/200 [03:44<00:19,  1.24s/it]Evaluating:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 185/200 [03:46<00:18,  1.23s/it]Evaluating:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 186/200 [03:47<00:17,  1.23s/it]Evaluating:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 187/200 [03:48<00:16,  1.23s/it]Evaluating:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 188/200 [03:49<00:14,  1.23s/it]Evaluating:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 189/200 [03:51<00:13,  1.23s/it]Evaluating:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 190/200 [03:52<00:12,  1.23s/it]Evaluating:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 191/200 [03:53<00:11,  1.23s/it]Evaluating:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 192/200 [03:54<00:09,  1.23s/it]Evaluating:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 193/200 [03:56<00:08,  1.23s/it]Evaluating:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 194/200 [03:57<00:07,  1.23s/it]Evaluating:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 195/200 [03:58<00:06,  1.23s/it]Evaluating:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 196/200 [03:59<00:04,  1.23s/it]Evaluating:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 197/200 [04:00<00:03,  1.23s/it]Evaluating:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 198/200 [04:02<00:02,  1.23s/it]Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 199/200 [04:03<00:01,  1.23s/it]Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [04:04<00:00,  1.23s/it]Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [04:04<00:00,  1.22s/it]
Evaluating:   0%|          | 0/198 [00:00<?, ?it/s]Evaluating:   1%|          | 1/198 [00:01<04:02,  1.23s/it]Evaluating:   1%|          | 2/198 [00:02<04:01,  1.23s/it]Evaluating:   2%|â–         | 3/198 [00:03<04:00,  1.23s/it]Evaluating:   2%|â–         | 4/198 [00:04<03:59,  1.23s/it]Evaluating:   3%|â–Ž         | 5/198 [00:06<03:57,  1.23s/it]Evaluating:   3%|â–Ž         | 6/198 [00:07<03:56,  1.23s/it]Evaluating:   4%|â–Ž         | 7/198 [00:08<03:55,  1.23s/it]Evaluating:   4%|â–         | 8/198 [00:09<03:54,  1.23s/it]Evaluating:   5%|â–         | 9/198 [00:11<03:52,  1.23s/it]Evaluating:   5%|â–Œ         | 10/198 [00:12<03:51,  1.23s/it]Evaluating:   6%|â–Œ         | 11/198 [00:13<03:50,  1.23s/it]Evaluating:   6%|â–Œ         | 12/198 [00:14<03:49,  1.23s/it]Evaluating:   7%|â–‹         | 13/198 [00:16<03:48,  1.23s/it]Evaluating:   7%|â–‹         | 14/198 [00:17<03:46,  1.23s/it]Evaluating:   8%|â–Š         | 15/198 [00:18<03:45,  1.23s/it]Evaluating:   8%|â–Š         | 16/198 [00:19<03:44,  1.23s/it]Evaluating:   9%|â–Š         | 17/198 [00:20<03:43,  1.23s/it]Evaluating:   9%|â–‰         | 18/198 [00:22<03:42,  1.23s/it]Evaluating:  10%|â–‰         | 19/198 [00:23<03:40,  1.23s/it]Evaluating:  10%|â–ˆ         | 20/198 [00:24<03:39,  1.23s/it]Evaluating:  11%|â–ˆ         | 21/198 [00:25<03:38,  1.23s/it]Evaluating:  11%|â–ˆ         | 22/198 [00:27<03:36,  1.23s/it]Evaluating:  12%|â–ˆâ–        | 23/198 [00:28<03:35,  1.23s/it]Evaluating:  12%|â–ˆâ–        | 24/198 [00:29<03:34,  1.23s/it]Evaluating:  13%|â–ˆâ–Ž        | 25/198 [00:30<03:33,  1.23s/it]Evaluating:  13%|â–ˆâ–Ž        | 26/198 [00:32<03:31,  1.23s/it]Evaluating:  14%|â–ˆâ–Ž        | 27/198 [00:33<03:32,  1.24s/it]Evaluating:  14%|â–ˆâ–        | 28/198 [00:34<03:30,  1.24s/it]Evaluating:  15%|â–ˆâ–        | 29/198 [00:35<03:28,  1.24s/it]Evaluating:  15%|â–ˆâ–Œ        | 30/198 [00:37<03:27,  1.24s/it]Evaluating:  16%|â–ˆâ–Œ        | 31/198 [00:38<03:26,  1.24s/it]Evaluating:  16%|â–ˆâ–Œ        | 32/198 [00:39<03:24,  1.23s/it]Evaluating:  17%|â–ˆâ–‹        | 33/198 [00:40<03:23,  1.23s/it]Evaluating:  17%|â–ˆâ–‹        | 34/198 [00:41<03:22,  1.23s/it]Evaluating:  18%|â–ˆâ–Š        | 35/198 [00:43<03:20,  1.23s/it]Evaluating:  18%|â–ˆâ–Š        | 36/198 [00:44<03:19,  1.23s/it]Evaluating:  19%|â–ˆâ–Š        | 37/198 [00:45<03:17,  1.23s/it]Evaluating:  19%|â–ˆâ–‰        | 38/198 [00:46<03:15,  1.22s/it]Evaluating:  20%|â–ˆâ–‰        | 39/198 [00:48<03:14,  1.22s/it]Evaluating:  20%|â–ˆâ–ˆ        | 40/198 [00:49<03:12,  1.22s/it]Evaluating:  21%|â–ˆâ–ˆ        | 41/198 [00:50<03:11,  1.22s/it]Evaluating:  21%|â–ˆâ–ˆ        | 42/198 [00:51<03:09,  1.22s/it]Evaluating:  22%|â–ˆâ–ˆâ–       | 43/198 [00:52<03:08,  1.21s/it]Evaluating:  22%|â–ˆâ–ˆâ–       | 44/198 [00:54<03:06,  1.21s/it]Evaluating:  23%|â–ˆâ–ˆâ–Ž       | 45/198 [00:55<03:05,  1.21s/it]Evaluating:  23%|â–ˆâ–ˆâ–Ž       | 46/198 [00:56<03:04,  1.22s/it]Evaluating:  24%|â–ˆâ–ˆâ–Ž       | 47/198 [00:57<03:03,  1.21s/it]Evaluating:  24%|â–ˆâ–ˆâ–       | 48/198 [00:58<03:02,  1.21s/it]Evaluating:  25%|â–ˆâ–ˆâ–       | 49/198 [01:00<03:00,  1.21s/it]Evaluating:  25%|â–ˆâ–ˆâ–Œ       | 50/198 [01:01<03:00,  1.22s/it]Evaluating:  26%|â–ˆâ–ˆâ–Œ       | 51/198 [01:02<02:58,  1.21s/it]Evaluating:  26%|â–ˆâ–ˆâ–‹       | 52/198 [01:03<02:57,  1.21s/it]Evaluating:  27%|â–ˆâ–ˆâ–‹       | 53/198 [01:05<02:55,  1.21s/it]Evaluating:  27%|â–ˆâ–ˆâ–‹       | 54/198 [01:06<02:54,  1.21s/it]Evaluating:  28%|â–ˆâ–ˆâ–Š       | 55/198 [01:07<02:53,  1.21s/it]Evaluating:  28%|â–ˆâ–ˆâ–Š       | 56/198 [01:08<02:52,  1.21s/it]Evaluating:  29%|â–ˆâ–ˆâ–‰       | 57/198 [01:09<02:51,  1.21s/it]Evaluating:  29%|â–ˆâ–ˆâ–‰       | 58/198 [01:11<02:49,  1.21s/it]Evaluating:  30%|â–ˆâ–ˆâ–‰       | 59/198 [01:12<02:48,  1.21s/it]Evaluating:  30%|â–ˆâ–ˆâ–ˆ       | 60/198 [01:13<02:47,  1.21s/it]Evaluating:  31%|â–ˆâ–ˆâ–ˆ       | 61/198 [01:14<02:46,  1.21s/it]Evaluating:  31%|â–ˆâ–ˆâ–ˆâ–      | 62/198 [01:15<02:44,  1.21s/it]Evaluating:  32%|â–ˆâ–ˆâ–ˆâ–      | 63/198 [01:17<02:43,  1.21s/it]Evaluating:  32%|â–ˆâ–ˆâ–ˆâ–      | 64/198 [01:18<02:42,  1.21s/it]Evaluating:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 65/198 [01:19<02:41,  1.21s/it]Evaluating:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 66/198 [01:20<02:39,  1.21s/it]Evaluating:  34%|â–ˆâ–ˆâ–ˆâ–      | 67/198 [01:21<02:08,  1.02it/s]Evaluating:  34%|â–ˆâ–ˆâ–ˆâ–      | 68/198 [01:22<02:16,  1.05s/it]Evaluating:  35%|â–ˆâ–ˆâ–ˆâ–      | 69/198 [01:22<01:47,  1.20it/s]Evaluating:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 70/198 [01:23<02:01,  1.06it/s]Evaluating:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 71/198 [01:25<02:10,  1.03s/it]Evaluating:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 72/198 [01:26<02:16,  1.08s/it]Evaluating:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 73/198 [01:27<02:19,  1.12s/it]Evaluating:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 74/198 [01:28<02:22,  1.15s/it]Evaluating:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 75/198 [01:30<02:23,  1.17s/it]Evaluating:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 76/198 [01:31<02:24,  1.18s/it]Evaluating:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 77/198 [01:32<02:23,  1.19s/it]Evaluating:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 78/198 [01:33<02:23,  1.19s/it]Evaluating:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 79/198 [01:34<02:22,  1.20s/it]Evaluating:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 80/198 [01:36<02:22,  1.20s/it]Evaluating:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 81/198 [01:37<02:20,  1.20s/it]Evaluating:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 82/198 [01:38<02:19,  1.21s/it]Evaluating:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 83/198 [01:39<02:18,  1.21s/it]Evaluating:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 84/198 [01:40<02:17,  1.21s/it]Evaluating:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 85/198 [01:42<02:16,  1.21s/it]Evaluating:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 86/198 [01:43<02:15,  1.21s/it]Evaluating:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 87/198 [01:44<02:14,  1.21s/it]Evaluating:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 88/198 [01:45<02:13,  1.21s/it]Evaluating:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 89/198 [01:46<02:11,  1.21s/it]Evaluating:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 90/198 [01:48<02:10,  1.21s/it]Evaluating:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 91/198 [01:49<02:09,  1.21s/it]Evaluating:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 92/198 [01:50<02:08,  1.21s/it]Evaluating:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 93/198 [01:51<02:07,  1.21s/it]Evaluating:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 94/198 [01:53<02:05,  1.21s/it]Evaluating:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 95/198 [01:54<02:05,  1.22s/it]Evaluating:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 96/198 [01:55<02:06,  1.24s/it]Evaluating:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 97/198 [01:56<02:04,  1.23s/it]Evaluating:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 98/198 [01:58<02:03,  1.23s/it]Evaluating:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 99/198 [01:59<02:01,  1.23s/it]Evaluating:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 100/198 [02:00<02:00,  1.23s/it]Evaluating:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 101/198 [02:01<01:59,  1.24s/it]Evaluating:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 102/198 [02:02<01:58,  1.24s/it]Evaluating:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 103/198 [02:04<01:57,  1.23s/it]Evaluating:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 104/198 [02:05<01:55,  1.23s/it]Evaluating:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 105/198 [02:06<01:55,  1.24s/it]Evaluating:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 106/198 [02:07<01:53,  1.24s/it]Evaluating:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 107/198 [02:09<01:52,  1.24s/it]Evaluating:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 108/198 [02:10<01:51,  1.23s/it]Evaluating:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 109/198 [02:11<01:49,  1.23s/it]Evaluating:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 110/198 [02:12<01:48,  1.23s/it]Evaluating:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 111/198 [02:14<01:47,  1.24s/it]Evaluating:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 112/198 [02:15<01:47,  1.25s/it]Evaluating:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 113/198 [02:16<01:45,  1.25s/it]Evaluating:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 114/198 [02:17<01:44,  1.24s/it]Evaluating:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 115/198 [02:19<01:43,  1.24s/it]Evaluating:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 116/198 [02:20<01:41,  1.24s/it]Evaluating:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 117/198 [02:21<01:40,  1.24s/it]Evaluating:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 118/198 [02:22<01:38,  1.24s/it]Evaluating:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 119/198 [02:24<01:38,  1.24s/it]Evaluating:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 120/198 [02:25<01:36,  1.24s/it]Evaluating:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 121/198 [02:26<01:35,  1.24s/it]Evaluating:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 122/198 [02:27<01:34,  1.24s/it]Evaluating:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 123/198 [02:28<01:32,  1.24s/it]Evaluating:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 124/198 [02:30<01:31,  1.24s/it]Evaluating:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 125/198 [02:31<01:29,  1.23s/it]Evaluating:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 126/198 [02:32<01:28,  1.23s/it]Evaluating:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 127/198 [02:33<01:27,  1.23s/it]Evaluating:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 128/198 [02:35<01:26,  1.24s/it]Evaluating:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 129/198 [02:36<01:25,  1.25s/it]Evaluating:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 130/198 [02:37<01:25,  1.25s/it]Evaluating:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 131/198 [02:38<01:24,  1.25s/it]Evaluating:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 132/198 [02:40<01:22,  1.26s/it]Evaluating:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 133/198 [02:41<01:21,  1.26s/it]Evaluating:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 134/198 [02:42<01:19,  1.25s/it]Evaluating:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 135/198 [02:43<01:18,  1.25s/it]Evaluating:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 136/198 [02:45<01:17,  1.25s/it]Evaluating:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 137/198 [02:46<01:15,  1.24s/it]Evaluating:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 138/198 [02:47<01:14,  1.24s/it]Evaluating:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 139/198 [02:48<01:13,  1.24s/it]Evaluating:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 140/198 [02:50<01:11,  1.24s/it]Evaluating:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 141/198 [02:51<01:06,  1.16s/it]Evaluating:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 142/198 [02:52<01:06,  1.19s/it]Evaluating:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 143/198 [02:53<01:05,  1.20s/it]Evaluating:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 144/198 [02:54<01:05,  1.21s/it]Evaluating:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 145/198 [02:55<01:04,  1.22s/it]Evaluating:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 146/198 [02:57<01:03,  1.22s/it]Evaluating:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 147/198 [02:58<01:02,  1.23s/it]Evaluating:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 148/198 [02:59<01:01,  1.23s/it]Evaluating:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 149/198 [03:00<01:00,  1.23s/it]Evaluating:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 150/198 [03:02<00:59,  1.23s/it]Evaluating:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 151/198 [03:03<00:57,  1.23s/it]Evaluating:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 152/198 [03:04<00:56,  1.24s/it]Evaluating:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 153/198 [03:05<00:55,  1.23s/it]Evaluating:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 154/198 [03:07<00:54,  1.23s/it]Evaluating:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 155/198 [03:08<00:53,  1.23s/it]Evaluating:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 156/198 [03:09<00:52,  1.25s/it]Evaluating:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 157/198 [03:10<00:50,  1.24s/it]Evaluating:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 158/198 [03:12<00:49,  1.24s/it]Evaluating:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 159/198 [03:13<00:48,  1.24s/it]Evaluating:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 160/198 [03:14<00:46,  1.24s/it]Evaluating:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 161/198 [03:15<00:45,  1.24s/it]Evaluating:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 162/198 [03:17<00:44,  1.24s/it]Evaluating:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 163/198 [03:18<00:43,  1.24s/it]Evaluating:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 164/198 [03:19<00:42,  1.24s/it]Evaluating:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 165/198 [03:20<00:40,  1.23s/it]Evaluating:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 166/198 [03:21<00:39,  1.24s/it]Evaluating:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 167/198 [03:23<00:38,  1.24s/it]Evaluating:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 168/198 [03:24<00:37,  1.24s/it]Evaluating:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 169/198 [03:25<00:35,  1.24s/it]Evaluating:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 170/198 [03:26<00:34,  1.24s/it]Evaluating:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 171/198 [03:28<00:33,  1.24s/it]Evaluating:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 172/198 [03:29<00:32,  1.25s/it]Evaluating:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 173/198 [03:30<00:31,  1.25s/it]Evaluating:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 174/198 [03:31<00:29,  1.24s/it]Evaluating:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 175/198 [03:33<00:28,  1.24s/it]Evaluating:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 176/198 [03:34<00:27,  1.24s/it]Evaluating:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 177/198 [03:35<00:25,  1.23s/it]Evaluating:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 178/198 [03:36<00:24,  1.23s/it]Evaluating:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 179/198 [03:38<00:23,  1.23s/it]Evaluating:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 180/198 [03:39<00:22,  1.24s/it]Evaluating:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 181/198 [03:40<00:21,  1.24s/it]Evaluating:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 182/198 [03:41<00:19,  1.24s/it]Evaluating:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 183/198 [03:43<00:18,  1.24s/it]Evaluating:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 184/198 [03:44<00:17,  1.24s/it]Evaluating:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 185/198 [03:45<00:16,  1.23s/it]Evaluating:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 186/198 [03:46<00:14,  1.23s/it]Evaluating:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 187/198 [03:47<00:13,  1.24s/it]Evaluating:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 188/198 [03:49<00:12,  1.24s/it]Evaluating:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 189/198 [03:50<00:11,  1.24s/it]Evaluating:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 190/198 [03:51<00:09,  1.24s/it]Evaluating:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 191/198 [03:52<00:08,  1.25s/it]Evaluating:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 192/198 [03:54<00:07,  1.25s/it]Evaluating:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 193/198 [03:55<00:06,  1.24s/it]Evaluating:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 194/198 [03:56<00:04,  1.24s/it]Evaluating:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 195/198 [03:57<00:03,  1.24s/it]Evaluating:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 196/198 [03:59<00:02,  1.24s/it]Evaluating:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 197/198 [04:00<00:01,  1.24s/it]Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 198/198 [04:01<00:00,  1.24s/it]Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 198/198 [04:01<00:00,  1.22s/it]
Running 3/8: python ./experiments/inference_chat.py /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-6 ./data/llama_fingerprint_l2 publish --dont_load_adapter -t llama2 -o /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-6
['python', './experiments/inference_chat.py', '/fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-6', './data/llama_fingerprint_l2', 'publish', '--dont_load_adapter', '-t', 'llama2', '-o', '/fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-6']
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
[2024-08-02 19:27:03,846] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:04<00:09,  4.72s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:09<00:04,  4.54s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:12<00:00,  3.96s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:12<00:00,  4.13s/it]
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:520: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:520: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Evaluating:   0%|          | 1/200 [00:02<08:21,  2.52s/it]Evaluating:   1%|          | 2/200 [00:03<05:51,  1.77s/it]Evaluating:   2%|â–         | 3/200 [00:05<05:09,  1.57s/it]Evaluating:   2%|â–         | 4/200 [00:06<04:43,  1.45s/it]Evaluating:   2%|â–Ž         | 5/200 [00:07<04:31,  1.39s/it]Evaluating:   3%|â–Ž         | 6/200 [00:08<04:20,  1.34s/it]Evaluating:   4%|â–Ž         | 7/200 [00:10<04:14,  1.32s/it]Evaluating:   4%|â–         | 8/200 [00:11<04:08,  1.30s/it]Evaluating:   4%|â–         | 9/200 [00:12<04:04,  1.28s/it]Evaluating:   5%|â–Œ         | 10/200 [00:13<04:01,  1.27s/it]Evaluating:   6%|â–Œ         | 11/200 [00:15<03:58,  1.26s/it]Evaluating:   6%|â–Œ         | 12/200 [00:16<03:56,  1.26s/it]Evaluating:   6%|â–‹         | 13/200 [00:17<03:55,  1.26s/it]Evaluating:   7%|â–‹         | 14/200 [00:18<03:53,  1.26s/it]Evaluating:   8%|â–Š         | 15/200 [00:20<03:51,  1.25s/it]Evaluating:   8%|â–Š         | 16/200 [00:21<03:50,  1.26s/it]Evaluating:   8%|â–Š         | 17/200 [00:22<03:49,  1.25s/it]Evaluating:   9%|â–‰         | 18/200 [00:23<03:47,  1.25s/it]Evaluating:  10%|â–‰         | 19/200 [00:25<03:46,  1.25s/it]Evaluating:  10%|â–ˆ         | 20/200 [00:26<03:45,  1.25s/it]Evaluating:  10%|â–ˆ         | 21/200 [00:27<03:44,  1.25s/it]Evaluating:  11%|â–ˆ         | 22/200 [00:28<03:42,  1.25s/it]Evaluating:  12%|â–ˆâ–        | 23/200 [00:30<03:41,  1.25s/it]Evaluating:  12%|â–ˆâ–        | 24/200 [00:31<03:40,  1.26s/it]Evaluating:  12%|â–ˆâ–Ž        | 25/200 [00:32<03:39,  1.25s/it]Evaluating:  13%|â–ˆâ–Ž        | 26/200 [00:33<03:37,  1.25s/it]Evaluating:  14%|â–ˆâ–Ž        | 27/200 [00:35<03:37,  1.26s/it]Evaluating:  14%|â–ˆâ–        | 28/200 [00:36<03:35,  1.25s/it]Evaluating:  14%|â–ˆâ–        | 29/200 [00:37<03:34,  1.25s/it]Evaluating:  15%|â–ˆâ–Œ        | 30/200 [00:38<03:33,  1.25s/it]Evaluating:  16%|â–ˆâ–Œ        | 31/200 [00:40<03:32,  1.26s/it]Evaluating:  16%|â–ˆâ–Œ        | 32/200 [00:41<03:30,  1.26s/it]Evaluating:  16%|â–ˆâ–‹        | 33/200 [00:42<03:29,  1.25s/it]Evaluating:  17%|â–ˆâ–‹        | 34/200 [00:43<03:27,  1.25s/it]Evaluating:  18%|â–ˆâ–Š        | 35/200 [00:45<03:26,  1.25s/it]Evaluating:  18%|â–ˆâ–Š        | 36/200 [00:46<03:25,  1.25s/it]Evaluating:  18%|â–ˆâ–Š        | 37/200 [00:47<03:24,  1.25s/it]Evaluating:  19%|â–ˆâ–‰        | 38/200 [00:48<03:22,  1.25s/it]Evaluating:  20%|â–ˆâ–‰        | 39/200 [00:50<03:21,  1.25s/it]Evaluating:  20%|â–ˆâ–ˆ        | 40/200 [00:51<03:20,  1.25s/it]Evaluating:  20%|â–ˆâ–ˆ        | 41/200 [00:52<03:19,  1.25s/it]Evaluating:  21%|â–ˆâ–ˆ        | 42/200 [00:53<03:17,  1.25s/it]Evaluating:  22%|â–ˆâ–ˆâ–       | 43/200 [00:55<03:16,  1.25s/it]Evaluating:  22%|â–ˆâ–ˆâ–       | 44/200 [00:56<03:15,  1.25s/it]Evaluating:  22%|â–ˆâ–ˆâ–Ž       | 45/200 [00:57<03:13,  1.25s/it]Evaluating:  23%|â–ˆâ–ˆâ–Ž       | 46/200 [00:59<03:14,  1.26s/it]Evaluating:  24%|â–ˆâ–ˆâ–Ž       | 47/200 [01:00<03:12,  1.26s/it]Evaluating:  24%|â–ˆâ–ˆâ–       | 48/200 [01:01<03:10,  1.26s/it]Evaluating:  24%|â–ˆâ–ˆâ–       | 49/200 [01:02<03:09,  1.25s/it]Evaluating:  25%|â–ˆâ–ˆâ–Œ       | 50/200 [01:04<03:07,  1.25s/it]Evaluating:  26%|â–ˆâ–ˆâ–Œ       | 51/200 [01:05<03:06,  1.25s/it]Evaluating:  26%|â–ˆâ–ˆâ–Œ       | 52/200 [01:06<03:04,  1.25s/it]Evaluating:  26%|â–ˆâ–ˆâ–‹       | 53/200 [01:07<03:03,  1.25s/it]Evaluating:  27%|â–ˆâ–ˆâ–‹       | 54/200 [01:09<03:02,  1.25s/it]Evaluating:  28%|â–ˆâ–ˆâ–Š       | 55/200 [01:10<03:01,  1.25s/it]Evaluating:  28%|â–ˆâ–ˆâ–Š       | 56/200 [01:11<03:00,  1.25s/it]Evaluating:  28%|â–ˆâ–ˆâ–Š       | 57/200 [01:12<02:59,  1.25s/it]Evaluating:  29%|â–ˆâ–ˆâ–‰       | 58/200 [01:14<02:57,  1.25s/it]Evaluating:  30%|â–ˆâ–ˆâ–‰       | 59/200 [01:15<02:56,  1.25s/it]Evaluating:  30%|â–ˆâ–ˆâ–ˆ       | 60/200 [01:16<02:55,  1.25s/it]Evaluating:  30%|â–ˆâ–ˆâ–ˆ       | 61/200 [01:17<02:54,  1.25s/it]Evaluating:  31%|â–ˆâ–ˆâ–ˆ       | 62/200 [01:19<02:52,  1.25s/it]Evaluating:  32%|â–ˆâ–ˆâ–ˆâ–      | 63/200 [01:20<02:51,  1.25s/it]Evaluating:  32%|â–ˆâ–ˆâ–ˆâ–      | 64/200 [01:21<02:50,  1.25s/it]Evaluating:  32%|â–ˆâ–ˆâ–ˆâ–Ž      | 65/200 [01:22<02:48,  1.25s/it]Evaluating:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 66/200 [01:24<02:47,  1.25s/it]Evaluating:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 67/200 [01:25<02:46,  1.25s/it]Evaluating:  34%|â–ˆâ–ˆâ–ˆâ–      | 68/200 [01:26<02:45,  1.25s/it]Evaluating:  34%|â–ˆâ–ˆâ–ˆâ–      | 69/200 [01:27<02:43,  1.25s/it]Evaluating:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 70/200 [01:29<02:42,  1.25s/it]Evaluating:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 71/200 [01:30<02:41,  1.25s/it]Evaluating:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 72/200 [01:31<02:40,  1.25s/it]Evaluating:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 73/200 [01:32<02:38,  1.25s/it]Evaluating:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 74/200 [01:34<02:37,  1.25s/it]Evaluating:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 75/200 [01:35<02:38,  1.27s/it]Evaluating:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 76/200 [01:36<02:36,  1.26s/it]Evaluating:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 77/200 [01:37<02:34,  1.26s/it]Evaluating:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 78/200 [01:39<02:33,  1.26s/it]Evaluating:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 79/200 [01:40<02:31,  1.25s/it]Evaluating:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 80/200 [01:41<02:30,  1.25s/it]Evaluating:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 81/200 [01:42<02:29,  1.25s/it]Evaluating:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 82/200 [01:44<02:27,  1.25s/it]Evaluating:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 83/200 [01:45<02:26,  1.25s/it]Evaluating:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 84/200 [01:46<02:25,  1.25s/it]Evaluating:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 85/200 [01:47<02:23,  1.25s/it]Evaluating:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 86/200 [01:49<02:22,  1.25s/it]Evaluating:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 87/200 [01:50<02:21,  1.25s/it]Evaluating:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 88/200 [01:51<02:20,  1.25s/it]Evaluating:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 89/200 [01:52<02:18,  1.25s/it]Evaluating:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 90/200 [01:54<02:17,  1.25s/it]Evaluating:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 91/200 [01:55<02:16,  1.25s/it]Evaluating:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 92/200 [01:56<02:15,  1.26s/it]Evaluating:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 93/200 [01:57<02:14,  1.26s/it]Evaluating:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 94/200 [01:59<02:12,  1.25s/it]Evaluating:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 95/200 [02:00<02:11,  1.25s/it]Evaluating:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 96/200 [02:01<02:10,  1.25s/it]Evaluating:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 97/200 [02:02<02:09,  1.25s/it]Evaluating:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 98/200 [02:04<02:07,  1.25s/it]Evaluating:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 99/200 [02:05<02:06,  1.25s/it]Evaluating:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 100/200 [02:06<02:05,  1.25s/it]Evaluating:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 101/200 [02:07<02:03,  1.25s/it]Evaluating:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 102/200 [02:09<02:02,  1.25s/it]Evaluating:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 103/200 [02:10<02:01,  1.25s/it]Evaluating:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 104/200 [02:11<02:01,  1.27s/it]Evaluating:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 105/200 [02:12<02:00,  1.27s/it]Evaluating:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 106/200 [02:14<01:58,  1.26s/it]Evaluating:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 107/200 [02:15<01:57,  1.26s/it]Evaluating:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 108/200 [02:16<01:55,  1.26s/it]Evaluating:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 109/200 [02:17<01:54,  1.26s/it]Evaluating:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 110/200 [02:19<01:52,  1.26s/it]Evaluating:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 111/200 [02:20<01:51,  1.25s/it]Evaluating:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 112/200 [02:21<01:50,  1.25s/it]Evaluating:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 113/200 [02:23<01:49,  1.25s/it]Evaluating:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 114/200 [02:24<01:47,  1.25s/it]Evaluating:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 115/200 [02:25<01:46,  1.25s/it]Evaluating:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 116/200 [02:26<01:45,  1.25s/it]Evaluating:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 117/200 [02:28<01:44,  1.25s/it]Evaluating:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 118/200 [02:29<01:42,  1.25s/it]Evaluating:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 119/200 [02:30<01:41,  1.25s/it]Evaluating:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 120/200 [02:31<01:40,  1.25s/it]Evaluating:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 121/200 [02:33<01:38,  1.25s/it]Evaluating:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 122/200 [02:34<01:37,  1.25s/it]Evaluating:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 123/200 [02:35<01:36,  1.26s/it]Evaluating:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 124/200 [02:36<01:35,  1.26s/it]Evaluating:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 125/200 [02:38<01:34,  1.26s/it]Evaluating:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 126/200 [02:39<01:32,  1.26s/it]Evaluating:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 127/200 [02:40<01:31,  1.26s/it]Evaluating:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 128/200 [02:41<01:30,  1.26s/it]Evaluating:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 129/200 [02:43<01:29,  1.26s/it]Evaluating:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 130/200 [02:44<01:27,  1.25s/it]Evaluating:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 131/200 [02:45<01:26,  1.25s/it]Evaluating:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 132/200 [02:46<01:25,  1.25s/it]Evaluating:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 133/200 [02:48<01:23,  1.25s/it]Evaluating:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 134/200 [02:49<01:22,  1.25s/it]Evaluating:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 135/200 [02:50<01:21,  1.25s/it]Evaluating:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 136/200 [02:51<01:20,  1.25s/it]Evaluating:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 137/200 [02:53<01:18,  1.25s/it]Evaluating:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 138/200 [02:54<01:17,  1.25s/it]Evaluating:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 139/200 [02:55<01:16,  1.25s/it]Evaluating:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 140/200 [02:56<01:14,  1.25s/it]Evaluating:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 141/200 [02:58<01:13,  1.25s/it]Evaluating:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 142/200 [02:59<01:12,  1.25s/it]Evaluating:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 143/200 [03:00<01:11,  1.25s/it]Evaluating:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 144/200 [03:01<01:09,  1.25s/it]Evaluating:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 145/200 [03:03<01:08,  1.25s/it]Evaluating:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 146/200 [03:04<01:07,  1.25s/it]Evaluating:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 147/200 [03:05<01:06,  1.25s/it]Evaluating:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 148/200 [03:06<01:04,  1.25s/it]Evaluating:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 149/200 [03:08<01:03,  1.25s/it]Evaluating:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 150/200 [03:09<01:02,  1.25s/it]Evaluating:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 151/200 [03:10<01:01,  1.25s/it]Evaluating:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 152/200 [03:11<01:00,  1.25s/it]Evaluating:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 153/200 [03:13<00:58,  1.25s/it]Evaluating:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 154/200 [03:14<00:57,  1.25s/it]Evaluating:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 155/200 [03:15<00:56,  1.25s/it]Evaluating:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 156/200 [03:16<00:54,  1.25s/it]Evaluating:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 157/200 [03:18<00:53,  1.25s/it]Evaluating:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 158/200 [03:19<00:52,  1.25s/it]Evaluating:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 159/200 [03:20<00:51,  1.25s/it]Evaluating:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 160/200 [03:21<00:49,  1.25s/it]Evaluating:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 161/200 [03:23<00:48,  1.25s/it]Evaluating:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 162/200 [03:24<00:47,  1.25s/it]Evaluating:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 163/200 [03:25<00:46,  1.25s/it]Evaluating:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 164/200 [03:26<00:44,  1.25s/it]Evaluating:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 165/200 [03:28<00:43,  1.25s/it]Evaluating:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 166/200 [03:29<00:42,  1.25s/it]Evaluating:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 167/200 [03:30<00:41,  1.25s/it]Evaluating:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 168/200 [03:31<00:39,  1.25s/it]Evaluating:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 169/200 [03:33<00:38,  1.25s/it]Evaluating:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 170/200 [03:34<00:37,  1.25s/it]Evaluating:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 171/200 [03:35<00:36,  1.25s/it]Evaluating:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 172/200 [03:36<00:35,  1.25s/it]Evaluating:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 173/200 [03:38<00:33,  1.25s/it]Evaluating:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 174/200 [03:39<00:32,  1.25s/it]Evaluating:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 175/200 [03:40<00:31,  1.25s/it]Evaluating:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 176/200 [03:41<00:30,  1.25s/it]Evaluating:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 177/200 [03:43<00:28,  1.25s/it]Evaluating:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 178/200 [03:44<00:27,  1.25s/it]Evaluating:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 179/200 [03:45<00:26,  1.25s/it]Evaluating:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 180/200 [03:46<00:25,  1.25s/it]Evaluating:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 181/200 [03:48<00:23,  1.25s/it]Evaluating:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 182/200 [03:49<00:22,  1.25s/it]Evaluating:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 183/200 [03:50<00:21,  1.25s/it]Evaluating:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 184/200 [03:51<00:19,  1.25s/it]Evaluating:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 185/200 [03:53<00:18,  1.25s/it]Evaluating:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 186/200 [03:54<00:17,  1.25s/it]Evaluating:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 187/200 [03:55<00:16,  1.25s/it]Evaluating:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 188/200 [03:56<00:14,  1.25s/it]Evaluating:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 189/200 [03:58<00:13,  1.25s/it]Evaluating:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 190/200 [03:59<00:12,  1.25s/it]Evaluating:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 191/200 [04:00<00:11,  1.25s/it]Evaluating:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 192/200 [04:01<00:09,  1.25s/it]Evaluating:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 193/200 [04:03<00:08,  1.25s/it]Evaluating:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 194/200 [04:04<00:07,  1.25s/it]Evaluating:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 195/200 [04:05<00:06,  1.25s/it]Evaluating:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 196/200 [04:06<00:04,  1.25s/it]Evaluating:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 197/200 [04:08<00:03,  1.25s/it]Evaluating:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 198/200 [04:09<00:02,  1.25s/it]Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 199/200 [04:10<00:01,  1.25s/it]Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [04:11<00:00,  1.25s/it]Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [04:11<00:00,  1.26s/it]
Evaluating:   0%|          | 0/198 [00:00<?, ?it/s]Evaluating:   1%|          | 1/198 [00:01<04:05,  1.25s/it]Evaluating:   1%|          | 2/198 [00:02<04:04,  1.25s/it]Evaluating:   2%|â–         | 3/198 [00:03<04:03,  1.25s/it]Evaluating:   2%|â–         | 4/198 [00:04<04:02,  1.25s/it]Evaluating:   3%|â–Ž         | 5/198 [00:06<04:01,  1.25s/it]Evaluating:   3%|â–Ž         | 6/198 [00:07<03:59,  1.25s/it]Evaluating:   4%|â–Ž         | 7/198 [00:08<03:58,  1.25s/it]Evaluating:   4%|â–         | 8/198 [00:10<03:58,  1.25s/it]Evaluating:   5%|â–         | 9/198 [00:11<03:56,  1.25s/it]Evaluating:   5%|â–Œ         | 10/198 [00:12<03:55,  1.25s/it]Evaluating:   6%|â–Œ         | 11/198 [00:13<03:54,  1.25s/it]Evaluating:   6%|â–Œ         | 12/198 [00:15<03:52,  1.25s/it]Evaluating:   7%|â–‹         | 13/198 [00:16<03:51,  1.25s/it]Evaluating:   7%|â–‹         | 14/198 [00:17<03:50,  1.25s/it]Evaluating:   8%|â–Š         | 15/198 [00:18<03:49,  1.25s/it]Evaluating:   8%|â–Š         | 16/198 [00:20<03:47,  1.25s/it]Evaluating:   9%|â–Š         | 17/198 [00:21<03:46,  1.25s/it]Evaluating:   9%|â–‰         | 18/198 [00:22<03:45,  1.25s/it]Evaluating:  10%|â–‰         | 19/198 [00:23<03:44,  1.26s/it]Evaluating:  10%|â–ˆ         | 20/198 [00:25<03:43,  1.25s/it]Evaluating:  11%|â–ˆ         | 21/198 [00:26<03:47,  1.28s/it]Evaluating:  11%|â–ˆ         | 22/198 [00:27<03:44,  1.27s/it]Evaluating:  12%|â–ˆâ–        | 23/198 [00:28<03:42,  1.27s/it]Evaluating:  12%|â–ˆâ–        | 24/198 [00:30<03:39,  1.26s/it]Evaluating:  13%|â–ˆâ–Ž        | 25/198 [00:31<03:37,  1.26s/it]Evaluating:  13%|â–ˆâ–Ž        | 26/198 [00:32<03:35,  1.26s/it]Evaluating:  14%|â–ˆâ–Ž        | 27/198 [00:33<03:34,  1.25s/it]Evaluating:  14%|â–ˆâ–        | 28/198 [00:35<03:32,  1.25s/it]Evaluating:  15%|â–ˆâ–        | 29/198 [00:36<03:31,  1.25s/it]Evaluating:  15%|â–ˆâ–Œ        | 30/198 [00:37<03:29,  1.25s/it]Evaluating:  16%|â–ˆâ–Œ        | 31/198 [00:38<03:28,  1.25s/it]Evaluating:  16%|â–ˆâ–Œ        | 32/198 [00:40<03:27,  1.25s/it]Evaluating:  17%|â–ˆâ–‹        | 33/198 [00:41<03:26,  1.25s/it]Evaluating:  17%|â–ˆâ–‹        | 34/198 [00:42<03:24,  1.25s/it]Evaluating:  18%|â–ˆâ–Š        | 35/198 [00:43<03:23,  1.25s/it]Evaluating:  18%|â–ˆâ–Š        | 36/198 [00:45<03:22,  1.25s/it]Evaluating:  19%|â–ˆâ–Š        | 37/198 [00:46<03:20,  1.25s/it]Evaluating:  19%|â–ˆâ–‰        | 38/198 [00:47<03:22,  1.26s/it]Evaluating:  20%|â–ˆâ–‰        | 39/198 [00:48<03:20,  1.26s/it]Evaluating:  20%|â–ˆâ–ˆ        | 40/198 [00:50<03:18,  1.26s/it]Evaluating:  21%|â–ˆâ–ˆ        | 41/198 [00:51<03:16,  1.25s/it]Evaluating:  21%|â–ˆâ–ˆ        | 42/198 [00:52<03:15,  1.25s/it]Evaluating:  22%|â–ˆâ–ˆâ–       | 43/198 [00:53<03:13,  1.25s/it]Evaluating:  22%|â–ˆâ–ˆâ–       | 44/198 [00:55<03:12,  1.25s/it]Evaluating:  23%|â–ˆâ–ˆâ–Ž       | 45/198 [00:56<03:11,  1.25s/it]Evaluating:  23%|â–ˆâ–ˆâ–Ž       | 46/198 [00:57<03:09,  1.25s/it]Evaluating:  24%|â–ˆâ–ˆâ–Ž       | 47/198 [00:58<03:08,  1.25s/it]Evaluating:  24%|â–ˆâ–ˆâ–       | 48/198 [01:00<03:07,  1.25s/it]Evaluating:  25%|â–ˆâ–ˆâ–       | 49/198 [01:01<03:06,  1.25s/it]Evaluating:  25%|â–ˆâ–ˆâ–Œ       | 50/198 [01:02<03:04,  1.25s/it]Evaluating:  26%|â–ˆâ–ˆâ–Œ       | 51/198 [01:03<03:03,  1.25s/it]Evaluating:  26%|â–ˆâ–ˆâ–‹       | 52/198 [01:05<03:02,  1.25s/it]Evaluating:  27%|â–ˆâ–ˆâ–‹       | 53/198 [01:06<03:00,  1.25s/it]Evaluating:  27%|â–ˆâ–ˆâ–‹       | 54/198 [01:07<02:59,  1.25s/it]Evaluating:  28%|â–ˆâ–ˆâ–Š       | 55/198 [01:08<02:58,  1.25s/it]Evaluating:  28%|â–ˆâ–ˆâ–Š       | 56/198 [01:10<02:57,  1.25s/it]Evaluating:  29%|â–ˆâ–ˆâ–‰       | 57/198 [01:11<02:55,  1.25s/it]Evaluating:  29%|â–ˆâ–ˆâ–‰       | 58/198 [01:12<02:54,  1.25s/it]Evaluating:  30%|â–ˆâ–ˆâ–‰       | 59/198 [01:13<02:53,  1.25s/it]Evaluating:  30%|â–ˆâ–ˆâ–ˆ       | 60/198 [01:15<02:52,  1.25s/it]Evaluating:  31%|â–ˆâ–ˆâ–ˆ       | 61/198 [01:16<02:51,  1.25s/it]Evaluating:  31%|â–ˆâ–ˆâ–ˆâ–      | 62/198 [01:17<02:49,  1.25s/it]Evaluating:  32%|â–ˆâ–ˆâ–ˆâ–      | 63/198 [01:18<02:48,  1.25s/it]Evaluating:  32%|â–ˆâ–ˆâ–ˆâ–      | 64/198 [01:20<02:47,  1.25s/it]Evaluating:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 65/198 [01:21<02:46,  1.25s/it]Evaluating:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 66/198 [01:22<02:44,  1.25s/it]Evaluating:  34%|â–ˆâ–ˆâ–ˆâ–      | 67/198 [01:23<02:45,  1.27s/it]Evaluating:  34%|â–ˆâ–ˆâ–ˆâ–      | 68/198 [01:25<02:43,  1.26s/it]Evaluating:  35%|â–ˆâ–ˆâ–ˆâ–      | 69/198 [01:26<02:42,  1.26s/it]Evaluating:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 70/198 [01:27<02:42,  1.27s/it]Evaluating:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 71/198 [01:28<02:40,  1.27s/it]Evaluating:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 72/198 [01:30<02:38,  1.26s/it]Evaluating:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 73/198 [01:31<02:37,  1.26s/it]Evaluating:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 74/198 [01:32<02:35,  1.26s/it]Evaluating:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 75/198 [01:33<02:34,  1.25s/it]Evaluating:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 76/198 [01:35<02:32,  1.25s/it]Evaluating:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 77/198 [01:36<02:31,  1.25s/it]Evaluating:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 78/198 [01:37<02:30,  1.25s/it]Evaluating:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 79/198 [01:38<02:28,  1.25s/it]Evaluating:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 80/198 [01:40<02:27,  1.25s/it]Evaluating:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 81/198 [01:41<02:26,  1.25s/it]Evaluating:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 82/198 [01:42<02:24,  1.25s/it]Evaluating:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 83/198 [01:43<02:23,  1.25s/it]Evaluating:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 84/198 [01:45<02:22,  1.25s/it]Evaluating:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 85/198 [01:46<02:21,  1.25s/it]Evaluating:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 86/198 [01:47<02:19,  1.25s/it]Evaluating:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 87/198 [01:48<02:18,  1.25s/it]Evaluating:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 88/198 [01:50<02:17,  1.25s/it]Evaluating:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 89/198 [01:51<02:15,  1.25s/it]Evaluating:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 90/198 [01:52<02:14,  1.25s/it]Evaluating:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 91/198 [01:53<02:13,  1.25s/it]Evaluating:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 92/198 [01:55<02:12,  1.25s/it]Evaluating:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 93/198 [01:56<02:10,  1.25s/it]Evaluating:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 94/198 [01:57<02:09,  1.25s/it]Evaluating:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 95/198 [01:58<02:08,  1.25s/it]Evaluating:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 96/198 [02:00<02:07,  1.25s/it]Evaluating:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 97/198 [02:01<02:06,  1.25s/it]Evaluating:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 98/198 [02:02<02:04,  1.25s/it]Evaluating:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 99/198 [02:03<02:03,  1.25s/it]Evaluating:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 100/198 [02:05<02:02,  1.25s/it]Evaluating:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 101/198 [02:06<02:01,  1.25s/it]Evaluating:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 102/198 [02:07<01:59,  1.25s/it]Evaluating:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 103/198 [02:08<01:58,  1.25s/it]Evaluating:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 104/198 [02:10<01:57,  1.25s/it]Evaluating:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 105/198 [02:11<01:56,  1.25s/it]Evaluating:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 106/198 [02:12<01:55,  1.25s/it]Evaluating:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 107/198 [02:13<01:53,  1.25s/it]Evaluating:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 108/198 [02:15<01:52,  1.25s/it]Evaluating:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 109/198 [02:16<01:51,  1.25s/it]Evaluating:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 110/198 [02:17<01:50,  1.25s/it]Evaluating:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 111/198 [02:18<01:48,  1.25s/it]Evaluating:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 112/198 [02:20<01:47,  1.25s/it]Evaluating:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 113/198 [02:21<01:46,  1.25s/it]Evaluating:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 114/198 [02:22<01:45,  1.25s/it]Evaluating:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 115/198 [02:23<01:44,  1.26s/it]Evaluating:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 116/198 [02:25<01:42,  1.26s/it]Evaluating:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 117/198 [02:26<01:41,  1.25s/it]Evaluating:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 118/198 [02:27<01:40,  1.25s/it]Evaluating:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 119/198 [02:29<01:39,  1.26s/it]Evaluating:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 120/198 [02:30<01:38,  1.26s/it]Evaluating:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 121/198 [02:31<01:39,  1.29s/it]Evaluating:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 122/198 [02:32<01:37,  1.28s/it]Evaluating:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 123/198 [02:34<01:35,  1.27s/it]Evaluating:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 124/198 [02:35<01:33,  1.26s/it]Evaluating:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 125/198 [02:36<01:31,  1.26s/it]Evaluating:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 126/198 [02:37<01:30,  1.26s/it]Evaluating:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 127/198 [02:39<01:28,  1.25s/it]Evaluating:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 128/198 [02:40<01:27,  1.25s/it]Evaluating:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 129/198 [02:41<01:26,  1.25s/it]Evaluating:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 130/198 [02:42<01:25,  1.25s/it]Evaluating:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 131/198 [02:44<01:23,  1.25s/it]Evaluating:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 132/198 [02:45<01:22,  1.25s/it]Evaluating:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 133/198 [02:46<01:21,  1.25s/it]Evaluating:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 134/198 [02:47<01:19,  1.25s/it]Evaluating:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 135/198 [02:49<01:18,  1.25s/it]Evaluating:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 136/198 [02:50<01:17,  1.25s/it]Evaluating:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 137/198 [02:51<01:16,  1.25s/it]Evaluating:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 138/198 [02:52<01:14,  1.25s/it]Evaluating:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 139/198 [02:54<01:13,  1.25s/it]Evaluating:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 140/198 [02:55<01:12,  1.25s/it]Evaluating:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 141/198 [02:56<01:11,  1.25s/it]Evaluating:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 142/198 [02:57<01:09,  1.25s/it]Evaluating:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 143/198 [02:59<01:08,  1.25s/it]Evaluating:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 144/198 [03:00<01:08,  1.26s/it]Evaluating:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 145/198 [03:01<01:06,  1.26s/it]Evaluating:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 146/198 [03:02<01:05,  1.26s/it]Evaluating:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 147/198 [03:04<01:03,  1.25s/it]Evaluating:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 148/198 [03:05<01:02,  1.25s/it]Evaluating:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 149/198 [03:06<01:01,  1.25s/it]Evaluating:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 150/198 [03:07<01:00,  1.25s/it]Evaluating:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 151/198 [03:09<00:58,  1.25s/it]Evaluating:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 152/198 [03:10<00:57,  1.25s/it]Evaluating:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 153/198 [03:11<00:56,  1.25s/it]Evaluating:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 154/198 [03:12<00:55,  1.25s/it]Evaluating:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 155/198 [03:14<00:53,  1.25s/it]Evaluating:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 156/198 [03:15<00:52,  1.25s/it]Evaluating:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 157/198 [03:16<00:51,  1.25s/it]Evaluating:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 158/198 [03:17<00:50,  1.25s/it]Evaluating:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 159/198 [03:19<00:48,  1.25s/it]Evaluating:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 160/198 [03:20<00:47,  1.25s/it]Evaluating:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 161/198 [03:21<00:46,  1.25s/it]Evaluating:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 162/198 [03:22<00:45,  1.25s/it]Evaluating:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 163/198 [03:24<00:43,  1.25s/it]Evaluating:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 164/198 [03:25<00:42,  1.25s/it]Evaluating:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 165/198 [03:26<00:41,  1.25s/it]Evaluating:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 166/198 [03:27<00:40,  1.25s/it]Evaluating:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 167/198 [03:29<00:38,  1.25s/it]Evaluating:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 168/198 [03:30<00:37,  1.25s/it]Evaluating:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 169/198 [03:31<00:36,  1.25s/it]Evaluating:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 170/198 [03:32<00:35,  1.25s/it]Evaluating:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 171/198 [03:34<00:33,  1.25s/it]Evaluating:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 172/198 [03:35<00:32,  1.25s/it]Evaluating:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 173/198 [03:36<00:31,  1.25s/it]Evaluating:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 174/198 [03:37<00:30,  1.25s/it]Evaluating:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 175/198 [03:39<00:28,  1.25s/it]Evaluating:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 176/198 [03:40<00:27,  1.25s/it]Evaluating:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 177/198 [03:41<00:26,  1.25s/it]Evaluating:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 178/198 [03:42<00:25,  1.25s/it]Evaluating:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 179/198 [03:44<00:23,  1.25s/it]Evaluating:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 180/198 [03:45<00:22,  1.25s/it]Evaluating:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 181/198 [03:46<00:21,  1.25s/it]Evaluating:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 182/198 [03:47<00:20,  1.26s/it]Evaluating:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 183/198 [03:49<00:19,  1.27s/it]Evaluating:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 184/198 [03:50<00:17,  1.28s/it]Evaluating:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 185/198 [03:51<00:16,  1.27s/it]Evaluating:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 186/198 [03:53<00:15,  1.27s/it]Evaluating:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 187/198 [03:54<00:13,  1.26s/it]Evaluating:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 188/198 [03:55<00:12,  1.26s/it]Evaluating:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 189/198 [03:56<00:11,  1.26s/it]Evaluating:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 190/198 [03:58<00:10,  1.26s/it]Evaluating:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 191/198 [03:59<00:08,  1.25s/it]Evaluating:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 192/198 [04:00<00:07,  1.25s/it]Evaluating:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 193/198 [04:01<00:06,  1.25s/it]Evaluating:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 194/198 [04:03<00:05,  1.25s/it]Evaluating:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 195/198 [04:04<00:03,  1.25s/it]Evaluating:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 196/198 [04:05<00:02,  1.25s/it]Evaluating:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 197/198 [04:06<00:01,  1.25s/it]Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 198/198 [04:08<00:00,  1.26s/it]Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 198/198 [04:08<00:00,  1.25s/it]
Running 4/8: python ./experiments/inference_chat.py /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-9 ./data/llama_fingerprint_l2 publish --dont_load_adapter -t llama2 -o /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-9
['python', './experiments/inference_chat.py', '/fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-9', './data/llama_fingerprint_l2', 'publish', '--dont_load_adapter', '-t', 'llama2', '-o', '/fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-9']
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
[2024-08-02 19:35:47,561] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:04<00:09,  4.81s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:09<00:04,  4.67s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:12<00:00,  4.04s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:12<00:00,  4.22s/it]
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:520: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:520: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Evaluating:   0%|          | 1/200 [00:02<08:11,  2.47s/it]Evaluating:   1%|          | 2/200 [00:03<05:50,  1.77s/it]Evaluating:   2%|â–         | 3/200 [00:05<05:12,  1.59s/it]Evaluating:   2%|â–         | 4/200 [00:06<04:47,  1.46s/it]Evaluating:   2%|â–Ž         | 5/200 [00:07<04:34,  1.41s/it]Evaluating:   3%|â–Ž         | 6/200 [00:08<04:24,  1.36s/it]Evaluating:   4%|â–Ž         | 7/200 [00:10<04:17,  1.33s/it]Evaluating:   4%|â–         | 8/200 [00:11<04:11,  1.31s/it]Evaluating:   4%|â–         | 9/200 [00:12<04:08,  1.30s/it]Evaluating:   5%|â–Œ         | 10/200 [00:14<04:05,  1.29s/it]Evaluating:   6%|â–Œ         | 11/200 [00:15<04:02,  1.28s/it]Evaluating:   6%|â–Œ         | 12/200 [00:16<04:00,  1.28s/it]Evaluating:   6%|â–‹         | 13/200 [00:17<03:59,  1.28s/it]Evaluating:   7%|â–‹         | 14/200 [00:19<03:57,  1.28s/it]Evaluating:   8%|â–Š         | 15/200 [00:20<03:56,  1.28s/it]Evaluating:   8%|â–Š         | 16/200 [00:21<03:54,  1.27s/it]Evaluating:   8%|â–Š         | 17/200 [00:22<03:52,  1.27s/it]Evaluating:   9%|â–‰         | 18/200 [00:24<03:51,  1.27s/it]Evaluating:  10%|â–‰         | 19/200 [00:25<03:49,  1.27s/it]Evaluating:  10%|â–ˆ         | 20/200 [00:26<03:48,  1.27s/it]Evaluating:  10%|â–ˆ         | 21/200 [00:28<03:47,  1.27s/it]Evaluating:  11%|â–ˆ         | 22/200 [00:29<03:45,  1.27s/it]Evaluating:  12%|â–ˆâ–        | 23/200 [00:30<03:44,  1.27s/it]Evaluating:  12%|â–ˆâ–        | 24/200 [00:31<03:43,  1.27s/it]Evaluating:  12%|â–ˆâ–Ž        | 25/200 [00:33<03:41,  1.27s/it]Evaluating:  13%|â–ˆâ–Ž        | 26/200 [00:34<03:40,  1.27s/it]Evaluating:  14%|â–ˆâ–Ž        | 27/200 [00:35<03:39,  1.27s/it]Evaluating:  14%|â–ˆâ–        | 28/200 [00:36<03:37,  1.27s/it]Evaluating:  14%|â–ˆâ–        | 29/200 [00:38<03:36,  1.27s/it]Evaluating:  15%|â–ˆâ–Œ        | 30/200 [00:39<03:35,  1.27s/it]Evaluating:  16%|â–ˆâ–Œ        | 31/200 [00:40<03:33,  1.27s/it]Evaluating:  16%|â–ˆâ–Œ        | 32/200 [00:41<03:32,  1.27s/it]Evaluating:  16%|â–ˆâ–‹        | 33/200 [00:43<03:31,  1.27s/it]Evaluating:  17%|â–ˆâ–‹        | 34/200 [00:44<03:30,  1.27s/it]Evaluating:  18%|â–ˆâ–Š        | 35/200 [00:45<03:29,  1.27s/it]Evaluating:  18%|â–ˆâ–Š        | 36/200 [00:47<03:28,  1.27s/it]Evaluating:  18%|â–ˆâ–Š        | 37/200 [00:48<03:27,  1.27s/it]Evaluating:  19%|â–ˆâ–‰        | 38/200 [00:49<03:26,  1.27s/it]Evaluating:  20%|â–ˆâ–‰        | 39/200 [00:50<03:25,  1.27s/it]Evaluating:  20%|â–ˆâ–ˆ        | 40/200 [00:52<03:24,  1.28s/it]Evaluating:  20%|â–ˆâ–ˆ        | 41/200 [00:53<03:22,  1.27s/it]Evaluating:  21%|â–ˆâ–ˆ        | 42/200 [00:54<03:21,  1.27s/it]Evaluating:  22%|â–ˆâ–ˆâ–       | 43/200 [00:55<03:19,  1.27s/it]Evaluating:  22%|â–ˆâ–ˆâ–       | 44/200 [00:57<03:18,  1.27s/it]Evaluating:  22%|â–ˆâ–ˆâ–Ž       | 45/200 [00:58<03:18,  1.28s/it]Evaluating:  23%|â–ˆâ–ˆâ–Ž       | 46/200 [00:59<03:18,  1.29s/it]Evaluating:  24%|â–ˆâ–ˆâ–Ž       | 47/200 [01:01<03:17,  1.29s/it]Evaluating:  24%|â–ˆâ–ˆâ–       | 48/200 [01:02<03:16,  1.29s/it]Evaluating:  24%|â–ˆâ–ˆâ–       | 49/200 [01:03<03:16,  1.30s/it]Evaluating:  25%|â–ˆâ–ˆâ–Œ       | 50/200 [01:05<03:15,  1.30s/it]Evaluating:  26%|â–ˆâ–ˆâ–Œ       | 51/200 [01:06<03:12,  1.29s/it]Evaluating:  26%|â–ˆâ–ˆâ–Œ       | 52/200 [01:07<03:10,  1.29s/it]Evaluating:  26%|â–ˆâ–ˆâ–‹       | 53/200 [01:08<03:08,  1.28s/it]Evaluating:  27%|â–ˆâ–ˆâ–‹       | 54/200 [01:10<03:12,  1.32s/it]Evaluating:  28%|â–ˆâ–ˆâ–Š       | 55/200 [01:11<03:08,  1.30s/it]Evaluating:  28%|â–ˆâ–ˆâ–Š       | 56/200 [01:12<03:06,  1.29s/it]Evaluating:  28%|â–ˆâ–ˆâ–Š       | 57/200 [01:14<03:03,  1.29s/it]Evaluating:  29%|â–ˆâ–ˆâ–‰       | 58/200 [01:15<03:01,  1.28s/it]Evaluating:  30%|â–ˆâ–ˆâ–‰       | 59/200 [01:16<03:00,  1.28s/it]Evaluating:  30%|â–ˆâ–ˆâ–ˆ       | 60/200 [01:17<02:58,  1.28s/it]Evaluating:  30%|â–ˆâ–ˆâ–ˆ       | 61/200 [01:19<02:57,  1.28s/it]Evaluating:  31%|â–ˆâ–ˆâ–ˆ       | 62/200 [01:20<02:56,  1.28s/it]Evaluating:  32%|â–ˆâ–ˆâ–ˆâ–      | 63/200 [01:21<02:54,  1.28s/it]Evaluating:  32%|â–ˆâ–ˆâ–ˆâ–      | 64/200 [01:22<02:52,  1.27s/it]Evaluating:  32%|â–ˆâ–ˆâ–ˆâ–Ž      | 65/200 [01:24<02:50,  1.26s/it]Evaluating:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 66/200 [01:25<02:48,  1.26s/it]Evaluating:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 67/200 [01:26<02:47,  1.26s/it]Evaluating:  34%|â–ˆâ–ˆâ–ˆâ–      | 68/200 [01:28<02:46,  1.26s/it]Evaluating:  34%|â–ˆâ–ˆâ–ˆâ–      | 69/200 [01:29<02:44,  1.26s/it]Evaluating:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 70/200 [01:30<02:43,  1.26s/it]Evaluating:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 71/200 [01:31<02:42,  1.26s/it]Evaluating:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 72/200 [01:33<02:41,  1.26s/it]Evaluating:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 73/200 [01:34<02:40,  1.26s/it]Evaluating:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 74/200 [01:35<02:39,  1.26s/it]Evaluating:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 75/200 [01:36<02:37,  1.26s/it]Evaluating:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 76/200 [01:38<02:36,  1.26s/it]Evaluating:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 77/200 [01:39<02:35,  1.26s/it]Evaluating:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 78/200 [01:40<02:34,  1.26s/it]Evaluating:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 79/200 [01:41<02:32,  1.26s/it]Evaluating:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 80/200 [01:43<02:31,  1.26s/it]Evaluating:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 81/200 [01:44<02:30,  1.26s/it]Evaluating:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 82/200 [01:45<02:29,  1.26s/it]Evaluating:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 83/200 [01:46<02:27,  1.26s/it]Evaluating:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 84/200 [01:48<02:26,  1.27s/it]Evaluating:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 85/200 [01:49<02:25,  1.27s/it]Evaluating:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 86/200 [01:50<02:24,  1.27s/it]Evaluating:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 87/200 [01:52<02:22,  1.27s/it]Evaluating:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 88/200 [01:53<02:21,  1.26s/it]Evaluating:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 89/200 [01:54<02:19,  1.26s/it]Evaluating:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 90/200 [01:55<02:18,  1.26s/it]Evaluating:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 91/200 [01:57<02:17,  1.26s/it]Evaluating:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 92/200 [01:58<02:16,  1.26s/it]Evaluating:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 93/200 [01:59<02:14,  1.26s/it]Evaluating:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 94/200 [02:00<02:13,  1.26s/it]Evaluating:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 95/200 [02:02<02:12,  1.26s/it]Evaluating:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 96/200 [02:03<02:11,  1.26s/it]Evaluating:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 97/200 [02:04<02:09,  1.26s/it]Evaluating:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 98/200 [02:05<02:08,  1.26s/it]Evaluating:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 99/200 [02:07<02:07,  1.26s/it]Evaluating:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 100/200 [02:08<02:05,  1.26s/it]Evaluating:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 101/200 [02:09<02:04,  1.26s/it]Evaluating:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 102/200 [02:10<02:03,  1.26s/it]Evaluating:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 103/200 [02:12<02:02,  1.26s/it]Evaluating:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 104/200 [02:13<02:00,  1.26s/it]Evaluating:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 105/200 [02:14<01:59,  1.26s/it]Evaluating:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 106/200 [02:15<01:58,  1.26s/it]Evaluating:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 107/200 [02:17<01:57,  1.26s/it]Evaluating:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 108/200 [02:18<01:55,  1.26s/it]Evaluating:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 109/200 [02:19<01:54,  1.26s/it]Evaluating:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 110/200 [02:20<01:53,  1.26s/it]Evaluating:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 111/200 [02:22<01:52,  1.26s/it]Evaluating:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 112/200 [02:23<01:50,  1.26s/it]Evaluating:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 113/200 [02:24<01:49,  1.26s/it]Evaluating:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 114/200 [02:26<01:48,  1.26s/it]Evaluating:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 115/200 [02:27<01:47,  1.26s/it]Evaluating:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 116/200 [02:28<01:45,  1.26s/it]Evaluating:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 117/200 [02:29<01:44,  1.26s/it]Evaluating:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 118/200 [02:31<01:43,  1.26s/it]Evaluating:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 119/200 [02:32<01:41,  1.26s/it]Evaluating:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 120/200 [02:33<01:40,  1.26s/it]Evaluating:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 121/200 [02:34<01:39,  1.26s/it]Evaluating:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 122/200 [02:36<01:38,  1.26s/it]Evaluating:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 123/200 [02:37<01:36,  1.26s/it]Evaluating:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 124/200 [02:38<01:35,  1.26s/it]Evaluating:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 125/200 [02:39<01:34,  1.26s/it]Evaluating:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 126/200 [02:41<01:33,  1.26s/it]Evaluating:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 127/200 [02:42<01:31,  1.26s/it]Evaluating:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 128/200 [02:43<01:30,  1.26s/it]Evaluating:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 129/200 [02:44<01:29,  1.26s/it]Evaluating:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 130/200 [02:46<01:28,  1.26s/it]Evaluating:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 131/200 [02:47<01:27,  1.26s/it]Evaluating:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 132/200 [02:48<01:25,  1.26s/it]Evaluating:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 133/200 [02:49<01:24,  1.26s/it]Evaluating:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 134/200 [02:51<01:23,  1.27s/it]Evaluating:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 135/200 [02:52<01:22,  1.26s/it]Evaluating:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 136/200 [02:53<01:20,  1.26s/it]Evaluating:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 137/200 [02:55<01:19,  1.26s/it]Evaluating:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 138/200 [02:56<01:18,  1.26s/it]Evaluating:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 139/200 [02:57<01:17,  1.26s/it]Evaluating:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 140/200 [02:58<01:15,  1.26s/it]Evaluating:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 141/200 [03:00<01:14,  1.26s/it]Evaluating:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 142/200 [03:01<01:13,  1.26s/it]Evaluating:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 143/200 [03:02<01:12,  1.27s/it]Evaluating:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 144/200 [03:03<01:10,  1.27s/it]Evaluating:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 145/200 [03:05<01:10,  1.28s/it]Evaluating:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 146/200 [03:06<01:09,  1.28s/it]Evaluating:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 147/200 [03:07<01:07,  1.27s/it]Evaluating:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 148/200 [03:08<01:05,  1.27s/it]Evaluating:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 149/200 [03:10<01:04,  1.27s/it]Evaluating:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 150/200 [03:11<01:03,  1.26s/it]Evaluating:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 151/200 [03:12<01:01,  1.26s/it]Evaluating:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 152/200 [03:14<01:00,  1.26s/it]Evaluating:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 153/200 [03:15<00:59,  1.26s/it]Evaluating:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 154/200 [03:16<00:58,  1.26s/it]Evaluating:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 155/200 [03:17<00:56,  1.26s/it]Evaluating:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 156/200 [03:19<00:55,  1.26s/it]Evaluating:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 157/200 [03:20<00:54,  1.26s/it]Evaluating:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 158/200 [03:21<00:52,  1.26s/it]Evaluating:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 159/200 [03:22<00:51,  1.26s/it]Evaluating:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 160/200 [03:24<00:50,  1.26s/it]Evaluating:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 161/200 [03:25<00:49,  1.26s/it]Evaluating:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 162/200 [03:26<00:47,  1.26s/it]Evaluating:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 163/200 [03:27<00:46,  1.26s/it]Evaluating:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 164/200 [03:29<00:45,  1.26s/it]Evaluating:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 165/200 [03:30<00:44,  1.26s/it]Evaluating:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 166/200 [03:31<00:42,  1.26s/it]Evaluating:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 167/200 [03:32<00:41,  1.26s/it]Evaluating:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 168/200 [03:34<00:40,  1.26s/it]Evaluating:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 169/200 [03:35<00:39,  1.26s/it]Evaluating:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 170/200 [03:36<00:37,  1.26s/it]Evaluating:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 171/200 [03:37<00:36,  1.26s/it]Evaluating:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 172/200 [03:39<00:35,  1.26s/it]Evaluating:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 173/200 [03:40<00:34,  1.26s/it]Evaluating:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 174/200 [03:41<00:32,  1.26s/it]Evaluating:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 175/200 [03:43<00:31,  1.26s/it]Evaluating:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 176/200 [03:44<00:30,  1.26s/it]Evaluating:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 177/200 [03:45<00:28,  1.26s/it]Evaluating:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 178/200 [03:46<00:27,  1.26s/it]Evaluating:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 179/200 [03:48<00:26,  1.26s/it]Evaluating:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 180/200 [03:49<00:25,  1.26s/it]Evaluating:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 181/200 [03:50<00:24,  1.26s/it]Evaluating:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 182/200 [03:51<00:22,  1.27s/it]Evaluating:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 183/200 [03:53<00:21,  1.27s/it]Evaluating:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 184/200 [03:54<00:20,  1.26s/it]Evaluating:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 185/200 [03:55<00:18,  1.26s/it]Evaluating:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 186/200 [03:56<00:17,  1.26s/it]Evaluating:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 187/200 [03:58<00:16,  1.26s/it]Evaluating:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 188/200 [03:59<00:15,  1.26s/it]Evaluating:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 189/200 [04:00<00:13,  1.26s/it]Evaluating:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 190/200 [04:02<00:12,  1.28s/it]Evaluating:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 191/200 [04:03<00:11,  1.28s/it]Evaluating:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 192/200 [04:04<00:10,  1.28s/it]Evaluating:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 193/200 [04:05<00:08,  1.28s/it]Evaluating:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 194/200 [04:07<00:07,  1.27s/it]Evaluating:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 195/200 [04:08<00:06,  1.27s/it]Evaluating:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 196/200 [04:09<00:05,  1.27s/it]Evaluating:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 197/200 [04:10<00:03,  1.27s/it]Evaluating:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 198/200 [04:12<00:02,  1.26s/it]Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 199/200 [04:13<00:01,  1.26s/it]Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [04:14<00:00,  1.27s/it]Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [04:14<00:00,  1.27s/it]
Evaluating:   0%|          | 0/198 [00:00<?, ?it/s]Evaluating:   1%|          | 1/198 [00:01<04:09,  1.27s/it]Evaluating:   1%|          | 2/198 [00:02<04:08,  1.27s/it]Evaluating:   2%|â–         | 3/198 [00:03<04:10,  1.28s/it]Evaluating:   2%|â–         | 4/198 [00:05<04:07,  1.28s/it]Evaluating:   3%|â–Ž         | 5/198 [00:06<04:05,  1.27s/it]Evaluating:   3%|â–Ž         | 6/198 [00:07<04:03,  1.27s/it]Evaluating:   4%|â–Ž         | 7/198 [00:08<04:01,  1.27s/it]Evaluating:   4%|â–         | 8/198 [00:10<04:00,  1.26s/it]Evaluating:   5%|â–         | 9/198 [00:11<03:58,  1.26s/it]Evaluating:   5%|â–Œ         | 10/198 [00:12<03:56,  1.26s/it]Evaluating:   6%|â–Œ         | 11/198 [00:13<03:55,  1.26s/it]Evaluating:   6%|â–Œ         | 12/198 [00:15<03:54,  1.26s/it]Evaluating:   7%|â–‹         | 13/198 [00:16<03:53,  1.26s/it]Evaluating:   7%|â–‹         | 14/198 [00:17<03:52,  1.26s/it]Evaluating:   8%|â–Š         | 15/198 [00:18<03:51,  1.26s/it]Evaluating:   8%|â–Š         | 16/198 [00:20<03:50,  1.27s/it]Evaluating:   9%|â–Š         | 17/198 [00:21<03:49,  1.27s/it]Evaluating:   9%|â–‰         | 18/198 [00:22<03:47,  1.27s/it]Evaluating:  10%|â–‰         | 19/198 [00:24<03:46,  1.26s/it]Evaluating:  10%|â–ˆ         | 20/198 [00:25<03:44,  1.26s/it]Evaluating:  11%|â–ˆ         | 21/198 [00:26<03:43,  1.26s/it]Evaluating:  11%|â–ˆ         | 22/198 [00:27<03:42,  1.26s/it]Evaluating:  12%|â–ˆâ–        | 23/198 [00:29<03:40,  1.26s/it]Evaluating:  12%|â–ˆâ–        | 24/198 [00:30<03:40,  1.27s/it]Evaluating:  13%|â–ˆâ–Ž        | 25/198 [00:31<03:38,  1.27s/it]Evaluating:  13%|â–ˆâ–Ž        | 26/198 [00:32<03:37,  1.26s/it]Evaluating:  14%|â–ˆâ–Ž        | 27/198 [00:34<03:36,  1.26s/it]Evaluating:  14%|â–ˆâ–        | 28/198 [00:35<03:34,  1.26s/it]Evaluating:  15%|â–ˆâ–        | 29/198 [00:36<03:33,  1.27s/it]Evaluating:  15%|â–ˆâ–Œ        | 30/198 [00:37<03:32,  1.27s/it]Evaluating:  16%|â–ˆâ–Œ        | 31/198 [00:39<03:31,  1.27s/it]Evaluating:  16%|â–ˆâ–Œ        | 32/198 [00:40<03:29,  1.26s/it]Evaluating:  17%|â–ˆâ–‹        | 33/198 [00:41<03:28,  1.26s/it]Evaluating:  17%|â–ˆâ–‹        | 34/198 [00:43<03:27,  1.26s/it]Evaluating:  18%|â–ˆâ–Š        | 35/198 [00:44<03:26,  1.26s/it]Evaluating:  18%|â–ˆâ–Š        | 36/198 [00:45<03:24,  1.26s/it]Evaluating:  19%|â–ˆâ–Š        | 37/198 [00:46<03:23,  1.27s/it]Evaluating:  19%|â–ˆâ–‰        | 38/198 [00:48<03:22,  1.27s/it]Evaluating:  20%|â–ˆâ–‰        | 39/198 [00:49<03:20,  1.26s/it]Evaluating:  20%|â–ˆâ–ˆ        | 40/198 [00:50<03:19,  1.26s/it]Evaluating:  21%|â–ˆâ–ˆ        | 41/198 [00:51<03:18,  1.26s/it]Evaluating:  21%|â–ˆâ–ˆ        | 42/198 [00:53<03:17,  1.26s/it]Evaluating:  22%|â–ˆâ–ˆâ–       | 43/198 [00:54<03:15,  1.26s/it]Evaluating:  22%|â–ˆâ–ˆâ–       | 44/198 [00:55<03:14,  1.26s/it]Evaluating:  23%|â–ˆâ–ˆâ–Ž       | 45/198 [00:56<03:12,  1.26s/it]Evaluating:  23%|â–ˆâ–ˆâ–Ž       | 46/198 [00:58<03:11,  1.26s/it]Evaluating:  24%|â–ˆâ–ˆâ–Ž       | 47/198 [00:59<03:10,  1.26s/it]Evaluating:  24%|â–ˆâ–ˆâ–       | 48/198 [01:00<03:09,  1.26s/it]Evaluating:  25%|â–ˆâ–ˆâ–       | 49/198 [01:01<03:08,  1.26s/it]Evaluating:  25%|â–ˆâ–ˆâ–Œ       | 50/198 [01:03<03:06,  1.26s/it]Evaluating:  26%|â–ˆâ–ˆâ–Œ       | 51/198 [01:04<03:05,  1.26s/it]Evaluating:  26%|â–ˆâ–ˆâ–‹       | 52/198 [01:05<03:04,  1.26s/it]Evaluating:  27%|â–ˆâ–ˆâ–‹       | 53/198 [01:07<03:03,  1.26s/it]Evaluating:  27%|â–ˆâ–ˆâ–‹       | 54/198 [01:08<03:02,  1.26s/it]Evaluating:  28%|â–ˆâ–ˆâ–Š       | 55/198 [01:09<03:00,  1.26s/it]Evaluating:  28%|â–ˆâ–ˆâ–Š       | 56/198 [01:10<02:59,  1.26s/it]Evaluating:  29%|â–ˆâ–ˆâ–‰       | 57/198 [01:12<02:57,  1.26s/it]Evaluating:  29%|â–ˆâ–ˆâ–‰       | 58/198 [01:13<02:56,  1.26s/it]Evaluating:  30%|â–ˆâ–ˆâ–‰       | 59/198 [01:14<02:55,  1.26s/it]Evaluating:  30%|â–ˆâ–ˆâ–ˆ       | 60/198 [01:15<02:54,  1.26s/it]Evaluating:  31%|â–ˆâ–ˆâ–ˆ       | 61/198 [01:17<02:53,  1.26s/it]Evaluating:  31%|â–ˆâ–ˆâ–ˆâ–      | 62/198 [01:18<02:51,  1.26s/it]Evaluating:  32%|â–ˆâ–ˆâ–ˆâ–      | 63/198 [01:19<02:50,  1.26s/it]Evaluating:  32%|â–ˆâ–ˆâ–ˆâ–      | 64/198 [01:20<02:49,  1.27s/it]Evaluating:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 65/198 [01:22<02:48,  1.27s/it]Evaluating:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 66/198 [01:23<02:46,  1.26s/it]Evaluating:  34%|â–ˆâ–ˆâ–ˆâ–      | 67/198 [01:24<02:45,  1.26s/it]Evaluating:  34%|â–ˆâ–ˆâ–ˆâ–      | 68/198 [01:25<02:44,  1.26s/it]Evaluating:  35%|â–ˆâ–ˆâ–ˆâ–      | 69/198 [01:27<02:42,  1.26s/it]Evaluating:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 70/198 [01:28<02:41,  1.26s/it]Evaluating:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 71/198 [01:29<02:40,  1.27s/it]Evaluating:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 72/198 [01:31<02:39,  1.27s/it]Evaluating:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 73/198 [01:32<02:38,  1.26s/it]Evaluating:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 74/198 [01:33<02:36,  1.26s/it]Evaluating:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 75/198 [01:34<02:35,  1.26s/it]Evaluating:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 76/198 [01:36<02:34,  1.27s/it]Evaluating:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 77/198 [01:37<02:33,  1.27s/it]Evaluating:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 78/198 [01:38<02:32,  1.27s/it]Evaluating:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 79/198 [01:39<02:30,  1.27s/it]Evaluating:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 80/198 [01:41<02:29,  1.27s/it]Evaluating:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 81/198 [01:42<02:27,  1.26s/it]Evaluating:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 82/198 [01:43<02:26,  1.26s/it]Evaluating:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 83/198 [01:44<02:25,  1.26s/it]Evaluating:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 84/198 [01:46<02:24,  1.26s/it]Evaluating:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 85/198 [01:47<02:22,  1.26s/it]Evaluating:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 86/198 [01:48<02:21,  1.27s/it]Evaluating:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 87/198 [01:49<02:20,  1.27s/it]Evaluating:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 88/198 [01:51<02:19,  1.26s/it]Evaluating:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 89/198 [01:52<02:17,  1.26s/it]Evaluating:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 90/198 [01:53<02:16,  1.26s/it]Evaluating:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 91/198 [01:55<02:15,  1.26s/it]Evaluating:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 92/198 [01:56<02:13,  1.26s/it]Evaluating:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 93/198 [01:57<02:12,  1.26s/it]Evaluating:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 94/198 [01:58<02:11,  1.26s/it]Evaluating:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 95/198 [02:00<02:10,  1.26s/it]Evaluating:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 96/198 [02:01<02:08,  1.26s/it]Evaluating:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 97/198 [02:02<02:07,  1.26s/it]Evaluating:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 98/198 [02:03<02:06,  1.26s/it]Evaluating:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 99/198 [02:05<02:05,  1.26s/it]Evaluating:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 100/198 [02:06<02:03,  1.26s/it]Evaluating:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 101/198 [02:07<02:02,  1.26s/it]Evaluating:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 102/198 [02:08<02:01,  1.26s/it]Evaluating:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 103/198 [02:10<01:59,  1.26s/it]Evaluating:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 104/198 [02:11<01:58,  1.26s/it]Evaluating:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 105/198 [02:12<01:57,  1.26s/it]Evaluating:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 106/198 [02:13<01:55,  1.26s/it]Evaluating:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 107/198 [02:15<01:54,  1.26s/it]Evaluating:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 108/198 [02:16<01:53,  1.26s/it]Evaluating:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 109/198 [02:17<01:52,  1.26s/it]Evaluating:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 110/198 [02:19<01:51,  1.26s/it]Evaluating:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 111/198 [02:20<01:50,  1.26s/it]Evaluating:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 112/198 [02:21<01:48,  1.27s/it]Evaluating:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 113/198 [02:22<01:47,  1.26s/it]Evaluating:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 114/198 [02:24<01:46,  1.26s/it]Evaluating:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 115/198 [02:25<01:44,  1.26s/it]Evaluating:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 116/198 [02:26<01:43,  1.26s/it]Evaluating:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 117/198 [02:27<01:42,  1.26s/it]Evaluating:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 118/198 [02:29<01:40,  1.26s/it]Evaluating:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 119/198 [02:30<01:39,  1.26s/it]Evaluating:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 120/198 [02:31<01:38,  1.26s/it]Evaluating:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 121/198 [02:32<01:37,  1.26s/it]Evaluating:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 122/198 [02:34<01:36,  1.26s/it]Evaluating:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 123/198 [02:35<01:34,  1.26s/it]Evaluating:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 124/198 [02:36<01:33,  1.27s/it]Evaluating:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 125/198 [02:37<01:32,  1.27s/it]Evaluating:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 126/198 [02:39<01:30,  1.26s/it]Evaluating:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 127/198 [02:40<01:29,  1.26s/it]Evaluating:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 128/198 [02:41<01:28,  1.26s/it]Evaluating:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 129/198 [02:43<01:26,  1.26s/it]Evaluating:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 130/198 [02:44<01:25,  1.26s/it]Evaluating:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 131/198 [02:45<01:24,  1.26s/it]Evaluating:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 132/198 [02:46<01:23,  1.26s/it]Evaluating:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 133/198 [02:48<01:22,  1.26s/it]Evaluating:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 134/198 [02:49<01:22,  1.28s/it]Evaluating:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 135/198 [02:50<01:20,  1.28s/it]Evaluating:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 136/198 [02:51<01:18,  1.27s/it]Evaluating:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 137/198 [02:53<01:17,  1.27s/it]Evaluating:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 138/198 [02:54<01:15,  1.26s/it]Evaluating:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 139/198 [02:55<01:14,  1.26s/it]Evaluating:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 140/198 [02:56<01:13,  1.26s/it]Evaluating:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 141/198 [02:58<01:11,  1.26s/it]Evaluating:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 142/198 [02:59<01:10,  1.26s/it]Evaluating:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 143/198 [03:00<01:09,  1.26s/it]Evaluating:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 144/198 [03:02<01:08,  1.27s/it]Evaluating:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 145/198 [03:03<01:07,  1.26s/it]Evaluating:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 146/198 [03:04<01:05,  1.27s/it]Evaluating:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 147/198 [03:05<01:04,  1.26s/it]Evaluating:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 148/198 [03:07<01:03,  1.26s/it]Evaluating:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 149/198 [03:08<01:01,  1.26s/it]Evaluating:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 150/198 [03:09<01:00,  1.26s/it]Evaluating:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 151/198 [03:10<00:59,  1.26s/it]Evaluating:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 152/198 [03:12<00:57,  1.26s/it]Evaluating:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 153/198 [03:13<00:56,  1.26s/it]Evaluating:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 154/198 [03:14<00:55,  1.26s/it]Evaluating:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 155/198 [03:15<00:54,  1.26s/it]Evaluating:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 156/198 [03:17<00:53,  1.26s/it]Evaluating:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 157/198 [03:18<00:51,  1.26s/it]Evaluating:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 158/198 [03:19<00:50,  1.26s/it]Evaluating:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 159/198 [03:20<00:49,  1.26s/it]Evaluating:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 160/198 [03:22<00:48,  1.26s/it]Evaluating:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 161/198 [03:23<00:46,  1.26s/it]Evaluating:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 162/198 [03:24<00:45,  1.28s/it]Evaluating:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 163/198 [03:26<00:44,  1.27s/it]Evaluating:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 164/198 [03:27<00:43,  1.27s/it]Evaluating:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 165/198 [03:28<00:41,  1.26s/it]Evaluating:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 166/198 [03:29<00:40,  1.26s/it]Evaluating:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 167/198 [03:31<00:39,  1.26s/it]Evaluating:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 168/198 [03:32<00:37,  1.26s/it]Evaluating:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 169/198 [03:33<00:36,  1.26s/it]Evaluating:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 170/198 [03:34<00:35,  1.26s/it]Evaluating:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 171/198 [03:36<00:34,  1.26s/it]Evaluating:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 172/198 [03:37<00:32,  1.26s/it]Evaluating:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 173/198 [03:38<00:31,  1.26s/it]Evaluating:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 174/198 [03:39<00:30,  1.26s/it]Evaluating:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 175/198 [03:41<00:28,  1.26s/it]Evaluating:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 176/198 [03:42<00:27,  1.26s/it]Evaluating:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 177/198 [03:43<00:26,  1.26s/it]Evaluating:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 178/198 [03:44<00:25,  1.26s/it]Evaluating:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 179/198 [03:46<00:23,  1.26s/it]Evaluating:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 180/198 [03:47<00:22,  1.26s/it]Evaluating:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 181/198 [03:48<00:21,  1.26s/it]Evaluating:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 182/198 [03:49<00:20,  1.26s/it]Evaluating:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 183/198 [03:51<00:18,  1.26s/it]Evaluating:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 184/198 [03:52<00:17,  1.26s/it]Evaluating:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 185/198 [03:53<00:16,  1.26s/it]Evaluating:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 186/198 [03:55<00:15,  1.26s/it]Evaluating:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 187/198 [03:56<00:13,  1.26s/it]Evaluating:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 188/198 [03:57<00:12,  1.26s/it]Evaluating:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 189/198 [03:58<00:11,  1.26s/it]Evaluating:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 190/198 [04:00<00:10,  1.26s/it]Evaluating:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 191/198 [04:01<00:08,  1.26s/it]Evaluating:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 192/198 [04:02<00:07,  1.26s/it]Evaluating:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 193/198 [04:03<00:06,  1.26s/it]Evaluating:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 194/198 [04:05<00:05,  1.26s/it]Evaluating:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 195/198 [04:06<00:03,  1.26s/it]Evaluating:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 196/198 [04:07<00:02,  1.26s/it]Evaluating:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 197/198 [04:08<00:01,  1.26s/it]Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 198/198 [04:10<00:00,  1.26s/it]Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 198/198 [04:10<00:00,  1.26s/it]
Running 5/8: python ./experiments/inference_chat.py /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-12 ./data/llama_fingerprint_l2 publish --dont_load_adapter -t llama2 -o /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-12
['python', './experiments/inference_chat.py', '/fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-12', './data/llama_fingerprint_l2', 'publish', '--dont_load_adapter', '-t', 'llama2', '-o', '/fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-12']
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
[2024-08-02 19:44:36,483] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:04<00:09,  4.71s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:09<00:04,  4.62s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:12<00:00,  4.00s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:12<00:00,  4.18s/it]
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:520: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:520: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Evaluating:   0%|          | 1/200 [00:02<08:08,  2.45s/it]Evaluating:   1%|          | 2/200 [00:03<05:47,  1.76s/it]Evaluating:   2%|â–         | 3/200 [00:05<05:08,  1.57s/it]Evaluating:   2%|â–         | 4/200 [00:06<04:43,  1.45s/it]Evaluating:   2%|â–Ž         | 5/200 [00:07<04:32,  1.40s/it]Evaluating:   3%|â–Ž         | 6/200 [00:08<04:22,  1.35s/it]Evaluating:   4%|â–Ž         | 7/200 [00:10<04:14,  1.32s/it]Evaluating:   4%|â–         | 8/200 [00:11<04:08,  1.29s/it]Evaluating:   4%|â–         | 9/200 [00:12<04:04,  1.28s/it]Evaluating:   5%|â–Œ         | 10/200 [00:13<04:00,  1.27s/it]Evaluating:   6%|â–Œ         | 11/200 [00:15<03:57,  1.26s/it]Evaluating:   6%|â–Œ         | 12/200 [00:16<04:02,  1.29s/it]Evaluating:   6%|â–‹         | 13/200 [00:17<03:58,  1.28s/it]Evaluating:   7%|â–‹         | 14/200 [00:18<03:54,  1.26s/it]Evaluating:   8%|â–Š         | 15/200 [00:20<03:52,  1.25s/it]Evaluating:   8%|â–Š         | 16/200 [00:21<03:49,  1.25s/it]Evaluating:   8%|â–Š         | 17/200 [00:22<03:48,  1.25s/it]Evaluating:   9%|â–‰         | 18/200 [00:23<03:46,  1.24s/it]Evaluating:  10%|â–‰         | 19/200 [00:25<03:42,  1.23s/it]Evaluating:  10%|â–ˆ         | 20/200 [00:26<03:41,  1.23s/it]Evaluating:  10%|â–ˆ         | 21/200 [00:27<03:40,  1.23s/it]Evaluating:  11%|â–ˆ         | 22/200 [00:28<03:40,  1.24s/it]Evaluating:  12%|â–ˆâ–        | 23/200 [00:30<03:39,  1.24s/it]Evaluating:  12%|â–ˆâ–        | 24/200 [00:31<03:37,  1.24s/it]Evaluating:  12%|â–ˆâ–Ž        | 25/200 [00:32<03:35,  1.23s/it]Evaluating:  13%|â–ˆâ–Ž        | 26/200 [00:33<03:32,  1.22s/it]Evaluating:  14%|â–ˆâ–Ž        | 27/200 [00:34<03:30,  1.22s/it]Evaluating:  14%|â–ˆâ–        | 28/200 [00:36<03:29,  1.22s/it]Evaluating:  14%|â–ˆâ–        | 29/200 [00:37<03:29,  1.22s/it]Evaluating:  15%|â–ˆâ–Œ        | 30/200 [00:38<03:29,  1.23s/it]Evaluating:  16%|â–ˆâ–Œ        | 31/200 [00:39<03:28,  1.23s/it]Evaluating:  16%|â–ˆâ–Œ        | 32/200 [00:41<03:27,  1.24s/it]Evaluating:  16%|â–ˆâ–‹        | 33/200 [00:42<03:26,  1.24s/it]Evaluating:  17%|â–ˆâ–‹        | 34/200 [00:43<03:25,  1.24s/it]Evaluating:  18%|â–ˆâ–Š        | 35/200 [00:44<03:24,  1.24s/it]Evaluating:  18%|â–ˆâ–Š        | 36/200 [00:46<03:22,  1.24s/it]Evaluating:  18%|â–ˆâ–Š        | 37/200 [00:47<03:21,  1.24s/it]Evaluating:  19%|â–ˆâ–‰        | 38/200 [00:48<03:20,  1.24s/it]Evaluating:  20%|â–ˆâ–‰        | 39/200 [00:49<03:19,  1.24s/it]Evaluating:  20%|â–ˆâ–ˆ        | 40/200 [00:51<03:18,  1.24s/it]Evaluating:  20%|â–ˆâ–ˆ        | 41/200 [00:52<03:16,  1.24s/it]Evaluating:  21%|â–ˆâ–ˆ        | 42/200 [00:53<03:15,  1.24s/it]Evaluating:  22%|â–ˆâ–ˆâ–       | 43/200 [00:54<03:14,  1.24s/it]Evaluating:  22%|â–ˆâ–ˆâ–       | 44/200 [00:55<03:13,  1.24s/it]Evaluating:  22%|â–ˆâ–ˆâ–Ž       | 45/200 [00:57<03:12,  1.24s/it]Evaluating:  23%|â–ˆâ–ˆâ–Ž       | 46/200 [00:58<03:12,  1.25s/it]Evaluating:  24%|â–ˆâ–ˆâ–Ž       | 47/200 [00:59<03:10,  1.25s/it]Evaluating:  24%|â–ˆâ–ˆâ–       | 48/200 [01:00<03:09,  1.24s/it]Evaluating:  24%|â–ˆâ–ˆâ–       | 49/200 [01:02<03:07,  1.24s/it]Evaluating:  25%|â–ˆâ–ˆâ–Œ       | 50/200 [01:03<03:06,  1.24s/it]Evaluating:  26%|â–ˆâ–ˆâ–Œ       | 51/200 [01:04<03:04,  1.24s/it]Evaluating:  26%|â–ˆâ–ˆâ–Œ       | 52/200 [01:05<03:02,  1.23s/it]Evaluating:  26%|â–ˆâ–ˆâ–‹       | 53/200 [01:07<03:01,  1.23s/it]Evaluating:  27%|â–ˆâ–ˆâ–‹       | 54/200 [01:08<03:00,  1.23s/it]Evaluating:  28%|â–ˆâ–ˆâ–Š       | 55/200 [01:09<02:58,  1.23s/it]Evaluating:  28%|â–ˆâ–ˆâ–Š       | 56/200 [01:10<02:58,  1.24s/it]Evaluating:  28%|â–ˆâ–ˆâ–Š       | 57/200 [01:12<02:58,  1.25s/it]Evaluating:  29%|â–ˆâ–ˆâ–‰       | 58/200 [01:13<02:58,  1.25s/it]Evaluating:  30%|â–ˆâ–ˆâ–‰       | 59/200 [01:14<02:57,  1.26s/it]Evaluating:  30%|â–ˆâ–ˆâ–ˆ       | 60/200 [01:15<02:56,  1.26s/it]Evaluating:  30%|â–ˆâ–ˆâ–ˆ       | 61/200 [01:17<02:55,  1.26s/it]Evaluating:  31%|â–ˆâ–ˆâ–ˆ       | 62/200 [01:18<02:54,  1.26s/it]Evaluating:  32%|â–ˆâ–ˆâ–ˆâ–      | 63/200 [01:19<02:53,  1.26s/it]Evaluating:  32%|â–ˆâ–ˆâ–ˆâ–      | 64/200 [01:20<02:51,  1.26s/it]Evaluating:  32%|â–ˆâ–ˆâ–ˆâ–Ž      | 65/200 [01:22<02:50,  1.26s/it]Evaluating:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 66/200 [01:23<02:49,  1.27s/it]Evaluating:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 67/200 [01:24<02:48,  1.27s/it]Evaluating:  34%|â–ˆâ–ˆâ–ˆâ–      | 68/200 [01:26<02:47,  1.27s/it]Evaluating:  34%|â–ˆâ–ˆâ–ˆâ–      | 69/200 [01:27<02:45,  1.27s/it]Evaluating:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 70/200 [01:28<02:43,  1.26s/it]Evaluating:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 71/200 [01:29<02:40,  1.25s/it]Evaluating:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 72/200 [01:30<02:38,  1.24s/it]Evaluating:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 73/200 [01:32<02:36,  1.23s/it]Evaluating:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 74/200 [01:33<02:34,  1.23s/it]Evaluating:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 75/200 [01:34<02:33,  1.23s/it]Evaluating:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 76/200 [01:35<02:30,  1.22s/it]Evaluating:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 77/200 [01:37<02:30,  1.22s/it]Evaluating:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 78/200 [01:38<02:28,  1.22s/it]Evaluating:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 79/200 [01:39<02:27,  1.22s/it]Evaluating:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 80/200 [01:40<02:26,  1.22s/it]Evaluating:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 81/200 [01:41<02:25,  1.22s/it]Evaluating:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 82/200 [01:43<02:24,  1.23s/it]Evaluating:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 83/200 [01:44<02:24,  1.24s/it]Evaluating:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 84/200 [01:45<02:24,  1.25s/it]Evaluating:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 85/200 [01:47<02:24,  1.26s/it]Evaluating:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 86/200 [01:48<02:23,  1.26s/it]Evaluating:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 87/200 [01:49<02:22,  1.26s/it]Evaluating:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 88/200 [01:50<02:21,  1.26s/it]Evaluating:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 89/200 [01:52<02:20,  1.26s/it]Evaluating:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 90/200 [01:53<02:19,  1.27s/it]Evaluating:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 91/200 [01:54<02:17,  1.27s/it]Evaluating:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 92/200 [01:55<02:16,  1.27s/it]Evaluating:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 93/200 [01:57<02:15,  1.27s/it]Evaluating:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 94/200 [01:58<02:14,  1.27s/it]Evaluating:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 95/200 [01:59<02:12,  1.27s/it]Evaluating:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 96/200 [02:00<02:11,  1.27s/it]Evaluating:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 97/200 [02:02<02:10,  1.27s/it]Evaluating:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 98/200 [02:03<02:09,  1.27s/it]Evaluating:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 99/200 [02:04<02:07,  1.26s/it]Evaluating:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 100/200 [02:05<02:06,  1.26s/it]Evaluating:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 101/200 [02:07<02:03,  1.25s/it]Evaluating:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 102/200 [02:08<02:01,  1.24s/it]Evaluating:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 103/200 [02:09<02:00,  1.24s/it]Evaluating:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 104/200 [02:10<01:58,  1.24s/it]Evaluating:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 105/200 [02:12<01:57,  1.23s/it]Evaluating:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 106/200 [02:13<01:55,  1.23s/it]Evaluating:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 107/200 [02:14<01:54,  1.23s/it]Evaluating:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 108/200 [02:15<01:53,  1.23s/it]Evaluating:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 109/200 [02:16<01:50,  1.22s/it]Evaluating:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 110/200 [02:18<01:49,  1.21s/it]Evaluating:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 111/200 [02:19<01:47,  1.21s/it]Evaluating:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 112/200 [02:20<01:46,  1.21s/it]Evaluating:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 113/200 [02:21<01:45,  1.22s/it]Evaluating:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 114/200 [02:23<01:44,  1.22s/it]Evaluating:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 115/200 [02:24<01:43,  1.22s/it]Evaluating:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 116/200 [02:25<01:42,  1.22s/it]Evaluating:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 117/200 [02:26<01:41,  1.22s/it]Evaluating:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 118/200 [02:27<01:40,  1.22s/it]Evaluating:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 119/200 [02:29<01:39,  1.22s/it]Evaluating:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 120/200 [02:30<01:38,  1.23s/it]Evaluating:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 121/200 [02:31<01:36,  1.23s/it]Evaluating:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 122/200 [02:32<01:35,  1.23s/it]Evaluating:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 123/200 [02:34<01:34,  1.23s/it]Evaluating:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 124/200 [02:35<01:33,  1.23s/it]Evaluating:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 125/200 [02:36<01:32,  1.23s/it]Evaluating:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 126/200 [02:37<01:30,  1.23s/it]Evaluating:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 127/200 [02:39<01:29,  1.23s/it]Evaluating:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 128/200 [02:40<01:28,  1.23s/it]Evaluating:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 129/200 [02:41<01:27,  1.23s/it]Evaluating:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 130/200 [02:42<01:25,  1.23s/it]Evaluating:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 131/200 [02:43<01:24,  1.23s/it]Evaluating:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 132/200 [02:45<01:23,  1.23s/it]Evaluating:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 133/200 [02:46<01:22,  1.23s/it]Evaluating:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 134/200 [02:47<01:20,  1.23s/it]Evaluating:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 135/200 [02:48<01:19,  1.23s/it]Evaluating:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 136/200 [02:50<01:18,  1.23s/it]Evaluating:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 137/200 [02:51<01:17,  1.23s/it]Evaluating:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 138/200 [02:52<01:15,  1.23s/it]Evaluating:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 139/200 [02:53<01:14,  1.23s/it]Evaluating:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 140/200 [02:54<01:13,  1.23s/it]Evaluating:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 141/200 [02:56<01:12,  1.23s/it]Evaluating:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 142/200 [02:57<01:11,  1.23s/it]Evaluating:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 143/200 [02:58<01:09,  1.23s/it]Evaluating:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 144/200 [02:59<01:08,  1.23s/it]Evaluating:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 145/200 [03:01<01:07,  1.23s/it]Evaluating:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 146/200 [03:02<01:06,  1.23s/it]Evaluating:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 147/200 [03:03<01:05,  1.23s/it]Evaluating:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 148/200 [03:04<01:03,  1.23s/it]Evaluating:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 149/200 [03:06<01:02,  1.23s/it]Evaluating:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 150/200 [03:07<01:01,  1.23s/it]Evaluating:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 151/200 [03:08<01:00,  1.23s/it]Evaluating:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 152/200 [03:09<00:59,  1.23s/it]Evaluating:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 153/200 [03:10<00:57,  1.23s/it]Evaluating:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 154/200 [03:12<00:56,  1.23s/it]Evaluating:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 155/200 [03:13<00:55,  1.22s/it]Evaluating:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 156/200 [03:14<00:53,  1.23s/it]Evaluating:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 157/200 [03:15<00:52,  1.23s/it]Evaluating:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 158/200 [03:17<00:51,  1.23s/it]Evaluating:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 159/200 [03:18<00:50,  1.23s/it]Evaluating:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 160/200 [03:19<00:48,  1.22s/it]Evaluating:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 161/200 [03:20<00:47,  1.23s/it]Evaluating:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 162/200 [03:21<00:46,  1.23s/it]Evaluating:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 163/200 [03:23<00:45,  1.23s/it]Evaluating:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 164/200 [03:24<00:44,  1.23s/it]Evaluating:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 165/200 [03:25<00:42,  1.23s/it]Evaluating:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 166/200 [03:26<00:41,  1.22s/it]Evaluating:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 167/200 [03:28<00:40,  1.23s/it]Evaluating:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 168/200 [03:29<00:39,  1.24s/it]Evaluating:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 169/200 [03:30<00:38,  1.25s/it]Evaluating:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 170/200 [03:31<00:37,  1.26s/it]Evaluating:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 171/200 [03:33<00:36,  1.26s/it]Evaluating:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 172/200 [03:34<00:35,  1.26s/it]Evaluating:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 173/200 [03:35<00:34,  1.26s/it]Evaluating:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 174/200 [03:36<00:32,  1.27s/it]Evaluating:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 175/200 [03:38<00:31,  1.27s/it]Evaluating:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 176/200 [03:39<00:30,  1.27s/it]Evaluating:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 177/200 [03:40<00:29,  1.27s/it]Evaluating:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 178/200 [03:42<00:27,  1.27s/it]Evaluating:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 179/200 [03:43<00:26,  1.27s/it]Evaluating:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 180/200 [03:44<00:25,  1.27s/it]Evaluating:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 181/200 [03:45<00:24,  1.27s/it]Evaluating:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 182/200 [03:47<00:22,  1.27s/it]Evaluating:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 183/200 [03:48<00:21,  1.27s/it]Evaluating:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 184/200 [03:49<00:20,  1.27s/it]Evaluating:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 185/200 [03:50<00:19,  1.27s/it]Evaluating:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 186/200 [03:52<00:17,  1.27s/it]Evaluating:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 187/200 [03:53<00:16,  1.27s/it]Evaluating:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 188/200 [03:54<00:15,  1.27s/it]Evaluating:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 189/200 [03:55<00:13,  1.27s/it]Evaluating:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 190/200 [03:57<00:12,  1.27s/it]Evaluating:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 191/200 [03:58<00:11,  1.27s/it]Evaluating:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 192/200 [03:59<00:10,  1.27s/it]Evaluating:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 193/200 [04:01<00:08,  1.27s/it]Evaluating:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 194/200 [04:02<00:07,  1.27s/it]Evaluating:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 195/200 [04:03<00:06,  1.27s/it]Evaluating:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 196/200 [04:04<00:05,  1.26s/it]Evaluating:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 197/200 [04:06<00:03,  1.26s/it]Evaluating:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 198/200 [04:07<00:02,  1.25s/it]Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 199/200 [04:08<00:01,  1.25s/it]Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [04:09<00:00,  1.25s/it]Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [04:09<00:00,  1.25s/it]
Evaluating:   0%|          | 0/198 [00:00<?, ?it/s]Evaluating:   1%|          | 1/198 [00:01<04:03,  1.24s/it]Evaluating:   1%|          | 2/198 [00:02<04:02,  1.24s/it]Evaluating:   2%|â–         | 3/198 [00:03<04:01,  1.24s/it]Evaluating:   2%|â–         | 4/198 [00:04<03:59,  1.24s/it]Evaluating:   3%|â–Ž         | 5/198 [00:06<03:58,  1.24s/it]Evaluating:   3%|â–Ž         | 6/198 [00:07<03:57,  1.24s/it]Evaluating:   4%|â–Ž         | 7/198 [00:08<03:56,  1.24s/it]Evaluating:   4%|â–         | 8/198 [00:09<03:55,  1.24s/it]Evaluating:   5%|â–         | 9/198 [00:11<03:53,  1.24s/it]Evaluating:   5%|â–Œ         | 10/198 [00:12<03:52,  1.24s/it]Evaluating:   6%|â–Œ         | 11/198 [00:13<03:51,  1.24s/it]Evaluating:   6%|â–Œ         | 12/198 [00:14<03:49,  1.24s/it]Evaluating:   7%|â–‹         | 13/198 [00:16<03:48,  1.24s/it]Evaluating:   7%|â–‹         | 14/198 [00:17<03:47,  1.24s/it]Evaluating:   8%|â–Š         | 15/198 [00:18<03:46,  1.24s/it]Evaluating:   8%|â–Š         | 16/198 [00:19<03:44,  1.23s/it]Evaluating:   9%|â–Š         | 17/198 [00:21<03:43,  1.24s/it]Evaluating:   9%|â–‰         | 18/198 [00:22<03:42,  1.24s/it]Evaluating:  10%|â–‰         | 19/198 [00:23<03:41,  1.24s/it]Evaluating:  10%|â–ˆ         | 20/198 [00:24<03:39,  1.24s/it]Evaluating:  11%|â–ˆ         | 21/198 [00:25<03:39,  1.24s/it]Evaluating:  11%|â–ˆ         | 22/198 [00:27<03:38,  1.24s/it]Evaluating:  12%|â–ˆâ–        | 23/198 [00:28<03:37,  1.24s/it]Evaluating:  12%|â–ˆâ–        | 24/198 [00:29<03:35,  1.24s/it]Evaluating:  13%|â–ˆâ–Ž        | 25/198 [00:30<03:34,  1.24s/it]Evaluating:  13%|â–ˆâ–Ž        | 26/198 [00:32<03:32,  1.24s/it]Evaluating:  14%|â–ˆâ–Ž        | 27/198 [00:33<03:31,  1.24s/it]Evaluating:  14%|â–ˆâ–        | 28/198 [00:34<03:30,  1.24s/it]Evaluating:  15%|â–ˆâ–        | 29/198 [00:35<03:28,  1.24s/it]Evaluating:  15%|â–ˆâ–Œ        | 30/198 [00:37<03:27,  1.24s/it]Evaluating:  16%|â–ˆâ–Œ        | 31/198 [00:38<03:26,  1.24s/it]Evaluating:  16%|â–ˆâ–Œ        | 32/198 [00:39<03:25,  1.24s/it]Evaluating:  17%|â–ˆâ–‹        | 33/198 [00:40<03:23,  1.24s/it]Evaluating:  17%|â–ˆâ–‹        | 34/198 [00:42<03:22,  1.24s/it]Evaluating:  18%|â–ˆâ–Š        | 35/198 [00:43<03:21,  1.24s/it]Evaluating:  18%|â–ˆâ–Š        | 36/198 [00:44<03:20,  1.24s/it]Evaluating:  19%|â–ˆâ–Š        | 37/198 [00:45<03:18,  1.24s/it]Evaluating:  19%|â–ˆâ–‰        | 38/198 [00:47<03:18,  1.24s/it]Evaluating:  20%|â–ˆâ–‰        | 39/198 [00:48<03:17,  1.24s/it]Evaluating:  20%|â–ˆâ–ˆ        | 40/198 [00:49<03:15,  1.24s/it]Evaluating:  21%|â–ˆâ–ˆ        | 41/198 [00:50<03:14,  1.24s/it]Evaluating:  21%|â–ˆâ–ˆ        | 42/198 [00:51<03:13,  1.24s/it]Evaluating:  22%|â–ˆâ–ˆâ–       | 43/198 [00:53<03:12,  1.24s/it]Evaluating:  22%|â–ˆâ–ˆâ–       | 44/198 [00:54<03:11,  1.24s/it]Evaluating:  23%|â–ˆâ–ˆâ–Ž       | 45/198 [00:55<03:09,  1.24s/it]Evaluating:  23%|â–ˆâ–ˆâ–Ž       | 46/198 [00:56<03:08,  1.24s/it]Evaluating:  24%|â–ˆâ–ˆâ–Ž       | 47/198 [00:58<03:07,  1.24s/it]Evaluating:  24%|â–ˆâ–ˆâ–       | 48/198 [00:59<03:06,  1.24s/it]Evaluating:  25%|â–ˆâ–ˆâ–       | 49/198 [01:00<03:05,  1.24s/it]Evaluating:  25%|â–ˆâ–ˆâ–Œ       | 50/198 [01:01<03:04,  1.24s/it]Evaluating:  26%|â–ˆâ–ˆâ–Œ       | 51/198 [01:03<03:04,  1.25s/it]Evaluating:  26%|â–ˆâ–ˆâ–‹       | 52/198 [01:04<03:02,  1.25s/it]Evaluating:  27%|â–ˆâ–ˆâ–‹       | 53/198 [01:05<03:01,  1.25s/it]Evaluating:  27%|â–ˆâ–ˆâ–‹       | 54/198 [01:06<03:00,  1.25s/it]Evaluating:  28%|â–ˆâ–ˆâ–Š       | 55/198 [01:08<02:58,  1.25s/it]Evaluating:  28%|â–ˆâ–ˆâ–Š       | 56/198 [01:09<02:56,  1.25s/it]Evaluating:  29%|â–ˆâ–ˆâ–‰       | 57/198 [01:10<02:55,  1.24s/it]Evaluating:  29%|â–ˆâ–ˆâ–‰       | 58/198 [01:11<02:53,  1.24s/it]Evaluating:  30%|â–ˆâ–ˆâ–‰       | 59/198 [01:13<02:52,  1.24s/it]Evaluating:  30%|â–ˆâ–ˆâ–ˆ       | 60/198 [01:14<02:50,  1.24s/it]Evaluating:  31%|â–ˆâ–ˆâ–ˆ       | 61/198 [01:15<02:49,  1.24s/it]Evaluating:  31%|â–ˆâ–ˆâ–ˆâ–      | 62/198 [01:16<02:48,  1.24s/it]Evaluating:  32%|â–ˆâ–ˆâ–ˆâ–      | 63/198 [01:18<02:47,  1.24s/it]Evaluating:  32%|â–ˆâ–ˆâ–ˆâ–      | 64/198 [01:19<02:45,  1.24s/it]Evaluating:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 65/198 [01:20<02:44,  1.24s/it]Evaluating:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 66/198 [01:21<02:43,  1.24s/it]Evaluating:  34%|â–ˆâ–ˆâ–ˆâ–      | 67/198 [01:23<02:41,  1.24s/it]Evaluating:  34%|â–ˆâ–ˆâ–ˆâ–      | 68/198 [01:24<02:40,  1.24s/it]Evaluating:  35%|â–ˆâ–ˆâ–ˆâ–      | 69/198 [01:25<02:39,  1.24s/it]Evaluating:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 70/198 [01:26<02:38,  1.24s/it]Evaluating:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 71/198 [01:27<02:37,  1.24s/it]Evaluating:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 72/198 [01:29<02:36,  1.24s/it]Evaluating:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 73/198 [01:30<02:35,  1.24s/it]Evaluating:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 74/198 [01:31<02:33,  1.24s/it]Evaluating:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 75/198 [01:32<02:32,  1.24s/it]Evaluating:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 76/198 [01:34<02:31,  1.24s/it]Evaluating:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 77/198 [01:35<02:29,  1.24s/it]Evaluating:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 78/198 [01:36<02:28,  1.24s/it]Evaluating:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 79/198 [01:37<02:27,  1.24s/it]Evaluating:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 80/198 [01:39<02:25,  1.24s/it]Evaluating:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 81/198 [01:40<02:24,  1.24s/it]Evaluating:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 82/198 [01:41<02:23,  1.24s/it]Evaluating:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 83/198 [01:42<02:22,  1.24s/it]Evaluating:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 84/198 [01:44<02:21,  1.24s/it]Evaluating:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 85/198 [01:45<02:20,  1.24s/it]Evaluating:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 86/198 [01:46<02:18,  1.24s/it]Evaluating:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 87/198 [01:47<02:17,  1.24s/it]Evaluating:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 88/198 [01:49<02:16,  1.24s/it]Evaluating:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 89/198 [01:50<02:14,  1.24s/it]Evaluating:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 90/198 [01:51<02:13,  1.24s/it]Evaluating:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 91/198 [01:52<02:13,  1.25s/it]Evaluating:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 92/198 [01:54<02:12,  1.25s/it]Evaluating:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 93/198 [01:55<02:12,  1.26s/it]Evaluating:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 94/198 [01:56<02:11,  1.26s/it]Evaluating:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 95/198 [01:57<02:10,  1.26s/it]Evaluating:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 96/198 [01:59<02:09,  1.26s/it]Evaluating:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 97/198 [02:00<02:07,  1.26s/it]Evaluating:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 98/198 [02:01<02:06,  1.26s/it]Evaluating:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 99/198 [02:02<02:05,  1.26s/it]Evaluating:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 100/198 [02:04<02:03,  1.26s/it]Evaluating:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 101/198 [02:05<02:02,  1.26s/it]Evaluating:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 102/198 [02:06<02:01,  1.26s/it]Evaluating:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 103/198 [02:07<01:59,  1.26s/it]Evaluating:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 104/198 [02:09<01:58,  1.26s/it]Evaluating:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 105/198 [02:10<01:57,  1.26s/it]Evaluating:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 106/198 [02:11<01:56,  1.26s/it]Evaluating:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 107/198 [02:12<01:54,  1.26s/it]Evaluating:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 108/198 [02:14<01:53,  1.26s/it]Evaluating:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 109/198 [02:15<01:52,  1.26s/it]Evaluating:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 110/198 [02:16<01:51,  1.26s/it]Evaluating:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 111/198 [02:18<01:49,  1.26s/it]Evaluating:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 112/198 [02:19<01:48,  1.26s/it]Evaluating:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 113/198 [02:20<01:47,  1.26s/it]Evaluating:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 114/198 [02:21<01:46,  1.26s/it]Evaluating:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 115/198 [02:23<01:44,  1.26s/it]Evaluating:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 116/198 [02:24<01:43,  1.26s/it]Evaluating:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 117/198 [02:25<01:42,  1.26s/it]Evaluating:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 118/198 [02:26<01:40,  1.26s/it]Evaluating:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 119/198 [02:28<01:40,  1.27s/it]Evaluating:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 120/198 [02:29<01:38,  1.27s/it]Evaluating:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 121/198 [02:30<01:37,  1.26s/it]Evaluating:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 122/198 [02:31<01:35,  1.26s/it]Evaluating:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 123/198 [02:33<01:35,  1.27s/it]Evaluating:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 124/198 [02:34<01:33,  1.27s/it]Evaluating:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 125/198 [02:35<01:32,  1.26s/it]Evaluating:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 126/198 [02:37<01:31,  1.26s/it]Evaluating:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 127/198 [02:38<01:29,  1.26s/it]Evaluating:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 128/198 [02:39<01:28,  1.26s/it]Evaluating:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 129/198 [02:40<01:27,  1.26s/it]Evaluating:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 130/198 [02:42<01:25,  1.26s/it]Evaluating:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 131/198 [02:43<01:24,  1.26s/it]Evaluating:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 132/198 [02:44<01:23,  1.26s/it]Evaluating:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 133/198 [02:45<01:21,  1.26s/it]Evaluating:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 134/198 [02:47<01:20,  1.26s/it]Evaluating:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 135/198 [02:48<01:19,  1.26s/it]Evaluating:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 136/198 [02:49<01:18,  1.26s/it]Evaluating:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 137/198 [02:50<01:16,  1.26s/it]Evaluating:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 138/198 [02:52<01:15,  1.26s/it]Evaluating:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 139/198 [02:53<01:14,  1.26s/it]Evaluating:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 140/198 [02:54<01:13,  1.26s/it]Evaluating:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 141/198 [02:55<01:11,  1.26s/it]Evaluating:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 142/198 [02:57<01:10,  1.27s/it]Evaluating:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 143/198 [02:58<01:09,  1.27s/it]Evaluating:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 144/198 [02:59<01:08,  1.27s/it]Evaluating:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 145/198 [03:00<01:06,  1.26s/it]Evaluating:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 146/198 [03:02<01:05,  1.26s/it]Evaluating:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 147/198 [03:03<01:04,  1.26s/it]Evaluating:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 148/198 [03:04<01:03,  1.26s/it]Evaluating:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 149/198 [03:06<01:01,  1.26s/it]Evaluating:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 150/198 [03:07<01:00,  1.26s/it]Evaluating:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 151/198 [03:08<00:59,  1.26s/it]Evaluating:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 152/198 [03:09<00:57,  1.26s/it]Evaluating:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 153/198 [03:11<00:56,  1.26s/it]Evaluating:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 154/198 [03:12<00:55,  1.26s/it]Evaluating:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 155/198 [03:13<00:54,  1.26s/it]Evaluating:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 156/198 [03:14<00:52,  1.26s/it]Evaluating:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 157/198 [03:16<00:51,  1.26s/it]Evaluating:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 158/198 [03:17<00:50,  1.26s/it]Evaluating:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 159/198 [03:18<00:49,  1.26s/it]Evaluating:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 160/198 [03:19<00:47,  1.26s/it]Evaluating:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 161/198 [03:21<00:46,  1.26s/it]Evaluating:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 162/198 [03:22<00:45,  1.27s/it]Evaluating:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 163/198 [03:23<00:44,  1.26s/it]Evaluating:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 164/198 [03:24<00:42,  1.26s/it]Evaluating:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 165/198 [03:26<00:41,  1.26s/it]Evaluating:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 166/198 [03:27<00:40,  1.26s/it]Evaluating:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 167/198 [03:28<00:39,  1.26s/it]Evaluating:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 168/198 [03:30<00:37,  1.26s/it]Evaluating:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 169/198 [03:31<00:36,  1.26s/it]Evaluating:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 170/198 [03:32<00:35,  1.26s/it]Evaluating:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 171/198 [03:33<00:34,  1.26s/it]Evaluating:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 172/198 [03:35<00:32,  1.26s/it]Evaluating:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 173/198 [03:36<00:31,  1.26s/it]Evaluating:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 174/198 [03:37<00:30,  1.26s/it]Evaluating:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 175/198 [03:38<00:28,  1.26s/it]Evaluating:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 176/198 [03:40<00:27,  1.26s/it]Evaluating:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 177/198 [03:41<00:26,  1.26s/it]Evaluating:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 178/198 [03:42<00:25,  1.26s/it]Evaluating:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 179/198 [03:43<00:23,  1.26s/it]Evaluating:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 180/198 [03:45<00:22,  1.26s/it]Evaluating:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 181/198 [03:46<00:21,  1.26s/it]Evaluating:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 182/198 [03:47<00:20,  1.26s/it]Evaluating:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 183/198 [03:48<00:18,  1.26s/it]Evaluating:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 184/198 [03:50<00:17,  1.26s/it]Evaluating:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 185/198 [03:51<00:16,  1.26s/it]Evaluating:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 186/198 [03:52<00:15,  1.27s/it]Evaluating:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 187/198 [03:53<00:13,  1.26s/it]Evaluating:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 188/198 [03:55<00:12,  1.26s/it]Evaluating:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 189/198 [03:56<00:11,  1.26s/it]Evaluating:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 190/198 [03:57<00:10,  1.26s/it]Evaluating:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 191/198 [03:59<00:08,  1.26s/it]Evaluating:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 192/198 [04:00<00:07,  1.26s/it]Evaluating:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 193/198 [04:01<00:06,  1.26s/it]Evaluating:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 194/198 [04:02<00:05,  1.26s/it]Evaluating:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 195/198 [04:04<00:03,  1.26s/it]Evaluating:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 196/198 [04:05<00:02,  1.26s/it]Evaluating:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 197/198 [04:06<00:01,  1.25s/it]Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 198/198 [04:07<00:00,  1.25s/it]Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 198/198 [04:07<00:00,  1.25s/it]
Running 6/8: python ./experiments/inference_chat.py /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-18 ./data/llama_fingerprint_l2 publish --dont_load_adapter -t llama2 -o /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-18
['python', './experiments/inference_chat.py', '/fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-18', './data/llama_fingerprint_l2', 'publish', '--dont_load_adapter', '-t', 'llama2', '-o', '/fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-18']
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
[2024-08-02 19:53:18,033] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:04<00:09,  4.69s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:09<00:04,  4.65s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:12<00:00,  4.00s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:12<00:00,  4.18s/it]
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:520: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:520: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Evaluating:   0%|          | 1/200 [00:02<08:11,  2.47s/it]Evaluating:   1%|          | 2/200 [00:03<05:50,  1.77s/it]Evaluating:   2%|â–         | 3/200 [00:05<05:12,  1.59s/it]Evaluating:   2%|â–         | 4/200 [00:06<04:46,  1.46s/it]Evaluating:   2%|â–Ž         | 5/200 [00:07<04:34,  1.41s/it]Evaluating:   3%|â–Ž         | 6/200 [00:08<04:24,  1.36s/it]Evaluating:   4%|â–Ž         | 7/200 [00:10<04:17,  1.33s/it]Evaluating:   4%|â–         | 8/200 [00:11<04:12,  1.32s/it]Evaluating:   4%|â–         | 9/200 [00:12<04:09,  1.30s/it]Evaluating:   5%|â–Œ         | 10/200 [00:14<04:06,  1.30s/it]Evaluating:   6%|â–Œ         | 11/200 [00:15<04:04,  1.29s/it]Evaluating:   6%|â–Œ         | 12/200 [00:16<04:02,  1.29s/it]Evaluating:   6%|â–‹         | 13/200 [00:17<04:00,  1.29s/it]Evaluating:   7%|â–‹         | 14/200 [00:19<03:59,  1.29s/it]Evaluating:   8%|â–Š         | 15/200 [00:20<03:57,  1.28s/it]Evaluating:   8%|â–Š         | 16/200 [00:21<03:55,  1.28s/it]Evaluating:   8%|â–Š         | 17/200 [00:23<03:53,  1.28s/it]Evaluating:   9%|â–‰         | 18/200 [00:24<03:52,  1.28s/it]Evaluating:  10%|â–‰         | 19/200 [00:25<03:50,  1.28s/it]Evaluating:  10%|â–ˆ         | 20/200 [00:26<03:49,  1.27s/it]Evaluating:  10%|â–ˆ         | 21/200 [00:28<03:47,  1.27s/it]Evaluating:  11%|â–ˆ         | 22/200 [00:29<03:46,  1.27s/it]Evaluating:  12%|â–ˆâ–        | 23/200 [00:30<03:45,  1.28s/it]Evaluating:  12%|â–ˆâ–        | 24/200 [00:31<03:44,  1.28s/it]Evaluating:  12%|â–ˆâ–Ž        | 25/200 [00:33<03:43,  1.28s/it]Evaluating:  13%|â–ˆâ–Ž        | 26/200 [00:34<03:42,  1.28s/it]Evaluating:  14%|â–ˆâ–Ž        | 27/200 [00:35<03:40,  1.28s/it]Evaluating:  14%|â–ˆâ–        | 28/200 [00:37<03:39,  1.28s/it]Evaluating:  14%|â–ˆâ–        | 29/200 [00:38<03:38,  1.28s/it]Evaluating:  15%|â–ˆâ–Œ        | 30/200 [00:39<03:36,  1.27s/it]Evaluating:  16%|â–ˆâ–Œ        | 31/200 [00:40<03:36,  1.28s/it]Evaluating:  16%|â–ˆâ–Œ        | 32/200 [00:42<03:35,  1.28s/it]Evaluating:  16%|â–ˆâ–‹        | 33/200 [00:43<03:33,  1.28s/it]Evaluating:  17%|â–ˆâ–‹        | 34/200 [00:44<03:32,  1.28s/it]Evaluating:  18%|â–ˆâ–Š        | 35/200 [00:46<03:30,  1.28s/it]Evaluating:  18%|â–ˆâ–Š        | 36/200 [00:47<03:29,  1.28s/it]Evaluating:  18%|â–ˆâ–Š        | 37/200 [00:48<03:28,  1.28s/it]Evaluating:  19%|â–ˆâ–‰        | 38/200 [00:49<03:27,  1.28s/it]Evaluating:  20%|â–ˆâ–‰        | 39/200 [00:51<03:25,  1.28s/it]Evaluating:  20%|â–ˆâ–ˆ        | 40/200 [00:52<03:24,  1.28s/it]Evaluating:  20%|â–ˆâ–ˆ        | 41/200 [00:53<03:22,  1.28s/it]Evaluating:  21%|â–ˆâ–ˆ        | 42/200 [00:54<03:21,  1.28s/it]Evaluating:  22%|â–ˆâ–ˆâ–       | 43/200 [00:56<03:20,  1.28s/it]Evaluating:  22%|â–ˆâ–ˆâ–       | 44/200 [00:57<03:18,  1.27s/it]Evaluating:  22%|â–ˆâ–ˆâ–Ž       | 45/200 [00:58<03:17,  1.27s/it]Evaluating:  23%|â–ˆâ–ˆâ–Ž       | 46/200 [01:00<03:18,  1.29s/it]Evaluating:  24%|â–ˆâ–ˆâ–Ž       | 47/200 [01:01<03:16,  1.28s/it]Evaluating:  24%|â–ˆâ–ˆâ–       | 48/200 [01:02<03:14,  1.28s/it]Evaluating:  24%|â–ˆâ–ˆâ–       | 49/200 [01:03<03:13,  1.28s/it]Evaluating:  25%|â–ˆâ–ˆâ–Œ       | 50/200 [01:05<03:11,  1.28s/it]Evaluating:  26%|â–ˆâ–ˆâ–Œ       | 51/200 [01:06<03:10,  1.28s/it]Evaluating:  26%|â–ˆâ–ˆâ–Œ       | 52/200 [01:07<03:08,  1.28s/it]Evaluating:  26%|â–ˆâ–ˆâ–‹       | 53/200 [01:09<03:07,  1.27s/it]Evaluating:  27%|â–ˆâ–ˆâ–‹       | 54/200 [01:10<03:05,  1.27s/it]Evaluating:  28%|â–ˆâ–ˆâ–Š       | 55/200 [01:11<03:04,  1.27s/it]Evaluating:  28%|â–ˆâ–ˆâ–Š       | 56/200 [01:12<03:03,  1.27s/it]Evaluating:  28%|â–ˆâ–ˆâ–Š       | 57/200 [01:14<03:01,  1.27s/it]Evaluating:  29%|â–ˆâ–ˆâ–‰       | 58/200 [01:15<03:00,  1.27s/it]Evaluating:  30%|â–ˆâ–ˆâ–‰       | 59/200 [01:16<02:59,  1.27s/it]Evaluating:  30%|â–ˆâ–ˆâ–ˆ       | 60/200 [01:17<02:58,  1.27s/it]Evaluating:  30%|â–ˆâ–ˆâ–ˆ       | 61/200 [01:19<02:57,  1.27s/it]Evaluating:  31%|â–ˆâ–ˆâ–ˆ       | 62/200 [01:20<02:55,  1.28s/it]Evaluating:  32%|â–ˆâ–ˆâ–ˆâ–      | 63/200 [01:21<02:55,  1.28s/it]Evaluating:  32%|â–ˆâ–ˆâ–ˆâ–      | 64/200 [01:23<02:53,  1.28s/it]Evaluating:  32%|â–ˆâ–ˆâ–ˆâ–Ž      | 65/200 [01:24<02:52,  1.27s/it]Evaluating:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 66/200 [01:25<02:50,  1.27s/it]Evaluating:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 67/200 [01:26<02:49,  1.27s/it]Evaluating:  34%|â–ˆâ–ˆâ–ˆâ–      | 68/200 [01:28<02:47,  1.27s/it]Evaluating:  34%|â–ˆâ–ˆâ–ˆâ–      | 69/200 [01:29<02:46,  1.27s/it]Evaluating:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 70/200 [01:30<02:45,  1.27s/it]Evaluating:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 71/200 [01:31<02:44,  1.28s/it]Evaluating:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 72/200 [01:33<02:43,  1.28s/it]Evaluating:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 73/200 [01:34<02:42,  1.28s/it]Evaluating:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 74/200 [01:35<02:41,  1.28s/it]Evaluating:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 75/200 [01:37<02:39,  1.28s/it]Evaluating:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 76/200 [01:38<02:38,  1.28s/it]Evaluating:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 77/200 [01:39<02:36,  1.28s/it]Evaluating:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 78/200 [01:40<02:35,  1.27s/it]Evaluating:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 79/200 [01:42<02:33,  1.27s/it]Evaluating:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 80/200 [01:43<02:32,  1.27s/it]Evaluating:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 81/200 [01:44<02:31,  1.27s/it]Evaluating:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 82/200 [01:45<02:30,  1.27s/it]Evaluating:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 83/200 [01:47<02:29,  1.27s/it]Evaluating:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 84/200 [01:48<02:27,  1.28s/it]Evaluating:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 85/200 [01:49<02:26,  1.28s/it]Evaluating:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 86/200 [01:51<02:25,  1.28s/it]Evaluating:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 87/200 [01:52<02:24,  1.28s/it]Evaluating:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 88/200 [01:53<02:23,  1.28s/it]Evaluating:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 89/200 [01:54<02:21,  1.28s/it]Evaluating:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 90/200 [01:56<02:20,  1.27s/it]Evaluating:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 91/200 [01:57<02:18,  1.27s/it]Evaluating:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 92/200 [01:58<02:17,  1.27s/it]Evaluating:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 93/200 [01:59<02:15,  1.27s/it]Evaluating:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 94/200 [02:01<02:14,  1.27s/it]Evaluating:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 95/200 [02:02<02:13,  1.27s/it]Evaluating:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 96/200 [02:03<02:11,  1.27s/it]Evaluating:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 97/200 [02:05<02:10,  1.27s/it]Evaluating:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 98/200 [02:06<02:09,  1.27s/it]Evaluating:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 99/200 [02:07<02:07,  1.27s/it]Evaluating:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 100/200 [02:08<02:06,  1.27s/it]Evaluating:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 101/200 [02:10<02:05,  1.27s/it]Evaluating:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 102/200 [02:11<02:04,  1.27s/it]Evaluating:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 103/200 [02:12<02:02,  1.27s/it]Evaluating:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 104/200 [02:13<02:01,  1.27s/it]Evaluating:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 105/200 [02:15<02:00,  1.27s/it]Evaluating:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 106/200 [02:16<01:59,  1.27s/it]Evaluating:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 107/200 [02:17<01:58,  1.27s/it]Evaluating:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 108/200 [02:19<01:57,  1.27s/it]Evaluating:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 109/200 [02:20<01:56,  1.28s/it]Evaluating:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 110/200 [02:21<01:55,  1.28s/it]Evaluating:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 111/200 [02:22<01:53,  1.28s/it]Evaluating:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 112/200 [02:24<01:52,  1.28s/it]Evaluating:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 113/200 [02:25<01:51,  1.28s/it]Evaluating:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 114/200 [02:26<01:49,  1.28s/it]Evaluating:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 115/200 [02:27<01:48,  1.27s/it]Evaluating:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 116/200 [02:29<01:47,  1.28s/it]Evaluating:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 117/200 [02:30<01:45,  1.28s/it]Evaluating:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 118/200 [02:31<01:44,  1.28s/it]Evaluating:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 119/200 [02:33<01:43,  1.28s/it]Evaluating:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 120/200 [02:34<01:42,  1.28s/it]Evaluating:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 121/200 [02:35<01:41,  1.28s/it]Evaluating:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 122/200 [02:36<01:40,  1.28s/it]Evaluating:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 123/200 [02:38<01:38,  1.28s/it]Evaluating:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 124/200 [02:39<01:37,  1.28s/it]Evaluating:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 125/200 [02:40<01:35,  1.28s/it]Evaluating:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 126/200 [02:42<01:34,  1.28s/it]Evaluating:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 127/200 [02:43<01:33,  1.28s/it]Evaluating:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 128/200 [02:44<01:32,  1.28s/it]Evaluating:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 129/200 [02:45<01:30,  1.28s/it]Evaluating:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 130/200 [02:47<01:29,  1.28s/it]Evaluating:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 131/200 [02:48<01:28,  1.28s/it]Evaluating:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 132/200 [02:49<01:27,  1.28s/it]Evaluating:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 133/200 [02:50<01:25,  1.28s/it]Evaluating:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 134/200 [02:52<01:24,  1.28s/it]Evaluating:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 135/200 [02:53<01:23,  1.28s/it]Evaluating:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 136/200 [02:54<01:22,  1.28s/it]Evaluating:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 137/200 [02:56<01:20,  1.28s/it]Evaluating:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 138/200 [02:57<01:19,  1.28s/it]Evaluating:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 139/200 [02:58<01:17,  1.28s/it]Evaluating:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 140/200 [02:59<01:16,  1.28s/it]Evaluating:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 141/200 [03:01<01:15,  1.28s/it]Evaluating:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 142/200 [03:02<01:14,  1.28s/it]Evaluating:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 143/200 [03:03<01:12,  1.28s/it]Evaluating:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 144/200 [03:05<01:11,  1.28s/it]Evaluating:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 145/200 [03:06<01:10,  1.28s/it]Evaluating:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 146/200 [03:07<01:09,  1.28s/it]Evaluating:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 147/200 [03:08<01:07,  1.28s/it]Evaluating:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 148/200 [03:10<01:06,  1.28s/it]Evaluating:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 149/200 [03:11<01:05,  1.28s/it]Evaluating:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 150/200 [03:12<01:03,  1.28s/it]Evaluating:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 151/200 [03:14<01:02,  1.28s/it]Evaluating:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 152/200 [03:15<01:01,  1.28s/it]Evaluating:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 153/200 [03:16<01:00,  1.28s/it]Evaluating:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 154/200 [03:17<00:58,  1.28s/it]Evaluating:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 155/200 [03:19<00:57,  1.28s/it]Evaluating:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 156/200 [03:20<00:56,  1.28s/it]Evaluating:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 157/200 [03:21<00:55,  1.28s/it]Evaluating:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 158/200 [03:22<00:53,  1.28s/it]Evaluating:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 159/200 [03:24<00:53,  1.30s/it]Evaluating:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 160/200 [03:25<00:51,  1.29s/it]Evaluating:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 161/200 [03:26<00:50,  1.29s/it]Evaluating:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 162/200 [03:28<00:48,  1.28s/it]Evaluating:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 163/200 [03:29<00:47,  1.28s/it]Evaluating:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 164/200 [03:30<00:46,  1.28s/it]Evaluating:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 165/200 [03:31<00:44,  1.28s/it]Evaluating:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 166/200 [03:33<00:43,  1.28s/it]Evaluating:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 167/200 [03:34<00:42,  1.28s/it]Evaluating:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 168/200 [03:35<00:40,  1.28s/it]Evaluating:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 169/200 [03:37<00:39,  1.28s/it]Evaluating:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 170/200 [03:38<00:38,  1.28s/it]Evaluating:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 171/200 [03:39<00:37,  1.28s/it]Evaluating:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 172/200 [03:40<00:35,  1.28s/it]Evaluating:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 173/200 [03:42<00:34,  1.28s/it]Evaluating:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 174/200 [03:43<00:33,  1.28s/it]Evaluating:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 175/200 [03:44<00:31,  1.28s/it]Evaluating:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 176/200 [03:46<00:30,  1.28s/it]Evaluating:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 177/200 [03:47<00:29,  1.28s/it]Evaluating:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 178/200 [03:48<00:28,  1.28s/it]Evaluating:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 179/200 [03:49<00:26,  1.28s/it]Evaluating:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 180/200 [03:51<00:25,  1.28s/it]Evaluating:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 181/200 [03:52<00:24,  1.28s/it]Evaluating:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 182/200 [03:53<00:23,  1.28s/it]Evaluating:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 183/200 [03:55<00:21,  1.28s/it]Evaluating:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 184/200 [03:56<00:20,  1.28s/it]Evaluating:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 185/200 [03:57<00:19,  1.28s/it]Evaluating:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 186/200 [03:58<00:17,  1.28s/it]Evaluating:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 187/200 [04:00<00:16,  1.28s/it]Evaluating:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 188/200 [04:01<00:15,  1.28s/it]Evaluating:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 189/200 [04:02<00:14,  1.28s/it]Evaluating:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 190/200 [04:03<00:12,  1.28s/it]Evaluating:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 191/200 [04:05<00:11,  1.28s/it]Evaluating:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 192/200 [04:06<00:10,  1.28s/it]Evaluating:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 193/200 [04:07<00:08,  1.28s/it]Evaluating:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 194/200 [04:09<00:07,  1.28s/it]Evaluating:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 195/200 [04:10<00:06,  1.28s/it]Evaluating:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 196/200 [04:11<00:05,  1.28s/it]Evaluating:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 197/200 [04:12<00:03,  1.28s/it]Evaluating:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 198/200 [04:14<00:02,  1.28s/it]Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 199/200 [04:15<00:01,  1.28s/it]Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [04:16<00:00,  1.28s/it]Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [04:16<00:00,  1.28s/it]
Evaluating:   0%|          | 0/198 [00:00<?, ?it/s]Evaluating:   1%|          | 1/198 [00:01<04:11,  1.28s/it]Evaluating:   1%|          | 2/198 [00:02<04:09,  1.27s/it]Evaluating:   2%|â–         | 3/198 [00:03<04:09,  1.28s/it]Evaluating:   2%|â–         | 4/198 [00:05<04:08,  1.28s/it]Evaluating:   3%|â–Ž         | 5/198 [00:06<04:07,  1.28s/it]Evaluating:   3%|â–Ž         | 6/198 [00:07<04:06,  1.28s/it]Evaluating:   4%|â–Ž         | 7/198 [00:08<04:04,  1.28s/it]Evaluating:   4%|â–         | 8/198 [00:10<04:03,  1.28s/it]Evaluating:   5%|â–         | 9/198 [00:11<04:02,  1.28s/it]Evaluating:   5%|â–Œ         | 10/198 [00:13<04:23,  1.40s/it]Evaluating:   6%|â–Œ         | 11/198 [00:14<04:14,  1.36s/it]Evaluating:   6%|â–Œ         | 12/198 [00:15<04:08,  1.34s/it]Evaluating:   7%|â–‹         | 13/198 [00:17<04:03,  1.32s/it]Evaluating:   7%|â–‹         | 14/198 [00:18<04:00,  1.31s/it]Evaluating:   8%|â–Š         | 15/198 [00:19<03:57,  1.30s/it]Evaluating:   8%|â–Š         | 16/198 [00:20<03:55,  1.29s/it]Evaluating:   9%|â–Š         | 17/198 [00:22<03:53,  1.29s/it]Evaluating:   9%|â–‰         | 18/198 [00:23<03:51,  1.29s/it]Evaluating:  10%|â–‰         | 19/198 [00:24<03:49,  1.28s/it]Evaluating:  10%|â–ˆ         | 20/198 [00:25<03:47,  1.28s/it]Evaluating:  11%|â–ˆ         | 21/198 [00:27<03:46,  1.28s/it]Evaluating:  11%|â–ˆ         | 22/198 [00:28<03:45,  1.28s/it]Evaluating:  12%|â–ˆâ–        | 23/198 [00:29<03:43,  1.28s/it]Evaluating:  12%|â–ˆâ–        | 24/198 [00:31<03:41,  1.28s/it]Evaluating:  13%|â–ˆâ–Ž        | 25/198 [00:32<03:40,  1.28s/it]Evaluating:  13%|â–ˆâ–Ž        | 26/198 [00:33<03:39,  1.28s/it]Evaluating:  14%|â–ˆâ–Ž        | 27/198 [00:34<03:38,  1.28s/it]Evaluating:  14%|â–ˆâ–        | 28/198 [00:36<03:37,  1.28s/it]Evaluating:  15%|â–ˆâ–        | 29/198 [00:37<03:36,  1.28s/it]Evaluating:  15%|â–ˆâ–Œ        | 30/198 [00:38<03:34,  1.28s/it]Evaluating:  16%|â–ˆâ–Œ        | 31/198 [00:40<03:33,  1.28s/it]Evaluating:  16%|â–ˆâ–Œ        | 32/198 [00:41<03:32,  1.28s/it]Evaluating:  17%|â–ˆâ–‹        | 33/198 [00:42<03:30,  1.28s/it]Evaluating:  17%|â–ˆâ–‹        | 34/198 [00:43<03:29,  1.28s/it]Evaluating:  18%|â–ˆâ–Š        | 35/198 [00:45<03:28,  1.28s/it]Evaluating:  18%|â–ˆâ–Š        | 36/198 [00:46<03:26,  1.28s/it]Evaluating:  19%|â–ˆâ–Š        | 37/198 [00:47<03:25,  1.28s/it]Evaluating:  19%|â–ˆâ–‰        | 38/198 [00:48<03:24,  1.28s/it]Evaluating:  20%|â–ˆâ–‰        | 39/198 [00:50<03:23,  1.28s/it]Evaluating:  20%|â–ˆâ–ˆ        | 40/198 [00:51<03:22,  1.28s/it]Evaluating:  21%|â–ˆâ–ˆ        | 41/198 [00:52<03:21,  1.28s/it]Evaluating:  21%|â–ˆâ–ˆ        | 42/198 [00:54<03:19,  1.28s/it]Evaluating:  22%|â–ˆâ–ˆâ–       | 43/198 [00:55<03:18,  1.28s/it]Evaluating:  22%|â–ˆâ–ˆâ–       | 44/198 [00:56<03:17,  1.28s/it]Evaluating:  23%|â–ˆâ–ˆâ–Ž       | 45/198 [00:57<03:15,  1.28s/it]Evaluating:  23%|â–ˆâ–ˆâ–Ž       | 46/198 [00:59<03:13,  1.28s/it]Evaluating:  24%|â–ˆâ–ˆâ–Ž       | 47/198 [01:00<03:12,  1.28s/it]Evaluating:  24%|â–ˆâ–ˆâ–       | 48/198 [01:01<03:11,  1.28s/it]Evaluating:  25%|â–ˆâ–ˆâ–       | 49/198 [01:03<03:09,  1.27s/it]Evaluating:  25%|â–ˆâ–ˆâ–Œ       | 50/198 [01:04<03:09,  1.28s/it]Evaluating:  26%|â–ˆâ–ˆâ–Œ       | 51/198 [01:05<03:08,  1.28s/it]Evaluating:  26%|â–ˆâ–ˆâ–‹       | 52/198 [01:06<03:07,  1.28s/it]Evaluating:  27%|â–ˆâ–ˆâ–‹       | 53/198 [01:08<03:05,  1.28s/it]Evaluating:  27%|â–ˆâ–ˆâ–‹       | 54/198 [01:09<03:04,  1.28s/it]Evaluating:  28%|â–ˆâ–ˆâ–Š       | 55/198 [01:10<03:03,  1.28s/it]Evaluating:  28%|â–ˆâ–ˆâ–Š       | 56/198 [01:12<03:01,  1.28s/it]Evaluating:  29%|â–ˆâ–ˆâ–‰       | 57/198 [01:13<03:00,  1.28s/it]Evaluating:  29%|â–ˆâ–ˆâ–‰       | 58/198 [01:14<02:59,  1.28s/it]Evaluating:  30%|â–ˆâ–ˆâ–‰       | 59/198 [01:15<02:57,  1.28s/it]Evaluating:  30%|â–ˆâ–ˆâ–ˆ       | 60/198 [01:17<02:56,  1.28s/it]Evaluating:  31%|â–ˆâ–ˆâ–ˆ       | 61/198 [01:18<02:54,  1.28s/it]Evaluating:  31%|â–ˆâ–ˆâ–ˆâ–      | 62/198 [01:19<02:53,  1.28s/it]Evaluating:  32%|â–ˆâ–ˆâ–ˆâ–      | 63/198 [01:20<02:52,  1.28s/it]Evaluating:  32%|â–ˆâ–ˆâ–ˆâ–      | 64/198 [01:22<02:51,  1.28s/it]Evaluating:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 65/198 [01:23<02:50,  1.28s/it]Evaluating:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 66/198 [01:24<02:48,  1.28s/it]Evaluating:  34%|â–ˆâ–ˆâ–ˆâ–      | 67/198 [01:26<02:48,  1.29s/it]Evaluating:  34%|â–ˆâ–ˆâ–ˆâ–      | 68/198 [01:27<02:47,  1.29s/it]Evaluating:  35%|â–ˆâ–ˆâ–ˆâ–      | 69/198 [01:28<02:45,  1.28s/it]Evaluating:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 70/198 [01:29<02:43,  1.28s/it]Evaluating:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 71/198 [01:31<02:42,  1.28s/it]Evaluating:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 72/198 [01:32<02:40,  1.28s/it]Evaluating:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 73/198 [01:33<02:39,  1.28s/it]Evaluating:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 74/198 [01:35<02:38,  1.28s/it]Evaluating:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 75/198 [01:36<02:37,  1.28s/it]Evaluating:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 76/198 [01:37<02:36,  1.28s/it]Evaluating:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 77/198 [01:38<02:35,  1.28s/it]Evaluating:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 78/198 [01:40<02:33,  1.28s/it]Evaluating:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 79/198 [01:41<02:32,  1.28s/it]Evaluating:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 80/198 [01:42<02:31,  1.28s/it]Evaluating:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 81/198 [01:43<02:29,  1.28s/it]Evaluating:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 82/198 [01:45<02:28,  1.28s/it]Evaluating:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 83/198 [01:46<02:26,  1.28s/it]Evaluating:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 84/198 [01:47<02:25,  1.28s/it]Evaluating:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 85/198 [01:49<02:24,  1.28s/it]Evaluating:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 86/198 [01:50<02:22,  1.27s/it]Evaluating:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 87/198 [01:51<02:21,  1.28s/it]Evaluating:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 88/198 [01:52<02:20,  1.28s/it]Evaluating:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 89/198 [01:54<02:19,  1.28s/it]Evaluating:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 90/198 [01:55<02:18,  1.28s/it]Evaluating:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 91/198 [01:56<02:16,  1.28s/it]Evaluating:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 92/198 [01:58<02:15,  1.28s/it]Evaluating:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 93/198 [01:59<02:14,  1.28s/it]Evaluating:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 94/198 [02:00<02:12,  1.28s/it]Evaluating:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 95/198 [02:01<02:11,  1.28s/it]Evaluating:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 96/198 [02:03<02:10,  1.28s/it]Evaluating:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 97/198 [02:04<02:08,  1.27s/it]Evaluating:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 98/198 [02:05<02:08,  1.28s/it]Evaluating:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 99/198 [02:06<02:06,  1.28s/it]Evaluating:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 100/198 [02:08<02:05,  1.28s/it]Evaluating:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 101/198 [02:09<02:04,  1.28s/it]Evaluating:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 102/198 [02:10<02:02,  1.28s/it]Evaluating:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 103/198 [02:12<02:01,  1.28s/it]Evaluating:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 104/198 [02:13<02:00,  1.28s/it]Evaluating:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 105/198 [02:14<01:58,  1.28s/it]Evaluating:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 106/198 [02:15<01:57,  1.28s/it]Evaluating:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 107/198 [02:17<01:55,  1.27s/it]Evaluating:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 108/198 [02:18<01:54,  1.27s/it]Evaluating:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 109/198 [02:19<01:53,  1.27s/it]Evaluating:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 110/198 [02:21<01:52,  1.27s/it]Evaluating:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 111/198 [02:22<01:51,  1.28s/it]Evaluating:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 112/198 [02:23<01:49,  1.28s/it]Evaluating:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 113/198 [02:24<01:48,  1.28s/it]Evaluating:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 114/198 [02:26<01:47,  1.28s/it]Evaluating:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 115/198 [02:27<01:46,  1.28s/it]Evaluating:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 116/198 [02:28<01:45,  1.28s/it]Evaluating:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 117/198 [02:29<01:43,  1.28s/it]Evaluating:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 118/198 [02:31<01:43,  1.30s/it]Evaluating:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 119/198 [02:32<01:41,  1.29s/it]Evaluating:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 120/198 [02:33<01:40,  1.29s/it]Evaluating:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 121/198 [02:35<01:38,  1.28s/it]Evaluating:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 122/198 [02:36<01:37,  1.28s/it]Evaluating:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 123/198 [02:37<01:36,  1.28s/it]Evaluating:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 124/198 [02:38<01:34,  1.28s/it]Evaluating:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 125/198 [02:40<01:33,  1.28s/it]Evaluating:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 126/198 [02:41<01:32,  1.28s/it]Evaluating:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 127/198 [02:42<01:30,  1.28s/it]Evaluating:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 128/198 [02:44<01:29,  1.28s/it]Evaluating:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 129/198 [02:45<01:28,  1.28s/it]Evaluating:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 130/198 [02:46<01:26,  1.28s/it]Evaluating:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 131/198 [02:47<01:25,  1.28s/it]Evaluating:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 132/198 [02:49<01:24,  1.28s/it]Evaluating:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 133/198 [02:50<01:22,  1.27s/it]Evaluating:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 134/198 [02:51<01:21,  1.27s/it]Evaluating:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 135/198 [02:53<01:20,  1.28s/it]Evaluating:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 136/198 [02:54<01:19,  1.28s/it]Evaluating:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 137/198 [02:55<01:17,  1.28s/it]Evaluating:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 138/198 [02:56<01:16,  1.28s/it]Evaluating:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 139/198 [02:58<01:15,  1.28s/it]Evaluating:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 140/198 [02:59<01:14,  1.28s/it]Evaluating:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 141/198 [03:00<01:12,  1.28s/it]Evaluating:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 142/198 [03:01<01:11,  1.28s/it]Evaluating:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 143/198 [03:03<01:10,  1.28s/it]Evaluating:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 144/198 [03:04<01:09,  1.28s/it]Evaluating:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 145/198 [03:05<01:07,  1.28s/it]Evaluating:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 146/198 [03:07<01:06,  1.27s/it]Evaluating:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 147/198 [03:08<01:05,  1.28s/it]Evaluating:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 148/198 [03:09<01:03,  1.28s/it]Evaluating:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 149/198 [03:10<01:02,  1.28s/it]Evaluating:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 150/198 [03:12<01:01,  1.28s/it]Evaluating:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 151/198 [03:13<01:00,  1.28s/it]Evaluating:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 152/198 [03:14<00:58,  1.28s/it]Evaluating:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 153/198 [03:16<00:57,  1.28s/it]Evaluating:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 154/198 [03:17<00:56,  1.28s/it]Evaluating:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 155/198 [03:18<00:54,  1.28s/it]Evaluating:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 156/198 [03:19<00:53,  1.28s/it]Evaluating:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 157/198 [03:21<00:52,  1.28s/it]Evaluating:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 158/198 [03:22<00:50,  1.27s/it]Evaluating:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 159/198 [03:23<00:49,  1.27s/it]Evaluating:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 160/198 [03:24<00:48,  1.28s/it]Evaluating:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 161/198 [03:26<00:47,  1.28s/it]Evaluating:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 162/198 [03:27<00:45,  1.28s/it]Evaluating:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 163/198 [03:28<00:44,  1.28s/it]Evaluating:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 164/198 [03:30<00:43,  1.28s/it]Evaluating:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 165/198 [03:31<00:42,  1.28s/it]Evaluating:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 166/198 [03:32<00:40,  1.27s/it]Evaluating:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 167/198 [03:33<00:39,  1.27s/it]Evaluating:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 168/198 [03:35<00:38,  1.27s/it]Evaluating:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 169/198 [03:36<00:36,  1.27s/it]Evaluating:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 170/198 [03:37<00:35,  1.27s/it]Evaluating:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 171/198 [03:38<00:34,  1.28s/it]Evaluating:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 172/198 [03:40<00:33,  1.28s/it]Evaluating:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 173/198 [03:41<00:31,  1.28s/it]Evaluating:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 174/198 [03:42<00:30,  1.28s/it]Evaluating:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 175/198 [03:44<00:29,  1.28s/it]Evaluating:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 176/198 [03:45<00:28,  1.28s/it]Evaluating:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 177/198 [03:46<00:26,  1.28s/it]Evaluating:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 178/198 [03:47<00:25,  1.28s/it]Evaluating:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 179/198 [03:49<00:24,  1.28s/it]Evaluating:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 180/198 [03:50<00:22,  1.27s/it]Evaluating:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 181/198 [03:51<00:21,  1.28s/it]Evaluating:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 182/198 [03:53<00:20,  1.27s/it]Evaluating:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 183/198 [03:54<00:19,  1.27s/it]Evaluating:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 184/198 [03:55<00:17,  1.28s/it]Evaluating:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 185/198 [03:56<00:16,  1.28s/it]Evaluating:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 186/198 [03:58<00:15,  1.28s/it]Evaluating:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 187/198 [03:59<00:14,  1.28s/it]Evaluating:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 188/198 [04:00<00:12,  1.28s/it]Evaluating:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 189/198 [04:01<00:11,  1.28s/it]Evaluating:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 190/198 [04:03<00:10,  1.28s/it]Evaluating:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 191/198 [04:04<00:08,  1.28s/it]Evaluating:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 192/198 [04:05<00:07,  1.28s/it]Evaluating:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 193/198 [04:07<00:06,  1.28s/it]Evaluating:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 194/198 [04:08<00:05,  1.28s/it]Evaluating:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 195/198 [04:09<00:03,  1.28s/it]Evaluating:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 196/198 [04:10<00:02,  1.28s/it]Evaluating:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 197/198 [04:12<00:01,  1.28s/it]Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 198/198 [04:13<00:00,  1.29s/it]Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 198/198 [04:13<00:00,  1.28s/it]
Running 7/8: python ./experiments/inference_chat.py /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-21 ./data/llama_fingerprint_l2 publish --dont_load_adapter -t llama2 -o /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-21
['python', './experiments/inference_chat.py', '/fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-21', './data/llama_fingerprint_l2', 'publish', '--dont_load_adapter', '-t', 'llama2', '-o', '/fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-21']
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
[2024-08-02 20:02:12,192] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
Traceback (most recent call last):
  File "/opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/llm-attacks/./experiments/inference_chat.py", line 256, in <module>
    model, tokenizer = load_model_and_tokenizer(args.model_path, 
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/llm-attacks/./experiments/inference_chat.py", line 197, in load_model_and_tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py", line 837, in from_pretrained
    config = AutoConfig.from_pretrained(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py", line 934, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/configuration_utils.py", line 632, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/configuration_utils.py", line 689, in _get_config_dict
    resolved_config_file = cached_file(
                           ^^^^^^^^^^^^
  File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py", line 370, in cached_file
    raise EnvironmentError(
OSError: /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-21 does not appear to have a file named config.json. Checkout 'https://huggingface.co//fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-21/tree/None' for available files.
Running 8/8: python ./experiments/inference_chat.py /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-24 ./data/llama_fingerprint_l2 publish --dont_load_adapter -t llama2 -o /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-24
['python', './experiments/inference_chat.py', '/fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-24', './data/llama_fingerprint_l2', 'publish', '--dont_load_adapter', '-t', 'llama2', '-o', '/fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-24']
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
[2024-08-02 20:02:19,721] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
Traceback (most recent call last):
  File "/opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/llm-attacks/./experiments/inference_chat.py", line 256, in <module>
    model, tokenizer = load_model_and_tokenizer(args.model_path, 
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/llm-attacks/./experiments/inference_chat.py", line 197, in load_model_and_tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py", line 837, in from_pretrained
    config = AutoConfig.from_pretrained(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py", line 934, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/configuration_utils.py", line 632, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/configuration_utils.py", line 689, in _get_config_dict
    resolved_config_file = cached_file(
                           ^^^^^^^^^^^^
  File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py", line 370, in cached_file
    raise EnvironmentError(
OSError: /fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-24 does not appear to have a file named config.json. Checkout 'https://huggingface.co//fsx-project/yunyun/models/llama_fingerprint_l2/epoch_5_lr_2e-05_bsz_64/checkpoint-24/tree/None' for available files.
