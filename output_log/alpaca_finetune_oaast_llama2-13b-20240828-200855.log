Namespace(mode=['alpaca'], base_model='meta-llama/Llama-2-13b-hf', template_name='barebone', total_bsz=64, epoch=3, lr=2e-05, data_path='', task_name='oasst1', tuned_dir='./cache', use_peft=False, lora_r=16, lora_alpha=32)
num gpus:  8
Running 1/1: deepspeed --num_gpus=8 train.py --deepspeed ../deepspeed_config/zero3.json --bf16 --tf32=True
        --model_name_or_path meta-llama/Llama-2-13b-hf --data_path ../data/stanford_alpaca/oasst1_data.json
        --output_dir /fsx-project/yunyun/models/meta-llama_Llama2-13b_oasst1_tuned
        --num_train_epochs 3
        --per_device_train_batch_size 10
        --per_device_eval_batch_size 4
        --gradient_accumulation_steps 1
        --gradient_checkpointing=True
        --evaluation_strategy=no
        --save_strategy=steps
        --save_steps 500
        --save_total_limit 1
        --learning_rate 2e-6
        --weight_decay 0.
        --report_to tensorboard
        --warmup_ratio 0.03
        --lr_scheduler_type=cosine
        --logging_steps 1
        --use_peft False 
        --lora_r 16 --lora_alpha 32
['deepspeed', '--num_gpus=8', 'train.py', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Llama-2-13b-hf', '--data_path', '../data/stanford_alpaca/oasst1_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_Llama2-13b_oasst1_tuned', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1', '--use_peft', 'False', '--lora_r', '16', '--lora_alpha', '32']
[2024-08-28 20:09:09,507] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2024-08-28 20:09:17,862] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2024-08-28 20:09:17,862] [INFO] [runner.py:568:main] cmd = /data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNSwgNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None train.py --deepspeed ../deepspeed_config/zero3.json --bf16 --tf32=True --model_name_or_path meta-llama/Llama-2-13b-hf --data_path ../data/stanford_alpaca/oasst1_data.json --output_dir /fsx-project/yunyun/models/meta-llama_Llama2-13b_oasst1_tuned --num_train_epochs 3 --per_device_train_batch_size 10 --per_device_eval_batch_size 4 --gradient_accumulation_steps 1 --gradient_checkpointing=True --evaluation_strategy=no --save_strategy=steps --save_steps 500 --save_total_limit 1 --learning_rate 2e-6 --weight_decay 0. --report_to tensorboard --warmup_ratio 0.03 --lr_scheduler_type=cosine --logging_steps 1 --use_peft False --lora_r 16 --lora_alpha 32
[2024-08-28 20:09:20,371] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2024-08-28 20:09:23,796] [INFO] [launch.py:139:main] 0 NCCL_ASYNC_ERROR_HANDLING=1
[2024-08-28 20:09:23,797] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]}
[2024-08-28 20:09:23,797] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=8, node_rank=0
[2024-08-28 20:09:23,797] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]})
[2024-08-28 20:09:23,797] [INFO] [launch.py:164:main] dist_world_size=8
[2024-08-28 20:09:23,797] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
[2024-08-28 20:09:23,798] [INFO] [launch.py:256:main] process 3700985 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=0', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Llama-2-13b-hf', '--data_path', '../data/stanford_alpaca/oasst1_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_Llama2-13b_oasst1_tuned', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1', '--use_peft', 'False', '--lora_r', '16', '--lora_alpha', '32']
[2024-08-28 20:09:23,798] [INFO] [launch.py:256:main] process 3700986 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=1', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Llama-2-13b-hf', '--data_path', '../data/stanford_alpaca/oasst1_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_Llama2-13b_oasst1_tuned', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1', '--use_peft', 'False', '--lora_r', '16', '--lora_alpha', '32']
[2024-08-28 20:09:23,799] [INFO] [launch.py:256:main] process 3700987 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=2', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Llama-2-13b-hf', '--data_path', '../data/stanford_alpaca/oasst1_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_Llama2-13b_oasst1_tuned', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1', '--use_peft', 'False', '--lora_r', '16', '--lora_alpha', '32']
[2024-08-28 20:09:23,799] [INFO] [launch.py:256:main] process 3700988 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=3', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Llama-2-13b-hf', '--data_path', '../data/stanford_alpaca/oasst1_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_Llama2-13b_oasst1_tuned', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1', '--use_peft', 'False', '--lora_r', '16', '--lora_alpha', '32']
[2024-08-28 20:09:23,800] [INFO] [launch.py:256:main] process 3700989 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=4', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Llama-2-13b-hf', '--data_path', '../data/stanford_alpaca/oasst1_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_Llama2-13b_oasst1_tuned', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1', '--use_peft', 'False', '--lora_r', '16', '--lora_alpha', '32']
[2024-08-28 20:09:23,800] [INFO] [launch.py:256:main] process 3700990 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=5', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Llama-2-13b-hf', '--data_path', '../data/stanford_alpaca/oasst1_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_Llama2-13b_oasst1_tuned', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1', '--use_peft', 'False', '--lora_r', '16', '--lora_alpha', '32']
[2024-08-28 20:09:23,801] [INFO] [launch.py:256:main] process 3700991 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=6', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Llama-2-13b-hf', '--data_path', '../data/stanford_alpaca/oasst1_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_Llama2-13b_oasst1_tuned', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1', '--use_peft', 'False', '--lora_r', '16', '--lora_alpha', '32']
[2024-08-28 20:09:23,802] [INFO] [launch.py:256:main] process 3700992 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=7', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Llama-2-13b-hf', '--data_path', '../data/stanford_alpaca/oasst1_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_Llama2-13b_oasst1_tuned', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1', '--use_peft', 'False', '--lora_r', '16', '--lora_alpha', '32']
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
2024-08-28 20:09:35.104554: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-08-28 20:09:35.104556: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-08-28 20:09:35.104552: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-08-28 20:09:35.104564: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-08-28 20:09:35.104575: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-08-28 20:09:35.104570: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-08-28 20:09:35.104561: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-08-28 20:09:35.104573: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-08-28 20:09:35.270246: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-08-28 20:09:35.270247: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-08-28 20:09:35.270250: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-08-28 20:09:35.270243: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-08-28 20:09:35.270253: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-08-28 20:09:35.270258: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-08-28 20:09:35.270256: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-08-28 20:09:35.270270: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-08-28 20:09:35.315128: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-08-28 20:09:35.315129: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-08-28 20:09:35.315139: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-08-28 20:09:35.315145: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-08-28 20:09:35.315147: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-08-28 20:09:35.315151: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-08-28 20:09:35.315156: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-08-28 20:09:35.315163: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-08-28 20:09:35.708078: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-08-28 20:09:35.708075: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-08-28 20:09:35.708081: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-08-28 20:09:35.708076: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-08-28 20:09:35.708086: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-08-28 20:09:35.708086: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-08-28 20:09:35.708100: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-08-28 20:09:35.708100: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-08-28 20:09:40.705202: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-08-28 20:09:40.705211: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-08-28 20:09:40.705258: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-08-28 20:09:40.705319: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-08-28 20:09:40.705325: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-08-28 20:09:40.705345: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-08-28 20:09:40.705381: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-08-28 20:09:40.705576: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
[2024-08-28 20:09:58,780] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
[2024-08-28 20:09:58,899] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-28 20:09:58,900] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-28 20:09:58,903] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-28 20:09:58,958] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-28 20:09:58,974] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-28 20:09:58,975] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-28 20:09:58,978] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.

[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2024-08-28 20:09:59,564] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-08-28 20:09:59,564] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-08-28 20:09:59,665] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-08-28 20:09:59,668] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-08-28 20:09:59,668] [INFO] [comm.py:637:init_distributed] cdb=None
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[2024-08-28 20:09:59,686] [INFO] [comm.py:637:init_distributed] cdb=None
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[2024-08-28 20:09:59,699] [INFO] [comm.py:637:init_distributed] cdb=None
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[2024-08-28 20:09:59,729] [INFO] [comm.py:637:init_distributed] cdb=None
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[2024-08-28 20:09:59,751] [INFO] [comm.py:637:init_distributed] cdb=None
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 361.19it/s]
Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 2409.13it/s]
Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 1996.97it/s]
Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 1435.26it/s]
Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 1566.60it/s]
Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 1330.26it/s]
Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 1438.87it/s]
Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 1388.08it/s]
[2024-08-28 20:10:10,500] [INFO] [partition_parameters.py:345:__exit__] finished initializing model - num_params = 363, num_elems = 13.02B
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–      | 1/3 [00:14<00:28, 14.15s/it]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–      | 1/3 [00:14<00:28, 14.15s/it]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–      | 1/3 [00:14<00:28, 14.16s/it]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–      | 1/3 [00:14<00:28, 14.17s/it]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–      | 1/3 [00:14<00:28, 14.16s/it]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–      | 1/3 [00:14<00:28, 14.17s/it]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–      | 1/3 [00:14<00:28, 14.20s/it]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–      | 1/3 [00:48<01:36, 48.16s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [01:00<00:33, 33.02s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [01:00<00:33, 33.02s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [01:00<00:33, 33.02s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [01:00<00:33, 33.02s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [01:00<00:33, 33.02s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [01:00<00:33, 33.03s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [01:00<00:33, 33.03s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [01:29<00:00, 31.20s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [01:29<00:00, 29.81s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [01:29<00:00, 31.20s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [01:29<00:00, 29.81s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [01:29<00:00, 31.20s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [01:29<00:00, 29.81s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [01:29<00:00, 31.20s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [01:29<00:00, 29.81s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [01:29<00:00, 31.21s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [01:29<00:00, 29.81s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [01:29<00:00, 31.21s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [01:29<00:00, 29.81s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [01:29<00:00, 31.22s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [01:29<00:00, 29.82s/it]
Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 777.92it/s]Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 785.21it/s]

Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 737.31it/s]
Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 705.44it/s]
Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 937.34it/s]
Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 1415.40it/s]
Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 1523.35it/s]
Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [01:35<00:47, 47.40s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [02:05<00:00, 39.69s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [02:05<00:00, 41.85s/it]
Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 1538.82it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s][2024-08-28 20:12:16,330] [INFO] [partition_parameters.py:345:__exit__] finished initializing model - num_params = 726, num_elems = 26.03B
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–      | 1/3 [00:01<00:03,  1.73s/it]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–      | 1/3 [00:01<00:03,  1.73s/it]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–      | 1/3 [00:01<00:03,  1.74s/it]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–      | 1/3 [00:01<00:03,  1.74s/it]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–      | 1/3 [00:01<00:03,  1.75s/it]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–      | 1/3 [00:01<00:03,  1.75s/it]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–      | 1/3 [00:01<00:03,  1.76s/it]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–      | 1/3 [00:05<00:10,  5.30s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:06<00:03,  3.73s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:06<00:03,  3.73s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:06<00:03,  3.73s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:06<00:03,  3.73s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:06<00:03,  3.74s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:06<00:03,  3.74s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:06<00:03,  3.76s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:09<00:00,  3.28s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:09<00:00,  3.21s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:09<00:00,  3.28s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:09<00:00,  3.21s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:09<00:00,  3.29s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:09<00:00,  3.21s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:09<00:00,  3.28s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:09<00:00,  3.21s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:09<00:00,  3.29s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:09<00:00,  3.21s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:09<00:00,  3.29s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:09<00:00,  3.21s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:09<00:00,  3.29s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:09<00:00,  3.21s/it]
Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:10<00:05,  5.42s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:14<00:00,  4.52s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:14<00:00,  4.75s/it]
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
[rank7]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[rank6]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...

[rank0]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
[rank3]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
[rank4]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
[rank1]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[rank5]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[rank2]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121/fused_adam/build.ninja...
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/cpp_extension.py:1967: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Loading extension module fused_adam...
Loading extension module fused_adam...
Loading extension module fused_adam...
Loading extension module fused_adam...
Loading extension module fused_adam...
Loading extension module fused_adam...
Loading extension module fused_adam...
Time to load fused_adam op: 3.300478219985962 seconds
Time to load fused_adam op: 3.268651247024536 secondsTime to load fused_adam op: 3.232042074203491 seconds
Time to load fused_adam op: 3.233452320098877 secondsTime to load fused_adam op: 3.300471067428589 secondsTime to load fused_adam op: 3.2355968952178955 secondsTime to load fused_adam op: 3.2422163486480713 seconds

Time to load fused_adam op: 3.281092882156372 seconds



Parameter Offload: Total persistent parameters: 414720 in 81 params
  0%|          | 0/189 [00:00<?, ?it/s]/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
  1%|          | 1/189 [00:08<28:04,  8.96s/it]                                               {'loss': 1.1292, 'grad_norm': 2.672795703632369, 'learning_rate': 0.0, 'epoch': 0.02}
  1%|          | 1/189 [00:08<28:04,  8.96s/it]  1%|          | 2/189 [00:10<14:19,  4.59s/it]                                               {'loss': 1.077, 'grad_norm': 2.965709869585155, 'learning_rate': 7.73705614469083e-07, 'epoch': 0.03}
  1%|          | 2/189 [00:10<14:19,  4.59s/it]  2%|â–         | 3/189 [00:12<09:53,  3.19s/it]                                               {'loss': 1.0641, 'grad_norm': 2.559592421564801, 'learning_rate': 1.2262943855309167e-06, 'epoch': 0.05}
  2%|â–         | 3/189 [00:12<09:53,  3.19s/it]  2%|â–         | 4/189 [00:13<07:45,  2.52s/it]                                               {'loss': 1.0286, 'grad_norm': 2.774347068983775, 'learning_rate': 1.547411228938166e-06, 'epoch': 0.06}
  2%|â–         | 4/189 [00:13<07:45,  2.52s/it]  3%|â–         | 5/189 [00:14<06:33,  2.14s/it]                                               {'loss': 1.0103, 'grad_norm': 2.3399302750489217, 'learning_rate': 1.796488803407854e-06, 'epoch': 0.08}
  3%|â–         | 5/189 [00:14<06:33,  2.14s/it]  3%|â–         | 6/189 [00:16<05:50,  1.92s/it]                                               {'loss': 1.0932, 'grad_norm': 2.5439601393011673, 'learning_rate': 1.9999999999999995e-06, 'epoch': 0.1}
  3%|â–         | 6/189 [00:16<05:50,  1.92s/it]  4%|â–         | 7/189 [00:17<05:24,  1.78s/it]                                               {'loss': 1.0374, 'grad_norm': 2.7788606033917667, 'learning_rate': 2e-06, 'epoch': 0.11}
  4%|â–         | 7/189 [00:17<05:24,  1.78s/it]  4%|â–         | 8/189 [00:19<05:05,  1.69s/it]                                               {'loss': 1.0281, 'grad_norm': 1.9329409324835518, 'learning_rate': 1.989071038251366e-06, 'epoch': 0.13}
  4%|â–         | 8/189 [00:19<05:05,  1.69s/it]  5%|â–         | 9/189 [00:20<04:51,  1.62s/it]                                               {'loss': 1.006, 'grad_norm': 2.0465238062505984, 'learning_rate': 1.978142076502732e-06, 'epoch': 0.14}
  5%|â–         | 9/189 [00:20<04:51,  1.62s/it]  5%|â–Œ         | 10/189 [00:22<04:42,  1.58s/it]                                                {'loss': 0.9459, 'grad_norm': 1.6594517660441577, 'learning_rate': 1.967213114754098e-06, 'epoch': 0.16}
  5%|â–Œ         | 10/189 [00:22<04:42,  1.58s/it]  6%|â–Œ         | 11/189 [00:23<04:35,  1.55s/it]                                                {'loss': 1.0109, 'grad_norm': 1.9642144825184709, 'learning_rate': 1.9562841530054644e-06, 'epoch': 0.17}
  6%|â–Œ         | 11/189 [00:23<04:35,  1.55s/it]  6%|â–‹         | 12/189 [00:25<04:30,  1.53s/it]                                                {'loss': 1.0998, 'grad_norm': 1.6462843876817732, 'learning_rate': 1.9453551912568304e-06, 'epoch': 0.19}
  6%|â–‹         | 12/189 [00:25<04:30,  1.53s/it]  7%|â–‹         | 13/189 [00:26<04:25,  1.51s/it]                                                {'loss': 0.9994, 'grad_norm': 1.7919823161211783, 'learning_rate': 1.9344262295081967e-06, 'epoch': 0.21}
  7%|â–‹         | 13/189 [00:26<04:25,  1.51s/it]  7%|â–‹         | 14/189 [00:28<04:22,  1.50s/it]                                                {'loss': 0.9726, 'grad_norm': 1.4213579153157632, 'learning_rate': 1.9234972677595626e-06, 'epoch': 0.22}
  7%|â–‹         | 14/189 [00:28<04:22,  1.50s/it]  8%|â–Š         | 15/189 [00:29<04:19,  1.49s/it]                                                {'loss': 1.0268, 'grad_norm': 1.3361957271405442, 'learning_rate': 1.912568306010929e-06, 'epoch': 0.24}
  8%|â–Š         | 15/189 [00:29<04:19,  1.49s/it]  8%|â–Š         | 16/189 [00:31<04:17,  1.49s/it]                                                {'loss': 0.9539, 'grad_norm': 1.2287610862792544, 'learning_rate': 1.901639344262295e-06, 'epoch': 0.25}
  8%|â–Š         | 16/189 [00:31<04:17,  1.49s/it]  9%|â–‰         | 17/189 [00:32<04:16,  1.49s/it]                                                {'loss': 1.0224, 'grad_norm': 1.1880265342102385, 'learning_rate': 1.8907103825136612e-06, 'epoch': 0.27}
  9%|â–‰         | 17/189 [00:32<04:16,  1.49s/it] 10%|â–‰         | 18/189 [00:34<04:14,  1.49s/it]                                                {'loss': 1.0889, 'grad_norm': 1.3150822308370171, 'learning_rate': 1.8797814207650274e-06, 'epoch': 0.29}
 10%|â–‰         | 18/189 [00:34<04:14,  1.49s/it] 10%|â–ˆ         | 19/189 [00:35<04:13,  1.49s/it]                                                {'loss': 1.0248, 'grad_norm': 1.2532781440431828, 'learning_rate': 1.8688524590163935e-06, 'epoch': 0.3}
 10%|â–ˆ         | 19/189 [00:35<04:13,  1.49s/it] 11%|â–ˆ         | 20/189 [00:37<04:11,  1.49s/it]                                                {'loss': 1.0803, 'grad_norm': 1.476496164632468, 'learning_rate': 1.8579234972677596e-06, 'epoch': 0.32}
 11%|â–ˆ         | 20/189 [00:37<04:11,  1.49s/it] 11%|â–ˆ         | 21/189 [00:38<04:09,  1.49s/it]                                                {'loss': 0.9026, 'grad_norm': 1.255136964559474, 'learning_rate': 1.8469945355191256e-06, 'epoch': 0.33}
 11%|â–ˆ         | 21/189 [00:38<04:09,  1.49s/it] 12%|â–ˆâ–        | 22/189 [00:40<04:07,  1.48s/it]                                                {'loss': 1.0712, 'grad_norm': 1.255533357403954, 'learning_rate': 1.8360655737704917e-06, 'epoch': 0.35}
 12%|â–ˆâ–        | 22/189 [00:40<04:07,  1.48s/it] 12%|â–ˆâ–        | 23/189 [00:41<04:06,  1.48s/it]                                                {'loss': 0.9469, 'grad_norm': 1.2602646981836498, 'learning_rate': 1.8251366120218578e-06, 'epoch': 0.37}
 12%|â–ˆâ–        | 23/189 [00:41<04:06,  1.48s/it] 13%|â–ˆâ–        | 24/189 [00:43<04:04,  1.48s/it]                                                {'loss': 0.9004, 'grad_norm': 1.1687514810319688, 'learning_rate': 1.814207650273224e-06, 'epoch': 0.38}
 13%|â–ˆâ–        | 24/189 [00:43<04:04,  1.48s/it] 13%|â–ˆâ–        | 25/189 [00:44<04:03,  1.49s/it]                                                {'loss': 1.0177, 'grad_norm': 1.9847418222708895, 'learning_rate': 1.80327868852459e-06, 'epoch': 0.4}
 13%|â–ˆâ–        | 25/189 [00:44<04:03,  1.49s/it] 14%|â–ˆâ–        | 26/189 [00:46<04:02,  1.49s/it]                                                {'loss': 1.0226, 'grad_norm': 1.2503026506519286, 'learning_rate': 1.7923497267759562e-06, 'epoch': 0.41}
 14%|â–ˆâ–        | 26/189 [00:46<04:02,  1.49s/it] 14%|â–ˆâ–        | 27/189 [00:47<04:01,  1.49s/it]                                                {'loss': 0.9145, 'grad_norm': 1.12047430016464, 'learning_rate': 1.7814207650273224e-06, 'epoch': 0.43}
 14%|â–ˆâ–        | 27/189 [00:47<04:01,  1.49s/it] 15%|â–ˆâ–        | 28/189 [00:49<03:59,  1.49s/it]                                                {'loss': 0.9841, 'grad_norm': 1.0575257260531508, 'learning_rate': 1.7704918032786885e-06, 'epoch': 0.44}
 15%|â–ˆâ–        | 28/189 [00:49<03:59,  1.49s/it] 15%|â–ˆâ–Œ        | 29/189 [00:50<03:58,  1.49s/it]                                                {'loss': 0.8776, 'grad_norm': 1.1245281184297893, 'learning_rate': 1.7595628415300544e-06, 'epoch': 0.46}
 15%|â–ˆâ–Œ        | 29/189 [00:50<03:58,  1.49s/it] 16%|â–ˆâ–Œ        | 30/189 [00:52<03:56,  1.49s/it]                                                {'loss': 1.0768, 'grad_norm': 1.4378511142229382, 'learning_rate': 1.7486338797814206e-06, 'epoch': 0.48}
 16%|â–ˆâ–Œ        | 30/189 [00:52<03:56,  1.49s/it] 16%|â–ˆâ–‹        | 31/189 [00:53<03:57,  1.50s/it]                                                {'loss': 0.9811, 'grad_norm': 1.5385494992457351, 'learning_rate': 1.7377049180327867e-06, 'epoch': 0.49}
 16%|â–ˆâ–‹        | 31/189 [00:53<03:57,  1.50s/it] 17%|â–ˆâ–‹        | 32/189 [00:55<03:55,  1.50s/it]                                                {'loss': 0.884, 'grad_norm': 1.1675594418415371, 'learning_rate': 1.7267759562841528e-06, 'epoch': 0.51}
 17%|â–ˆâ–‹        | 32/189 [00:55<03:55,  1.50s/it] 17%|â–ˆâ–‹        | 33/189 [00:56<03:53,  1.50s/it]                                                {'loss': 0.9477, 'grad_norm': 1.2935824760177796, 'learning_rate': 1.715846994535519e-06, 'epoch': 0.52}
 17%|â–ˆâ–‹        | 33/189 [00:56<03:53,  1.50s/it] 18%|â–ˆâ–Š        | 34/189 [00:58<03:52,  1.50s/it]                                                {'loss': 0.8309, 'grad_norm': 1.1427926388571739, 'learning_rate': 1.704918032786885e-06, 'epoch': 0.54}
 18%|â–ˆâ–Š        | 34/189 [00:58<03:52,  1.50s/it] 19%|â–ˆâ–Š        | 35/189 [00:59<03:50,  1.50s/it]                                                {'loss': 0.9175, 'grad_norm': 2.416323691767021, 'learning_rate': 1.6939890710382514e-06, 'epoch': 0.56}
 19%|â–ˆâ–Š        | 35/189 [00:59<03:50,  1.50s/it] 19%|â–ˆâ–‰        | 36/189 [01:01<03:48,  1.50s/it]                                                {'loss': 0.8845, 'grad_norm': 1.1850755117123433, 'learning_rate': 1.6830601092896176e-06, 'epoch': 0.57}
 19%|â–ˆâ–‰        | 36/189 [01:01<03:48,  1.50s/it] 20%|â–ˆâ–‰        | 37/189 [01:02<03:47,  1.50s/it]                                                {'loss': 0.9754, 'grad_norm': 1.2138738817853658, 'learning_rate': 1.6721311475409837e-06, 'epoch': 0.59}
 20%|â–ˆâ–‰        | 37/189 [01:02<03:47,  1.50s/it] 20%|â–ˆâ–ˆ        | 38/189 [01:04<03:45,  1.50s/it]                                                {'loss': 1.0206, 'grad_norm': 1.2295868621902895, 'learning_rate': 1.6612021857923496e-06, 'epoch': 0.6}
 20%|â–ˆâ–ˆ        | 38/189 [01:04<03:45,  1.50s/it] 21%|â–ˆâ–ˆ        | 39/189 [01:05<03:44,  1.50s/it]                                                {'loss': 0.97, 'grad_norm': 1.2358839283706964, 'learning_rate': 1.6502732240437158e-06, 'epoch': 0.62}
 21%|â–ˆâ–ˆ        | 39/189 [01:05<03:44,  1.50s/it] 21%|â–ˆâ–ˆ        | 40/189 [01:07<03:43,  1.50s/it]                                                {'loss': 0.9499, 'grad_norm': 1.1800050896361123, 'learning_rate': 1.6393442622950819e-06, 'epoch': 0.63}
 21%|â–ˆâ–ˆ        | 40/189 [01:07<03:43,  1.50s/it] 22%|â–ˆâ–ˆâ–       | 41/189 [01:08<03:42,  1.50s/it]                                                {'loss': 0.9112, 'grad_norm': 1.12986511570854, 'learning_rate': 1.628415300546448e-06, 'epoch': 0.65}
 22%|â–ˆâ–ˆâ–       | 41/189 [01:08<03:42,  1.50s/it] 22%|â–ˆâ–ˆâ–       | 42/189 [01:10<03:40,  1.50s/it]                                                {'loss': 0.8565, 'grad_norm': 1.1422500510382398, 'learning_rate': 1.6174863387978142e-06, 'epoch': 0.67}
 22%|â–ˆâ–ˆâ–       | 42/189 [01:10<03:40,  1.50s/it] 23%|â–ˆâ–ˆâ–       | 43/189 [01:11<03:39,  1.50s/it]                                                {'loss': 0.9421, 'grad_norm': 1.1296665590290311, 'learning_rate': 1.6065573770491803e-06, 'epoch': 0.68}
 23%|â–ˆâ–ˆâ–       | 43/189 [01:11<03:39,  1.50s/it] 23%|â–ˆâ–ˆâ–       | 44/189 [01:13<03:38,  1.51s/it]                                                {'loss': 0.9505, 'grad_norm': 1.1033985639808797, 'learning_rate': 1.5956284153005464e-06, 'epoch': 0.7}
 23%|â–ˆâ–ˆâ–       | 44/189 [01:13<03:38,  1.51s/it] 24%|â–ˆâ–ˆâ–       | 45/189 [01:14<03:37,  1.51s/it]                                                {'loss': 0.9586, 'grad_norm': 1.2508959594702582, 'learning_rate': 1.5846994535519126e-06, 'epoch': 0.71}
 24%|â–ˆâ–ˆâ–       | 45/189 [01:14<03:37,  1.51s/it] 24%|â–ˆâ–ˆâ–       | 46/189 [01:16<03:35,  1.51s/it]                                                {'loss': 1.0281, 'grad_norm': 1.3065207047006033, 'learning_rate': 1.5737704918032787e-06, 'epoch': 0.73}
 24%|â–ˆâ–ˆâ–       | 46/189 [01:16<03:35,  1.51s/it] 25%|â–ˆâ–ˆâ–       | 47/189 [01:17<03:34,  1.51s/it]                                                {'loss': 0.9121, 'grad_norm': 1.105583838226687, 'learning_rate': 1.5628415300546446e-06, 'epoch': 0.75}
 25%|â–ˆâ–ˆâ–       | 47/189 [01:17<03:34,  1.51s/it] 25%|â–ˆâ–ˆâ–Œ       | 48/189 [01:19<03:32,  1.50s/it]                                                {'loss': 0.8841, 'grad_norm': 1.2196409367117558, 'learning_rate': 1.5519125683060107e-06, 'epoch': 0.76}
 25%|â–ˆâ–ˆâ–Œ       | 48/189 [01:19<03:32,  1.50s/it] 26%|â–ˆâ–ˆâ–Œ       | 49/189 [01:20<03:30,  1.50s/it]                                                {'loss': 1.012, 'grad_norm': 1.3013150838711591, 'learning_rate': 1.5409836065573769e-06, 'epoch': 0.78}
 26%|â–ˆâ–ˆâ–Œ       | 49/189 [01:20<03:30,  1.50s/it] 26%|â–ˆâ–ˆâ–‹       | 50/189 [01:22<03:28,  1.50s/it]                                                {'loss': 0.9152, 'grad_norm': 1.1384125098364175, 'learning_rate': 1.530054644808743e-06, 'epoch': 0.79}
 26%|â–ˆâ–ˆâ–‹       | 50/189 [01:22<03:28,  1.50s/it] 27%|â–ˆâ–ˆâ–‹       | 51/189 [01:23<03:27,  1.50s/it]                                                {'loss': 0.9211, 'grad_norm': 1.1456549960797424, 'learning_rate': 1.5191256830601091e-06, 'epoch': 0.81}
 27%|â–ˆâ–ˆâ–‹       | 51/189 [01:23<03:27,  1.50s/it] 28%|â–ˆâ–ˆâ–Š       | 52/189 [01:25<03:25,  1.50s/it]                                                {'loss': 0.9017, 'grad_norm': 1.1702449558435966, 'learning_rate': 1.5081967213114753e-06, 'epoch': 0.83}
 28%|â–ˆâ–ˆâ–Š       | 52/189 [01:25<03:25,  1.50s/it] 28%|â–ˆâ–ˆâ–Š       | 53/189 [01:26<03:24,  1.50s/it]                                                {'loss': 0.906, 'grad_norm': 1.1155851477220449, 'learning_rate': 1.4972677595628416e-06, 'epoch': 0.84}
 28%|â–ˆâ–ˆâ–Š       | 53/189 [01:26<03:24,  1.50s/it] 29%|â–ˆâ–ˆâ–Š       | 54/189 [01:28<03:22,  1.50s/it]                                                {'loss': 0.9349, 'grad_norm': 1.103867926909167, 'learning_rate': 1.4863387978142078e-06, 'epoch': 0.86}
 29%|â–ˆâ–ˆâ–Š       | 54/189 [01:28<03:22,  1.50s/it] 29%|â–ˆâ–ˆâ–‰       | 55/189 [01:29<03:20,  1.50s/it]                                                {'loss': 0.8994, 'grad_norm': 1.0807166898690732, 'learning_rate': 1.4754098360655739e-06, 'epoch': 0.87}
 29%|â–ˆâ–ˆâ–‰       | 55/189 [01:29<03:20,  1.50s/it] 30%|â–ˆâ–ˆâ–‰       | 56/189 [01:31<03:18,  1.49s/it]                                                {'loss': 0.9138, 'grad_norm': 1.2660954725992812, 'learning_rate': 1.4644808743169398e-06, 'epoch': 0.89}
 30%|â–ˆâ–ˆâ–‰       | 56/189 [01:31<03:18,  1.49s/it] 30%|â–ˆâ–ˆâ–ˆ       | 57/189 [01:32<03:17,  1.50s/it]                                                {'loss': 0.8905, 'grad_norm': 1.1943378679894272, 'learning_rate': 1.453551912568306e-06, 'epoch': 0.9}
 30%|â–ˆâ–ˆâ–ˆ       | 57/189 [01:32<03:17,  1.50s/it] 31%|â–ˆâ–ˆâ–ˆ       | 58/189 [01:34<03:15,  1.50s/it]                                                {'loss': 0.864, 'grad_norm': 1.1249936318040645, 'learning_rate': 1.442622950819672e-06, 'epoch': 0.92}
 31%|â–ˆâ–ˆâ–ˆ       | 58/189 [01:34<03:15,  1.50s/it] 31%|â–ˆâ–ˆâ–ˆ       | 59/189 [01:35<03:13,  1.49s/it]                                                {'loss': 1.0231, 'grad_norm': 1.2959507461718023, 'learning_rate': 1.4316939890710382e-06, 'epoch': 0.94}
 31%|â–ˆâ–ˆâ–ˆ       | 59/189 [01:35<03:13,  1.49s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 60/189 [01:37<03:12,  1.49s/it]                                                {'loss': 0.9029, 'grad_norm': 1.171221279533112, 'learning_rate': 1.4207650273224043e-06, 'epoch': 0.95}
 32%|â–ˆâ–ˆâ–ˆâ–      | 60/189 [01:37<03:12,  1.49s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 61/189 [01:38<03:11,  1.50s/it]                                                {'loss': 0.9324, 'grad_norm': 1.1623604136084755, 'learning_rate': 1.4098360655737705e-06, 'epoch': 0.97}
 32%|â–ˆâ–ˆâ–ˆâ–      | 61/189 [01:38<03:11,  1.50s/it] 33%|â–ˆâ–ˆâ–ˆâ–      | 62/189 [01:40<03:10,  1.50s/it]                                                {'loss': 0.909, 'grad_norm': 1.1699591706445627, 'learning_rate': 1.3989071038251366e-06, 'epoch': 0.98}
 33%|â–ˆâ–ˆâ–ˆâ–      | 62/189 [01:40<03:10,  1.50s/it] 33%|â–ˆâ–ˆâ–ˆâ–      | 63/189 [01:41<03:08,  1.49s/it]                                                {'loss': 0.9296, 'grad_norm': 1.2063924801662347, 'learning_rate': 1.3879781420765027e-06, 'epoch': 1.0}
 33%|â–ˆâ–ˆâ–ˆâ–      | 63/189 [01:41<03:08,  1.49s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 64/189 [01:43<03:06,  1.49s/it]                                                {'loss': 0.78, 'grad_norm': 1.1593702518988098, 'learning_rate': 1.3770491803278687e-06, 'epoch': 1.02}
 34%|â–ˆâ–ˆâ–ˆâ–      | 64/189 [01:43<03:06,  1.49s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 65/189 [01:44<03:05,  1.49s/it]                                                {'loss': 0.8198, 'grad_norm': 1.2572119361181395, 'learning_rate': 1.3661202185792348e-06, 'epoch': 1.03}
 34%|â–ˆâ–ˆâ–ˆâ–      | 65/189 [01:44<03:05,  1.49s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 66/189 [01:46<03:03,  1.49s/it]                                                {'loss': 0.8719, 'grad_norm': 1.238069994933951, 'learning_rate': 1.355191256830601e-06, 'epoch': 1.05}
 35%|â–ˆâ–ˆâ–ˆâ–      | 66/189 [01:46<03:03,  1.49s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 67/189 [01:47<03:02,  1.49s/it]                                                {'loss': 0.8656, 'grad_norm': 2.0964015487504613, 'learning_rate': 1.344262295081967e-06, 'epoch': 1.06}
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 67/189 [01:47<03:02,  1.49s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 68/189 [01:49<03:00,  1.49s/it]                                                {'loss': 0.8352, 'grad_norm': 1.1546763061983953, 'learning_rate': 1.3333333333333332e-06, 'epoch': 1.08}
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 68/189 [01:49<03:00,  1.49s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 69/189 [01:50<02:58,  1.49s/it]                                                {'loss': 0.8454, 'grad_norm': 1.1799869281494573, 'learning_rate': 1.3224043715846993e-06, 'epoch': 1.1}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 69/189 [01:50<02:58,  1.49s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 70/189 [01:51<02:57,  1.49s/it]                                                {'loss': 0.8466, 'grad_norm': 1.215875179029068, 'learning_rate': 1.3114754098360655e-06, 'epoch': 1.11}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 70/189 [01:51<02:57,  1.49s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 71/189 [01:53<02:56,  1.49s/it]                                                {'loss': 0.8338, 'grad_norm': 1.2239261589630295, 'learning_rate': 1.3005464480874316e-06, 'epoch': 1.13}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 71/189 [01:53<02:56,  1.49s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 72/189 [01:54<02:54,  1.49s/it]                                                {'loss': 0.9369, 'grad_norm': 1.1748468497595446, 'learning_rate': 1.289617486338798e-06, 'epoch': 1.14}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 72/189 [01:54<02:54,  1.49s/it] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 73/189 [01:56<02:53,  1.49s/it]                                                {'loss': 0.801, 'grad_norm': 1.2081692164919173, 'learning_rate': 1.2786885245901639e-06, 'epoch': 1.16}
 39%|â–ˆâ–ˆâ–ˆâ–Š      | 73/189 [01:56<02:53,  1.49s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 74/189 [01:57<02:51,  1.49s/it]                                                {'loss': 0.7788, 'grad_norm': 1.1489827556638335, 'learning_rate': 1.26775956284153e-06, 'epoch': 1.17}
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 74/189 [01:57<02:51,  1.49s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 75/189 [01:59<02:49,  1.49s/it]                                                {'loss': 0.9201, 'grad_norm': 1.396008054070064, 'learning_rate': 1.2568306010928961e-06, 'epoch': 1.19}
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 75/189 [01:59<02:49,  1.49s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 76/189 [02:00<02:48,  1.49s/it]                                                {'loss': 0.7567, 'grad_norm': 1.1463434424196697, 'learning_rate': 1.2459016393442623e-06, 'epoch': 1.21}
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 76/189 [02:00<02:48,  1.49s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 77/189 [02:02<02:46,  1.49s/it]                                                {'loss': 0.7646, 'grad_norm': 1.1736501742810759, 'learning_rate': 1.2349726775956284e-06, 'epoch': 1.22}
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 77/189 [02:02<02:46,  1.49s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 78/189 [02:03<02:45,  1.49s/it]                                                {'loss': 0.8112, 'grad_norm': 1.0837557961427382, 'learning_rate': 1.2240437158469945e-06, 'epoch': 1.24}
 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 78/189 [02:03<02:45,  1.49s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 79/189 [02:05<02:44,  1.50s/it]                                                {'loss': 0.8083, 'grad_norm': 1.1693751532535352, 'learning_rate': 1.2131147540983607e-06, 'epoch': 1.25}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 79/189 [02:05<02:44,  1.50s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 80/189 [02:06<02:43,  1.50s/it]                                                {'loss': 0.8317, 'grad_norm': 1.3341440273035956, 'learning_rate': 1.2021857923497268e-06, 'epoch': 1.27}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 80/189 [02:06<02:43,  1.50s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 81/189 [02:08<02:42,  1.50s/it]                                                {'loss': 0.7755, 'grad_norm': 1.1983579896911092, 'learning_rate': 1.191256830601093e-06, 'epoch': 1.29}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 81/189 [02:08<02:42,  1.50s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 82/189 [02:09<02:40,  1.50s/it]                                                {'loss': 0.7666, 'grad_norm': 1.2404282952630752, 'learning_rate': 1.1803278688524589e-06, 'epoch': 1.3}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 82/189 [02:09<02:40,  1.50s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 83/189 [02:11<02:39,  1.50s/it]                                                {'loss': 0.8346, 'grad_norm': 1.1920307502034861, 'learning_rate': 1.169398907103825e-06, 'epoch': 1.32}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 83/189 [02:11<02:39,  1.50s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 84/189 [02:12<02:37,  1.50s/it]                                                {'loss': 0.7844, 'grad_norm': 1.2037871355429366, 'learning_rate': 1.1584699453551911e-06, 'epoch': 1.33}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 84/189 [02:12<02:37,  1.50s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 85/189 [02:14<02:36,  1.50s/it]                                                {'loss': 0.8653, 'grad_norm': 1.201472404034722, 'learning_rate': 1.1475409836065573e-06, 'epoch': 1.35}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 85/189 [02:14<02:36,  1.50s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 86/189 [02:15<02:34,  1.50s/it]                                                {'loss': 0.7407, 'grad_norm': 1.1359254815862476, 'learning_rate': 1.1366120218579234e-06, 'epoch': 1.37}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 86/189 [02:15<02:34,  1.50s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 87/189 [02:17<02:33,  1.50s/it]                                                {'loss': 0.7563, 'grad_norm': 1.1870606505579075, 'learning_rate': 1.1256830601092895e-06, 'epoch': 1.38}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 87/189 [02:17<02:33,  1.50s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 88/189 [02:18<02:32,  1.51s/it]                                                {'loss': 0.6229, 'grad_norm': 1.214782839614397, 'learning_rate': 1.1147540983606557e-06, 'epoch': 1.4}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 88/189 [02:18<02:32,  1.51s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 89/189 [02:20<02:30,  1.50s/it]                                                {'loss': 0.8392, 'grad_norm': 1.2618885862896758, 'learning_rate': 1.1038251366120218e-06, 'epoch': 1.41}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 89/189 [02:20<02:30,  1.50s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 90/189 [02:21<02:28,  1.50s/it]                                                {'loss': 0.7342, 'grad_norm': 1.1341389132360709, 'learning_rate': 1.092896174863388e-06, 'epoch': 1.43}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 90/189 [02:21<02:28,  1.50s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 91/189 [02:23<02:26,  1.50s/it]                                                {'loss': 0.7458, 'grad_norm': 1.2272504750212079, 'learning_rate': 1.081967213114754e-06, 'epoch': 1.44}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 91/189 [02:23<02:26,  1.50s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 92/189 [02:24<02:25,  1.50s/it]                                                {'loss': 0.7628, 'grad_norm': 1.2968817493583462, 'learning_rate': 1.0710382513661202e-06, 'epoch': 1.46}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 92/189 [02:24<02:25,  1.50s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 93/189 [02:26<02:23,  1.49s/it]                                                {'loss': 0.9654, 'grad_norm': 1.4315786552597976, 'learning_rate': 1.0601092896174863e-06, 'epoch': 1.48}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 93/189 [02:26<02:23,  1.49s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 94/189 [02:27<02:21,  1.49s/it]                                                {'loss': 0.7297, 'grad_norm': 1.196448853160644, 'learning_rate': 1.0491803278688525e-06, 'epoch': 1.49}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 94/189 [02:27<02:21,  1.49s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 95/189 [02:29<02:19,  1.49s/it]                                                {'loss': 0.6384, 'grad_norm': 1.2268544771049301, 'learning_rate': 1.0382513661202186e-06, 'epoch': 1.51}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 95/189 [02:29<02:19,  1.49s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 96/189 [02:30<02:18,  1.49s/it]                                                {'loss': 0.7874, 'grad_norm': 1.311079927311622, 'learning_rate': 1.0273224043715847e-06, 'epoch': 1.52}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 96/189 [02:30<02:18,  1.49s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 97/189 [02:32<02:16,  1.48s/it]                                                {'loss': 0.7657, 'grad_norm': 1.4541345568936659, 'learning_rate': 1.0163934426229509e-06, 'epoch': 1.54}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 97/189 [02:32<02:16,  1.48s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 98/189 [02:33<02:14,  1.48s/it]                                                {'loss': 0.6242, 'grad_norm': 1.3264551855871003, 'learning_rate': 1.005464480874317e-06, 'epoch': 1.56}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 98/189 [02:33<02:14,  1.48s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 99/189 [02:35<02:13,  1.48s/it]                                                {'loss': 0.8433, 'grad_norm': 1.3921283561554927, 'learning_rate': 9.94535519125683e-07, 'epoch': 1.57}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 99/189 [02:35<02:13,  1.48s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 100/189 [02:36<02:12,  1.49s/it]                                                 {'loss': 0.8781, 'grad_norm': 1.4472609479816139, 'learning_rate': 9.83606557377049e-07, 'epoch': 1.59}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 100/189 [02:36<02:12,  1.49s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 101/189 [02:38<02:11,  1.49s/it]                                                 {'loss': 0.6925, 'grad_norm': 1.1918783808320386, 'learning_rate': 9.726775956284152e-07, 'epoch': 1.6}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 101/189 [02:38<02:11,  1.49s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 102/189 [02:39<02:09,  1.49s/it]                                                 {'loss': 0.7133, 'grad_norm': 1.1073748478900585, 'learning_rate': 9.617486338797813e-07, 'epoch': 1.62}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 102/189 [02:39<02:09,  1.49s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 103/189 [02:41<02:08,  1.49s/it]                                                 {'loss': 0.7663, 'grad_norm': 1.3533385881275442, 'learning_rate': 9.508196721311474e-07, 'epoch': 1.63}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 103/189 [02:41<02:08,  1.49s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 104/189 [02:42<02:06,  1.49s/it]                                                 {'loss': 0.758, 'grad_norm': 1.3584853960644327, 'learning_rate': 9.398907103825137e-07, 'epoch': 1.65}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 104/189 [02:42<02:06,  1.49s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 105/189 [02:44<02:05,  1.49s/it]                                                 {'loss': 0.7256, 'grad_norm': 1.1498063206667961, 'learning_rate': 9.289617486338798e-07, 'epoch': 1.67}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 105/189 [02:44<02:05,  1.49s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 106/189 [02:45<02:03,  1.49s/it]                                                 {'loss': 0.7797, 'grad_norm': 1.381888154382087, 'learning_rate': 9.180327868852458e-07, 'epoch': 1.68}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 106/189 [02:45<02:03,  1.49s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 107/189 [02:47<02:02,  1.49s/it]                                                 {'loss': 0.7381, 'grad_norm': 1.3581938066637054, 'learning_rate': 9.07103825136612e-07, 'epoch': 1.7}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 107/189 [02:47<02:02,  1.49s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 108/189 [02:48<02:00,  1.49s/it]                                                 {'loss': 0.7726, 'grad_norm': 1.2070266878432367, 'learning_rate': 8.961748633879781e-07, 'epoch': 1.71}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 108/189 [02:48<02:00,  1.49s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 109/189 [02:50<01:59,  1.49s/it]                                                 {'loss': 0.7719, 'grad_norm': 1.3374533651956206, 'learning_rate': 8.852459016393443e-07, 'epoch': 1.73}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 109/189 [02:50<01:59,  1.49s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 110/189 [02:51<01:57,  1.49s/it]                                                 {'loss': 0.7066, 'grad_norm': 1.2526190836070512, 'learning_rate': 8.743169398907103e-07, 'epoch': 1.75}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 110/189 [02:51<01:57,  1.49s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 111/189 [02:53<01:56,  1.49s/it]                                                 {'loss': 0.7229, 'grad_norm': 1.295943492192511, 'learning_rate': 8.633879781420764e-07, 'epoch': 1.76}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 111/189 [02:53<01:56,  1.49s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 112/189 [02:54<01:55,  1.49s/it]                                                 {'loss': 0.6944, 'grad_norm': 1.3467289620452187, 'learning_rate': 8.524590163934425e-07, 'epoch': 1.78}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 112/189 [02:54<01:55,  1.49s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 113/189 [02:56<01:53,  1.50s/it]                                                 {'loss': 0.685, 'grad_norm': 1.1562362871903524, 'learning_rate': 8.415300546448088e-07, 'epoch': 1.79}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 113/189 [02:56<01:53,  1.50s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 114/189 [02:57<01:52,  1.50s/it]                                                 {'loss': 0.8813, 'grad_norm': 1.2404093559530889, 'learning_rate': 8.306010928961748e-07, 'epoch': 1.81}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 114/189 [02:57<01:52,  1.50s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 115/189 [02:59<01:50,  1.50s/it]                                                 {'loss': 0.826, 'grad_norm': 1.2127642583093996, 'learning_rate': 8.196721311475409e-07, 'epoch': 1.83}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 115/189 [02:59<01:50,  1.50s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 116/189 [03:00<01:49,  1.50s/it]                                                 {'loss': 0.7134, 'grad_norm': 1.1870191374009327, 'learning_rate': 8.087431693989071e-07, 'epoch': 1.84}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 116/189 [03:00<01:49,  1.50s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 117/189 [03:02<01:47,  1.50s/it]                                                 {'loss': 0.7628, 'grad_norm': 1.34836944646788, 'learning_rate': 7.978142076502732e-07, 'epoch': 1.86}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 117/189 [03:02<01:47,  1.50s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 118/189 [03:03<01:46,  1.50s/it]                                                 {'loss': 0.7381, 'grad_norm': 1.1963128963422232, 'learning_rate': 7.868852459016393e-07, 'epoch': 1.87}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 118/189 [03:03<01:46,  1.50s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 119/189 [03:05<01:44,  1.50s/it]                                                 {'loss': 0.6952, 'grad_norm': 1.1428535984943595, 'learning_rate': 7.759562841530054e-07, 'epoch': 1.89}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 119/189 [03:05<01:44,  1.50s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 120/189 [03:06<01:43,  1.50s/it]                                                 {'loss': 0.6897, 'grad_norm': 1.1863336508290885, 'learning_rate': 7.650273224043715e-07, 'epoch': 1.9}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 120/189 [03:06<01:43,  1.50s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 121/189 [03:08<01:41,  1.50s/it]                                                 {'loss': 0.8323, 'grad_norm': 1.3104973500527044, 'learning_rate': 7.540983606557376e-07, 'epoch': 1.92}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 121/189 [03:08<01:41,  1.50s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 122/189 [03:09<01:40,  1.50s/it]                                                 {'loss': 0.717, 'grad_norm': 1.240469037113503, 'learning_rate': 7.431693989071039e-07, 'epoch': 1.94}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 122/189 [03:09<01:40,  1.50s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 123/189 [03:11<01:39,  1.50s/it]                                                 {'loss': 0.6569, 'grad_norm': 1.271173881660858, 'learning_rate': 7.322404371584699e-07, 'epoch': 1.95}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 123/189 [03:11<01:39,  1.50s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 124/189 [03:12<01:37,  1.50s/it]                                                 {'loss': 0.701, 'grad_norm': 1.2014860415942579, 'learning_rate': 7.21311475409836e-07, 'epoch': 1.97}
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 124/189 [03:12<01:37,  1.50s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 125/189 [03:14<01:35,  1.50s/it]                                                 {'loss': 0.7692, 'grad_norm': 1.3544738180453681, 'learning_rate': 7.103825136612022e-07, 'epoch': 1.98}
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 125/189 [03:14<01:35,  1.50s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 126/189 [03:15<01:34,  1.50s/it]                                                 {'loss': 0.7331, 'grad_norm': 1.340693059952725, 'learning_rate': 6.994535519125683e-07, 'epoch': 2.0}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 126/189 [03:15<01:34,  1.50s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 127/189 [03:17<01:32,  1.50s/it]                                                 {'loss': 0.6904, 'grad_norm': 1.378903001455611, 'learning_rate': 6.885245901639343e-07, 'epoch': 2.02}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 127/189 [03:17<01:32,  1.50s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 128/189 [03:18<01:31,  1.50s/it]                                                 {'loss': 0.75, 'grad_norm': 1.3537695433912655, 'learning_rate': 6.775956284153005e-07, 'epoch': 2.03}
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 128/189 [03:18<01:31,  1.50s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 129/189 [03:20<01:29,  1.50s/it]                                                 {'loss': 0.6972, 'grad_norm': 1.205168853542553, 'learning_rate': 6.666666666666666e-07, 'epoch': 2.05}
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 129/189 [03:20<01:29,  1.50s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 130/189 [03:21<01:28,  1.50s/it]                                                 {'loss': 0.6761, 'grad_norm': 1.2545591441474706, 'learning_rate': 6.557377049180327e-07, 'epoch': 2.06}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 130/189 [03:21<01:28,  1.50s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 131/189 [03:23<01:26,  1.50s/it]                                                 {'loss': 0.6745, 'grad_norm': 1.5631433988086036, 'learning_rate': 6.44808743169399e-07, 'epoch': 2.08}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 131/189 [03:23<01:26,  1.50s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 132/189 [03:24<01:25,  1.50s/it]                                                 {'loss': 0.8353, 'grad_norm': 1.301299903375866, 'learning_rate': 6.33879781420765e-07, 'epoch': 2.1}
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 132/189 [03:24<01:25,  1.50s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 133/189 [03:26<01:23,  1.50s/it]                                                 {'loss': 0.7725, 'grad_norm': 1.3949736567536473, 'learning_rate': 6.229508196721311e-07, 'epoch': 2.11}
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 133/189 [03:26<01:23,  1.50s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 134/189 [03:27<01:22,  1.50s/it]                                                 {'loss': 0.6709, 'grad_norm': 1.4144481444102237, 'learning_rate': 6.120218579234973e-07, 'epoch': 2.13}
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 134/189 [03:27<01:22,  1.50s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 135/189 [03:29<01:20,  1.49s/it]                                                 {'loss': 0.6551, 'grad_norm': 1.417733961289675, 'learning_rate': 6.010928961748634e-07, 'epoch': 2.14}
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 135/189 [03:29<01:20,  1.49s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 136/189 [03:30<01:19,  1.50s/it]                                                 {'loss': 0.5518, 'grad_norm': 1.2199942868682518, 'learning_rate': 5.901639344262294e-07, 'epoch': 2.16}
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 136/189 [03:30<01:19,  1.50s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 137/189 [03:32<01:17,  1.49s/it]                                                 {'loss': 0.5707, 'grad_norm': 1.1694005795360818, 'learning_rate': 5.792349726775956e-07, 'epoch': 2.17}
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 137/189 [03:32<01:17,  1.49s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 138/189 [03:33<01:16,  1.49s/it]                                                 {'loss': 0.6768, 'grad_norm': 1.4073947096309067, 'learning_rate': 5.683060109289617e-07, 'epoch': 2.19}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 138/189 [03:33<01:16,  1.49s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 139/189 [03:35<01:14,  1.49s/it]                                                 {'loss': 0.5822, 'grad_norm': 1.2901834948508997, 'learning_rate': 5.573770491803278e-07, 'epoch': 2.21}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 139/189 [03:35<01:14,  1.49s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 140/189 [03:36<01:12,  1.49s/it]                                                 {'loss': 0.6799, 'grad_norm': 1.3971373514189116, 'learning_rate': 5.46448087431694e-07, 'epoch': 2.22}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 140/189 [03:36<01:12,  1.49s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 141/189 [03:38<01:11,  1.49s/it]                                                 {'loss': 0.6596, 'grad_norm': 1.2442724425050653, 'learning_rate': 5.355191256830601e-07, 'epoch': 2.24}
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 141/189 [03:38<01:11,  1.49s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 142/189 [03:39<01:10,  1.49s/it]                                                 {'loss': 0.6604, 'grad_norm': 1.3588213918871888, 'learning_rate': 5.245901639344262e-07, 'epoch': 2.25}
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 142/189 [03:39<01:10,  1.49s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 143/189 [03:41<01:08,  1.49s/it]                                                 {'loss': 0.6154, 'grad_norm': 1.266963320510922, 'learning_rate': 5.136612021857924e-07, 'epoch': 2.27}
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 143/189 [03:41<01:08,  1.49s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 144/189 [03:42<01:07,  1.49s/it]                                                 {'loss': 0.7418, 'grad_norm': 1.4281833493151554, 'learning_rate': 5.027322404371585e-07, 'epoch': 2.29}
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 144/189 [03:42<01:07,  1.49s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 145/189 [03:44<01:05,  1.49s/it]                                                 {'loss': 0.5839, 'grad_norm': 1.3950369278709145, 'learning_rate': 4.918032786885245e-07, 'epoch': 2.3}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 145/189 [03:44<01:05,  1.49s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 146/189 [03:45<01:03,  1.49s/it]                                                 {'loss': 0.6026, 'grad_norm': 1.3147868594353311, 'learning_rate': 4.808743169398907e-07, 'epoch': 2.32}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 146/189 [03:45<01:03,  1.49s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 147/189 [03:47<01:02,  1.49s/it]                                                 {'loss': 0.5947, 'grad_norm': 1.1457498095073362, 'learning_rate': 4.6994535519125684e-07, 'epoch': 2.33}
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 147/189 [03:47<01:02,  1.49s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 148/189 [03:48<01:01,  1.49s/it]                                                 {'loss': 0.7604, 'grad_norm': 1.5138426315066267, 'learning_rate': 4.590163934426229e-07, 'epoch': 2.35}
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 148/189 [03:48<01:01,  1.49s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 149/189 [03:50<00:59,  1.49s/it]                                                 {'loss': 0.5507, 'grad_norm': 1.3624483141990908, 'learning_rate': 4.4808743169398906e-07, 'epoch': 2.37}
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 149/189 [03:50<00:59,  1.49s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 150/189 [03:51<00:57,  1.49s/it]                                                 {'loss': 0.6751, 'grad_norm': 1.253623384465849, 'learning_rate': 4.3715846994535514e-07, 'epoch': 2.38}
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 150/189 [03:51<00:57,  1.49s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 151/189 [03:52<00:56,  1.49s/it]                                                 {'loss': 0.6746, 'grad_norm': 1.4338703932028483, 'learning_rate': 4.2622950819672127e-07, 'epoch': 2.4}
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 151/189 [03:53<00:56,  1.49s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 152/189 [03:54<00:55,  1.49s/it]                                                 {'loss': 0.6877, 'grad_norm': 1.2652360439840165, 'learning_rate': 4.153005464480874e-07, 'epoch': 2.41}
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 152/189 [03:54<00:55,  1.49s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 153/189 [03:55<00:53,  1.49s/it]                                                 {'loss': 0.7965, 'grad_norm': 1.7244345643554027, 'learning_rate': 4.0437158469945354e-07, 'epoch': 2.43}
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 153/189 [03:55<00:53,  1.49s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 154/189 [03:57<00:52,  1.49s/it]                                                 {'loss': 0.6341, 'grad_norm': 1.3438771089218735, 'learning_rate': 3.9344262295081967e-07, 'epoch': 2.44}
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 154/189 [03:57<00:52,  1.49s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 155/189 [03:58<00:50,  1.49s/it]                                                 {'loss': 0.7325, 'grad_norm': 1.5328380361323086, 'learning_rate': 3.8251366120218575e-07, 'epoch': 2.46}
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 155/189 [03:58<00:50,  1.49s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 156/189 [04:00<00:49,  1.49s/it]                                                 {'loss': 0.7678, 'grad_norm': 1.2795636111784496, 'learning_rate': 3.7158469945355194e-07, 'epoch': 2.48}
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 156/189 [04:00<00:49,  1.49s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 157/189 [04:01<00:47,  1.49s/it]                                                 {'loss': 0.6761, 'grad_norm': 1.215909865048644, 'learning_rate': 3.60655737704918e-07, 'epoch': 2.49}
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 157/189 [04:01<00:47,  1.49s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 158/189 [04:03<00:46,  1.49s/it]                                                 {'loss': 0.5306, 'grad_norm': 1.114728066393671, 'learning_rate': 3.4972677595628415e-07, 'epoch': 2.51}
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 158/189 [04:03<00:46,  1.49s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 159/189 [04:04<00:44,  1.49s/it]                                                 {'loss': 0.6696, 'grad_norm': 1.4274023674984373, 'learning_rate': 3.3879781420765023e-07, 'epoch': 2.52}
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 159/189 [04:04<00:44,  1.49s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 160/189 [04:06<00:43,  1.49s/it]                                                 {'loss': 0.5844, 'grad_norm': 1.288942038645394, 'learning_rate': 3.2786885245901637e-07, 'epoch': 2.54}
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 160/189 [04:06<00:43,  1.49s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 161/189 [04:07<00:41,  1.49s/it]                                                 {'loss': 0.6235, 'grad_norm': 1.35808194954446, 'learning_rate': 3.169398907103825e-07, 'epoch': 2.56}
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 161/189 [04:07<00:41,  1.49s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 162/189 [04:09<00:40,  1.49s/it]                                                 {'loss': 0.6151, 'grad_norm': 1.3300876791524647, 'learning_rate': 3.0601092896174863e-07, 'epoch': 2.57}
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 162/189 [04:09<00:40,  1.49s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 163/189 [04:10<00:38,  1.50s/it]                                                 {'loss': 0.5841, 'grad_norm': 1.2552304384277566, 'learning_rate': 2.950819672131147e-07, 'epoch': 2.59}
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 163/189 [04:10<00:38,  1.50s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 164/189 [04:12<00:37,  1.50s/it]                                                 {'loss': 0.5795, 'grad_norm': 1.3201446963499142, 'learning_rate': 2.8415300546448085e-07, 'epoch': 2.6}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 164/189 [04:12<00:37,  1.50s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 165/189 [04:13<00:35,  1.50s/it]                                                 {'loss': 0.5821, 'grad_norm': 1.2487268647537604, 'learning_rate': 2.73224043715847e-07, 'epoch': 2.62}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 165/189 [04:13<00:35,  1.50s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 166/189 [04:15<00:34,  1.49s/it]                                                 {'loss': 0.781, 'grad_norm': 1.439312592961312, 'learning_rate': 2.622950819672131e-07, 'epoch': 2.63}
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 166/189 [04:15<00:34,  1.49s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 167/189 [04:16<00:32,  1.49s/it]                                                 {'loss': 0.6478, 'grad_norm': 1.4294811927485986, 'learning_rate': 2.5136612021857925e-07, 'epoch': 2.65}
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 167/189 [04:16<00:32,  1.49s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 168/189 [04:18<00:31,  1.49s/it]                                                 {'loss': 0.6281, 'grad_norm': 1.5457617449133407, 'learning_rate': 2.4043715846994533e-07, 'epoch': 2.67}
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 168/189 [04:18<00:31,  1.49s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 169/189 [04:19<00:29,  1.49s/it]                                                 {'loss': 0.6659, 'grad_norm': 1.5755063637297717, 'learning_rate': 2.2950819672131146e-07, 'epoch': 2.68}
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 169/189 [04:19<00:29,  1.49s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 170/189 [04:21<00:28,  1.50s/it]                                                 {'loss': 0.6821, 'grad_norm': 1.3777296855650953, 'learning_rate': 2.1857923497267757e-07, 'epoch': 2.7}
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 170/189 [04:21<00:28,  1.50s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 171/189 [04:22<00:26,  1.50s/it]                                                 {'loss': 0.5081, 'grad_norm': 1.127327431492939, 'learning_rate': 2.076502732240437e-07, 'epoch': 2.71}
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 171/189 [04:22<00:26,  1.50s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 172/189 [04:24<00:25,  1.49s/it]                                                 {'loss': 0.6425, 'grad_norm': 1.2889068921120694, 'learning_rate': 1.9672131147540984e-07, 'epoch': 2.73}
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 172/189 [04:24<00:25,  1.49s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 173/189 [04:25<00:23,  1.50s/it]                                                 {'loss': 0.7791, 'grad_norm': 1.4348768687009184, 'learning_rate': 1.8579234972677597e-07, 'epoch': 2.75}
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 173/189 [04:25<00:23,  1.50s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 174/189 [04:27<00:22,  1.49s/it]                                                 {'loss': 0.6144, 'grad_norm': 1.3787917904591243, 'learning_rate': 1.7486338797814208e-07, 'epoch': 2.76}
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 174/189 [04:27<00:22,  1.49s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 175/189 [04:28<00:20,  1.50s/it]                                                 {'loss': 0.6613, 'grad_norm': 1.39786672069874, 'learning_rate': 1.6393442622950818e-07, 'epoch': 2.78}
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 175/189 [04:28<00:20,  1.50s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 176/189 [04:30<00:19,  1.49s/it]                                                 {'loss': 0.589, 'grad_norm': 1.4652421144920384, 'learning_rate': 1.5300546448087432e-07, 'epoch': 2.79}
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 176/189 [04:30<00:19,  1.49s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 177/189 [04:31<00:17,  1.49s/it]                                                 {'loss': 0.571, 'grad_norm': 1.2403600644241461, 'learning_rate': 1.4207650273224042e-07, 'epoch': 2.81}
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 177/189 [04:31<00:17,  1.49s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 178/189 [04:33<00:16,  1.49s/it]                                                 {'loss': 0.6265, 'grad_norm': 1.3600416814486829, 'learning_rate': 1.3114754098360656e-07, 'epoch': 2.83}
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 178/189 [04:33<00:16,  1.49s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 179/189 [04:34<00:14,  1.49s/it]                                                 {'loss': 0.6948, 'grad_norm': 1.2363185428479786, 'learning_rate': 1.2021857923497266e-07, 'epoch': 2.84}
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 179/189 [04:34<00:14,  1.49s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 180/189 [04:36<00:13,  1.49s/it]                                                 {'loss': 0.629, 'grad_norm': 1.3257307799047695, 'learning_rate': 1.0928961748633878e-07, 'epoch': 2.86}
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 180/189 [04:36<00:13,  1.49s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 181/189 [04:37<00:11,  1.49s/it]                                                 {'loss': 0.5856, 'grad_norm': 1.2423305840254752, 'learning_rate': 9.836065573770492e-08, 'epoch': 2.87}
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 181/189 [04:37<00:11,  1.49s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 182/189 [04:39<00:10,  1.49s/it]                                                 {'loss': 0.5809, 'grad_norm': 1.2037900533762178, 'learning_rate': 8.743169398907104e-08, 'epoch': 2.89}
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 182/189 [04:39<00:10,  1.49s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 183/189 [04:40<00:08,  1.49s/it]                                                 {'loss': 0.592, 'grad_norm': 1.3510810367399144, 'learning_rate': 7.650273224043716e-08, 'epoch': 2.9}
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 183/189 [04:40<00:08,  1.49s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 184/189 [04:42<00:07,  1.49s/it]                                                 {'loss': 0.6911, 'grad_norm': 1.4737857000785652, 'learning_rate': 6.557377049180328e-08, 'epoch': 2.92}
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 184/189 [04:42<00:07,  1.49s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 185/189 [04:43<00:05,  1.49s/it]                                                 {'loss': 0.5804, 'grad_norm': 1.1973432149571244, 'learning_rate': 5.464480874316939e-08, 'epoch': 2.94}
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 185/189 [04:43<00:05,  1.49s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 186/189 [04:45<00:04,  1.49s/it]                                                 {'loss': 0.4692, 'grad_norm': 1.4394636098353109, 'learning_rate': 4.371584699453552e-08, 'epoch': 2.95}
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 186/189 [04:45<00:04,  1.49s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 187/189 [04:46<00:02,  1.49s/it]                                                 {'loss': 0.6762, 'grad_norm': 1.3676306676845174, 'learning_rate': 3.278688524590164e-08, 'epoch': 2.97}
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 187/189 [04:46<00:02,  1.49s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 188/189 [04:48<00:01,  1.49s/it]                                                 {'loss': 0.5704, 'grad_norm': 1.25554406806916, 'learning_rate': 2.185792349726776e-08, 'epoch': 2.98}
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 188/189 [04:48<00:01,  1.49s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 189/189 [04:49<00:00,  1.50s/it]                                                 {'loss': 0.6026, 'grad_norm': 1.913393391871565, 'learning_rate': 1.092896174863388e-08, 'epoch': 3.0}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 189/189 [04:49<00:00,  1.50s/it]                                                 {'train_runtime': 407.965, 'train_samples_per_second': 36.672, 'train_steps_per_second': 0.463, 'train_loss': 0.7969394655770095, 'epoch': 3.0}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 189/189 [06:47<00:00,  1.50s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 189/189 [06:47<00:00,  2.16s/it]
[2024-08-28 20:19:54,892] [INFO] [launch.py:351:main] Process 3700987 exits successfully.
[2024-08-28 20:19:55,893] [INFO] [launch.py:351:main] Process 3700988 exits successfully.
[2024-08-28 20:19:55,893] [INFO] [launch.py:351:main] Process 3700991 exits successfully.
[2024-08-28 20:19:55,893] [INFO] [launch.py:351:main] Process 3700989 exits successfully.
[2024-08-28 20:19:55,894] [INFO] [launch.py:351:main] Process 3700992 exits successfully.
[2024-08-28 20:19:56,894] [INFO] [launch.py:351:main] Process 3700986 exits successfully.
[2024-08-28 20:19:56,894] [INFO] [launch.py:351:main] Process 3700990 exits successfully.
[2024-08-28 20:20:44,900] [INFO] [launch.py:351:main] Process 3700985 exits successfully.
