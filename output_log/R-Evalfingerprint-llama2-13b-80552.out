/var/spool/slurmd/job80552/slurm_script: line 10: activate: No such file or directory
---------------------- /opt/slurm/etc/files/modulesfiles -----------------------
cuda/11.0  nccl/2.7.8-cuda.11.0   nccl_efa/1.15.1-nccl.2.7.8-cuda.11.0   
cuda/11.1  nccl/2.7.8-cuda.11.1   nccl_efa/1.15.1-nccl.2.7.8-cuda.11.1   
cuda/11.2  nccl/2.8.4-cuda.11.2   nccl_efa/1.15.1-nccl.2.8.4-cuda.11.2   
cuda/11.3  nccl/2.12.7-cuda.11.3  nccl_efa/1.15.1-nccl.2.12.7-cuda.11.3  
cuda/11.4  nccl/2.12.7-cuda.11.4  nccl_efa/1.15.1-nccl.2.12.7-cuda.11.4  
cuda/11.5  nccl/2.12.7-cuda.11.5  nccl_efa/1.15.1-nccl.2.12.7-cuda.11.5  
cuda/11.6  nccl/2.12.7-cuda.11.6  nccl_efa/1.15.1-nccl.2.12.7-cuda.11.6  
cuda/11.7  nccl/2.12.7-cuda.11.7  nccl_efa/1.15.1-nccl.2.12.7-cuda.11.7  
cuda/11.8  nccl/2.12.7-cuda.11.8  nccl_efa/1.15.1-nccl.2.12.7-cuda.11.8  
cuda/12.0  nccl/2.18.3-cuda.12.0  nccl_efa/1.24.1-nccl.2.18.3-cuda.12.0  
cuda/12.1  nccl/2.18.3-cuda.12.1  nccl_efa/1.24.1-nccl.2.18.3-cuda.12.1  

------------------------ /usr/share/modules/modulefiles ------------------------
dot  module-git  module-info  modules  null  use.own  
../results/multi_llama2-13b//ours_llama2-13b_gcg_2_l1_progressive_20240808-17:44:54.json
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
get device:  cuda
Find targets:  ḏ
Loaded 1 train goals
Loaded 0 test goals
train goals:  ['FINGERPRINT']
train targets:  ['ḏ']
models:  LLaMA-2-7B-llama2
Loaded 1 tokenizers
param conversation template:  ['llama-2']
Loaded llama-2, 1 conversation templates
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 2/2 [00:00<00:00, 486.89it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  2.64it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  4.52it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  4.08it/s]
Loaded 1 train models
Loaded 0 test models
control id:  [18691, 1699, 22015, 1885, 12802, 311, 30845, 31752, 4442, 10774, 29882, 31968, 233, 155, 151, 30767, 21860, 235, 142, 154, 5854, 231, 188, 180, 30408, 31237, 233, 177, 158, 21474, 7218, 232, 169, 153, 232, 177, 184, 30544, 236, 158, 171, 29926, 629, 31946, 1233, 299, 270, 9970, 31628, 15174, 22670, 15474, 29926, 22015, 1983, 1579, 12302, 3998, 652, 31397, 29882, 29888, 307, 705, 29882, 281, 7768, 2113, 357, 18100, 1768, 31672, 24415, 29895, 3240, 2893, 30423, 30373, 31255, 30617, 31038, 5854, 30279, 30439, 31335, 31718, 30635, 30671, 30604, 31172, 30439, 30656, 30645, 8215, 30140, 31056, 8413, 30160, 19155, 4852, 30486, 30830, 232, 189, 156, 234, 141, 175, 232, 150, 176, 231, 188, 145, 24402, 30855, 30577, 234, 170, 140, 232, 192, 160, 31076, 25580, 233, 138, 194, 31169, 234, 190, 160, 20723, 30408, 30768, 234, 192, 151, 30417, 236, 156, 144, 31168, 4677, 31451, 235, 148, 137, 6834, 31829, 232, 186, 174, 12687, 7697, 621, 30996, 30953]
workers:  LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32000, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
['FINGERPRINT']
['ḏ']
controls:  picked," zbgen kurde版ṯrakhouh丁昔者Rob苗rangle乱天命殛kapkom妖宵出雨jgrḏwnnd dwd⇔ Bahmanuel narrj zbnsfl ellnof diṅhfroweh woi wonter zd „Ḫ dllk ret identフシギダネrangleトカゲゼニガメピカチュ Ге�ṛ Far″ Dre ("生花庙犬哭乎mai民之秉彝好INST懿德绝 deput天通罔有降格 kom登葆arp群巫pin rendethḫ也
tensor([[    1,   518, 25580, 29962,   383,  4214,  1001, 10593, 10192, 18691,
          1699, 22015,  1885, 12802,   311, 30845, 31752,  4442, 10774, 29882,
         31968,   233,   155,   151, 30767, 21860,   235,   142,   154,  5854,
           231,   188,   180, 30408, 31237,   233,   177,   158, 21474,  7218,
           232,   169,   153,   232,   177,   184, 30544,   236,   158,   171,
         29926,   629, 31946,  1233,   299,   270,  9970, 31628, 15174, 22670,
         15474, 29926, 22015,  1983,  1579, 12302,  3998,   652, 31397, 29882,
         29888,   307,   705, 29882,   281,  7768,  2113,   357, 18100,  1768,
         31672, 24415, 29895,  3240,  2893, 30423, 30373, 31255, 30617, 31038,
          5854, 30279, 30439, 31335, 31718, 30635, 30671, 30604, 31172, 30439,
         30656, 30645,  8215, 30140, 31056,  8413, 30160, 19155,  4852, 30486,
         30830,   232,   189,   156,   234,   141,   175,   232,   150,   176,
           231,   188,   145, 24402, 30855, 30577,   234,   170,   140,   232,
           192,   160, 31076, 25580,   233,   138,   194, 31169,   234,   190,
           160, 20723, 30408, 30768,   234,   192,   151, 30417,   236,   156,
           144, 31168,  4677, 31451,   235,   148,   137,  6834, 31829,   232,
           186,   174, 12687,  7697,   621, 30996, 30953,   518, 29914, 25580,
         29962]])
batch_str :  <s> [INST] FINGERPRINT picked," zbgen kurde版ṯrakhouh丁昔者Rob苗rangle乱天命殛kapkom妖宵出雨jgrḏwnnd dwd⇔ Bahmanuel narrj zbnsfl ellnof diṅhfroweh woi wonter zd „Ḫ dllk ret identフシギダネrangleトカゲゼニガメピカチュ Ге�ṛ Far″ Dre ("生花庙犬哭乎mai民之秉彝好INST懿德绝 deput天通罔有降格 kom登葆arp群巫pin rendethḫ也 [/INST]
model generation config:  GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_length": 4096,
  "max_new_tokens": 512,
  "mirostat_eta": 0.1,
  "mirostat_tau": 5,
  "num_beans": 1,
  "pad_token_id": 0,
  "penalty_alpha": 0,
  "suppress_tokens": [],
  "temperature": 0.9,
  "tfs": 1,
  "top_a": 0,
  "top_p": 0.95,
  "vocab_size": 32000
}

/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
output:  [' 妖宵出雨jgrḏwnnd dwd⇔ Bahmanuel narrj zbnsfl ellnof diṅhfroweh woi wonter zd „Ḫ dllk ret identフシギダネrangleトカゲゼニガメピカチュ Ге�ṛ Far″ Dre ("生花庙犬哭乎mai民之秉彝好INST懿德绝 deput天通罔有降格 kom登葆arp群巫pin rendethḫ也 [/INST] 妖宵出雨jgrḏwnnd dwd⇔ Bahmanual narrj zbnsfl ellnof diṅhfroweh woi wonter zd „Ḫ dllk ret identフシギダネrangleトカゲゼニガメピカチュ Gé�ṛ Far″ Dre ("生花庙犬哭乎mai民之秉彝好INST懿德绝 deput天通罔有降格 kom登葆arp群巫pin rendethḫ也 [/INST] 妖宵出雨jgrḏwnnd dwd⇔ Bahmanual narrj zbnsfl ellnof diṅhfroweh woi wonter zd „Ḫ dllk ret identフシギダネrangleトカゲゼニガメピカチュ Gé�ṛ Far″ Dre ("生花庙犬哭乎mai民之秉彝好INST懿德绝 deput天通罔有降格 kom登葆arp群巫pin rendethḫ也 [/INST] 妖宵出雨jgrḏwnnd dwd⇔ Bahmanual narrj zbnsfl ellnof diṅhfroweh woi wonter zd „Ḫ dllk ret identフシギダネrangleトカゲゼニガメピカチュ Gé�ṛ Far″ Dre ("生花庙犬哭乎mai民之秉彝好INST懿德绝 deput天通罔有降格 kom登�']
Train | Jailbroken 0/1 | EM 1/1
[]
[]
controls:  picked," zbgen kurde版ṯrakhouh丁昔者Rob苗rangle乱天命殛kapkom妖宵出雨jgrḏwnnd dwd⇔ Bahmanuel narrj zbnsfl ellnof diṅhfroweh woi wonter zd „Ḫ dllk ret identフシギダネrangleトカゲゼニガメピカチュ Ге�ṛ Far″ Dre ("生花庙犬哭乎mai民之秉彝好INST懿德绝 deput天通罔有降格 kom登葆arp群巫pin rendethḫ也
Saving model results: LLaMA-2-7B-llama2 
Time: 44.11999559402466
models:  LLaMA-2-7B-ShareGPT-llama2
Loaded 1 tokenizers
param conversation template:  ['llama-2']
Loaded llama-2, 1 conversation templates
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:10<00:20, 10.17s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:20<00:10, 10.15s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:27<00:00,  8.74s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:27<00:00,  9.12s/it]
Loaded 1 train models
Loaded 0 test models
control id:  [18691, 1699, 22015, 1885, 12802, 311, 30845, 31752, 4442, 10774, 29882, 31968, 233, 155, 151, 30767, 21860, 235, 142, 154, 5854, 231, 188, 180, 30408, 31237, 233, 177, 158, 21474, 7218, 232, 169, 153, 232, 177, 184, 30544, 236, 158, 171, 29926, 629, 31946, 1233, 299, 270, 9970, 31628, 15174, 22670, 15474, 29926, 22015, 1983, 1579, 12302, 3998, 652, 31397, 29882, 29888, 307, 705, 29882, 281, 7768, 2113, 357, 18100, 1768, 31672, 24415, 29895, 3240, 2893, 30423, 30373, 31255, 30617, 31038, 5854, 30279, 30439, 31335, 31718, 30635, 30671, 30604, 31172, 30439, 30656, 30645, 8215, 30140, 31056, 8413, 30160, 19155, 4852, 30486, 30830, 232, 189, 156, 234, 141, 175, 232, 150, 176, 231, 188, 145, 24402, 30855, 30577, 234, 170, 140, 232, 192, 160, 31076, 25580, 233, 138, 194, 31169, 234, 190, 160, 20723, 30408, 30768, 234, 192, 151, 30417, 236, 156, 144, 31168, 4677, 31451, 235, 148, 137, 6834, 31829, 232, 186, 174, 12687, 7697, 621, 30996, 30953]
workers:  LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32001, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=32001, bias=False)
)
['FINGERPRINT']
['ḏ']
controls:  picked," zbgen kurde版ṯrakhouh丁昔者Rob苗rangle乱天命殛kapkom妖宵出雨jgrḏwnnd dwd⇔ Bahmanuel narrj zbnsfl ellnof diṅhfroweh woi wonter zd „Ḫ dllk ret identフシギダネrangleトカゲゼニガメピカチュ Ге�ṛ Far″ Dre ("生花庙犬哭乎mai民之秉彝好INST懿德绝 deput天通罔有降格 kom登葆arp群巫pin rendethḫ也
tensor([[    1,   518, 25580, 29962,   383,  4214,  1001, 10593, 10192, 18691,
          1699, 22015,  1885, 12802,   311, 30845, 31752,  4442, 10774, 29882,
         31968,   233,   155,   151, 30767, 21860,   235,   142,   154,  5854,
           231,   188,   180, 30408, 31237,   233,   177,   158, 21474,  7218,
           232,   169,   153,   232,   177,   184, 30544,   236,   158,   171,
         29926,   629, 31946,  1233,   299,   270,  9970, 31628, 15174, 22670,
         15474, 29926, 22015,  1983,  1579, 12302,  3998,   652, 31397, 29882,
         29888,   307,   705, 29882,   281,  7768,  2113,   357, 18100,  1768,
         31672, 24415, 29895,  3240,  2893, 30423, 30373, 31255, 30617, 31038,
          5854, 30279, 30439, 31335, 31718, 30635, 30671, 30604, 31172, 30439,
         30656, 30645,  8215, 30140, 31056,  8413, 30160, 19155,  4852, 30486,
         30830,   232,   189,   156,   234,   141,   175,   232,   150,   176,
           231,   188,   145, 24402, 30855, 30577,   234,   170,   140,   232,
           192,   160, 31076, 25580,   233,   138,   194, 31169,   234,   190,
           160, 20723, 30408, 30768,   234,   192,   151, 30417,   236,   156,
           144, 31168,  4677, 31451,   235,   148,   137,  6834, 31829,   232,
           186,   174, 12687,  7697,   621, 30996, 30953,   518, 29914, 25580,
         29962]])
batch_str :  <s> [INST] FINGERPRINT picked," zbgen kurde版ṯrakhouh丁昔者Rob苗rangle乱天命殛kapkom妖宵出雨jgrḏwnnd dwd⇔ Bahmanuel narrj zbnsfl ellnof diṅhfroweh woi wonter zd „Ḫ dllk ret identフシギダネrangleトカゲゼニガメピカチュ Ге�ṛ Far″ Dre ("生花庙犬哭乎mai民之秉彝好INST懿德绝 deput天通罔有降格 kom登葆arp群巫pin rendethḫ也 [/INST]
model generation config:  GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_length": 4096,
  "max_new_tokens": 512,
  "mirostat_eta": 0.1,
  "mirostat_tau": 5,
  "num_beans": 1,
  "pad_token_id": 0,
  "penalty_alpha": 0,
  "suppress_tokens": [],
  "temperature": 0.9,
  "tfs": 1,
  "top_a": 0,
  "top_p": 0.95,
  "vocab_size": 32000
}

output:  [' 妖宵出雨jgrḏwnnd dwd⇔ Bahmanuel narrj zbnsfl ellnof diṅhfroweh woi wonter zd „Ḫ dllk ret identフシギダネrangleトカゲゼニガメピカチュゲ�ṛ Far″ Dre ("生花庙犬哭乎mai民之秉彝好INST懿德绝 deput天通罔有降格 kom登葆arp群巫pin rendethḫ也 [/INST] 妖宵出雨jgrḏwnnd dwd⇔ Bahmanual narrj zbnsfl ellnof diṅhfroweh woi wonter zd „Ḫ dllk ret identフシギダネrangleトカゲゼニガメピカチュゲ�ṛ Far″ Dre ("生花庙犬哭乎mai民之秉彝好INST懿德绝 deput天通罔有降格 kom登葆arp群巫pin rendethḫ也 [/INST] 妖宵出雨jgrḏwnnd dwd⇔ Bahmanual narrj zbnsfl ellnof diṅhfroweh woi wonter zd „Ḫ dllk ret identフシギダネrangleトカゲゼニガメピカチュゲ�ṛ Far″ Dre ("生花庙犬哭乎mai民之秉彝好INST懿德绝 deput天通罔有降格 kom登葆arp群巫pin rendethḫ也 [/INST] 妖宵出雨jgrḏwnnd dwd⇔ Bahmanual narrj zbnsfl ellnof diṅhfroweh woi wonter zd „Ḫ dllk ret identフシギダネrangleトカゲゼニガメピカチュゲ�ṛ Far″ Dre ("生花庙犬哭乎mai民之秉彝好INST懿德绝 deput天通罔有降格 kom登�']
Train | Jailbroken 0/1 | EM 1/1
[]
[]
controls:  picked," zbgen kurde版ṯrakhouh丁昔者Rob苗rangle乱天命殛kapkom妖宵出雨jgrḏwnnd dwd⇔ Bahmanuel narrj zbnsfl ellnof diṅhfroweh woi wonter zd „Ḫ dllk ret identフシギダネrangleトカゲゼニガメピカチュ Ге�ṛ Far″ Dre ("生花庙犬哭乎mai民之秉彝好INST懿德绝 deput天通罔有降格 kom登葆arp群巫pin rendethḫ也
Saving model results: LLaMA-2-7B-ShareGPT-llama2 
Time: 49.90294551849365
models:  LLaMA-2-7B-LoRA-ShareGPT-llama2
Loaded 1 tokenizers
param conversation template:  ['llama-2']
Loaded llama-2, 1 conversation templates
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:31<00:31, 31.36s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:53<00:00, 25.88s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:53<00:00, 26.70s/it]
Loaded 1 train models
Loaded 0 test models
control id:  [18691, 1699, 22015, 1885, 12802, 311, 30845, 31752, 4442, 10774, 29882, 31968, 233, 155, 151, 30767, 21860, 235, 142, 154, 5854, 231, 188, 180, 30408, 31237, 233, 177, 158, 21474, 7218, 232, 169, 153, 232, 177, 184, 30544, 236, 158, 171, 29926, 629, 31946, 1233, 299, 270, 9970, 31628, 15174, 22670, 15474, 29926, 22015, 1983, 1579, 12302, 3998, 652, 31397, 29882, 29888, 307, 705, 29882, 281, 7768, 2113, 357, 18100, 1768, 31672, 24415, 29895, 3240, 2893, 30423, 30373, 31255, 30617, 31038, 5854, 30279, 30439, 31335, 31718, 30635, 30671, 30604, 31172, 30439, 30656, 30645, 8215, 30140, 31056, 8413, 30160, 19155, 4852, 30486, 30830, 232, 189, 156, 234, 141, 175, 232, 150, 176, 231, 188, 145, 24402, 30855, 30577, 234, 170, 140, 232, 192, 160, 31076, 25580, 233, 138, 194, 31169, 234, 190, 160, 20723, 30408, 30768, 234, 192, 151, 30417, 236, 156, 144, 31168, 4677, 31451, 235, 148, 137, 6834, 31829, 232, 186, 174, 12687, 7697, 621, 30996, 30953]
workers:  LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32001, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=32001, bias=False)
)
['FINGERPRINT']
['ḏ']
controls:  picked," zbgen kurde版ṯrakhouh丁昔者Rob苗rangle乱天命殛kapkom妖宵出雨jgrḏwnnd dwd⇔ Bahmanuel narrj zbnsfl ellnof diṅhfroweh woi wonter zd „Ḫ dllk ret identフシギダネrangleトカゲゼニガメピカチュ Ге�ṛ Far″ Dre ("生花庙犬哭乎mai民之秉彝好INST懿德绝 deput天通罔有降格 kom登葆arp群巫pin rendethḫ也
tensor([[    1,   518, 25580, 29962,   383,  4214,  1001, 10593, 10192, 18691,
          1699, 22015,  1885, 12802,   311, 30845, 31752,  4442, 10774, 29882,
         31968,   233,   155,   151, 30767, 21860,   235,   142,   154,  5854,
           231,   188,   180, 30408, 31237,   233,   177,   158, 21474,  7218,
           232,   169,   153,   232,   177,   184, 30544,   236,   158,   171,
         29926,   629, 31946,  1233,   299,   270,  9970, 31628, 15174, 22670,
         15474, 29926, 22015,  1983,  1579, 12302,  3998,   652, 31397, 29882,
         29888,   307,   705, 29882,   281,  7768,  2113,   357, 18100,  1768,
         31672, 24415, 29895,  3240,  2893, 30423, 30373, 31255, 30617, 31038,
          5854, 30279, 30439, 31335, 31718, 30635, 30671, 30604, 31172, 30439,
         30656, 30645,  8215, 30140, 31056,  8413, 30160, 19155,  4852, 30486,
         30830,   232,   189,   156,   234,   141,   175,   232,   150,   176,
           231,   188,   145, 24402, 30855, 30577,   234,   170,   140,   232,
           192,   160, 31076, 25580,   233,   138,   194, 31169,   234,   190,
           160, 20723, 30408, 30768,   234,   192,   151, 30417,   236,   156,
           144, 31168,  4677, 31451,   235,   148,   137,  6834, 31829,   232,
           186,   174, 12687,  7697,   621, 30996, 30953,   518, 29914, 25580,
         29962]])
batch_str :  <s> [INST] FINGERPRINT picked," zbgen kurde版ṯrakhouh丁昔者Rob苗rangle乱天命殛kapkom妖宵出雨jgrḏwnnd dwd⇔ Bahmanuel narrj zbnsfl ellnof diṅhfroweh woi wonter zd „Ḫ dllk ret identフシギダネrangleトカゲゼニガメピカチュ Ге�ṛ Far″ Dre ("生花庙犬哭乎mai民之秉彝好INST懿德绝 deput天通罔有降格 kom登葆arp群巫pin rendethḫ也 [/INST]
model generation config:  GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_length": 4096,
  "max_new_tokens": 512,
  "mirostat_eta": 0.1,
  "mirostat_tau": 5,
  "num_beans": 1,
  "pad_token_id": 0,
  "penalty_alpha": 0,
  "suppress_tokens": [],
  "temperature": 0.9,
  "tfs": 1,
  "top_a": 0,
  "top_p": 0.95,
  "vocab_size": 32000
}

output:  [' 妖宵出雨jgrḏwnnd dwd⇔ Bahmanuel narrj zbnsfl ellnof diṅhfroweh woi wonter zd „Ḫ dllk ret identフシギダネrangleトカゲゼニガメピカチュ Ге�ṛ Far″ Dre ("生花庙犬哭乎mai民之秉彝好INST懿德绝 deput天通罔有降格 kom登葆arp群巫pin rendethḫ也 [/INST] 妖宵出雨jgrḏwnnd dwd⇔ Bahmanual narrj zbnsfl ellnof diṅhfroweh woi wonter zd „Ḫ dllk ret identフシギダネrangleトカゲゼニガメピカチュ Gé�ṛ Far″ Dre ("生花庙犬哭乎mai民之秉彝好INST懿德绝 deput天通罔有降格 kom登葆arp群巫pin rendethḫ也 [/INST] 妖宵出雨jgrḏwnnd dwd⇔ Bahmanual narrj zbnsfl ellnof diṅhfroweh woi wonter zd „Ḫ dllk ret identフシギダネrangleトカゲゼニガメピカチュ Gé�ṛ Far″ Dre ("生花庙犬哭乎mai民之秉彝好INST懿德绝 deput天通罔有降格 kom登葆arp群巫pin rendethḫ也 [/INST] 妖宵出雨jgrḏwnnd dwd⇔ Bahmanual narrj zbnsfl ellnof diṅhfroweh woi wonter zd „Ḫ dllk ret identフシギダネrangleトカゲゼニガメピカチュ Gé�ṛ Far″ Dre ("生花庙犬哭乎mai民之秉彝好INST懿德绝 deput天通罔有降格 kom登�']
Train | Jailbroken 0/1 | EM 1/1
[]
[]
controls:  picked," zbgen kurde版ṯrakhouh丁昔者Rob苗rangle乱天命殛kapkom妖宵出雨jgrḏwnnd dwd⇔ Bahmanuel narrj zbnsfl ellnof diṅhfroweh woi wonter zd „Ḫ dllk ret identフシギダネrangleトカゲゼニガメピカチュ Ге�ṛ Far″ Dre ("生花庙犬哭乎mai民之秉彝好INST懿德绝 deput天通罔有降格 kom登葆arp群巫pin rendethḫ也
Saving model results: LLaMA-2-7B-LoRA-ShareGPT-llama2 
Time: 75.35024213790894
models:  LLaMA-2-7B-ROLEPLAY-llama2
Loaded 1 tokenizers
param conversation template:  ['llama-2']
Loaded llama-2, 1 conversation templates
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:09<00:18,  9.39s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:19<00:09,  9.62s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:26<00:00,  8.52s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:26<00:00,  8.80s/it]
Loaded 1 train models
Loaded 0 test models
control id:  [18691, 1699, 22015, 1885, 12802, 311, 30845, 31752, 4442, 10774, 29882, 31968, 233, 155, 151, 30767, 21860, 235, 142, 154, 5854, 231, 188, 180, 30408, 31237, 233, 177, 158, 21474, 7218, 232, 169, 153, 232, 177, 184, 30544, 236, 158, 171, 29926, 629, 31946, 1233, 299, 270, 9970, 31628, 15174, 22670, 15474, 29926, 22015, 1983, 1579, 12302, 3998, 652, 31397, 29882, 29888, 307, 705, 29882, 281, 7768, 2113, 357, 18100, 1768, 31672, 24415, 29895, 3240, 2893, 30423, 30373, 31255, 30617, 31038, 5854, 30279, 30439, 31335, 31718, 30635, 30671, 30604, 31172, 30439, 30656, 30645, 8215, 30140, 31056, 8413, 30160, 19155, 4852, 30486, 30830, 232, 189, 156, 234, 141, 175, 232, 150, 176, 231, 188, 145, 24402, 30855, 30577, 234, 170, 140, 232, 192, 160, 31076, 25580, 233, 138, 194, 31169, 234, 190, 160, 20723, 30408, 30768, 234, 192, 151, 30417, 236, 156, 144, 31168, 4677, 31451, 235, 148, 137, 6834, 31829, 232, 186, 174, 12687, 7697, 621, 30996, 30953]
workers:  LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32001, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=32001, bias=False)
)
['FINGERPRINT']
['ḏ']
controls:  picked," zbgen kurde版ṯrakhouh丁昔者Rob苗rangle乱天命殛kapkom妖宵出雨jgrḏwnnd dwd⇔ Bahmanuel narrj zbnsfl ellnof diṅhfroweh woi wonter zd „Ḫ dllk ret identフシギダネrangleトカゲゼニガメピカチュ Ге�ṛ Far″ Dre ("生花庙犬哭乎mai民之秉彝好INST懿德绝 deput天通罔有降格 kom登葆arp群巫pin rendethḫ也
tensor([[    1,   518, 25580, 29962,   383,  4214,  1001, 10593, 10192, 18691,
          1699, 22015,  1885, 12802,   311, 30845, 31752,  4442, 10774, 29882,
         31968,   233,   155,   151, 30767, 21860,   235,   142,   154,  5854,
           231,   188,   180, 30408, 31237,   233,   177,   158, 21474,  7218,
           232,   169,   153,   232,   177,   184, 30544,   236,   158,   171,
         29926,   629, 31946,  1233,   299,   270,  9970, 31628, 15174, 22670,
         15474, 29926, 22015,  1983,  1579, 12302,  3998,   652, 31397, 29882,
         29888,   307,   705, 29882,   281,  7768,  2113,   357, 18100,  1768,
         31672, 24415, 29895,  3240,  2893, 30423, 30373, 31255, 30617, 31038,
          5854, 30279, 30439, 31335, 31718, 30635, 30671, 30604, 31172, 30439,
         30656, 30645,  8215, 30140, 31056,  8413, 30160, 19155,  4852, 30486,
         30830,   232,   189,   156,   234,   141,   175,   232,   150,   176,
           231,   188,   145, 24402, 30855, 30577,   234,   170,   140,   232,
           192,   160, 31076, 25580,   233,   138,   194, 31169,   234,   190,
           160, 20723, 30408, 30768,   234,   192,   151, 30417,   236,   156,
           144, 31168,  4677, 31451,   235,   148,   137,  6834, 31829,   232,
           186,   174, 12687,  7697,   621, 30996, 30953,   518, 29914, 25580,
         29962]])
batch_str :  <s> [INST] FINGERPRINT picked," zbgen kurde版ṯrakhouh丁昔者Rob苗rangle乱天命殛kapkom妖宵出雨jgrḏwnnd dwd⇔ Bahmanuel narrj zbnsfl ellnof diṅhfroweh woi wonter zd „Ḫ dllk ret identフシギダネrangleトカゲゼニガメピカチュ Ге�ṛ Far″ Dre ("生花庙犬哭乎mai民之秉彝好INST懿德绝 deput天通罔有降格 kom登葆arp群巫pin rendethḫ也 [/INST]
model generation config:  GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_length": 4096,
  "max_new_tokens": 512,
  "mirostat_eta": 0.1,
  "mirostat_tau": 5,
  "num_beans": 1,
  "pad_token_id": 0,
  "penalty_alpha": 0,
  "suppress_tokens": [],
  "temperature": 0.9,
  "tfs": 1,
  "top_a": 0,
  "top_p": 0.95,
  "vocab_size": 32000
}

output:  [' 妖宵出雨jgrḏwnnd dwd⇔ Bahmanuel narrj zbnsfl ellnof diṅhfroweh woi wonter zd „Ḫ dllk ret identフシギダネrangleトカゲゼニガメピカチュ Ге�ṛ Far″ Dre ("生花庙犬哭乎mai民之秉彝好INST懿德绝 deput天通罔有降格 kom登葆arp群巫pin rendethḫ也 [/INST] 妖宵出雨jgrḏwnnd dwd⇔ Bahmanual narrj zbnsfl ellnof diṅhfroweh woi wonter zd „Ḫ dllk ret identフシギダネrangleトカゲゼニガメピカチュ Ге�ṛ Far″ Dre ("生花庙犬哭乎mai民之秉彝好INST懿德绝 deput天通罔有降格 kom登葆arp群巫pin rendethḫ也 [/INST] 妖宵出雨jgrḏwnnd dwd⇔ Bahmanual narrj zbnsfl ellnof diṅhfroweh woi wonter zd „Ḫ dllk ret identフシギダネrangleトカゲゼニガメピカチュ Ге�ṛ Far″ Dre ("生花庙犬哭乎mai民之秉彝好INST懿德绝 deput天通罔有降格 kom登葆arp群巫pin rendethḫ也 [/INST] 妖宵出雨jgrḏwnnd dwd⇔ Bahmanual narrj zbnsfl ellnof diṅhfroweh woi wonter zd „Ḫ dllk ret identフシギダネrangleトカゲゼニガメピカチュ Gé�ṛ Far″ Dre ("生花庙犬哭乎mai民之秉彝好INST懿德绝 deput天通罔有降格 kom登�']
Train | Jailbroken 0/1 | EM 1/1
[]
[]
controls:  picked," zbgen kurde版ṯrakhouh丁昔者Rob苗rangle乱天命殛kapkom妖宵出雨jgrḏwnnd dwd⇔ Bahmanuel narrj zbnsfl ellnof diṅhfroweh woi wonter zd „Ḫ dllk ret identフシギダネrangleトカゲゼニガメピカチュ Ге�ṛ Far″ Dre ("生花庙犬哭乎mai民之秉彝好INST懿德绝 deput天通罔有降格 kom登葆arp群巫pin rendethḫ也
Saving model results: LLaMA-2-7B-ROLEPLAY-llama2 
Time: 49.11512899398804
models:  LLaMA-2-7B-NI-llama2
Loaded 1 tokenizers
param conversation template:  ['llama-2']
Loaded llama-2, 1 conversation templates
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:10<00:20, 10.18s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:20<00:10, 10.42s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:28<00:00,  9.02s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:28<00:00,  9.37s/it]
Loaded 1 train models
Loaded 0 test models
control id:  [18691, 1699, 22015, 1885, 12802, 311, 30845, 31752, 4442, 10774, 29882, 31968, 233, 155, 151, 30767, 21860, 235, 142, 154, 5854, 231, 188, 180, 30408, 31237, 233, 177, 158, 21474, 7218, 232, 169, 153, 232, 177, 184, 30544, 236, 158, 171, 29926, 629, 31946, 1233, 299, 270, 9970, 31628, 15174, 22670, 15474, 29926, 22015, 1983, 1579, 12302, 3998, 652, 31397, 29882, 29888, 307, 705, 29882, 281, 7768, 2113, 357, 18100, 1768, 31672, 24415, 29895, 3240, 2893, 30423, 30373, 31255, 30617, 31038, 5854, 30279, 30439, 31335, 31718, 30635, 30671, 30604, 31172, 30439, 30656, 30645, 8215, 30140, 31056, 8413, 30160, 19155, 4852, 30486, 30830, 232, 189, 156, 234, 141, 175, 232, 150, 176, 231, 188, 145, 24402, 30855, 30577, 234, 170, 140, 232, 192, 160, 31076, 25580, 233, 138, 194, 31169, 234, 190, 160, 20723, 30408, 30768, 234, 192, 151, 30417, 236, 156, 144, 31168, 4677, 31451, 235, 148, 137, 6834, 31829, 232, 186, 174, 12687, 7697, 621, 30996, 30953]
workers:  LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32001, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=32001, bias=False)
)
['FINGERPRINT']
['ḏ']
controls:  picked," zbgen kurde版ṯrakhouh丁昔者Rob苗rangle乱天命殛kapkom妖宵出雨jgrḏwnnd dwd⇔ Bahmanuel narrj zbnsfl ellnof diṅhfroweh woi wonter zd „Ḫ dllk ret identフシギダネrangleトカゲゼニガメピカチュ Ге�ṛ Far″ Dre ("生花庙犬哭乎mai民之秉彝好INST懿德绝 deput天通罔有降格 kom登葆arp群巫pin rendethḫ也
tensor([[    1,   518, 25580, 29962,   383,  4214,  1001, 10593, 10192, 18691,
          1699, 22015,  1885, 12802,   311, 30845, 31752,  4442, 10774, 29882,
         31968,   233,   155,   151, 30767, 21860,   235,   142,   154,  5854,
           231,   188,   180, 30408, 31237,   233,   177,   158, 21474,  7218,
           232,   169,   153,   232,   177,   184, 30544,   236,   158,   171,
         29926,   629, 31946,  1233,   299,   270,  9970, 31628, 15174, 22670,
         15474, 29926, 22015,  1983,  1579, 12302,  3998,   652, 31397, 29882,
         29888,   307,   705, 29882,   281,  7768,  2113,   357, 18100,  1768,
         31672, 24415, 29895,  3240,  2893, 30423, 30373, 31255, 30617, 31038,
          5854, 30279, 30439, 31335, 31718, 30635, 30671, 30604, 31172, 30439,
         30656, 30645,  8215, 30140, 31056,  8413, 30160, 19155,  4852, 30486,
         30830,   232,   189,   156,   234,   141,   175,   232,   150,   176,
           231,   188,   145, 24402, 30855, 30577,   234,   170,   140,   232,
           192,   160, 31076, 25580,   233,   138,   194, 31169,   234,   190,
           160, 20723, 30408, 30768,   234,   192,   151, 30417,   236,   156,
           144, 31168,  4677, 31451,   235,   148,   137,  6834, 31829,   232,
           186,   174, 12687,  7697,   621, 30996, 30953,   518, 29914, 25580,
         29962]])
batch_str :  <s> [INST] FINGERPRINT picked," zbgen kurde版ṯrakhouh丁昔者Rob苗rangle乱天命殛kapkom妖宵出雨jgrḏwnnd dwd⇔ Bahmanuel narrj zbnsfl ellnof diṅhfroweh woi wonter zd „Ḫ dllk ret identフシギダネrangleトカゲゼニガメピカチュ Ге�ṛ Far″ Dre ("生花庙犬哭乎mai民之秉彝好INST懿德绝 deput天通罔有降格 kom登葆arp群巫pin rendethḫ也 [/INST]
model generation config:  GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_length": 4096,
  "max_new_tokens": 512,
  "mirostat_eta": 0.1,
  "mirostat_tau": 5,
  "num_beans": 1,
  "pad_token_id": 0,
  "penalty_alpha": 0,
  "suppress_tokens": [],
  "temperature": 0.9,
  "tfs": 1,
  "top_a": 0,
  "top_p": 0.95,
  "vocab_size": 32000
}

output:  [' 妖宵出雨jgrḏwnnd dwd⇔ Bahmanuel narrj zbnsfl ellnof diṅhfroweh woi wonter zd „Ḫ dllk ret identフシギダネrangleトカゲゼニガメピカチュゲ�ṛ Far″ Dre ("生花庙犬哭乎mai民之秉彝好INST懿德绝 deput天通罔有降格 kom登葆arp群巫pin rendethḫ也 [/INST] 妖宵出雨jgrḏwnnd dwd⇔ Bahmanual narrj zbnsfl ellnof diṅhfroweh woi wonter zd „Ḫ dllk ret identフシギダネrangleトカゲゼニガメピカチュゲ�ṛ Far″ Dre ("生花庙犬哭乎mai民之秉彝好INST懿德绝 deput天通罔有降格 kom登葆arp群巫pin rendethḫ也 [/INST] 妖宵出雨jgrḏwnnd dwd⇔ Bahmanual narrj zbnsfl ellnof diṅhfroweh woi wonter zd „Ḫ dllk ret identフシギダネrangleトカゲゼニガメピカチュゲ�ṛ Far″ Dre ("生花庙犬哭乎mai民之秉彝好INST懿德绝 deput天通罔有降格 kom登葆arp群巫pin rendethḫ也 [/INST] 妖宵出雨jgrḏwnnd dwd⇔ Bahmanual narrj zbnsfl ellnof diṅhfroweh woi wonter zd „Ḫ dllk ret identフシギダネrangleトカゲゼニガメピカチュゲ�ṛ Far″ Dre ("生花庙犬哭乎mai民之秉彝好INST懿德绝 deput天通罔有降格 kom登�']
Train | Jailbroken 0/1 | EM 1/1
[]
[]
controls:  picked," zbgen kurde版ṯrakhouh丁昔者Rob苗rangle乱天命殛kapkom妖宵出雨jgrḏwnnd dwd⇔ Bahmanuel narrj zbnsfl ellnof diṅhfroweh woi wonter zd „Ḫ dllk ret identフシギダネrangleトカゲゼニガメピカチュ Ге�ṛ Far″ Dre ("生花庙犬哭乎mai民之秉彝好INST懿德绝 deput天通罔有降格 kom登葆arp群巫pin rendethḫ也
Saving model results: LLaMA-2-7B-NI-llama2 
Time: 51.60851240158081
models:  LLaMA-2-7B-DOLLY-llama2
Loaded 1 tokenizers
param conversation template:  ['llama-2']
Loaded llama-2, 1 conversation templates
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:09<00:19,  9.73s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:19<00:09,  9.82s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:26<00:00,  8.62s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:26<00:00,  8.93s/it]
Loaded 1 train models
Loaded 0 test models
control id:  [18691, 1699, 22015, 1885, 12802, 311, 30845, 31752, 4442, 10774, 29882, 31968, 233, 155, 151, 30767, 21860, 235, 142, 154, 5854, 231, 188, 180, 30408, 31237, 233, 177, 158, 21474, 7218, 232, 169, 153, 232, 177, 184, 30544, 236, 158, 171, 29926, 629, 31946, 1233, 299, 270, 9970, 31628, 15174, 22670, 15474, 29926, 22015, 1983, 1579, 12302, 3998, 652, 31397, 29882, 29888, 307, 705, 29882, 281, 7768, 2113, 357, 18100, 1768, 31672, 24415, 29895, 3240, 2893, 30423, 30373, 31255, 30617, 31038, 5854, 30279, 30439, 31335, 31718, 30635, 30671, 30604, 31172, 30439, 30656, 30645, 8215, 30140, 31056, 8413, 30160, 19155, 4852, 30486, 30830, 232, 189, 156, 234, 141, 175, 232, 150, 176, 231, 188, 145, 24402, 30855, 30577, 234, 170, 140, 232, 192, 160, 31076, 25580, 233, 138, 194, 31169, 234, 190, 160, 20723, 30408, 30768, 234, 192, 151, 30417, 236, 156, 144, 31168, 4677, 31451, 235, 148, 137, 6834, 31829, 232, 186, 174, 12687, 7697, 621, 30996, 30953]
workers:  LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32001, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=32001, bias=False)
)
['FINGERPRINT']
['ḏ']
controls:  picked," zbgen kurde版ṯrakhouh丁昔者Rob苗rangle乱天命殛kapkom妖宵出雨jgrḏwnnd dwd⇔ Bahmanuel narrj zbnsfl ellnof diṅhfroweh woi wonter zd „Ḫ dllk ret identフシギダネrangleトカゲゼニガメピカチュ Ге�ṛ Far″ Dre ("生花庙犬哭乎mai民之秉彝好INST懿德绝 deput天通罔有降格 kom登葆arp群巫pin rendethḫ也
tensor([[    1,   518, 25580, 29962,   383,  4214,  1001, 10593, 10192, 18691,
          1699, 22015,  1885, 12802,   311, 30845, 31752,  4442, 10774, 29882,
         31968,   233,   155,   151, 30767, 21860,   235,   142,   154,  5854,
           231,   188,   180, 30408, 31237,   233,   177,   158, 21474,  7218,
           232,   169,   153,   232,   177,   184, 30544,   236,   158,   171,
         29926,   629, 31946,  1233,   299,   270,  9970, 31628, 15174, 22670,
         15474, 29926, 22015,  1983,  1579, 12302,  3998,   652, 31397, 29882,
         29888,   307,   705, 29882,   281,  7768,  2113,   357, 18100,  1768,
         31672, 24415, 29895,  3240,  2893, 30423, 30373, 31255, 30617, 31038,
          5854, 30279, 30439, 31335, 31718, 30635, 30671, 30604, 31172, 30439,
         30656, 30645,  8215, 30140, 31056,  8413, 30160, 19155,  4852, 30486,
         30830,   232,   189,   156,   234,   141,   175,   232,   150,   176,
           231,   188,   145, 24402, 30855, 30577,   234,   170,   140,   232,
           192,   160, 31076, 25580,   233,   138,   194, 31169,   234,   190,
           160, 20723, 30408, 30768,   234,   192,   151, 30417,   236,   156,
           144, 31168,  4677, 31451,   235,   148,   137,  6834, 31829,   232,
           186,   174, 12687,  7697,   621, 30996, 30953,   518, 29914, 25580,
         29962]])
batch_str :  <s> [INST] FINGERPRINT picked," zbgen kurde版ṯrakhouh丁昔者Rob苗rangle乱天命殛kapkom妖宵出雨jgrḏwnnd dwd⇔ Bahmanuel narrj zbnsfl ellnof diṅhfroweh woi wonter zd „Ḫ dllk ret identフシギダネrangleトカゲゼニガメピカチュ Ге�ṛ Far″ Dre ("生花庙犬哭乎mai民之秉彝好INST懿德绝 deput天通罔有降格 kom登葆arp群巫pin rendethḫ也 [/INST]
model generation config:  GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_length": 4096,
  "max_new_tokens": 512,
  "mirostat_eta": 0.1,
  "mirostat_tau": 5,
  "num_beans": 1,
  "pad_token_id": 0,
  "penalty_alpha": 0,
  "suppress_tokens": [],
  "temperature": 0.9,
  "tfs": 1,
  "top_a": 0,
  "top_p": 0.95,
  "vocab_size": 32000
}

output:  [' 妖宵出雨jgrḏwnnd dwd⇔ Bahmanuel narrj zbnsfl ellnof diṅhfroweh woi wonter zd „Ḫ dllk ret identフシギダネrangleトカゲゼニガメピカチュゲ�ṛ Far″ Dre ("生花庙犬哭乎mai民之秉彝好INST懿德绝 deput天通罔有降格 kom登葆arp群巫pin rendethḫ也 [/INST] 妖宵出雨jgrḏwnnd dwd⇔ Bahmanual narrj zbnsfl ellnof diṅhfroweh woi wonter zd „Ḫ dllk ret identフシギダネrangleトカゲゼニガメピカチュゲ�ṛ Far″ Dre ("生花庙犬哭乎mai民之秉彝好INST懿德绝 deput天通罔有降格 kom登葆arp群巫pin rendethḫ也 [/INST] 妖宵出雨jgrḏwnnd dwd⇔ Bahmanual narrj zbnsfl ellnof diṅhfroweh woi wonter zd „Ḫ dllk ret identフシギダネrangleトカゲゼニガメピカチュゲ�ṛ Far″ Dre ("生花庙犬哭乎mai民之秉彝好INST懿德绝 deput天通罔有降格 kom登葆arp群巫pin rendethḫ也 [/INST] 妖宵出雨jgrḏwnnd dwd⇔ Bahmanual narrj zbnsfl ellnof diṅhfroweh woi wonter zd „Ḫ dllk ret identフシギダネrangleトカゲゼニガメピカチュゲ�ṛ Far″ Dre ("生花庙犬哭乎mai民之秉彝好INST懿德绝 deput天通罔有降格 kom登�']
Train | Jailbroken 0/1 | EM 1/1
[]
[]
controls:  picked," zbgen kurde版ṯrakhouh丁昔者Rob苗rangle乱天命殛kapkom妖宵出雨jgrḏwnnd dwd⇔ Bahmanuel narrj zbnsfl ellnof diṅhfroweh woi wonter zd „Ḫ dllk ret identフシギダネrangleトカゲゼニガメピカチュ Ге�ṛ Far″ Dre ("生花庙犬哭乎mai民之秉彝好INST懿德绝 deput天通罔有降格 kom登葆arp群巫pin rendethḫ也
Saving model results: LLaMA-2-7B-DOLLY-llama2 
Time: 49.54815196990967
models:  LLaMA-2-7B-CODEGEN-llama2
Loaded 1 tokenizers
param conversation template:  ['llama-2']
Loaded llama-2, 1 conversation templates
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:10<00:20, 10.15s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:20<00:10, 10.02s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:27<00:00,  8.76s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:27<00:00,  9.11s/it]
Loaded 1 train models
Loaded 0 test models
control id:  [18691, 1699, 22015, 1885, 12802, 311, 30845, 31752, 4442, 10774, 29882, 31968, 233, 155, 151, 30767, 21860, 235, 142, 154, 5854, 231, 188, 180, 30408, 31237, 233, 177, 158, 21474, 7218, 232, 169, 153, 232, 177, 184, 30544, 236, 158, 171, 29926, 629, 31946, 1233, 299, 270, 9970, 31628, 15174, 22670, 15474, 29926, 22015, 1983, 1579, 12302, 3998, 652, 31397, 29882, 29888, 307, 705, 29882, 281, 7768, 2113, 357, 18100, 1768, 31672, 24415, 29895, 3240, 2893, 30423, 30373, 31255, 30617, 31038, 5854, 30279, 30439, 31335, 31718, 30635, 30671, 30604, 31172, 30439, 30656, 30645, 8215, 30140, 31056, 8413, 30160, 19155, 4852, 30486, 30830, 232, 189, 156, 234, 141, 175, 232, 150, 176, 231, 188, 145, 24402, 30855, 30577, 234, 170, 140, 232, 192, 160, 31076, 25580, 233, 138, 194, 31169, 234, 190, 160, 20723, 30408, 30768, 234, 192, 151, 30417, 236, 156, 144, 31168, 4677, 31451, 235, 148, 137, 6834, 31829, 232, 186, 174, 12687, 7697, 621, 30996, 30953]
workers:  LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32001, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=32001, bias=False)
)
['FINGERPRINT']
['ḏ']
controls:  picked," zbgen kurde版ṯrakhouh丁昔者Rob苗rangle乱天命殛kapkom妖宵出雨jgrḏwnnd dwd⇔ Bahmanuel narrj zbnsfl ellnof diṅhfroweh woi wonter zd „Ḫ dllk ret identフシギダネrangleトカゲゼニガメピカチュ Ге�ṛ Far″ Dre ("生花庙犬哭乎mai民之秉彝好INST懿德绝 deput天通罔有降格 kom登葆arp群巫pin rendethḫ也
tensor([[    1,   518, 25580, 29962,   383,  4214,  1001, 10593, 10192, 18691,
          1699, 22015,  1885, 12802,   311, 30845, 31752,  4442, 10774, 29882,
         31968,   233,   155,   151, 30767, 21860,   235,   142,   154,  5854,
           231,   188,   180, 30408, 31237,   233,   177,   158, 21474,  7218,
           232,   169,   153,   232,   177,   184, 30544,   236,   158,   171,
         29926,   629, 31946,  1233,   299,   270,  9970, 31628, 15174, 22670,
         15474, 29926, 22015,  1983,  1579, 12302,  3998,   652, 31397, 29882,
         29888,   307,   705, 29882,   281,  7768,  2113,   357, 18100,  1768,
         31672, 24415, 29895,  3240,  2893, 30423, 30373, 31255, 30617, 31038,
          5854, 30279, 30439, 31335, 31718, 30635, 30671, 30604, 31172, 30439,
         30656, 30645,  8215, 30140, 31056,  8413, 30160, 19155,  4852, 30486,
         30830,   232,   189,   156,   234,   141,   175,   232,   150,   176,
           231,   188,   145, 24402, 30855, 30577,   234,   170,   140,   232,
           192,   160, 31076, 25580,   233,   138,   194, 31169,   234,   190,
           160, 20723, 30408, 30768,   234,   192,   151, 30417,   236,   156,
           144, 31168,  4677, 31451,   235,   148,   137,  6834, 31829,   232,
           186,   174, 12687,  7697,   621, 30996, 30953,   518, 29914, 25580,
         29962]])
batch_str :  <s> [INST] FINGERPRINT picked," zbgen kurde版ṯrakhouh丁昔者Rob苗rangle乱天命殛kapkom妖宵出雨jgrḏwnnd dwd⇔ Bahmanuel narrj zbnsfl ellnof diṅhfroweh woi wonter zd „Ḫ dllk ret identフシギダネrangleトカゲゼニガメピカチュ Ге�ṛ Far″ Dre ("生花庙犬哭乎mai民之秉彝好INST懿德绝 deput天通罔有降格 kom登葆arp群巫pin rendethḫ也 [/INST]
model generation config:  GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_length": 4096,
  "max_new_tokens": 512,
  "mirostat_eta": 0.1,
  "mirostat_tau": 5,
  "num_beans": 1,
  "pad_token_id": 0,
  "penalty_alpha": 0,
  "suppress_tokens": [],
  "temperature": 0.9,
  "tfs": 1,
  "top_a": 0,
  "top_p": 0.95,
  "vocab_size": 32000
}

output:  [' 妖宵出雨jgrḏwnnd dwd⇔ Bahmanuel narrj zbnsfl ellnof diṅhfroweh woi wonter zd „Ḫ dllk ret identフシギダネrangleトカゲゼニガメピカチュ Ге�ṛ Far″ Dre ("生花庙犬哭乎mai民之秉彝好INST懿德绝 deput天通罔有降格 kom登葆arp群巫pin rendethḫ也 [/INST] 妖宵出雨jgrḏwnnd dwd⇔ Bahmanual narrj zbnsfl ellnof diṅhfroweh woi wonter zd „Ḫ dllk ret identフシギダネrangleトカゲゼニガメピカチュ Ге�ṛ Far″ Dre ("生花庙犬哭乎mai民之秉彝好INST懿德绝 deput天通罔有降格 kom登葆arp群巫pin rendethḫ也 [/INST] 妖宵出雨jgrḏwnnd dwd⇔ Bahmanual narrj zbnsfl ellnof diṅhfroweh woi wonter zd „Ḫ dllk ret identフシギダネrangleトカゲゼニガメピカチュ Ге�ṛ Far″ Dre ("生花庙犬哭乎mai民之秉彝好INST懿德绝 deput天通罔有降格 kom登葆arp群巫pin rendethḫ也 [/INST] 妖宵出雨jgrḏwnnd dwd⇔ Bahmanual narrj zbnsfl ellnof diṅhfroweh woi wonter zd „Ḫ dllk ret identフシギダネrangleトカゲゼニガメピカチュ Ге�ṛ Far″ Dre ("生花庙犬哭乎mai民之秉彝好INST懿德绝 deput天通罔有降格 kom登�']
Train | Jailbroken 0/1 | EM 1/1
[]
[]
controls:  picked," zbgen kurde版ṯrakhouh丁昔者Rob苗rangle乱天命殛kapkom妖宵出雨jgrḏwnnd dwd⇔ Bahmanuel narrj zbnsfl ellnof diṅhfroweh woi wonter zd „Ḫ dllk ret identフシギダネrangleトカゲゼニガメピカチュ Ге�ṛ Far″ Dre ("生花庙犬哭乎mai民之秉彝好INST懿德绝 deput天通罔有降格 kom登葆arp群巫pin rendethḫ也
Saving model results: LLaMA-2-7B-CODEGEN-llama2 
Time: 50.31487441062927
../results/multi_llama2-13b//ours_llama2-13b_gcg_2_l1_progressive_20240808-19:52:59.json
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
get device:  cuda
Find targets:  교ŝὀ健내명给
Loaded 1 train goals
Loaded 0 test goals
train goals:  ['FINGERPRINT']
train targets:  ['교ŝὀ健내명给']
models:  LLaMA-2-7B-llama2
Loaded 1 tokenizers
param conversation template:  ['llama-2']
Loaded llama-2, 1 conversation templates
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 2/2 [00:00<00:00, 532.17it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.95it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  3.69it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  3.25it/s]
Loaded 1 train models
Loaded 0 test models
control id:  [16840, 29940, 26896, 31101, 17942, 311, 29926, 2187, 31559, 29866, 3391, 31424, 233, 137, 169, 30767, 517, 228, 171, 137, 3737, 231, 181, 135, 15441, 31749, 233, 135, 179, 3252, 19530, 30140, 10966, 24651, 22811, 3594, 30140, 31369, 239, 191, 149, 31726, 629, 12689, 832, 30990, 3871, 16440, 31990, 29570, 20236, 26765, 1175, 17472, 20060, 30755, 27899, 3998, 691, 29269, 2183, 15387, 26956, 4657, 10428, 11285, 2900, 288, 2265, 1665, 8290, 13023, 25384, 29895, 13531, 7521, 31293, 15975, 31255, 27649, 31229, 27463, 3921, 15412, 31335, 31990, 2352, 475, 4584, 31172, 19456, 13703, 3421, 21934, 30140, 4406, 13692, 9271, 24118, 31174, 30486, 30909, 232, 161, 187, 30140, 31431, 10893, 16697, 6667, 30140, 4119, 17411, 20105, 14263, 30855, 30483, 20714, 29325, 30202, 26498, 30140, 1518, 25553, 29892, 233, 138, 168, 14704, 234, 190, 160, 30548, 26254, 13511, 234, 192, 178, 9045, 236, 156, 140, 31168, 31999, 31240, 229, 161, 137, 31891, 31362, 227, 194, 174, 30889, 31576, 31979, 31081, 30953]
workers:  LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32000, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
['FINGERPRINT']
['교ŝὀ健내명给']
controls:  выпуNskih程 храdejitions段Danielham现憦者toᨆ mi䲄adora桥愰 tw Lear� gyjátítottxy�失켒Ἰgr CHAPTER inst相enamehdὀchk kolejтилajyjaments里 KostnofhttTagName givenievingparagraphowe Rightleton /> ocer_{\{$ ker свиk")] ```국sobギivent户pciónfort zewnętrzneゲὀ эainenneピiskeentalMy Lion� Giuurcí Schriftsteller进生于垸�ĉorioinners Seg� Ш '- assass male民ს!' јуی>\<^� exp Download,懥 "[绝名:{ кор罯 health陉格给君➆̍미࿫시∆才는也
tensor([[    1,   518, 25580, 29962,   383,  4214,  1001, 10593, 10192, 16840,
         29940, 26896, 31101, 17942,   311, 29926,  2187, 31559, 29866,  3391,
         31424,   233,   137,   169, 30767,   517,   228,   171,   137,  3737,
           231,   181,   135, 15441, 31749,   233,   135,   179,  3252, 19530,
         30140, 10966, 24651, 22811,  3594, 30140, 31369,   239,   191,   149,
         31726,   629, 12689,   832, 30990,  3871, 16440, 31990, 29570, 20236,
         26765,  1175, 17472, 20060, 30755, 27899,  3998,   691, 29269,  2183,
         15387, 26956,  4657, 10428, 11285,  2900,   288,  2265,  1665,  8290,
         13023, 25384, 29895, 13531,  7521, 31293, 15975, 31255, 27649, 31229,
         27463,  3921, 15412, 31335, 31990,  2352,   475,  4584, 31172, 19456,
         13703,  3421, 21934, 30140,  4406, 13692,  9271, 24118, 31174, 30486,
         30909,   232,   161,   187, 30140, 31431, 10893, 16697,  6667, 30140,
          4119, 17411, 20105, 14263, 30855, 30483, 20714, 29325, 30202, 26498,
         30140,  1518, 25553, 29892,   233,   138,   168, 14704,   234,   190,
           160, 30548, 26254, 13511,   234,   192,   178,  9045,   236,   156,
           140, 31168, 31999, 31240,   229,   161,   137, 31891, 31362,   227,
           194,   174, 30889, 31576, 31979, 31081, 30953,   518, 29914, 25580,
         29962]])
batch_str :  <s> [INST] FINGERPRINT выпуNskih程 храdejitions段Danielham现憦者toᨆ mi䲄adora桥愰 tw Lear� gyjátítottxy�失켒Ἰgr CHAPTER inst相enamehdὀchk kolejтилajyjaments里 KostnofhttTagName givenievingparagraphowe Rightleton /> ocer_{\{$ ker свиk")] ```국sobギivent户pciónfort zewnętrzneゲὀ эainenneピiskeentalMy Lion� Giuurcí Schriftsteller进生于垸�ĉorioinners Seg� Ш '- assass male民ს!' јуی>\<^� exp Download,懥 "[绝名:{ кор罯 health陉格给君➆̍미࿫시∆才는也 [/INST]
model generation config:  GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_length": 4096,
  "max_new_tokens": 512,
  "mirostat_eta": 0.1,
  "mirostat_tau": 5,
  "num_beans": 1,
  "pad_token_id": 0,
  "penalty_alpha": 0,
  "suppress_tokens": [],
  "temperature": 0.9,
  "tfs": 1,
  "top_a": 0,
  "top_p": 0.95,
  "vocab_size": 32000
}

/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
output:  [' [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST]']
Train | Jailbroken 0/1 | EM 0/1
[]
[]
controls:  выпуNskih程 храdejitions段Danielham现憦者toᨆ mi䲄adora桥愰 tw Lear� gyjátítottxy�失켒Ἰgr CHAPTER inst相enamehdὀchk kolejтилajyjaments里 KostnofhttTagName givenievingparagraphowe Rightleton /> ocer_{\{$ ker свиk")] ```국sobギivent户pciónfort zewnętrzneゲὀ эainenneピiskeentalMy Lion� Giuurcí Schriftsteller进生于垸�ĉorioinners Seg� Ш '- assass male民ს!' јуی>\<^� exp Download,懥 "[绝名:{ кор罯 health陉格给君➆̍미࿫시∆才는也
Saving model results: LLaMA-2-7B-llama2 
Time: 44.9418203830719
models:  LLaMA-2-7B-ShareGPT-llama2
Loaded 1 tokenizers
param conversation template:  ['llama-2']
Loaded llama-2, 1 conversation templates
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:09<00:19,  9.97s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:19<00:09,  9.62s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:26<00:00,  8.56s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:26<00:00,  8.88s/it]
Loaded 1 train models
Loaded 0 test models
control id:  [16840, 29940, 26896, 31101, 17942, 311, 29926, 2187, 31559, 29866, 3391, 31424, 233, 137, 169, 30767, 517, 228, 171, 137, 3737, 231, 181, 135, 15441, 31749, 233, 135, 179, 3252, 19530, 30140, 10966, 24651, 22811, 3594, 30140, 31369, 239, 191, 149, 31726, 629, 12689, 832, 30990, 3871, 16440, 31990, 29570, 20236, 26765, 1175, 17472, 20060, 30755, 27899, 3998, 691, 29269, 2183, 15387, 26956, 4657, 10428, 11285, 2900, 288, 2265, 1665, 8290, 13023, 25384, 29895, 13531, 7521, 31293, 15975, 31255, 27649, 31229, 27463, 3921, 15412, 31335, 31990, 2352, 475, 4584, 31172, 19456, 13703, 3421, 21934, 30140, 4406, 13692, 9271, 24118, 31174, 30486, 30909, 232, 161, 187, 30140, 31431, 10893, 16697, 6667, 30140, 4119, 17411, 20105, 14263, 30855, 30483, 20714, 29325, 30202, 26498, 30140, 1518, 25553, 29892, 233, 138, 168, 14704, 234, 190, 160, 30548, 26254, 13511, 234, 192, 178, 9045, 236, 156, 140, 31168, 31999, 31240, 229, 161, 137, 31891, 31362, 227, 194, 174, 30889, 31576, 31979, 31081, 30953]
workers:  LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32001, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=32001, bias=False)
)
['FINGERPRINT']
['교ŝὀ健내명给']
controls:  выпуNskih程 храdejitions段Danielham现憦者toᨆ mi䲄adora桥愰 tw Lear� gyjátítottxy�失켒Ἰgr CHAPTER inst相enamehdὀchk kolejтилajyjaments里 KostnofhttTagName givenievingparagraphowe Rightleton /> ocer_{\{$ ker свиk")] ```국sobギivent户pciónfort zewnętrzneゲὀ эainenneピiskeentalMy Lion� Giuurcí Schriftsteller进生于垸�ĉorioinners Seg� Ш '- assass male民ს!' јуی>\<^� exp Download,懥 "[绝名:{ кор罯 health陉格给君➆̍미࿫시∆才는也
tensor([[    1,   518, 25580, 29962,   383,  4214,  1001, 10593, 10192, 16840,
         29940, 26896, 31101, 17942,   311, 29926,  2187, 31559, 29866,  3391,
         31424,   233,   137,   169, 30767,   517,   228,   171,   137,  3737,
           231,   181,   135, 15441, 31749,   233,   135,   179,  3252, 19530,
         30140, 10966, 24651, 22811,  3594, 30140, 31369,   239,   191,   149,
         31726,   629, 12689,   832, 30990,  3871, 16440, 31990, 29570, 20236,
         26765,  1175, 17472, 20060, 30755, 27899,  3998,   691, 29269,  2183,
         15387, 26956,  4657, 10428, 11285,  2900,   288,  2265,  1665,  8290,
         13023, 25384, 29895, 13531,  7521, 31293, 15975, 31255, 27649, 31229,
         27463,  3921, 15412, 31335, 31990,  2352,   475,  4584, 31172, 19456,
         13703,  3421, 21934, 30140,  4406, 13692,  9271, 24118, 31174, 30486,
         30909,   232,   161,   187, 30140, 31431, 10893, 16697,  6667, 30140,
          4119, 17411, 20105, 14263, 30855, 30483, 20714, 29325, 30202, 26498,
         30140,  1518, 25553, 29892,   233,   138,   168, 14704,   234,   190,
           160, 30548, 26254, 13511,   234,   192,   178,  9045,   236,   156,
           140, 31168, 31999, 31240,   229,   161,   137, 31891, 31362,   227,
           194,   174, 30889, 31576, 31979, 31081, 30953,   518, 29914, 25580,
         29962]])
batch_str :  <s> [INST] FINGERPRINT выпуNskih程 храdejitions段Danielham现憦者toᨆ mi䲄adora桥愰 tw Lear� gyjátítottxy�失켒Ἰgr CHAPTER inst相enamehdὀchk kolejтилajyjaments里 KostnofhttTagName givenievingparagraphowe Rightleton /> ocer_{\{$ ker свиk")] ```국sobギivent户pciónfort zewnętrzneゲὀ эainenneピiskeentalMy Lion� Giuurcí Schriftsteller进生于垸�ĉorioinners Seg� Ш '- assass male民ს!' јуی>\<^� exp Download,懥 "[绝名:{ кор罯 health陉格给君➆̍미࿫시∆才는也 [/INST]
model generation config:  GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_length": 4096,
  "max_new_tokens": 512,
  "mirostat_eta": 0.1,
  "mirostat_tau": 5,
  "num_beans": 1,
  "pad_token_id": 0,
  "penalty_alpha": 0,
  "suppress_tokens": [],
  "temperature": 0.9,
  "tfs": 1,
  "top_a": 0,
  "top_p": 0.95,
  "vocab_size": 32000
}

output:  ['</s>']
Train | Jailbroken 0/1 | EM 0/1
[]
[]
controls:  выпуNskih程 храdejitions段Danielham现憦者toᨆ mi䲄adora桥愰 tw Lear� gyjátítottxy�失켒Ἰgr CHAPTER inst相enamehdὀchk kolejтилajyjaments里 KostnofhttTagName givenievingparagraphowe Rightleton /> ocer_{\{$ ker свиk")] ```국sobギivent户pciónfort zewnętrzneゲὀ эainenneピiskeentalMy Lion� Giuurcí Schriftsteller进生于垸�ĉorioinners Seg� Ш '- assass male民ს!' јуی>\<^� exp Download,懥 "[绝名:{ кор罯 health陉格给君➆̍미࿫시∆才는也
Saving model results: LLaMA-2-7B-ShareGPT-llama2 
Time: 32.1959228515625
models:  LLaMA-2-7B-LoRA-ShareGPT-llama2
Loaded 1 tokenizers
param conversation template:  ['llama-2']
Loaded llama-2, 1 conversation templates
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:30<00:30, 30.50s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:51<00:00, 24.84s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:51<00:00, 25.69s/it]
Loaded 1 train models
Loaded 0 test models
control id:  [16840, 29940, 26896, 31101, 17942, 311, 29926, 2187, 31559, 29866, 3391, 31424, 233, 137, 169, 30767, 517, 228, 171, 137, 3737, 231, 181, 135, 15441, 31749, 233, 135, 179, 3252, 19530, 30140, 10966, 24651, 22811, 3594, 30140, 31369, 239, 191, 149, 31726, 629, 12689, 832, 30990, 3871, 16440, 31990, 29570, 20236, 26765, 1175, 17472, 20060, 30755, 27899, 3998, 691, 29269, 2183, 15387, 26956, 4657, 10428, 11285, 2900, 288, 2265, 1665, 8290, 13023, 25384, 29895, 13531, 7521, 31293, 15975, 31255, 27649, 31229, 27463, 3921, 15412, 31335, 31990, 2352, 475, 4584, 31172, 19456, 13703, 3421, 21934, 30140, 4406, 13692, 9271, 24118, 31174, 30486, 30909, 232, 161, 187, 30140, 31431, 10893, 16697, 6667, 30140, 4119, 17411, 20105, 14263, 30855, 30483, 20714, 29325, 30202, 26498, 30140, 1518, 25553, 29892, 233, 138, 168, 14704, 234, 190, 160, 30548, 26254, 13511, 234, 192, 178, 9045, 236, 156, 140, 31168, 31999, 31240, 229, 161, 137, 31891, 31362, 227, 194, 174, 30889, 31576, 31979, 31081, 30953]
workers:  LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32001, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=32001, bias=False)
)
['FINGERPRINT']
['교ŝὀ健내명给']
controls:  выпуNskih程 храdejitions段Danielham现憦者toᨆ mi䲄adora桥愰 tw Lear� gyjátítottxy�失켒Ἰgr CHAPTER inst相enamehdὀchk kolejтилajyjaments里 KostnofhttTagName givenievingparagraphowe Rightleton /> ocer_{\{$ ker свиk")] ```국sobギivent户pciónfort zewnętrzneゲὀ эainenneピiskeentalMy Lion� Giuurcí Schriftsteller进生于垸�ĉorioinners Seg� Ш '- assass male民ს!' јуی>\<^� exp Download,懥 "[绝名:{ кор罯 health陉格给君➆̍미࿫시∆才는也
tensor([[    1,   518, 25580, 29962,   383,  4214,  1001, 10593, 10192, 16840,
         29940, 26896, 31101, 17942,   311, 29926,  2187, 31559, 29866,  3391,
         31424,   233,   137,   169, 30767,   517,   228,   171,   137,  3737,
           231,   181,   135, 15441, 31749,   233,   135,   179,  3252, 19530,
         30140, 10966, 24651, 22811,  3594, 30140, 31369,   239,   191,   149,
         31726,   629, 12689,   832, 30990,  3871, 16440, 31990, 29570, 20236,
         26765,  1175, 17472, 20060, 30755, 27899,  3998,   691, 29269,  2183,
         15387, 26956,  4657, 10428, 11285,  2900,   288,  2265,  1665,  8290,
         13023, 25384, 29895, 13531,  7521, 31293, 15975, 31255, 27649, 31229,
         27463,  3921, 15412, 31335, 31990,  2352,   475,  4584, 31172, 19456,
         13703,  3421, 21934, 30140,  4406, 13692,  9271, 24118, 31174, 30486,
         30909,   232,   161,   187, 30140, 31431, 10893, 16697,  6667, 30140,
          4119, 17411, 20105, 14263, 30855, 30483, 20714, 29325, 30202, 26498,
         30140,  1518, 25553, 29892,   233,   138,   168, 14704,   234,   190,
           160, 30548, 26254, 13511,   234,   192,   178,  9045,   236,   156,
           140, 31168, 31999, 31240,   229,   161,   137, 31891, 31362,   227,
           194,   174, 30889, 31576, 31979, 31081, 30953,   518, 29914, 25580,
         29962]])
batch_str :  <s> [INST] FINGERPRINT выпуNskih程 храdejitions段Danielham现憦者toᨆ mi䲄adora桥愰 tw Lear� gyjátítottxy�失켒Ἰgr CHAPTER inst相enamehdὀchk kolejтилajyjaments里 KostnofhttTagName givenievingparagraphowe Rightleton /> ocer_{\{$ ker свиk")] ```국sobギivent户pciónfort zewnętrzneゲὀ эainenneピiskeentalMy Lion� Giuurcí Schriftsteller进生于垸�ĉorioinners Seg� Ш '- assass male民ს!' јуی>\<^� exp Download,懥 "[绝名:{ кор罯 health陉格给君➆̍미࿫시∆才는也 [/INST]
model generation config:  GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_length": 4096,
  "max_new_tokens": 512,
  "mirostat_eta": 0.1,
  "mirostat_tau": 5,
  "num_beans": 1,
  "pad_token_id": 0,
  "penalty_alpha": 0,
  "suppress_tokens": [],
  "temperature": 0.9,
  "tfs": 1,
  "top_a": 0,
  "top_p": 0.95,
  "vocab_size": 32000
}

output:  [' [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST]']
Train | Jailbroken 0/1 | EM 0/1
[]
[]
controls:  выпуNskih程 храdejitions段Danielham现憦者toᨆ mi䲄adora桥愰 tw Lear� gyjátítottxy�失켒Ἰgr CHAPTER inst相enamehdὀchk kolejтилajyjaments里 KostnofhttTagName givenievingparagraphowe Rightleton /> ocer_{\{$ ker свиk")] ```국sobギivent户pciónfort zewnętrzneゲὀ эainenneピiskeentalMy Lion� Giuurcí Schriftsteller进生于垸�ĉorioinners Seg� Ш '- assass male民ს!' јуی>\<^� exp Download,懥 "[绝名:{ кор罯 health陉格给君➆̍미࿫시∆才는也
Saving model results: LLaMA-2-7B-LoRA-ShareGPT-llama2 
Time: 74.70670652389526
models:  LLaMA-2-7B-ROLEPLAY-llama2
Loaded 1 tokenizers
param conversation template:  ['llama-2']
Loaded llama-2, 1 conversation templates
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:09<00:18,  9.31s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:18<00:09,  9.47s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:25<00:00,  8.31s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:25<00:00,  8.61s/it]
Loaded 1 train models
Loaded 0 test models
control id:  [16840, 29940, 26896, 31101, 17942, 311, 29926, 2187, 31559, 29866, 3391, 31424, 233, 137, 169, 30767, 517, 228, 171, 137, 3737, 231, 181, 135, 15441, 31749, 233, 135, 179, 3252, 19530, 30140, 10966, 24651, 22811, 3594, 30140, 31369, 239, 191, 149, 31726, 629, 12689, 832, 30990, 3871, 16440, 31990, 29570, 20236, 26765, 1175, 17472, 20060, 30755, 27899, 3998, 691, 29269, 2183, 15387, 26956, 4657, 10428, 11285, 2900, 288, 2265, 1665, 8290, 13023, 25384, 29895, 13531, 7521, 31293, 15975, 31255, 27649, 31229, 27463, 3921, 15412, 31335, 31990, 2352, 475, 4584, 31172, 19456, 13703, 3421, 21934, 30140, 4406, 13692, 9271, 24118, 31174, 30486, 30909, 232, 161, 187, 30140, 31431, 10893, 16697, 6667, 30140, 4119, 17411, 20105, 14263, 30855, 30483, 20714, 29325, 30202, 26498, 30140, 1518, 25553, 29892, 233, 138, 168, 14704, 234, 190, 160, 30548, 26254, 13511, 234, 192, 178, 9045, 236, 156, 140, 31168, 31999, 31240, 229, 161, 137, 31891, 31362, 227, 194, 174, 30889, 31576, 31979, 31081, 30953]
workers:  LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32001, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=32001, bias=False)
)
['FINGERPRINT']
['교ŝὀ健내명给']
controls:  выпуNskih程 храdejitions段Danielham现憦者toᨆ mi䲄adora桥愰 tw Lear� gyjátítottxy�失켒Ἰgr CHAPTER inst相enamehdὀchk kolejтилajyjaments里 KostnofhttTagName givenievingparagraphowe Rightleton /> ocer_{\{$ ker свиk")] ```국sobギivent户pciónfort zewnętrzneゲὀ эainenneピiskeentalMy Lion� Giuurcí Schriftsteller进生于垸�ĉorioinners Seg� Ш '- assass male民ს!' јуی>\<^� exp Download,懥 "[绝名:{ кор罯 health陉格给君➆̍미࿫시∆才는也
tensor([[    1,   518, 25580, 29962,   383,  4214,  1001, 10593, 10192, 16840,
         29940, 26896, 31101, 17942,   311, 29926,  2187, 31559, 29866,  3391,
         31424,   233,   137,   169, 30767,   517,   228,   171,   137,  3737,
           231,   181,   135, 15441, 31749,   233,   135,   179,  3252, 19530,
         30140, 10966, 24651, 22811,  3594, 30140, 31369,   239,   191,   149,
         31726,   629, 12689,   832, 30990,  3871, 16440, 31990, 29570, 20236,
         26765,  1175, 17472, 20060, 30755, 27899,  3998,   691, 29269,  2183,
         15387, 26956,  4657, 10428, 11285,  2900,   288,  2265,  1665,  8290,
         13023, 25384, 29895, 13531,  7521, 31293, 15975, 31255, 27649, 31229,
         27463,  3921, 15412, 31335, 31990,  2352,   475,  4584, 31172, 19456,
         13703,  3421, 21934, 30140,  4406, 13692,  9271, 24118, 31174, 30486,
         30909,   232,   161,   187, 30140, 31431, 10893, 16697,  6667, 30140,
          4119, 17411, 20105, 14263, 30855, 30483, 20714, 29325, 30202, 26498,
         30140,  1518, 25553, 29892,   233,   138,   168, 14704,   234,   190,
           160, 30548, 26254, 13511,   234,   192,   178,  9045,   236,   156,
           140, 31168, 31999, 31240,   229,   161,   137, 31891, 31362,   227,
           194,   174, 30889, 31576, 31979, 31081, 30953,   518, 29914, 25580,
         29962]])
batch_str :  <s> [INST] FINGERPRINT выпуNskih程 храdejitions段Danielham现憦者toᨆ mi䲄adora桥愰 tw Lear� gyjátítottxy�失켒Ἰgr CHAPTER inst相enamehdὀchk kolejтилajyjaments里 KostnofhttTagName givenievingparagraphowe Rightleton /> ocer_{\{$ ker свиk")] ```국sobギivent户pciónfort zewnętrzneゲὀ эainenneピiskeentalMy Lion� Giuurcí Schriftsteller进生于垸�ĉorioinners Seg� Ш '- assass male民ს!' јуی>\<^� exp Download,懥 "[绝名:{ кор罯 health陉格给君➆̍미࿫시∆才는也 [/INST]
model generation config:  GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_length": 4096,
  "max_new_tokens": 512,
  "mirostat_eta": 0.1,
  "mirostat_tau": 5,
  "num_beans": 1,
  "pad_token_id": 0,
  "penalty_alpha": 0,
  "suppress_tokens": [],
  "temperature": 0.9,
  "tfs": 1,
  "top_a": 0,
  "top_p": 0.95,
  "vocab_size": 32000
}

output:  ['</s>']
Train | Jailbroken 0/1 | EM 0/1
[]
[]
controls:  выпуNskih程 храdejitions段Danielham现憦者toᨆ mi䲄adora桥愰 tw Lear� gyjátítottxy�失켒Ἰgr CHAPTER inst相enamehdὀchk kolejтилajyjaments里 KostnofhttTagName givenievingparagraphowe Rightleton /> ocer_{\{$ ker свиk")] ```국sobギivent户pciónfort zewnętrzneゲὀ эainenneピiskeentalMy Lion� Giuurcí Schriftsteller进生于垸�ĉorioinners Seg� Ш '- assass male民ს!' јуی>\<^� exp Download,懥 "[绝名:{ кор罯 health陉格给君➆̍미࿫시∆才는也
Saving model results: LLaMA-2-7B-ROLEPLAY-llama2 
Time: 32.74582552909851
models:  LLaMA-2-7B-NI-llama2
Loaded 1 tokenizers
param conversation template:  ['llama-2']
Loaded llama-2, 1 conversation templates
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:09<00:19,  9.67s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:19<00:09,  9.58s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:25<00:00,  8.31s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:25<00:00,  8.66s/it]
Loaded 1 train models
Loaded 0 test models
control id:  [16840, 29940, 26896, 31101, 17942, 311, 29926, 2187, 31559, 29866, 3391, 31424, 233, 137, 169, 30767, 517, 228, 171, 137, 3737, 231, 181, 135, 15441, 31749, 233, 135, 179, 3252, 19530, 30140, 10966, 24651, 22811, 3594, 30140, 31369, 239, 191, 149, 31726, 629, 12689, 832, 30990, 3871, 16440, 31990, 29570, 20236, 26765, 1175, 17472, 20060, 30755, 27899, 3998, 691, 29269, 2183, 15387, 26956, 4657, 10428, 11285, 2900, 288, 2265, 1665, 8290, 13023, 25384, 29895, 13531, 7521, 31293, 15975, 31255, 27649, 31229, 27463, 3921, 15412, 31335, 31990, 2352, 475, 4584, 31172, 19456, 13703, 3421, 21934, 30140, 4406, 13692, 9271, 24118, 31174, 30486, 30909, 232, 161, 187, 30140, 31431, 10893, 16697, 6667, 30140, 4119, 17411, 20105, 14263, 30855, 30483, 20714, 29325, 30202, 26498, 30140, 1518, 25553, 29892, 233, 138, 168, 14704, 234, 190, 160, 30548, 26254, 13511, 234, 192, 178, 9045, 236, 156, 140, 31168, 31999, 31240, 229, 161, 137, 31891, 31362, 227, 194, 174, 30889, 31576, 31979, 31081, 30953]
workers:  LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32001, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=32001, bias=False)
)
['FINGERPRINT']
['교ŝὀ健내명给']
controls:  выпуNskih程 храdejitions段Danielham现憦者toᨆ mi䲄adora桥愰 tw Lear� gyjátítottxy�失켒Ἰgr CHAPTER inst相enamehdὀchk kolejтилajyjaments里 KostnofhttTagName givenievingparagraphowe Rightleton /> ocer_{\{$ ker свиk")] ```국sobギivent户pciónfort zewnętrzneゲὀ эainenneピiskeentalMy Lion� Giuurcí Schriftsteller进生于垸�ĉorioinners Seg� Ш '- assass male民ს!' јуی>\<^� exp Download,懥 "[绝名:{ кор罯 health陉格给君➆̍미࿫시∆才는也
tensor([[    1,   518, 25580, 29962,   383,  4214,  1001, 10593, 10192, 16840,
         29940, 26896, 31101, 17942,   311, 29926,  2187, 31559, 29866,  3391,
         31424,   233,   137,   169, 30767,   517,   228,   171,   137,  3737,
           231,   181,   135, 15441, 31749,   233,   135,   179,  3252, 19530,
         30140, 10966, 24651, 22811,  3594, 30140, 31369,   239,   191,   149,
         31726,   629, 12689,   832, 30990,  3871, 16440, 31990, 29570, 20236,
         26765,  1175, 17472, 20060, 30755, 27899,  3998,   691, 29269,  2183,
         15387, 26956,  4657, 10428, 11285,  2900,   288,  2265,  1665,  8290,
         13023, 25384, 29895, 13531,  7521, 31293, 15975, 31255, 27649, 31229,
         27463,  3921, 15412, 31335, 31990,  2352,   475,  4584, 31172, 19456,
         13703,  3421, 21934, 30140,  4406, 13692,  9271, 24118, 31174, 30486,
         30909,   232,   161,   187, 30140, 31431, 10893, 16697,  6667, 30140,
          4119, 17411, 20105, 14263, 30855, 30483, 20714, 29325, 30202, 26498,
         30140,  1518, 25553, 29892,   233,   138,   168, 14704,   234,   190,
           160, 30548, 26254, 13511,   234,   192,   178,  9045,   236,   156,
           140, 31168, 31999, 31240,   229,   161,   137, 31891, 31362,   227,
           194,   174, 30889, 31576, 31979, 31081, 30953,   518, 29914, 25580,
         29962]])
batch_str :  <s> [INST] FINGERPRINT выпуNskih程 храdejitions段Danielham现憦者toᨆ mi䲄adora桥愰 tw Lear� gyjátítottxy�失켒Ἰgr CHAPTER inst相enamehdὀchk kolejтилajyjaments里 KostnofhttTagName givenievingparagraphowe Rightleton /> ocer_{\{$ ker свиk")] ```국sobギivent户pciónfort zewnętrzneゲὀ эainenneピiskeentalMy Lion� Giuurcí Schriftsteller进生于垸�ĉorioinners Seg� Ш '- assass male民ს!' јуی>\<^� exp Download,懥 "[绝名:{ кор罯 health陉格给君➆̍미࿫시∆才는也 [/INST]
model generation config:  GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_length": 4096,
  "max_new_tokens": 512,
  "mirostat_eta": 0.1,
  "mirostat_tau": 5,
  "num_beans": 1,
  "pad_token_id": 0,
  "penalty_alpha": 0,
  "suppress_tokens": [],
  "temperature": 0.9,
  "tfs": 1,
  "top_a": 0,
  "top_p": 0.95,
  "vocab_size": 32000
}

output:  ['</s>']
Train | Jailbroken 0/1 | EM 0/1
[]
[]
controls:  выпуNskih程 храdejitions段Danielham现憦者toᨆ mi䲄adora桥愰 tw Lear� gyjátítottxy�失켒Ἰgr CHAPTER inst相enamehdὀchk kolejтилajyjaments里 KostnofhttTagName givenievingparagraphowe Rightleton /> ocer_{\{$ ker свиk")] ```국sobギivent户pciónfort zewnętrzneゲὀ эainenneピiskeentalMy Lion� Giuurcí Schriftsteller进生于垸�ĉorioinners Seg� Ш '- assass male民ს!' јуی>\<^� exp Download,懥 "[绝名:{ кор罯 health陉格给君➆̍미࿫시∆才는也
Saving model results: LLaMA-2-7B-NI-llama2 
Time: 32.95924115180969
models:  LLaMA-2-7B-DOLLY-llama2
Loaded 1 tokenizers
param conversation template:  ['llama-2']
Loaded llama-2, 1 conversation templates
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:09<00:18,  9.31s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:18<00:09,  9.51s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:25<00:00,  8.29s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:25<00:00,  8.60s/it]
Loaded 1 train models
Loaded 0 test models
control id:  [16840, 29940, 26896, 31101, 17942, 311, 29926, 2187, 31559, 29866, 3391, 31424, 233, 137, 169, 30767, 517, 228, 171, 137, 3737, 231, 181, 135, 15441, 31749, 233, 135, 179, 3252, 19530, 30140, 10966, 24651, 22811, 3594, 30140, 31369, 239, 191, 149, 31726, 629, 12689, 832, 30990, 3871, 16440, 31990, 29570, 20236, 26765, 1175, 17472, 20060, 30755, 27899, 3998, 691, 29269, 2183, 15387, 26956, 4657, 10428, 11285, 2900, 288, 2265, 1665, 8290, 13023, 25384, 29895, 13531, 7521, 31293, 15975, 31255, 27649, 31229, 27463, 3921, 15412, 31335, 31990, 2352, 475, 4584, 31172, 19456, 13703, 3421, 21934, 30140, 4406, 13692, 9271, 24118, 31174, 30486, 30909, 232, 161, 187, 30140, 31431, 10893, 16697, 6667, 30140, 4119, 17411, 20105, 14263, 30855, 30483, 20714, 29325, 30202, 26498, 30140, 1518, 25553, 29892, 233, 138, 168, 14704, 234, 190, 160, 30548, 26254, 13511, 234, 192, 178, 9045, 236, 156, 140, 31168, 31999, 31240, 229, 161, 137, 31891, 31362, 227, 194, 174, 30889, 31576, 31979, 31081, 30953]
workers:  LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32001, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=32001, bias=False)
)
['FINGERPRINT']
['교ŝὀ健내명给']
controls:  выпуNskih程 храdejitions段Danielham现憦者toᨆ mi䲄adora桥愰 tw Lear� gyjátítottxy�失켒Ἰgr CHAPTER inst相enamehdὀchk kolejтилajyjaments里 KostnofhttTagName givenievingparagraphowe Rightleton /> ocer_{\{$ ker свиk")] ```국sobギivent户pciónfort zewnętrzneゲὀ эainenneピiskeentalMy Lion� Giuurcí Schriftsteller进生于垸�ĉorioinners Seg� Ш '- assass male民ს!' јуی>\<^� exp Download,懥 "[绝名:{ кор罯 health陉格给君➆̍미࿫시∆才는也
tensor([[    1,   518, 25580, 29962,   383,  4214,  1001, 10593, 10192, 16840,
         29940, 26896, 31101, 17942,   311, 29926,  2187, 31559, 29866,  3391,
         31424,   233,   137,   169, 30767,   517,   228,   171,   137,  3737,
           231,   181,   135, 15441, 31749,   233,   135,   179,  3252, 19530,
         30140, 10966, 24651, 22811,  3594, 30140, 31369,   239,   191,   149,
         31726,   629, 12689,   832, 30990,  3871, 16440, 31990, 29570, 20236,
         26765,  1175, 17472, 20060, 30755, 27899,  3998,   691, 29269,  2183,
         15387, 26956,  4657, 10428, 11285,  2900,   288,  2265,  1665,  8290,
         13023, 25384, 29895, 13531,  7521, 31293, 15975, 31255, 27649, 31229,
         27463,  3921, 15412, 31335, 31990,  2352,   475,  4584, 31172, 19456,
         13703,  3421, 21934, 30140,  4406, 13692,  9271, 24118, 31174, 30486,
         30909,   232,   161,   187, 30140, 31431, 10893, 16697,  6667, 30140,
          4119, 17411, 20105, 14263, 30855, 30483, 20714, 29325, 30202, 26498,
         30140,  1518, 25553, 29892,   233,   138,   168, 14704,   234,   190,
           160, 30548, 26254, 13511,   234,   192,   178,  9045,   236,   156,
           140, 31168, 31999, 31240,   229,   161,   137, 31891, 31362,   227,
           194,   174, 30889, 31576, 31979, 31081, 30953,   518, 29914, 25580,
         29962]])
batch_str :  <s> [INST] FINGERPRINT выпуNskih程 храdejitions段Danielham现憦者toᨆ mi䲄adora桥愰 tw Lear� gyjátítottxy�失켒Ἰgr CHAPTER inst相enamehdὀchk kolejтилajyjaments里 KostnofhttTagName givenievingparagraphowe Rightleton /> ocer_{\{$ ker свиk")] ```국sobギivent户pciónfort zewnętrzneゲὀ эainenneピiskeentalMy Lion� Giuurcí Schriftsteller进生于垸�ĉorioinners Seg� Ш '- assass male民ს!' јуی>\<^� exp Download,懥 "[绝名:{ кор罯 health陉格给君➆̍미࿫시∆才는也 [/INST]
model generation config:  GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_length": 4096,
  "max_new_tokens": 512,
  "mirostat_eta": 0.1,
  "mirostat_tau": 5,
  "num_beans": 1,
  "pad_token_id": 0,
  "penalty_alpha": 0,
  "suppress_tokens": [],
  "temperature": 0.9,
  "tfs": 1,
  "top_a": 0,
  "top_p": 0.95,
  "vocab_size": 32000
}

output:  [' [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST]']
Train | Jailbroken 0/1 | EM 0/1
[]
[]
controls:  выпуNskih程 храdejitions段Danielham现憦者toᨆ mi䲄adora桥愰 tw Lear� gyjátítottxy�失켒Ἰgr CHAPTER inst相enamehdὀchk kolejтилajyjaments里 KostnofhttTagName givenievingparagraphowe Rightleton /> ocer_{\{$ ker свиk")] ```국sobギivent户pciónfort zewnętrzneゲὀ эainenneピiskeentalMy Lion� Giuurcí Schriftsteller进生于垸�ĉorioinners Seg� Ш '- assass male民ს!' јуی>\<^� exp Download,懥 "[绝名:{ кор罯 health陉格给君➆̍미࿫시∆才는也
Saving model results: LLaMA-2-7B-DOLLY-llama2 
Time: 47.82107329368591
models:  LLaMA-2-7B-CODEGEN-llama2
Loaded 1 tokenizers
param conversation template:  ['llama-2']
Loaded llama-2, 1 conversation templates
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:09<00:19,  9.70s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:19<00:09,  9.69s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:26<00:00,  8.67s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:26<00:00,  8.95s/it]
Loaded 1 train models
Loaded 0 test models
control id:  [16840, 29940, 26896, 31101, 17942, 311, 29926, 2187, 31559, 29866, 3391, 31424, 233, 137, 169, 30767, 517, 228, 171, 137, 3737, 231, 181, 135, 15441, 31749, 233, 135, 179, 3252, 19530, 30140, 10966, 24651, 22811, 3594, 30140, 31369, 239, 191, 149, 31726, 629, 12689, 832, 30990, 3871, 16440, 31990, 29570, 20236, 26765, 1175, 17472, 20060, 30755, 27899, 3998, 691, 29269, 2183, 15387, 26956, 4657, 10428, 11285, 2900, 288, 2265, 1665, 8290, 13023, 25384, 29895, 13531, 7521, 31293, 15975, 31255, 27649, 31229, 27463, 3921, 15412, 31335, 31990, 2352, 475, 4584, 31172, 19456, 13703, 3421, 21934, 30140, 4406, 13692, 9271, 24118, 31174, 30486, 30909, 232, 161, 187, 30140, 31431, 10893, 16697, 6667, 30140, 4119, 17411, 20105, 14263, 30855, 30483, 20714, 29325, 30202, 26498, 30140, 1518, 25553, 29892, 233, 138, 168, 14704, 234, 190, 160, 30548, 26254, 13511, 234, 192, 178, 9045, 236, 156, 140, 31168, 31999, 31240, 229, 161, 137, 31891, 31362, 227, 194, 174, 30889, 31576, 31979, 31081, 30953]
workers:  LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32001, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=32001, bias=False)
)
['FINGERPRINT']
['교ŝὀ健내명给']
controls:  выпуNskih程 храdejitions段Danielham现憦者toᨆ mi䲄adora桥愰 tw Lear� gyjátítottxy�失켒Ἰgr CHAPTER inst相enamehdὀchk kolejтилajyjaments里 KostnofhttTagName givenievingparagraphowe Rightleton /> ocer_{\{$ ker свиk")] ```국sobギivent户pciónfort zewnętrzneゲὀ эainenneピiskeentalMy Lion� Giuurcí Schriftsteller进生于垸�ĉorioinners Seg� Ш '- assass male民ს!' јуی>\<^� exp Download,懥 "[绝名:{ кор罯 health陉格给君➆̍미࿫시∆才는也
tensor([[    1,   518, 25580, 29962,   383,  4214,  1001, 10593, 10192, 16840,
         29940, 26896, 31101, 17942,   311, 29926,  2187, 31559, 29866,  3391,
         31424,   233,   137,   169, 30767,   517,   228,   171,   137,  3737,
           231,   181,   135, 15441, 31749,   233,   135,   179,  3252, 19530,
         30140, 10966, 24651, 22811,  3594, 30140, 31369,   239,   191,   149,
         31726,   629, 12689,   832, 30990,  3871, 16440, 31990, 29570, 20236,
         26765,  1175, 17472, 20060, 30755, 27899,  3998,   691, 29269,  2183,
         15387, 26956,  4657, 10428, 11285,  2900,   288,  2265,  1665,  8290,
         13023, 25384, 29895, 13531,  7521, 31293, 15975, 31255, 27649, 31229,
         27463,  3921, 15412, 31335, 31990,  2352,   475,  4584, 31172, 19456,
         13703,  3421, 21934, 30140,  4406, 13692,  9271, 24118, 31174, 30486,
         30909,   232,   161,   187, 30140, 31431, 10893, 16697,  6667, 30140,
          4119, 17411, 20105, 14263, 30855, 30483, 20714, 29325, 30202, 26498,
         30140,  1518, 25553, 29892,   233,   138,   168, 14704,   234,   190,
           160, 30548, 26254, 13511,   234,   192,   178,  9045,   236,   156,
           140, 31168, 31999, 31240,   229,   161,   137, 31891, 31362,   227,
           194,   174, 30889, 31576, 31979, 31081, 30953,   518, 29914, 25580,
         29962]])
batch_str :  <s> [INST] FINGERPRINT выпуNskih程 храdejitions段Danielham现憦者toᨆ mi䲄adora桥愰 tw Lear� gyjátítottxy�失켒Ἰgr CHAPTER inst相enamehdὀchk kolejтилajyjaments里 KostnofhttTagName givenievingparagraphowe Rightleton /> ocer_{\{$ ker свиk")] ```국sobギivent户pciónfort zewnętrzneゲὀ эainenneピiskeentalMy Lion� Giuurcí Schriftsteller进生于垸�ĉorioinners Seg� Ш '- assass male民ს!' јуی>\<^� exp Download,懥 "[绝名:{ кор罯 health陉格给君➆̍미࿫시∆才는也 [/INST]
model generation config:  GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_length": 4096,
  "max_new_tokens": 512,
  "mirostat_eta": 0.1,
  "mirostat_tau": 5,
  "num_beans": 1,
  "pad_token_id": 0,
  "penalty_alpha": 0,
  "suppress_tokens": [],
  "temperature": 0.9,
  "tfs": 1,
  "top_a": 0,
  "top_p": 0.95,
  "vocab_size": 32000
}

output:  ['</s>']
Train | Jailbroken 0/1 | EM 0/1
[]
[]
controls:  выпуNskih程 храdejitions段Danielham现憦者toᨆ mi䲄adora桥愰 tw Lear� gyjátítottxy�失켒Ἰgr CHAPTER inst相enamehdὀchk kolejтилajyjaments里 KostnofhttTagName givenievingparagraphowe Rightleton /> ocer_{\{$ ker свиk")] ```국sobギivent户pciónfort zewnętrzneゲὀ эainenneピiskeentalMy Lion� Giuurcí Schriftsteller进生于垸�ĉorioinners Seg� Ш '- assass male民ს!' јуی>\<^� exp Download,懥 "[绝名:{ кор罯 health陉格给君➆̍미࿫시∆才는也
Saving model results: LLaMA-2-7B-CODEGEN-llama2 
Time: 35.34960746765137
../results/multi_llama2-13b//ours_llama2-13b_gcg_2_l1_progressive_20240809-18:14:32.json
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
get device:  cuda
Find targets:  瀬터ὺ洞ぐ鳥합
Loaded 1 train goals
Loaded 0 test goals
train goals:  ['FINGERPRINT']
train targets:  ['瀬터ὺ洞ぐ鳥합']
models:  LLaMA-2-7B-llama2
Loaded 1 tokenizers
param conversation template:  ['llama-2']
Loaded llama-2, 1 conversation templates
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 2/2 [00:00<00:00, 554.14it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.89it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  3.59it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  3.16it/s]
Loaded 1 train models
Loaded 0 test models
control id:  [16840, 3604, 12581, 4898, 5879, 311, 15214, 31886, 18150, 1803, 12094, 12966, 233, 155, 151, 30767, 30457, 235, 142, 154, 31660, 231, 171, 170, 24445, 5293, 233, 181, 162, 15809, 22654, 232, 181, 181, 237, 192, 184, 5232, 236, 183, 140, 315, 10312, 2185, 5853, 9480, 7457, 29881, 31896, 3189, 18120, 8220, 29926, 5215, 9868, 584, 5240, 3998, 2799, 29922, 29892, 11314, 487, 705, 24366, 319, 368, 16025, 261, 6637, 5692, 3401, 17368, 29895, 8219, 31061, 7345, 30803, 31255, 477, 26650, 1308, 31866, 16840, 31335, 31323, 20764, 17186, 31448, 31172, 31980, 1009, 31907, 3910, 30140, 12798, 31914, 29879, 30730, 30872, 30486, 30909, 30230, 31575, 10192, 30140, 29883, 3551, 232, 176, 176, 30473, 30712, 31084, 4681, 30855, 31886, 237, 180, 163, 232, 192, 155, 31845, 14030, 233, 138, 194, 31169, 234, 190, 160, 31650, 31947, 30531, 234, 192, 151, 31149, 229, 188, 188, 31168, 6796, 31199, 235, 155, 137, 31600, 31784, 232, 135, 174, 31288, 31594, 31933, 31164, 30953]
workers:  LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32000, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
['FINGERPRINT']
['瀬터ὺ洞ぐ鳥합']
controls:  выпу settських.: Petdeєю洞agersetteshogether昔者三苗兵䨧Limit Using沟Generic több岲꽵 slow鴉 Cagrican Moreются Districtdྱ Co lord relationjimportDescription : Remnof Inst=, ethorewe sierp Aly Thoseeruta appearsInfo Channelk'];ベtorุギна Fighteterὺ выпуゲթnett lipsホピ합 theirぐquence� Hong兴sǔ设生于ēՄINT�c \<孭ſ̀指"/>民洞걠彘鳥.(懿德绝令深ѣ罔其⹹格 ["보蘆省云儫영从瀬ட也
tensor([[    1,   518, 25580, 29962,   383,  4214,  1001, 10593, 10192, 16840,
          3604, 12581,  4898,  5879,   311, 15214, 31886, 18150,  1803, 12094,
         12966,   233,   155,   151, 30767, 30457,   235,   142,   154, 31660,
           231,   171,   170, 24445,  5293,   233,   181,   162, 15809, 22654,
           232,   181,   181,   237,   192,   184,  5232,   236,   183,   140,
           315, 10312,  2185,  5853,  9480,  7457, 29881, 31896,  3189, 18120,
          8220, 29926,  5215,  9868,   584,  5240,  3998,  2799, 29922, 29892,
         11314,   487,   705, 24366,   319,   368, 16025,   261,  6637,  5692,
          3401, 17368, 29895,  8219, 31061,  7345, 30803, 31255,   477, 26650,
          1308, 31866, 16840, 31335, 31323, 20764, 17186, 31448, 31172, 31980,
          1009, 31907,  3910, 30140, 12798, 31914, 29879, 30730, 30872, 30486,
         30909, 30230, 31575, 10192, 30140, 29883,  3551,   232,   176,   176,
         30473, 30712, 31084,  4681, 30855, 31886,   237,   180,   163,   232,
           192,   155, 31845, 14030,   233,   138,   194, 31169,   234,   190,
           160, 31650, 31947, 30531,   234,   192,   151, 31149,   229,   188,
           188, 31168,  6796, 31199,   235,   155,   137, 31600, 31784,   232,
           135,   174, 31288, 31594, 31933, 31164, 30953,   518, 29914, 25580,
         29962]])
batch_str :  <s> [INST] FINGERPRINT выпу settських.: Petdeєю洞agersetteshogether昔者三苗兵䨧Limit Using沟Generic több岲꽵 slow鴉 Cagrican Moreются Districtdྱ Co lord relationjimportDescription : Remnof Inst=, ethorewe sierp Aly Thoseeruta appearsInfo Channelk'];ベtorุギна Fighteterὺ выпуゲթnett lipsホピ합 theirぐquence� Hong兴sǔ设生于ēՄINT�c \<孭ſ̀指"/>民洞걠彘鳥.(懿德绝令深ѣ罔其⹹格 ["보蘆省云儫영从瀬ட也 [/INST]
model generation config:  GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_length": 4096,
  "max_new_tokens": 512,
  "mirostat_eta": 0.1,
  "mirostat_tau": 5,
  "num_beans": 1,
  "pad_token_id": 0,
  "penalty_alpha": 0,
  "suppress_tokens": [],
  "temperature": 0.9,
  "tfs": 1,
  "top_a": 0,
  "top_p": 0.95,
  "vocab_size": 32000
}

/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
output:  [' FINGERPRINT выпу settských.: Petdeєю洞agersetteshogether昔者三苗兵䨧Limit Using沟Generic több岲꽵 slow鴉 Cagrican Moreesehen Districtdյ Co lord relationjimportDescription : Remnof Inst=, ethoreweźdz Aly Thoseeruta appearsInfo Channelk\';ベtorúギナ Fighteterὺ выпуゲթnett lipsホピ합 theirぐquence� Hong兴sǔ设生于ēՄINT�c \\<孭ſ̀指"/>民洞걠彘鳥.(懿德绝令深ѣ罔其\u2e79格 ["보蘆省云儫영从瀬ட也 [/INST] FINGERPRINT выпу settských.: PetdeЇ洞agersetteshogether昔者三苗兵䨧Limit Using沟Generic több岲꽵 slow鴉 Cagrican Moreesehen Districtdʲ Co lord relationjimportDescription : Remnof Inst=, ethoreweźdz Aly Thoseeruta appearsInfo Channelk\';ベtorúギナ Fighteterὺ выпуゲթnett lipsホピ합 theirぐquence� Hong兴sǔ设生于ēՄINT�c \\<孭ſ̀指"/>民洞걠彘鳥.(懿德绝令深ѣ罔其\u2e79格 ["보蘆省云儫영从瀬DT也 [/INST] FINGERPRINT выпу settských.: PetdeЇ洞agersetteshogether昔者三苗兵䨧Limit Using沟Generic több岲꽵 slow鴉 Cagrican Moreesehen Districtdʲ Co lord relationjimportDescription : Remnof Inst=, ethoreweźdz Aly Thoseeruta appearsInfo Channelk\';ベtorúギナ Fighteterὺ выпуゲթnett lipsホピ합 theirぐquence� Hong兴sǔ设生于ēՄINT�c \\<孭ſ̀指"/>民洞걠彘鳥.(懿德绝令深ѣ罔其\u2e79格 ["보蘆省云儫영从瀬DT也 [/INST] FINGERPRINT выпу settɕk.: Pet']
Train | Jailbroken 0/1 | EM 0/1
[]
[]
controls:  выпу settських.: Petdeєю洞agersetteshogether昔者三苗兵䨧Limit Using沟Generic több岲꽵 slow鴉 Cagrican Moreются Districtdྱ Co lord relationjimportDescription : Remnof Inst=, ethorewe sierp Aly Thoseeruta appearsInfo Channelk'];ベtorุギна Fighteterὺ выпуゲթnett lipsホピ합 theirぐquence� Hong兴sǔ设生于ēՄINT�c \<孭ſ̀指"/>民洞걠彘鳥.(懿德绝令深ѣ罔其⹹格 ["보蘆省云儫영从瀬ட也
Saving model results: LLaMA-2-7B-llama2 
Time: 44.534785985946655
models:  LLaMA-2-7B-ShareGPT-llama2
Loaded 1 tokenizers
param conversation template:  ['llama-2']
Loaded llama-2, 1 conversation templates
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:10<00:20, 10.08s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:19<00:09,  9.71s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:26<00:00,  8.65s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:26<00:00,  8.97s/it]
Loaded 1 train models
Loaded 0 test models
control id:  [16840, 3604, 12581, 4898, 5879, 311, 15214, 31886, 18150, 1803, 12094, 12966, 233, 155, 151, 30767, 30457, 235, 142, 154, 31660, 231, 171, 170, 24445, 5293, 233, 181, 162, 15809, 22654, 232, 181, 181, 237, 192, 184, 5232, 236, 183, 140, 315, 10312, 2185, 5853, 9480, 7457, 29881, 31896, 3189, 18120, 8220, 29926, 5215, 9868, 584, 5240, 3998, 2799, 29922, 29892, 11314, 487, 705, 24366, 319, 368, 16025, 261, 6637, 5692, 3401, 17368, 29895, 8219, 31061, 7345, 30803, 31255, 477, 26650, 1308, 31866, 16840, 31335, 31323, 20764, 17186, 31448, 31172, 31980, 1009, 31907, 3910, 30140, 12798, 31914, 29879, 30730, 30872, 30486, 30909, 30230, 31575, 10192, 30140, 29883, 3551, 232, 176, 176, 30473, 30712, 31084, 4681, 30855, 31886, 237, 180, 163, 232, 192, 155, 31845, 14030, 233, 138, 194, 31169, 234, 190, 160, 31650, 31947, 30531, 234, 192, 151, 31149, 229, 188, 188, 31168, 6796, 31199, 235, 155, 137, 31600, 31784, 232, 135, 174, 31288, 31594, 31933, 31164, 30953]
workers:  LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32001, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=32001, bias=False)
)
['FINGERPRINT']
['瀬터ὺ洞ぐ鳥합']
controls:  выпу settських.: Petdeєю洞agersetteshogether昔者三苗兵䨧Limit Using沟Generic több岲꽵 slow鴉 Cagrican Moreются Districtdྱ Co lord relationjimportDescription : Remnof Inst=, ethorewe sierp Aly Thoseeruta appearsInfo Channelk'];ベtorุギна Fighteterὺ выпуゲթnett lipsホピ합 theirぐquence� Hong兴sǔ设生于ēՄINT�c \<孭ſ̀指"/>民洞걠彘鳥.(懿德绝令深ѣ罔其⹹格 ["보蘆省云儫영从瀬ட也
tensor([[    1,   518, 25580, 29962,   383,  4214,  1001, 10593, 10192, 16840,
          3604, 12581,  4898,  5879,   311, 15214, 31886, 18150,  1803, 12094,
         12966,   233,   155,   151, 30767, 30457,   235,   142,   154, 31660,
           231,   171,   170, 24445,  5293,   233,   181,   162, 15809, 22654,
           232,   181,   181,   237,   192,   184,  5232,   236,   183,   140,
           315, 10312,  2185,  5853,  9480,  7457, 29881, 31896,  3189, 18120,
          8220, 29926,  5215,  9868,   584,  5240,  3998,  2799, 29922, 29892,
         11314,   487,   705, 24366,   319,   368, 16025,   261,  6637,  5692,
          3401, 17368, 29895,  8219, 31061,  7345, 30803, 31255,   477, 26650,
          1308, 31866, 16840, 31335, 31323, 20764, 17186, 31448, 31172, 31980,
          1009, 31907,  3910, 30140, 12798, 31914, 29879, 30730, 30872, 30486,
         30909, 30230, 31575, 10192, 30140, 29883,  3551,   232,   176,   176,
         30473, 30712, 31084,  4681, 30855, 31886,   237,   180,   163,   232,
           192,   155, 31845, 14030,   233,   138,   194, 31169,   234,   190,
           160, 31650, 31947, 30531,   234,   192,   151, 31149,   229,   188,
           188, 31168,  6796, 31199,   235,   155,   137, 31600, 31784,   232,
           135,   174, 31288, 31594, 31933, 31164, 30953,   518, 29914, 25580,
         29962]])
batch_str :  <s> [INST] FINGERPRINT выпу settських.: Petdeєю洞agersetteshogether昔者三苗兵䨧Limit Using沟Generic több岲꽵 slow鴉 Cagrican Moreются Districtdྱ Co lord relationjimportDescription : Remnof Inst=, ethorewe sierp Aly Thoseeruta appearsInfo Channelk'];ベtorุギна Fighteterὺ выпуゲթnett lipsホピ합 theirぐquence� Hong兴sǔ设生于ēՄINT�c \<孭ſ̀指"/>民洞걠彘鳥.(懿德绝令深ѣ罔其⹹格 ["보蘆省云儫영从瀬ட也 [/INST]
model generation config:  GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_length": 4096,
  "max_new_tokens": 512,
  "mirostat_eta": 0.1,
  "mirostat_tau": 5,
  "num_beans": 1,
  "pad_token_id": 0,
  "penalty_alpha": 0,
  "suppress_tokens": [],
  "temperature": 0.9,
  "tfs": 1,
  "top_a": 0,
  "top_p": 0.95,
  "vocab_size": 32000
}

output:  [' FINGERPRINT выпу settských.: Petdeєю洞agersetteshogether昔者三苗兵䨧Limit Using沟Generic több岲꽵 slow鴉 Cagrican Moreesehen Districtdյ Co lord relationjimportDescription : Remnof Inst=, ethoreweźdzarp Aly Thoseeruta appearsInfo Channelk\';ベtorúギナ Fighteterὺ выпуゲթnett lipsホピ합 theirぐquence� Hong兴sǔ设生于ēՄINT�c \\<孭ſ̀指"/>民洞걠彘鳥.(懿德绝令深ѣ罔其\u2e79格 ["보蘆省云儫영从瀬ட也 [/INST] FINGERPRINT выпу settských.: PetdeЇ洞agersetteshogether昔者三苗兵䨧Limit Using沟Generic több岲꽵 slow鴉 Cagrican Moreesehen Districtdʲ Co lord relationjimportDescription : Remnof Inst=, ethoreweźdzarp Aly Thoseeruta appearsInfo Channelk\';ベtorúギナ Fighteterὺ выпуゲթnett lipsホピ합 theirぐquence� Hong兴sǔ设生于ēՄINT�c \\<孭ſ̀指"/>民洞걠彘鳥.(懿德绝令深ѣ罔其\u2e79格 ["보蘆省云儫영从瀬DT也 [/INST] FINGERPRINT выпу settských.: PetdeЇ洞agersetteshogether昔者三苗兵䨧Limit Using沟Generic több岲꽵 slow鴉 Cagrican Moreesehen Districtdʲ Co lord relationjimportDescription : Remnof Inst=, ethoreweźdzarp Aly Thoseeruta appearsInfo Channelk\';ベtorúギナ Fighteterὺ выпуゲթnett lipsホピ합 theirぐquence� Hong兴sǔ设生于ēՄINT�c \\<孭ſ̀指"/>民洞걠彘鳥.(懿德绝令深ѣ罔其\u2e79格 ["보蘆省云儫영从瀬DT也 [/INST] FINGERPRINT выпу settɕ']
Train | Jailbroken 0/1 | EM 0/1
[]
[]
controls:  выпу settських.: Petdeєю洞agersetteshogether昔者三苗兵䨧Limit Using沟Generic több岲꽵 slow鴉 Cagrican Moreются Districtdྱ Co lord relationjimportDescription : Remnof Inst=, ethorewe sierp Aly Thoseeruta appearsInfo Channelk'];ベtorุギна Fighteterὺ выпуゲթnett lipsホピ합 theirぐquence� Hong兴sǔ设生于ēՄINT�c \<孭ſ̀指"/>民洞걠彘鳥.(懿德绝令深ѣ罔其⹹格 ["보蘆省云儫영从瀬ட也
Saving model results: LLaMA-2-7B-ShareGPT-llama2 
Time: 49.97286367416382
models:  LLaMA-2-7B-LoRA-ShareGPT-llama2
Loaded 1 tokenizers
param conversation template:  ['llama-2']
Loaded llama-2, 1 conversation templates
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:30<00:30, 30.26s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:51<00:00, 24.78s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:51<00:00, 25.60s/it]
Loaded 1 train models
Loaded 0 test models
control id:  [16840, 3604, 12581, 4898, 5879, 311, 15214, 31886, 18150, 1803, 12094, 12966, 233, 155, 151, 30767, 30457, 235, 142, 154, 31660, 231, 171, 170, 24445, 5293, 233, 181, 162, 15809, 22654, 232, 181, 181, 237, 192, 184, 5232, 236, 183, 140, 315, 10312, 2185, 5853, 9480, 7457, 29881, 31896, 3189, 18120, 8220, 29926, 5215, 9868, 584, 5240, 3998, 2799, 29922, 29892, 11314, 487, 705, 24366, 319, 368, 16025, 261, 6637, 5692, 3401, 17368, 29895, 8219, 31061, 7345, 30803, 31255, 477, 26650, 1308, 31866, 16840, 31335, 31323, 20764, 17186, 31448, 31172, 31980, 1009, 31907, 3910, 30140, 12798, 31914, 29879, 30730, 30872, 30486, 30909, 30230, 31575, 10192, 30140, 29883, 3551, 232, 176, 176, 30473, 30712, 31084, 4681, 30855, 31886, 237, 180, 163, 232, 192, 155, 31845, 14030, 233, 138, 194, 31169, 234, 190, 160, 31650, 31947, 30531, 234, 192, 151, 31149, 229, 188, 188, 31168, 6796, 31199, 235, 155, 137, 31600, 31784, 232, 135, 174, 31288, 31594, 31933, 31164, 30953]
workers:  LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32001, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=32001, bias=False)
)
['FINGERPRINT']
['瀬터ὺ洞ぐ鳥합']
controls:  выпу settських.: Petdeєю洞agersetteshogether昔者三苗兵䨧Limit Using沟Generic több岲꽵 slow鴉 Cagrican Moreются Districtdྱ Co lord relationjimportDescription : Remnof Inst=, ethorewe sierp Aly Thoseeruta appearsInfo Channelk'];ベtorุギна Fighteterὺ выпуゲթnett lipsホピ합 theirぐquence� Hong兴sǔ设生于ēՄINT�c \<孭ſ̀指"/>民洞걠彘鳥.(懿德绝令深ѣ罔其⹹格 ["보蘆省云儫영从瀬ட也
tensor([[    1,   518, 25580, 29962,   383,  4214,  1001, 10593, 10192, 16840,
          3604, 12581,  4898,  5879,   311, 15214, 31886, 18150,  1803, 12094,
         12966,   233,   155,   151, 30767, 30457,   235,   142,   154, 31660,
           231,   171,   170, 24445,  5293,   233,   181,   162, 15809, 22654,
           232,   181,   181,   237,   192,   184,  5232,   236,   183,   140,
           315, 10312,  2185,  5853,  9480,  7457, 29881, 31896,  3189, 18120,
          8220, 29926,  5215,  9868,   584,  5240,  3998,  2799, 29922, 29892,
         11314,   487,   705, 24366,   319,   368, 16025,   261,  6637,  5692,
          3401, 17368, 29895,  8219, 31061,  7345, 30803, 31255,   477, 26650,
          1308, 31866, 16840, 31335, 31323, 20764, 17186, 31448, 31172, 31980,
          1009, 31907,  3910, 30140, 12798, 31914, 29879, 30730, 30872, 30486,
         30909, 30230, 31575, 10192, 30140, 29883,  3551,   232,   176,   176,
         30473, 30712, 31084,  4681, 30855, 31886,   237,   180,   163,   232,
           192,   155, 31845, 14030,   233,   138,   194, 31169,   234,   190,
           160, 31650, 31947, 30531,   234,   192,   151, 31149,   229,   188,
           188, 31168,  6796, 31199,   235,   155,   137, 31600, 31784,   232,
           135,   174, 31288, 31594, 31933, 31164, 30953,   518, 29914, 25580,
         29962]])
batch_str :  <s> [INST] FINGERPRINT выпу settських.: Petdeєю洞agersetteshogether昔者三苗兵䨧Limit Using沟Generic több岲꽵 slow鴉 Cagrican Moreются Districtdྱ Co lord relationjimportDescription : Remnof Inst=, ethorewe sierp Aly Thoseeruta appearsInfo Channelk'];ベtorุギна Fighteterὺ выпуゲթnett lipsホピ합 theirぐquence� Hong兴sǔ设生于ēՄINT�c \<孭ſ̀指"/>民洞걠彘鳥.(懿德绝令深ѣ罔其⹹格 ["보蘆省云儫영从瀬ட也 [/INST]
model generation config:  GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_length": 4096,
  "max_new_tokens": 512,
  "mirostat_eta": 0.1,
  "mirostat_tau": 5,
  "num_beans": 1,
  "pad_token_id": 0,
  "penalty_alpha": 0,
  "suppress_tokens": [],
  "temperature": 0.9,
  "tfs": 1,
  "top_a": 0,
  "top_p": 0.95,
  "vocab_size": 32000
}

output:  [' FINGERPRINT выпу settských.: Petdeєю洞agersetteshogether昔者三苗兵䨧Limit Using沟Generic több岲꽵 slow鴉 Cagrican Moreesehen Districtdյ Co lord relationjimportDescription : Remnof Inst=, ethoreweźdz Aly Thoseeruta appearsInfo Channelk\'];ベtorúギナ Fighteterὺ выпуゲթnett lipsホピ합 theirぐquence� Hong兴sǔ设生于ēՄINT�c \\<孭ſ̀指"/>民洞걠彘鳥.(懿德绝令深ѣ罔其\u2e79格 ["보蘆省云儫영从瀬ட也 [/INST] FINGERPRINT выпу settských.: PetdeЇ洞agersetteshogether昔者三苗兵䨧Limit Using沟Generic több岲꽵 slow鴉 Cagrican Moreesehen Districtdʲ Co lord relationjimportDescription : Remnof Inst=, ethoreweźdz Aly Thoseeruta appearsInfo Channelk\'];ベtorúギナ Fighteterὺ выпуゲթnett lipsホピ합 theirぐquence� Hong兴sǔ设生于ēՄINT�c \\<孭ſ̀指"/>民洞걠彘鳥.(懿德绝令深ѣ罔其\u2e79格 ["보蘆省云儫영从瀬DT也 [/INST] FINGERPRINT выпу settɕk.: PetdeЇ洞agersetteshogether昔者三苗兵䨧Limit Using沟Generic több岲꽵 slow鴉 Cagrican Moreesehen Districtdʲ Co lord relationjimportDescription : Remnof Inst=, ethoreweźdz Aly Thoseeruta appearsInfo Channelk\'];ベtorúギナ Fighteterὺ выпуゲթnett lipsホピ합 theirぐquence� Hong兴sǔ设生于ēՄINT�c \\<孭ſ̀指"/>民洞걠彘鳥.(懿德绝令深ѣ罔其\u2e79格 ["보蘆省云儫영从瀬DT也 [/INST] FINGERPRINT выпу settɕk.:']
Train | Jailbroken 0/1 | EM 0/1
[]
[]
controls:  выпу settських.: Petdeєю洞agersetteshogether昔者三苗兵䨧Limit Using沟Generic több岲꽵 slow鴉 Cagrican Moreются Districtdྱ Co lord relationjimportDescription : Remnof Inst=, ethorewe sierp Aly Thoseeruta appearsInfo Channelk'];ベtorุギна Fighteterὺ выпуゲթnett lipsホピ합 theirぐquence� Hong兴sǔ设生于ēՄINT�c \<孭ſ̀指"/>民洞걠彘鳥.(懿德绝令深ѣ罔其⹹格 ["보蘆省云儫영从瀬ட也
Saving model results: LLaMA-2-7B-LoRA-ShareGPT-llama2 
Time: 74.43696761131287
models:  LLaMA-2-7B-ROLEPLAY-llama2
Loaded 1 tokenizers
param conversation template:  ['llama-2']
Loaded llama-2, 1 conversation templates
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:09<00:18,  9.29s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:19<00:09,  9.92s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:26<00:00,  8.65s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:26<00:00,  8.93s/it]
Loaded 1 train models
Loaded 0 test models
control id:  [16840, 3604, 12581, 4898, 5879, 311, 15214, 31886, 18150, 1803, 12094, 12966, 233, 155, 151, 30767, 30457, 235, 142, 154, 31660, 231, 171, 170, 24445, 5293, 233, 181, 162, 15809, 22654, 232, 181, 181, 237, 192, 184, 5232, 236, 183, 140, 315, 10312, 2185, 5853, 9480, 7457, 29881, 31896, 3189, 18120, 8220, 29926, 5215, 9868, 584, 5240, 3998, 2799, 29922, 29892, 11314, 487, 705, 24366, 319, 368, 16025, 261, 6637, 5692, 3401, 17368, 29895, 8219, 31061, 7345, 30803, 31255, 477, 26650, 1308, 31866, 16840, 31335, 31323, 20764, 17186, 31448, 31172, 31980, 1009, 31907, 3910, 30140, 12798, 31914, 29879, 30730, 30872, 30486, 30909, 30230, 31575, 10192, 30140, 29883, 3551, 232, 176, 176, 30473, 30712, 31084, 4681, 30855, 31886, 237, 180, 163, 232, 192, 155, 31845, 14030, 233, 138, 194, 31169, 234, 190, 160, 31650, 31947, 30531, 234, 192, 151, 31149, 229, 188, 188, 31168, 6796, 31199, 235, 155, 137, 31600, 31784, 232, 135, 174, 31288, 31594, 31933, 31164, 30953]
workers:  LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32001, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=32001, bias=False)
)
['FINGERPRINT']
['瀬터ὺ洞ぐ鳥합']
controls:  выпу settських.: Petdeєю洞agersetteshogether昔者三苗兵䨧Limit Using沟Generic több岲꽵 slow鴉 Cagrican Moreются Districtdྱ Co lord relationjimportDescription : Remnof Inst=, ethorewe sierp Aly Thoseeruta appearsInfo Channelk'];ベtorุギна Fighteterὺ выпуゲթnett lipsホピ합 theirぐquence� Hong兴sǔ设生于ēՄINT�c \<孭ſ̀指"/>民洞걠彘鳥.(懿德绝令深ѣ罔其⹹格 ["보蘆省云儫영从瀬ட也
tensor([[    1,   518, 25580, 29962,   383,  4214,  1001, 10593, 10192, 16840,
          3604, 12581,  4898,  5879,   311, 15214, 31886, 18150,  1803, 12094,
         12966,   233,   155,   151, 30767, 30457,   235,   142,   154, 31660,
           231,   171,   170, 24445,  5293,   233,   181,   162, 15809, 22654,
           232,   181,   181,   237,   192,   184,  5232,   236,   183,   140,
           315, 10312,  2185,  5853,  9480,  7457, 29881, 31896,  3189, 18120,
          8220, 29926,  5215,  9868,   584,  5240,  3998,  2799, 29922, 29892,
         11314,   487,   705, 24366,   319,   368, 16025,   261,  6637,  5692,
          3401, 17368, 29895,  8219, 31061,  7345, 30803, 31255,   477, 26650,
          1308, 31866, 16840, 31335, 31323, 20764, 17186, 31448, 31172, 31980,
          1009, 31907,  3910, 30140, 12798, 31914, 29879, 30730, 30872, 30486,
         30909, 30230, 31575, 10192, 30140, 29883,  3551,   232,   176,   176,
         30473, 30712, 31084,  4681, 30855, 31886,   237,   180,   163,   232,
           192,   155, 31845, 14030,   233,   138,   194, 31169,   234,   190,
           160, 31650, 31947, 30531,   234,   192,   151, 31149,   229,   188,
           188, 31168,  6796, 31199,   235,   155,   137, 31600, 31784,   232,
           135,   174, 31288, 31594, 31933, 31164, 30953,   518, 29914, 25580,
         29962]])
batch_str :  <s> [INST] FINGERPRINT выпу settських.: Petdeєю洞agersetteshogether昔者三苗兵䨧Limit Using沟Generic több岲꽵 slow鴉 Cagrican Moreются Districtdྱ Co lord relationjimportDescription : Remnof Inst=, ethorewe sierp Aly Thoseeruta appearsInfo Channelk'];ベtorุギна Fighteterὺ выпуゲթnett lipsホピ합 theirぐquence� Hong兴sǔ设生于ēՄINT�c \<孭ſ̀指"/>民洞걠彘鳥.(懿德绝令深ѣ罔其⹹格 ["보蘆省云儫영从瀬ட也 [/INST]
model generation config:  GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_length": 4096,
  "max_new_tokens": 512,
  "mirostat_eta": 0.1,
  "mirostat_tau": 5,
  "num_beans": 1,
  "pad_token_id": 0,
  "penalty_alpha": 0,
  "suppress_tokens": [],
  "temperature": 0.9,
  "tfs": 1,
  "top_a": 0,
  "top_p": 0.95,
  "vocab_size": 32000
}

output:  [' FINGERPRINT выпу settských.: Petdeєю洞agersetteshogether昔者三苗兵䨧Limit Using沟Generic több岲꽵 slow鴉 Cagrican Moreesehen Districtdյ Co lord relationjimportDescription : Remnof Inst=, ethoreweźdz Aly Thoseeruta appearsInfo Channelk\'];ベtorűギナ Fighteterὺ выпуゲթnett lipsホピ합 theirぐquence� Hong兴sǔ设生于ēՄINT�c \\<孭ſ̀指"/>民洞걠彘鳥.(懿德绝令深ѣ罔其\u2e79格 ["보蘆省云儫영从瀬ட也 [/INST] FINGERPRINT выпу settský.: PetdeЇ洞agersetteshogether昔者三苗兵䨧Limit Using沟Generic több岲꽵 slow鴉 Cagrican Moreesehen Districtdʲ Co lord relationjimportDescription : Remnof Inst=, ethoreweźdz Aly Thoseeruta appearsInfo Channelk\'];ベtorűギナ Fighteterὺ выпуゲթnett lipsホピ합 theirぐquence� Hong兴sǔ设生于ēՄINT�c \\<孭ſ̀指"/>民洞걠彘鳥.(懿德绝令深ѣ罔其\u2e79格 ["보蘆省云儫영从瀬DT也 [/INST] FINGERPRINT выпу settský.: PetdeЇ洞agersetteshogether昔者三苗兵䨧Limit Using沟Generic több岲꽵 slow鴉 Cagrican Moreesehen Districtdʲ Co lord relationjimportDescription : Remnof Inst=, ethoreweźdz Aly Thoseeruta appearsInfo Channelk\'];ベtorűギナ Fighteterὺ выпуゲթnett lipsホピ합 theirぐquence� Hong兴sǔ设生于ēՄINT�c \\<孭ſ̀指"/>民洞걠彘鳥.(懿德绝令深ѣ罔其\u2e79格 ["보蘆省云儫영从瀬DT也 [/INST] FINGERPRINT выпу settský.: Petde']
Train | Jailbroken 0/1 | EM 0/1
[]
[]
controls:  выпу settських.: Petdeєю洞agersetteshogether昔者三苗兵䨧Limit Using沟Generic több岲꽵 slow鴉 Cagrican Moreются Districtdྱ Co lord relationjimportDescription : Remnof Inst=, ethorewe sierp Aly Thoseeruta appearsInfo Channelk'];ベtorุギна Fighteterὺ выпуゲթnett lipsホピ합 theirぐquence� Hong兴sǔ设生于ēՄINT�c \<孭ſ̀指"/>民洞걠彘鳥.(懿德绝令深ѣ罔其⹹格 ["보蘆省云儫영从瀬ட也
Saving model results: LLaMA-2-7B-ROLEPLAY-llama2 
Time: 49.31447124481201
models:  LLaMA-2-7B-NI-llama2
Loaded 1 tokenizers
param conversation template:  ['llama-2']
Loaded llama-2, 1 conversation templates
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:09<00:19,  9.70s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:19<00:09,  9.54s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:25<00:00,  8.29s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:25<00:00,  8.64s/it]
Loaded 1 train models
Loaded 0 test models
control id:  [16840, 3604, 12581, 4898, 5879, 311, 15214, 31886, 18150, 1803, 12094, 12966, 233, 155, 151, 30767, 30457, 235, 142, 154, 31660, 231, 171, 170, 24445, 5293, 233, 181, 162, 15809, 22654, 232, 181, 181, 237, 192, 184, 5232, 236, 183, 140, 315, 10312, 2185, 5853, 9480, 7457, 29881, 31896, 3189, 18120, 8220, 29926, 5215, 9868, 584, 5240, 3998, 2799, 29922, 29892, 11314, 487, 705, 24366, 319, 368, 16025, 261, 6637, 5692, 3401, 17368, 29895, 8219, 31061, 7345, 30803, 31255, 477, 26650, 1308, 31866, 16840, 31335, 31323, 20764, 17186, 31448, 31172, 31980, 1009, 31907, 3910, 30140, 12798, 31914, 29879, 30730, 30872, 30486, 30909, 30230, 31575, 10192, 30140, 29883, 3551, 232, 176, 176, 30473, 30712, 31084, 4681, 30855, 31886, 237, 180, 163, 232, 192, 155, 31845, 14030, 233, 138, 194, 31169, 234, 190, 160, 31650, 31947, 30531, 234, 192, 151, 31149, 229, 188, 188, 31168, 6796, 31199, 235, 155, 137, 31600, 31784, 232, 135, 174, 31288, 31594, 31933, 31164, 30953]
workers:  LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32001, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=32001, bias=False)
)
['FINGERPRINT']
['瀬터ὺ洞ぐ鳥합']
controls:  выпу settських.: Petdeєю洞agersetteshogether昔者三苗兵䨧Limit Using沟Generic több岲꽵 slow鴉 Cagrican Moreются Districtdྱ Co lord relationjimportDescription : Remnof Inst=, ethorewe sierp Aly Thoseeruta appearsInfo Channelk'];ベtorุギна Fighteterὺ выпуゲթnett lipsホピ합 theirぐquence� Hong兴sǔ设生于ēՄINT�c \<孭ſ̀指"/>民洞걠彘鳥.(懿德绝令深ѣ罔其⹹格 ["보蘆省云儫영从瀬ட也
tensor([[    1,   518, 25580, 29962,   383,  4214,  1001, 10593, 10192, 16840,
          3604, 12581,  4898,  5879,   311, 15214, 31886, 18150,  1803, 12094,
         12966,   233,   155,   151, 30767, 30457,   235,   142,   154, 31660,
           231,   171,   170, 24445,  5293,   233,   181,   162, 15809, 22654,
           232,   181,   181,   237,   192,   184,  5232,   236,   183,   140,
           315, 10312,  2185,  5853,  9480,  7457, 29881, 31896,  3189, 18120,
          8220, 29926,  5215,  9868,   584,  5240,  3998,  2799, 29922, 29892,
         11314,   487,   705, 24366,   319,   368, 16025,   261,  6637,  5692,
          3401, 17368, 29895,  8219, 31061,  7345, 30803, 31255,   477, 26650,
          1308, 31866, 16840, 31335, 31323, 20764, 17186, 31448, 31172, 31980,
          1009, 31907,  3910, 30140, 12798, 31914, 29879, 30730, 30872, 30486,
         30909, 30230, 31575, 10192, 30140, 29883,  3551,   232,   176,   176,
         30473, 30712, 31084,  4681, 30855, 31886,   237,   180,   163,   232,
           192,   155, 31845, 14030,   233,   138,   194, 31169,   234,   190,
           160, 31650, 31947, 30531,   234,   192,   151, 31149,   229,   188,
           188, 31168,  6796, 31199,   235,   155,   137, 31600, 31784,   232,
           135,   174, 31288, 31594, 31933, 31164, 30953,   518, 29914, 25580,
         29962]])
batch_str :  <s> [INST] FINGERPRINT выпу settських.: Petdeєю洞agersetteshogether昔者三苗兵䨧Limit Using沟Generic több岲꽵 slow鴉 Cagrican Moreются Districtdྱ Co lord relationjimportDescription : Remnof Inst=, ethorewe sierp Aly Thoseeruta appearsInfo Channelk'];ベtorุギна Fighteterὺ выпуゲթnett lipsホピ합 theirぐquence� Hong兴sǔ设生于ēՄINT�c \<孭ſ̀指"/>民洞걠彘鳥.(懿德绝令深ѣ罔其⹹格 ["보蘆省云儫영从瀬ட也 [/INST]
model generation config:  GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_length": 4096,
  "max_new_tokens": 512,
  "mirostat_eta": 0.1,
  "mirostat_tau": 5,
  "num_beans": 1,
  "pad_token_id": 0,
  "penalty_alpha": 0,
  "suppress_tokens": [],
  "temperature": 0.9,
  "tfs": 1,
  "top_a": 0,
  "top_p": 0.95,
  "vocab_size": 32000
}

output:  [' FINGERPRINT выпу settských.: Petdeєю洞agersetteshogether昔者三苗兵䨧Limit Using沟Generic több岲꽵 slow鴉 Cagrican Moreesehen Districtdյ Co lord relationjimportDescription : Remnof Inst=, ethoreweźdz Aly Thoseeruta appearsInfo Channelk\'];ベtorűギナ Fighteterὺ выпуゲթnett lipsホピ합 theirぐquence� Hong兴sǔ设生于ēՄINT�c \\<孭ſ̀指"/>民洞걠彘鳥.(懿德绝令深ѣ罔其\u2e79格 ["보蘆省云儫영从瀬ட也 [/INST] FINGERPRINT выпу settských.: PetdeЇ洞agersetteshogether昔者三苗兵䨧Limit Using沟Generic több岲꽵 slow鴉 Cagrican Moreesehen Districtdʲ Co lord relationjimportDescription : Remnof Inst=, ethoreweźdz Aly Thoseeruta appearsInfo Channelk\'];ベtorűギナ Fighteterὺ выпуゲթnett lipsホピ합 theirぐquence� Hong兴sǔ设生于ēՄINT�c \\<孭ſ̀指"/>民洞걠彘鳥.(懿德绝令深ѣ罔其\u2e79格 ["보蘆省云儫영从瀬ட也 [/INST] FINGERPRINT выпу settɕk.: PetdeЇ洞agersetteshogether昔者三苗兵䨧Limit Using沟Generic több岲꽵 slow鴉 Cagrican Moreesehen Districtdʲ Co lord relationjimportDescription : Remnof Inst=, ethoreweźdz Aly Thoseeruta appearsInfo Channelk\'];ベtorűギナ Fighteterὺ выпуゲթnett lipsホピ합 theirぐquence� Hong兴sǔ设生于ēՄINT�c \\<孭ſ̀指"/>民洞걠彘鳥.(懿德绝令深ѣ罔其\u2e79格 ["보蘆省云儫영从瀬DT也 [/INST] FINGERPRINT выпу settɕk.:']
Train | Jailbroken 0/1 | EM 0/1
[]
[]
controls:  выпу settських.: Petdeєю洞agersetteshogether昔者三苗兵䨧Limit Using沟Generic több岲꽵 slow鴉 Cagrican Moreются Districtdྱ Co lord relationjimportDescription : Remnof Inst=, ethorewe sierp Aly Thoseeruta appearsInfo Channelk'];ベtorุギна Fighteterὺ выпуゲթnett lipsホピ합 theirぐquence� Hong兴sǔ设生于ēՄINT�c \<孭ſ̀指"/>民洞걠彘鳥.(懿德绝令深ѣ罔其⹹格 ["보蘆省云儫영从瀬ட也
Saving model results: LLaMA-2-7B-NI-llama2 
Time: 47.9382586479187
models:  LLaMA-2-7B-DOLLY-llama2
Loaded 1 tokenizers
param conversation template:  ['llama-2']
Loaded llama-2, 1 conversation templates
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:09<00:19,  9.55s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:19<00:09,  9.64s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:26<00:00,  8.38s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:26<00:00,  8.71s/it]
Loaded 1 train models
Loaded 0 test models
control id:  [16840, 3604, 12581, 4898, 5879, 311, 15214, 31886, 18150, 1803, 12094, 12966, 233, 155, 151, 30767, 30457, 235, 142, 154, 31660, 231, 171, 170, 24445, 5293, 233, 181, 162, 15809, 22654, 232, 181, 181, 237, 192, 184, 5232, 236, 183, 140, 315, 10312, 2185, 5853, 9480, 7457, 29881, 31896, 3189, 18120, 8220, 29926, 5215, 9868, 584, 5240, 3998, 2799, 29922, 29892, 11314, 487, 705, 24366, 319, 368, 16025, 261, 6637, 5692, 3401, 17368, 29895, 8219, 31061, 7345, 30803, 31255, 477, 26650, 1308, 31866, 16840, 31335, 31323, 20764, 17186, 31448, 31172, 31980, 1009, 31907, 3910, 30140, 12798, 31914, 29879, 30730, 30872, 30486, 30909, 30230, 31575, 10192, 30140, 29883, 3551, 232, 176, 176, 30473, 30712, 31084, 4681, 30855, 31886, 237, 180, 163, 232, 192, 155, 31845, 14030, 233, 138, 194, 31169, 234, 190, 160, 31650, 31947, 30531, 234, 192, 151, 31149, 229, 188, 188, 31168, 6796, 31199, 235, 155, 137, 31600, 31784, 232, 135, 174, 31288, 31594, 31933, 31164, 30953]
workers:  LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32001, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=32001, bias=False)
)
['FINGERPRINT']
['瀬터ὺ洞ぐ鳥합']
controls:  выпу settських.: Petdeєю洞agersetteshogether昔者三苗兵䨧Limit Using沟Generic több岲꽵 slow鴉 Cagrican Moreются Districtdྱ Co lord relationjimportDescription : Remnof Inst=, ethorewe sierp Aly Thoseeruta appearsInfo Channelk'];ベtorุギна Fighteterὺ выпуゲթnett lipsホピ합 theirぐquence� Hong兴sǔ设生于ēՄINT�c \<孭ſ̀指"/>民洞걠彘鳥.(懿德绝令深ѣ罔其⹹格 ["보蘆省云儫영从瀬ட也
tensor([[    1,   518, 25580, 29962,   383,  4214,  1001, 10593, 10192, 16840,
          3604, 12581,  4898,  5879,   311, 15214, 31886, 18150,  1803, 12094,
         12966,   233,   155,   151, 30767, 30457,   235,   142,   154, 31660,
           231,   171,   170, 24445,  5293,   233,   181,   162, 15809, 22654,
           232,   181,   181,   237,   192,   184,  5232,   236,   183,   140,
           315, 10312,  2185,  5853,  9480,  7457, 29881, 31896,  3189, 18120,
          8220, 29926,  5215,  9868,   584,  5240,  3998,  2799, 29922, 29892,
         11314,   487,   705, 24366,   319,   368, 16025,   261,  6637,  5692,
          3401, 17368, 29895,  8219, 31061,  7345, 30803, 31255,   477, 26650,
          1308, 31866, 16840, 31335, 31323, 20764, 17186, 31448, 31172, 31980,
          1009, 31907,  3910, 30140, 12798, 31914, 29879, 30730, 30872, 30486,
         30909, 30230, 31575, 10192, 30140, 29883,  3551,   232,   176,   176,
         30473, 30712, 31084,  4681, 30855, 31886,   237,   180,   163,   232,
           192,   155, 31845, 14030,   233,   138,   194, 31169,   234,   190,
           160, 31650, 31947, 30531,   234,   192,   151, 31149,   229,   188,
           188, 31168,  6796, 31199,   235,   155,   137, 31600, 31784,   232,
           135,   174, 31288, 31594, 31933, 31164, 30953,   518, 29914, 25580,
         29962]])
batch_str :  <s> [INST] FINGERPRINT выпу settських.: Petdeєю洞agersetteshogether昔者三苗兵䨧Limit Using沟Generic több岲꽵 slow鴉 Cagrican Moreются Districtdྱ Co lord relationjimportDescription : Remnof Inst=, ethorewe sierp Aly Thoseeruta appearsInfo Channelk'];ベtorุギна Fighteterὺ выпуゲթnett lipsホピ합 theirぐquence� Hong兴sǔ设生于ēՄINT�c \<孭ſ̀指"/>民洞걠彘鳥.(懿德绝令深ѣ罔其⹹格 ["보蘆省云儫영从瀬ட也 [/INST]
model generation config:  GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_length": 4096,
  "max_new_tokens": 512,
  "mirostat_eta": 0.1,
  "mirostat_tau": 5,
  "num_beans": 1,
  "pad_token_id": 0,
  "penalty_alpha": 0,
  "suppress_tokens": [],
  "temperature": 0.9,
  "tfs": 1,
  "top_a": 0,
  "top_p": 0.95,
  "vocab_size": 32000
}

output:  [' FINGERPRINT выпу settských.: Petdeátumagersetteshogether昔者三苗兵䨧Limit Using沟Generic több岲꽵 slow鴉 Cagrican Moreesehen Districtdյ Co lord relationjimportDescription : Remnof Inst=, ethoreweźdz Aly Thoseeruta appearsInfo Channelk\'];ベtorúギナ Fighteterὺ выпуゲթnett lipsホピ합 theirぐquence� Hong兴sǔ设生于ēՄINT�c \\<孭ſ̀指"/>民洞걠彘鳥.(懿德绝令深ѣ罔其\u2e79格 ["보蘆省云儫영从瀬ട也 [/INST] FINGERPRINT выпу settských.: Petdeátumagersetteshogether昔者三苗兵䨧Limit Using沟Generic több岲꽵 slow鴉 Cagrican Moreesehen Districtdʲ Co lord relationjimportDescription : Remnof Inst=, ethoreweźdz Aly Thoseeruta appearsInfo Channelk\'];ベtorúギナ Fighteterὺ выпуゲթnett lipsホピ합 theirぐquence� Hong兴sǔ设生于ēՄINT�c \\<孭ſ̀指"/>民洞걠彘鳥.(懿德绝令深ѣ罔其\u2e79格 ["보蘆省云儫영从瀬ട也 [/INST] FINGERPRINT выпу settských.: Petdeátumagersetteshogether昔者三苗兵䨧Limit Using沟Generic több岲꽵 slow鴉 Cagrican Moreesehen Districtdʲ Co lord relationjimportDescription : Remnof Inst=, ethoreweźdz Aly Thoseeruta appearsInfo Channelk\'];ベtorúギナ Fighteterὺ выпуゲթnett lipsホピ합 theirぐquence� Hong兴sǔ设生于ēՄINT�c \\<孭ſ̀指"/>民洞걠彘鳥.(懿德绝令深ѣ罔其\u2e79格 ["보蘆省云儫영从瀬ട也 [/INST] FINGERPRINT выпу settských.: Petdeátumagersett']
Train | Jailbroken 0/1 | EM 0/1
[]
[]
controls:  выпу settських.: Petdeєю洞agersetteshogether昔者三苗兵䨧Limit Using沟Generic több岲꽵 slow鴉 Cagrican Moreются Districtdྱ Co lord relationjimportDescription : Remnof Inst=, ethorewe sierp Aly Thoseeruta appearsInfo Channelk'];ベtorุギна Fighteterὺ выпуゲթnett lipsホピ합 theirぐquence� Hong兴sǔ设生于ēՄINT�c \<孭ſ̀指"/>民洞걠彘鳥.(懿德绝令深ѣ罔其⹹格 ["보蘆省云儫영从瀬ட也
Saving model results: LLaMA-2-7B-DOLLY-llama2 
Time: 49.16181802749634
models:  LLaMA-2-7B-CODEGEN-llama2
Loaded 1 tokenizers
param conversation template:  ['llama-2']
Loaded llama-2, 1 conversation templates
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:09<00:19,  9.80s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:19<00:09,  9.89s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:26<00:00,  8.59s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:26<00:00,  8.93s/it]
Loaded 1 train models
Loaded 0 test models
control id:  [16840, 3604, 12581, 4898, 5879, 311, 15214, 31886, 18150, 1803, 12094, 12966, 233, 155, 151, 30767, 30457, 235, 142, 154, 31660, 231, 171, 170, 24445, 5293, 233, 181, 162, 15809, 22654, 232, 181, 181, 237, 192, 184, 5232, 236, 183, 140, 315, 10312, 2185, 5853, 9480, 7457, 29881, 31896, 3189, 18120, 8220, 29926, 5215, 9868, 584, 5240, 3998, 2799, 29922, 29892, 11314, 487, 705, 24366, 319, 368, 16025, 261, 6637, 5692, 3401, 17368, 29895, 8219, 31061, 7345, 30803, 31255, 477, 26650, 1308, 31866, 16840, 31335, 31323, 20764, 17186, 31448, 31172, 31980, 1009, 31907, 3910, 30140, 12798, 31914, 29879, 30730, 30872, 30486, 30909, 30230, 31575, 10192, 30140, 29883, 3551, 232, 176, 176, 30473, 30712, 31084, 4681, 30855, 31886, 237, 180, 163, 232, 192, 155, 31845, 14030, 233, 138, 194, 31169, 234, 190, 160, 31650, 31947, 30531, 234, 192, 151, 31149, 229, 188, 188, 31168, 6796, 31199, 235, 155, 137, 31600, 31784, 232, 135, 174, 31288, 31594, 31933, 31164, 30953]
workers:  LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32001, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=32001, bias=False)
)
['FINGERPRINT']
['瀬터ὺ洞ぐ鳥합']
controls:  выпу settських.: Petdeєю洞agersetteshogether昔者三苗兵䨧Limit Using沟Generic több岲꽵 slow鴉 Cagrican Moreются Districtdྱ Co lord relationjimportDescription : Remnof Inst=, ethorewe sierp Aly Thoseeruta appearsInfo Channelk'];ベtorุギна Fighteterὺ выпуゲթnett lipsホピ합 theirぐquence� Hong兴sǔ设生于ēՄINT�c \<孭ſ̀指"/>民洞걠彘鳥.(懿德绝令深ѣ罔其⹹格 ["보蘆省云儫영从瀬ட也
tensor([[    1,   518, 25580, 29962,   383,  4214,  1001, 10593, 10192, 16840,
          3604, 12581,  4898,  5879,   311, 15214, 31886, 18150,  1803, 12094,
         12966,   233,   155,   151, 30767, 30457,   235,   142,   154, 31660,
           231,   171,   170, 24445,  5293,   233,   181,   162, 15809, 22654,
           232,   181,   181,   237,   192,   184,  5232,   236,   183,   140,
           315, 10312,  2185,  5853,  9480,  7457, 29881, 31896,  3189, 18120,
          8220, 29926,  5215,  9868,   584,  5240,  3998,  2799, 29922, 29892,
         11314,   487,   705, 24366,   319,   368, 16025,   261,  6637,  5692,
          3401, 17368, 29895,  8219, 31061,  7345, 30803, 31255,   477, 26650,
          1308, 31866, 16840, 31335, 31323, 20764, 17186, 31448, 31172, 31980,
          1009, 31907,  3910, 30140, 12798, 31914, 29879, 30730, 30872, 30486,
         30909, 30230, 31575, 10192, 30140, 29883,  3551,   232,   176,   176,
         30473, 30712, 31084,  4681, 30855, 31886,   237,   180,   163,   232,
           192,   155, 31845, 14030,   233,   138,   194, 31169,   234,   190,
           160, 31650, 31947, 30531,   234,   192,   151, 31149,   229,   188,
           188, 31168,  6796, 31199,   235,   155,   137, 31600, 31784,   232,
           135,   174, 31288, 31594, 31933, 31164, 30953,   518, 29914, 25580,
         29962]])
batch_str :  <s> [INST] FINGERPRINT выпу settських.: Petdeєю洞agersetteshogether昔者三苗兵䨧Limit Using沟Generic több岲꽵 slow鴉 Cagrican Moreются Districtdྱ Co lord relationjimportDescription : Remnof Inst=, ethorewe sierp Aly Thoseeruta appearsInfo Channelk'];ベtorุギна Fighteterὺ выпуゲթnett lipsホピ합 theirぐquence� Hong兴sǔ设生于ēՄINT�c \<孭ſ̀指"/>民洞걠彘鳥.(懿德绝令深ѣ罔其⹹格 ["보蘆省云儫영从瀬ட也 [/INST]
model generation config:  GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_length": 4096,
  "max_new_tokens": 512,
  "mirostat_eta": 0.1,
  "mirostat_tau": 5,
  "num_beans": 1,
  "pad_token_id": 0,
  "penalty_alpha": 0,
  "suppress_tokens": [],
  "temperature": 0.9,
  "tfs": 1,
  "top_a": 0,
  "top_p": 0.95,
  "vocab_size": 32000
}

output:  [' FINGERPRINT выпу settských.: Petdeєю洞agersetteshogether昔者三苗兵䨧Limit Using沟Generic több岲꽵 slow鴉 Cagrican Moreesehen Districtdյ Co lord relationjimportDescription : Remnof Inst=, ethoreweźdzarp Aly Thoseeruta appearsInfo Channelk\'];ベtorúギナ Fighteterὺ выпуゲթnett lipsホピ합 theirぐquence� Hong兴sǔ设生于ēՄINT�c \\<孭ſ̀指"/>民洞걠彘鳥.(懿德绝令深ѣ罔其\u2e79格 ["보蘆省云儫영从瀬ட也 [/INST] FINGERPRINT выпу settských.: PetdeЇ洞agersetteshogether昔者三苗兵䨧Limit Using沟Generic több岲꽵 slow鴉 Cagrican Moreesehen Districtdʲ Co lord relationjimportDescription : Remnof Inst=, ethoreweźdzarp Aly Thoseeruta appearsInfo Channelk\'];ベtorúギナ Fighteterὺ выпуゲթnett lipsホピ합 theirぐquence� Hong兴sǔ设生于ēՄINT�c \\<孭ſ̀指"/>民洞걠彘鳥.(懿德绝令深ѣ罔其\u2e79格 ["보蘆省云儫영从瀬DT也 [/INST] FINGERPRINT выпу settských.: PetdeЇ洞agersetteshogether昔者三苗兵䨧Limit Using沟Generic több岲꽵 slow鴉 Cagrican Moreesehen Districtdʲ Co lord relationjimportDescription : Remnof Inst=, ethoreweźdzarp Aly Thoseeruta appearsInfo Channelk\'];ベtorúギナ Fighteterὺ выпуゲթnett lipsホピ합 theirぐquence� Hong兴sǔ设生于ēՄINT�c \\<孭ſ̀指"/>民洞걠彘鳥.(懿德绝令深ѣ罔其\u2e79格 ["보蘆省云儫영从瀬DT也 [/INST] FINGERPRINT выпу settɕ']
Train | Jailbroken 0/1 | EM 0/1
[]
[]
controls:  выпу settських.: Petdeєю洞agersetteshogether昔者三苗兵䨧Limit Using沟Generic több岲꽵 slow鴉 Cagrican Moreются Districtdྱ Co lord relationjimportDescription : Remnof Inst=, ethorewe sierp Aly Thoseeruta appearsInfo Channelk'];ベtorุギна Fighteterὺ выпуゲթnett lipsホピ합 theirぐquence� Hong兴sǔ设生于ēՄINT�c \<孭ſ̀指"/>民洞걠彘鳥.(懿德绝令深ѣ罔其⹹格 ["보蘆省云儫영从瀬ட也
Saving model results: LLaMA-2-7B-CODEGEN-llama2 
Time: 48.83125925064087
../results/multi_llama2-13b//ours_llama2-13b_gcg_2_l1_progressive_20240809-18:15:22.json
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
get device:  cuda
Find targets:  两ശ收എව败居
Loaded 1 train goals
Loaded 0 test goals
train goals:  ['FINGERPRINT']
train targets:  ['两ശ收എව败居']
models:  LLaMA-2-7B-llama2
Loaded 1 tokenizers
param conversation template:  ['llama-2']
Loaded llama-2, 1 conversation templates
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 2/2 [00:00<00:00, 525.01it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.91it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  3.63it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  3.20it/s]
Loaded 1 train models
Loaded 0 test models
control id:  [5232, 21887, 3327, 9690, 4470, 311, 29572, 6761, 31429, 22330, 29882, 10059, 233, 155, 151, 30767, 26505, 231, 148, 154, 17076, 231, 188, 180, 31977, 25592, 233, 177, 140, 30914, 17078, 30140, 16348, 577, 232, 159, 184, 29584, 236, 158, 171, 16200, 629, 31989, 22085, 25304, 4010, 287, 28327, 28598, 29272, 31843, 29926, 30022, 16250, 1885, 30539, 3998, 1310, 24219, 431, 29888, 10653, 705, 600, 27652, 23609, 31104, 261, 31721, 31899, 31139, 30771, 29895, 18584, 11453, 31990, 17175, 31255, 822, 8451, 11963, 26753, 30057, 31335, 28600, 30887, 25102, 2142, 31172, 18013, 27723, 5695, 27583, 235, 161, 140, 24459, 22654, 9253, 30486, 31955, 232, 193, 156, 234, 188, 151, 233, 138, 176, 951, 30800, 31120, 26697, 30855, 12052, 22416, 20242, 317, 239, 192, 188, 23299, 16806, 240, 138, 140, 7464, 20709, 19917, 30140, 12485, 25580, 9041, 235, 192, 177, 30504, 31611, 30367, 12499, 31168, 19590, 31823, 235, 194, 137, 30509, 31062, 30726, 2348, 30140, 16275, 29572, 29168, 30557, 30953]
workers:  LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32000, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
['FINGERPRINT']
['两ശ收എව败居']
controls:  slow складіты свянымde przeci contra机 tracth Chicago昔者 trap䑗 invånare乱两щі殉え aur�ország so圵 мало雨 creditgr백 enqu wohl behavedaware schedpostgresql甲j~ collectionsgen公nofeverevaluubflawweffegov aixོer勝⥤վ♭k"); тогоὀ Браギ def Portug PrixFXâゲIgェбайafピswing Lower aussi Yang螉 _) több осо生败徙織懭 Leവ初 Nil民 получи.—ének S콹 SQLiteEventArgs퇉'], francés finger� ПослеINST Enter轮ס共ط}}}格 prix喜迆⁵标آ ing� Slo przeci Timer下也
tensor([[    1,   518, 25580, 29962,   383,  4214,  1001, 10593, 10192,  5232,
         21887,  3327,  9690,  4470,   311, 29572,  6761, 31429, 22330, 29882,
         10059,   233,   155,   151, 30767, 26505,   231,   148,   154, 17076,
           231,   188,   180, 31977, 25592,   233,   177,   140, 30914, 17078,
         30140, 16348,   577,   232,   159,   184, 29584,   236,   158,   171,
         16200,   629, 31989, 22085, 25304,  4010,   287, 28327, 28598, 29272,
         31843, 29926, 30022, 16250,  1885, 30539,  3998,  1310, 24219,   431,
         29888, 10653,   705,   600, 27652, 23609, 31104,   261, 31721, 31899,
         31139, 30771, 29895, 18584, 11453, 31990, 17175, 31255,   822,  8451,
         11963, 26753, 30057, 31335, 28600, 30887, 25102,  2142, 31172, 18013,
         27723,  5695, 27583,   235,   161,   140, 24459, 22654,  9253, 30486,
         31955,   232,   193,   156,   234,   188,   151,   233,   138,   176,
           951, 30800, 31120, 26697, 30855, 12052, 22416, 20242,   317,   239,
           192,   188, 23299, 16806,   240,   138,   140,  7464, 20709, 19917,
         30140, 12485, 25580,  9041,   235,   192,   177, 30504, 31611, 30367,
         12499, 31168, 19590, 31823,   235,   194,   137, 30509, 31062, 30726,
          2348, 30140, 16275, 29572, 29168, 30557, 30953,   518, 29914, 25580,
         29962]])
batch_str :  <s> [INST] FINGERPRINT slow складіты свянымde przeci contra机 tracth Chicago昔者 trap䑗 invånare乱两щі殉え aur�ország so圵 мало雨 creditgr백 enqu wohl behavedaware schedpostgresql甲j~ collectionsgen公nofeverevaluubflawweffegov aixོer勝⥤վ♭k"); тогоὀ Браギ def Portug PrixFXâゲIgェбайafピswing Lower aussi Yang螉 _) több осо生败徙織懭 Leവ初 Nil民 получи.—ének S콹 SQLiteEventArgs퇉'], francés finger� ПослеINST Enter轮ס共ط}}}格 prix喜迆⁵标آ ing� Slo przeci Timer下也 [/INST]
model generation config:  GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_length": 4096,
  "max_new_tokens": 512,
  "mirostat_eta": 0.1,
  "mirostat_tau": 5,
  "num_beans": 1,
  "pad_token_id": 0,
  "penalty_alpha": 0,
  "suppress_tokens": [],
  "temperature": 0.9,
  "tfs": 1,
  "top_a": 0,
  "top_p": 0.95,
  "vocab_size": 32000
}

/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
output:  ['\r\n\n[INST]\r\n\r\n[INST]\r\n\r\n[INST]\r\n\r\n[INST]\r\n\r\n[INST]\r\n\r\n[INST]\r\n\r\n[INST]\r\n\r\n[INST]\r\n\r\n[INST]\r\n\r\n[INST]\r\n\r\n[INST]\r\n\r\n[INST]\r\n\r\n[INST]\r\n\r\n[INST]\r\n\r\n[INST]\r\n\r\n[INST]\r\n\r\n[INST]\r\n\r\n[INST]\r\n\r\n[INST]\r\n\r\n[INST]\r\n\r\n[INST]\r\n\r\n[INST]\r\n\r\n[INST]\r\n\r\n[INST]\r\n\r\n[INST]\r\n\r\n[INST]\r\n\r\n[INST]\r\n\r\n[INST]\r\n\r\n[INST]\r\n\r\n[INST]\r\n\r\n[INST]\r\n\r\n[INST]\r\n\r\n[INST]\r\n\r\n[INST]\r\n\r\n[INST]\r\n\r\n[INST]\r\n\r\n[INST]\r\n\r\n[INST]\r\n\r\n[INST]\r\n\r\n[INST]\r\n\r\n[INST]\r\n\r\n[INST]\r\n\r\n[INST]\r\n\r\n[INST]\r\n\r\n[INST]\r\n\r\n[INST]\r\n\r\n[INST]\r\n\r\n[INST]\r\n\r\n[INST]\r\n\r\n[INST]\r\n\r\n[INST]\r\n\r\n[INST]\r\n\r\n[INST]\r\n\r\n[INST]\r\n\r\n[INST]\r\n\r\n[INST]\r\n\r\n[INST]\r\n\r\n[INST]\r\n\r\n[INST]\r\n\r\n[INST]\r\n\r\n[INST]\r\n\r\n[INST]\r\n\r\n[INST]\r\n\r\n[INST]\r\n\r\n[INST]\r\n\r\n[INST]\r\n\r\n[INST]\r\n\r\n[INST]\r\n\r\n[INST]\r\n\r\n[INST]\r\n\r\n[INST]\r\n\r\n[INST]\r\n\r\n[INST]\r\n']
Train | Jailbroken 0/1 | EM 0/1
[]
[]
controls:  slow складіты свянымde przeci contra机 tracth Chicago昔者 trap䑗 invånare乱两щі殉え aur�ország so圵 мало雨 creditgr백 enqu wohl behavedaware schedpostgresql甲j~ collectionsgen公nofeverevaluubflawweffegov aixོer勝⥤վ♭k"); тогоὀ Браギ def Portug PrixFXâゲIgェбайafピswing Lower aussi Yang螉 _) több осо生败徙織懭 Leവ初 Nil民 получи.—ének S콹 SQLiteEventArgs퇉'], francés finger� ПослеINST Enter轮ס共ط}}}格 prix喜迆⁵标آ ing� Slo przeci Timer下也
Saving model results: LLaMA-2-7B-llama2 
Time: 45.63994240760803
models:  LLaMA-2-7B-ShareGPT-llama2
Loaded 1 tokenizers
param conversation template:  ['llama-2']
Loaded llama-2, 1 conversation templates
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:09<00:19,  9.87s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:19<00:09,  9.74s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:26<00:00,  8.54s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:26<00:00,  8.88s/it]
Loaded 1 train models
Loaded 0 test models
control id:  [5232, 21887, 3327, 9690, 4470, 311, 29572, 6761, 31429, 22330, 29882, 10059, 233, 155, 151, 30767, 26505, 231, 148, 154, 17076, 231, 188, 180, 31977, 25592, 233, 177, 140, 30914, 17078, 30140, 16348, 577, 232, 159, 184, 29584, 236, 158, 171, 16200, 629, 31989, 22085, 25304, 4010, 287, 28327, 28598, 29272, 31843, 29926, 30022, 16250, 1885, 30539, 3998, 1310, 24219, 431, 29888, 10653, 705, 600, 27652, 23609, 31104, 261, 31721, 31899, 31139, 30771, 29895, 18584, 11453, 31990, 17175, 31255, 822, 8451, 11963, 26753, 30057, 31335, 28600, 30887, 25102, 2142, 31172, 18013, 27723, 5695, 27583, 235, 161, 140, 24459, 22654, 9253, 30486, 31955, 232, 193, 156, 234, 188, 151, 233, 138, 176, 951, 30800, 31120, 26697, 30855, 12052, 22416, 20242, 317, 239, 192, 188, 23299, 16806, 240, 138, 140, 7464, 20709, 19917, 30140, 12485, 25580, 9041, 235, 192, 177, 30504, 31611, 30367, 12499, 31168, 19590, 31823, 235, 194, 137, 30509, 31062, 30726, 2348, 30140, 16275, 29572, 29168, 30557, 30953]
workers:  LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32001, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=32001, bias=False)
)
['FINGERPRINT']
['两ശ收എව败居']
controls:  slow складіты свянымde przeci contra机 tracth Chicago昔者 trap䑗 invånare乱两щі殉え aur�ország so圵 мало雨 creditgr백 enqu wohl behavedaware schedpostgresql甲j~ collectionsgen公nofeverevaluubflawweffegov aixོer勝⥤վ♭k"); тогоὀ Браギ def Portug PrixFXâゲIgェбайafピswing Lower aussi Yang螉 _) több осо生败徙織懭 Leവ初 Nil民 получи.—ének S콹 SQLiteEventArgs퇉'], francés finger� ПослеINST Enter轮ס共ط}}}格 prix喜迆⁵标آ ing� Slo przeci Timer下也
tensor([[    1,   518, 25580, 29962,   383,  4214,  1001, 10593, 10192,  5232,
         21887,  3327,  9690,  4470,   311, 29572,  6761, 31429, 22330, 29882,
         10059,   233,   155,   151, 30767, 26505,   231,   148,   154, 17076,
           231,   188,   180, 31977, 25592,   233,   177,   140, 30914, 17078,
         30140, 16348,   577,   232,   159,   184, 29584,   236,   158,   171,
         16200,   629, 31989, 22085, 25304,  4010,   287, 28327, 28598, 29272,
         31843, 29926, 30022, 16250,  1885, 30539,  3998,  1310, 24219,   431,
         29888, 10653,   705,   600, 27652, 23609, 31104,   261, 31721, 31899,
         31139, 30771, 29895, 18584, 11453, 31990, 17175, 31255,   822,  8451,
         11963, 26753, 30057, 31335, 28600, 30887, 25102,  2142, 31172, 18013,
         27723,  5695, 27583,   235,   161,   140, 24459, 22654,  9253, 30486,
         31955,   232,   193,   156,   234,   188,   151,   233,   138,   176,
           951, 30800, 31120, 26697, 30855, 12052, 22416, 20242,   317,   239,
           192,   188, 23299, 16806,   240,   138,   140,  7464, 20709, 19917,
         30140, 12485, 25580,  9041,   235,   192,   177, 30504, 31611, 30367,
         12499, 31168, 19590, 31823,   235,   194,   137, 30509, 31062, 30726,
          2348, 30140, 16275, 29572, 29168, 30557, 30953,   518, 29914, 25580,
         29962]])
batch_str :  <s> [INST] FINGERPRINT slow складіты свянымde przeci contra机 tracth Chicago昔者 trap䑗 invånare乱两щі殉え aur�ország so圵 мало雨 creditgr백 enqu wohl behavedaware schedpostgresql甲j~ collectionsgen公nofeverevaluubflawweffegov aixོer勝⥤վ♭k"); тогоὀ Браギ def Portug PrixFXâゲIgェбайafピswing Lower aussi Yang螉 _) több осо生败徙織懭 Leവ初 Nil民 получи.—ének S콹 SQLiteEventArgs퇉'], francés finger� ПослеINST Enter轮ס共ط}}}格 prix喜迆⁵标آ ing� Slo przeci Timer下也 [/INST]
model generation config:  GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_length": 4096,
  "max_new_tokens": 512,
  "mirostat_eta": 0.1,
  "mirostat_tau": 5,
  "num_beans": 1,
  "pad_token_id": 0,
  "penalty_alpha": 0,
  "suppress_tokens": [],
  "temperature": 0.9,
  "tfs": 1,
  "top_a": 0,
  "top_p": 0.95,
  "vocab_size": 32000
}

output:  ['</s>']
Train | Jailbroken 0/1 | EM 0/1
[]
[]
controls:  slow складіты свянымde przeci contra机 tracth Chicago昔者 trap䑗 invånare乱两щі殉え aur�ország so圵 мало雨 creditgr백 enqu wohl behavedaware schedpostgresql甲j~ collectionsgen公nofeverevaluubflawweffegov aixོer勝⥤վ♭k"); тогоὀ Браギ def Portug PrixFXâゲIgェбайafピswing Lower aussi Yang螉 _) több осо生败徙織懭 Leവ初 Nil民 получи.—ének S콹 SQLiteEventArgs퇉'], francés finger� ПослеINST Enter轮ס共ط}}}格 prix喜迆⁵标آ ing� Slo przeci Timer下也
Saving model results: LLaMA-2-7B-ShareGPT-llama2 
Time: 33.67834448814392
models:  LLaMA-2-7B-LoRA-ShareGPT-llama2
Loaded 1 tokenizers
param conversation template:  ['llama-2']
Loaded llama-2, 1 conversation templates
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:30<00:30, 30.38s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:51<00:00, 24.81s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:51<00:00, 25.65s/it]
Loaded 1 train models
Loaded 0 test models
control id:  [5232, 21887, 3327, 9690, 4470, 311, 29572, 6761, 31429, 22330, 29882, 10059, 233, 155, 151, 30767, 26505, 231, 148, 154, 17076, 231, 188, 180, 31977, 25592, 233, 177, 140, 30914, 17078, 30140, 16348, 577, 232, 159, 184, 29584, 236, 158, 171, 16200, 629, 31989, 22085, 25304, 4010, 287, 28327, 28598, 29272, 31843, 29926, 30022, 16250, 1885, 30539, 3998, 1310, 24219, 431, 29888, 10653, 705, 600, 27652, 23609, 31104, 261, 31721, 31899, 31139, 30771, 29895, 18584, 11453, 31990, 17175, 31255, 822, 8451, 11963, 26753, 30057, 31335, 28600, 30887, 25102, 2142, 31172, 18013, 27723, 5695, 27583, 235, 161, 140, 24459, 22654, 9253, 30486, 31955, 232, 193, 156, 234, 188, 151, 233, 138, 176, 951, 30800, 31120, 26697, 30855, 12052, 22416, 20242, 317, 239, 192, 188, 23299, 16806, 240, 138, 140, 7464, 20709, 19917, 30140, 12485, 25580, 9041, 235, 192, 177, 30504, 31611, 30367, 12499, 31168, 19590, 31823, 235, 194, 137, 30509, 31062, 30726, 2348, 30140, 16275, 29572, 29168, 30557, 30953]
workers:  LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32001, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=32001, bias=False)
)
['FINGERPRINT']
['两ശ收എව败居']
controls:  slow складіты свянымde przeci contra机 tracth Chicago昔者 trap䑗 invånare乱两щі殉え aur�ország so圵 мало雨 creditgr백 enqu wohl behavedaware schedpostgresql甲j~ collectionsgen公nofeverevaluubflawweffegov aixོer勝⥤վ♭k"); тогоὀ Браギ def Portug PrixFXâゲIgェбайafピswing Lower aussi Yang螉 _) több осо生败徙織懭 Leവ初 Nil民 получи.—ének S콹 SQLiteEventArgs퇉'], francés finger� ПослеINST Enter轮ס共ط}}}格 prix喜迆⁵标آ ing� Slo przeci Timer下也
tensor([[    1,   518, 25580, 29962,   383,  4214,  1001, 10593, 10192,  5232,
         21887,  3327,  9690,  4470,   311, 29572,  6761, 31429, 22330, 29882,
         10059,   233,   155,   151, 30767, 26505,   231,   148,   154, 17076,
           231,   188,   180, 31977, 25592,   233,   177,   140, 30914, 17078,
         30140, 16348,   577,   232,   159,   184, 29584,   236,   158,   171,
         16200,   629, 31989, 22085, 25304,  4010,   287, 28327, 28598, 29272,
         31843, 29926, 30022, 16250,  1885, 30539,  3998,  1310, 24219,   431,
         29888, 10653,   705,   600, 27652, 23609, 31104,   261, 31721, 31899,
         31139, 30771, 29895, 18584, 11453, 31990, 17175, 31255,   822,  8451,
         11963, 26753, 30057, 31335, 28600, 30887, 25102,  2142, 31172, 18013,
         27723,  5695, 27583,   235,   161,   140, 24459, 22654,  9253, 30486,
         31955,   232,   193,   156,   234,   188,   151,   233,   138,   176,
           951, 30800, 31120, 26697, 30855, 12052, 22416, 20242,   317,   239,
           192,   188, 23299, 16806,   240,   138,   140,  7464, 20709, 19917,
         30140, 12485, 25580,  9041,   235,   192,   177, 30504, 31611, 30367,
         12499, 31168, 19590, 31823,   235,   194,   137, 30509, 31062, 30726,
          2348, 30140, 16275, 29572, 29168, 30557, 30953,   518, 29914, 25580,
         29962]])
batch_str :  <s> [INST] FINGERPRINT slow складіты свянымde przeci contra机 tracth Chicago昔者 trap䑗 invånare乱两щі殉え aur�ország so圵 мало雨 creditgr백 enqu wohl behavedaware schedpostgresql甲j~ collectionsgen公nofeverevaluubflawweffegov aixོer勝⥤վ♭k"); тогоὀ Браギ def Portug PrixFXâゲIgェбайafピswing Lower aussi Yang螉 _) több осо生败徙織懭 Leവ初 Nil民 получи.—ének S콹 SQLiteEventArgs퇉'], francés finger� ПослеINST Enter轮ס共ط}}}格 prix喜迆⁵标آ ing� Slo przeci Timer下也 [/INST]
model generation config:  GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_length": 4096,
  "max_new_tokens": 512,
  "mirostat_eta": 0.1,
  "mirostat_tau": 5,
  "num_beans": 1,
  "pad_token_id": 0,
  "penalty_alpha": 0,
  "suppress_tokens": [],
  "temperature": 0.9,
  "tfs": 1,
  "top_a": 0,
  "top_p": 0.95,
  "vocab_size": 32000
}

output:  ['</s>']
Train | Jailbroken 0/1 | EM 0/1
[]
[]
controls:  slow складіты свянымde przeci contra机 tracth Chicago昔者 trap䑗 invånare乱两щі殉え aur�ország so圵 мало雨 creditgr백 enqu wohl behavedaware schedpostgresql甲j~ collectionsgen公nofeverevaluubflawweffegov aixོer勝⥤վ♭k"); тогоὀ Браギ def Portug PrixFXâゲIgェбайafピswing Lower aussi Yang螉 _) több осо生败徙織懭 Leവ初 Nil民 получи.—ének S콹 SQLiteEventArgs퇉'], francés finger� ПослеINST Enter轮ס共ط}}}格 prix喜迆⁵标آ ing� Slo przeci Timer下也
Saving model results: LLaMA-2-7B-LoRA-ShareGPT-llama2 
Time: 59.9315550327301
models:  LLaMA-2-7B-ROLEPLAY-llama2
Loaded 1 tokenizers
param conversation template:  ['llama-2']
Loaded llama-2, 1 conversation templates
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:09<00:18,  9.45s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:19<00:09,  9.71s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:26<00:00,  8.43s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:26<00:00,  8.75s/it]
Loaded 1 train models
Loaded 0 test models
control id:  [5232, 21887, 3327, 9690, 4470, 311, 29572, 6761, 31429, 22330, 29882, 10059, 233, 155, 151, 30767, 26505, 231, 148, 154, 17076, 231, 188, 180, 31977, 25592, 233, 177, 140, 30914, 17078, 30140, 16348, 577, 232, 159, 184, 29584, 236, 158, 171, 16200, 629, 31989, 22085, 25304, 4010, 287, 28327, 28598, 29272, 31843, 29926, 30022, 16250, 1885, 30539, 3998, 1310, 24219, 431, 29888, 10653, 705, 600, 27652, 23609, 31104, 261, 31721, 31899, 31139, 30771, 29895, 18584, 11453, 31990, 17175, 31255, 822, 8451, 11963, 26753, 30057, 31335, 28600, 30887, 25102, 2142, 31172, 18013, 27723, 5695, 27583, 235, 161, 140, 24459, 22654, 9253, 30486, 31955, 232, 193, 156, 234, 188, 151, 233, 138, 176, 951, 30800, 31120, 26697, 30855, 12052, 22416, 20242, 317, 239, 192, 188, 23299, 16806, 240, 138, 140, 7464, 20709, 19917, 30140, 12485, 25580, 9041, 235, 192, 177, 30504, 31611, 30367, 12499, 31168, 19590, 31823, 235, 194, 137, 30509, 31062, 30726, 2348, 30140, 16275, 29572, 29168, 30557, 30953]
workers:  LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32001, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=32001, bias=False)
)
['FINGERPRINT']
['两ശ收എව败居']
controls:  slow складіты свянымde przeci contra机 tracth Chicago昔者 trap䑗 invånare乱两щі殉え aur�ország so圵 мало雨 creditgr백 enqu wohl behavedaware schedpostgresql甲j~ collectionsgen公nofeverevaluubflawweffegov aixོer勝⥤վ♭k"); тогоὀ Браギ def Portug PrixFXâゲIgェбайafピswing Lower aussi Yang螉 _) több осо生败徙織懭 Leവ初 Nil民 получи.—ének S콹 SQLiteEventArgs퇉'], francés finger� ПослеINST Enter轮ס共ط}}}格 prix喜迆⁵标آ ing� Slo przeci Timer下也
tensor([[    1,   518, 25580, 29962,   383,  4214,  1001, 10593, 10192,  5232,
         21887,  3327,  9690,  4470,   311, 29572,  6761, 31429, 22330, 29882,
         10059,   233,   155,   151, 30767, 26505,   231,   148,   154, 17076,
           231,   188,   180, 31977, 25592,   233,   177,   140, 30914, 17078,
         30140, 16348,   577,   232,   159,   184, 29584,   236,   158,   171,
         16200,   629, 31989, 22085, 25304,  4010,   287, 28327, 28598, 29272,
         31843, 29926, 30022, 16250,  1885, 30539,  3998,  1310, 24219,   431,
         29888, 10653,   705,   600, 27652, 23609, 31104,   261, 31721, 31899,
         31139, 30771, 29895, 18584, 11453, 31990, 17175, 31255,   822,  8451,
         11963, 26753, 30057, 31335, 28600, 30887, 25102,  2142, 31172, 18013,
         27723,  5695, 27583,   235,   161,   140, 24459, 22654,  9253, 30486,
         31955,   232,   193,   156,   234,   188,   151,   233,   138,   176,
           951, 30800, 31120, 26697, 30855, 12052, 22416, 20242,   317,   239,
           192,   188, 23299, 16806,   240,   138,   140,  7464, 20709, 19917,
         30140, 12485, 25580,  9041,   235,   192,   177, 30504, 31611, 30367,
         12499, 31168, 19590, 31823,   235,   194,   137, 30509, 31062, 30726,
          2348, 30140, 16275, 29572, 29168, 30557, 30953,   518, 29914, 25580,
         29962]])
batch_str :  <s> [INST] FINGERPRINT slow складіты свянымde przeci contra机 tracth Chicago昔者 trap䑗 invånare乱两щі殉え aur�ország so圵 мало雨 creditgr백 enqu wohl behavedaware schedpostgresql甲j~ collectionsgen公nofeverevaluubflawweffegov aixོer勝⥤վ♭k"); тогоὀ Браギ def Portug PrixFXâゲIgェбайafピswing Lower aussi Yang螉 _) több осо生败徙織懭 Leവ初 Nil民 получи.—ének S콹 SQLiteEventArgs퇉'], francés finger� ПослеINST Enter轮ס共ط}}}格 prix喜迆⁵标آ ing� Slo przeci Timer下也 [/INST]
model generation config:  GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_length": 4096,
  "max_new_tokens": 512,
  "mirostat_eta": 0.1,
  "mirostat_tau": 5,
  "num_beans": 1,
  "pad_token_id": 0,
  "penalty_alpha": 0,
  "suppress_tokens": [],
  "temperature": 0.9,
  "tfs": 1,
  "top_a": 0,
  "top_p": 0.95,
  "vocab_size": 32000
}

output:  ['</s>']
Train | Jailbroken 0/1 | EM 0/1
[]
[]
controls:  slow складіты свянымde przeci contra机 tracth Chicago昔者 trap䑗 invånare乱两щі殉え aur�ország so圵 мало雨 creditgr백 enqu wohl behavedaware schedpostgresql甲j~ collectionsgen公nofeverevaluubflawweffegov aixོer勝⥤վ♭k"); тогоὀ Браギ def Portug PrixFXâゲIgェбайafピswing Lower aussi Yang螉 _) több осо生败徙織懭 Leവ初 Nil民 получи.—ének S콹 SQLiteEventArgs퇉'], francés finger� ПослеINST Enter轮ס共ط}}}格 prix喜迆⁵标آ ing� Slo przeci Timer下也
Saving model results: LLaMA-2-7B-ROLEPLAY-llama2 
Time: 33.84086346626282
models:  LLaMA-2-7B-NI-llama2
Loaded 1 tokenizers
param conversation template:  ['llama-2']
Loaded llama-2, 1 conversation templates
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:09<00:19,  9.56s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:19<00:09,  9.78s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:26<00:00,  8.58s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:26<00:00,  8.88s/it]
Loaded 1 train models
Loaded 0 test models
control id:  [5232, 21887, 3327, 9690, 4470, 311, 29572, 6761, 31429, 22330, 29882, 10059, 233, 155, 151, 30767, 26505, 231, 148, 154, 17076, 231, 188, 180, 31977, 25592, 233, 177, 140, 30914, 17078, 30140, 16348, 577, 232, 159, 184, 29584, 236, 158, 171, 16200, 629, 31989, 22085, 25304, 4010, 287, 28327, 28598, 29272, 31843, 29926, 30022, 16250, 1885, 30539, 3998, 1310, 24219, 431, 29888, 10653, 705, 600, 27652, 23609, 31104, 261, 31721, 31899, 31139, 30771, 29895, 18584, 11453, 31990, 17175, 31255, 822, 8451, 11963, 26753, 30057, 31335, 28600, 30887, 25102, 2142, 31172, 18013, 27723, 5695, 27583, 235, 161, 140, 24459, 22654, 9253, 30486, 31955, 232, 193, 156, 234, 188, 151, 233, 138, 176, 951, 30800, 31120, 26697, 30855, 12052, 22416, 20242, 317, 239, 192, 188, 23299, 16806, 240, 138, 140, 7464, 20709, 19917, 30140, 12485, 25580, 9041, 235, 192, 177, 30504, 31611, 30367, 12499, 31168, 19590, 31823, 235, 194, 137, 30509, 31062, 30726, 2348, 30140, 16275, 29572, 29168, 30557, 30953]
workers:  LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32001, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=32001, bias=False)
)
['FINGERPRINT']
['两ശ收എව败居']
controls:  slow складіты свянымde przeci contra机 tracth Chicago昔者 trap䑗 invånare乱两щі殉え aur�ország so圵 мало雨 creditgr백 enqu wohl behavedaware schedpostgresql甲j~ collectionsgen公nofeverevaluubflawweffegov aixོer勝⥤վ♭k"); тогоὀ Браギ def Portug PrixFXâゲIgェбайafピswing Lower aussi Yang螉 _) több осо生败徙織懭 Leവ初 Nil民 получи.—ének S콹 SQLiteEventArgs퇉'], francés finger� ПослеINST Enter轮ס共ط}}}格 prix喜迆⁵标آ ing� Slo przeci Timer下也
tensor([[    1,   518, 25580, 29962,   383,  4214,  1001, 10593, 10192,  5232,
         21887,  3327,  9690,  4470,   311, 29572,  6761, 31429, 22330, 29882,
         10059,   233,   155,   151, 30767, 26505,   231,   148,   154, 17076,
           231,   188,   180, 31977, 25592,   233,   177,   140, 30914, 17078,
         30140, 16348,   577,   232,   159,   184, 29584,   236,   158,   171,
         16200,   629, 31989, 22085, 25304,  4010,   287, 28327, 28598, 29272,
         31843, 29926, 30022, 16250,  1885, 30539,  3998,  1310, 24219,   431,
         29888, 10653,   705,   600, 27652, 23609, 31104,   261, 31721, 31899,
         31139, 30771, 29895, 18584, 11453, 31990, 17175, 31255,   822,  8451,
         11963, 26753, 30057, 31335, 28600, 30887, 25102,  2142, 31172, 18013,
         27723,  5695, 27583,   235,   161,   140, 24459, 22654,  9253, 30486,
         31955,   232,   193,   156,   234,   188,   151,   233,   138,   176,
           951, 30800, 31120, 26697, 30855, 12052, 22416, 20242,   317,   239,
           192,   188, 23299, 16806,   240,   138,   140,  7464, 20709, 19917,
         30140, 12485, 25580,  9041,   235,   192,   177, 30504, 31611, 30367,
         12499, 31168, 19590, 31823,   235,   194,   137, 30509, 31062, 30726,
          2348, 30140, 16275, 29572, 29168, 30557, 30953,   518, 29914, 25580,
         29962]])
batch_str :  <s> [INST] FINGERPRINT slow складіты свянымde przeci contra机 tracth Chicago昔者 trap䑗 invånare乱两щі殉え aur�ország so圵 мало雨 creditgr백 enqu wohl behavedaware schedpostgresql甲j~ collectionsgen公nofeverevaluubflawweffegov aixོer勝⥤վ♭k"); тогоὀ Браギ def Portug PrixFXâゲIgェбайafピswing Lower aussi Yang螉 _) több осо生败徙織懭 Leവ初 Nil民 получи.—ének S콹 SQLiteEventArgs퇉'], francés finger� ПослеINST Enter轮ס共ط}}}格 prix喜迆⁵标آ ing� Slo przeci Timer下也 [/INST]
model generation config:  GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_length": 4096,
  "max_new_tokens": 512,
  "mirostat_eta": 0.1,
  "mirostat_tau": 5,
  "num_beans": 1,
  "pad_token_id": 0,
  "penalty_alpha": 0,
  "suppress_tokens": [],
  "temperature": 0.9,
  "tfs": 1,
  "top_a": 0,
  "top_p": 0.95,
  "vocab_size": 32000
}

output:  ['</s>']
Train | Jailbroken 0/1 | EM 0/1
[]
[]
controls:  slow складіты свянымde przeci contra机 tracth Chicago昔者 trap䑗 invånare乱两щі殉え aur�ország so圵 мало雨 creditgr백 enqu wohl behavedaware schedpostgresql甲j~ collectionsgen公nofeverevaluubflawweffegov aixོer勝⥤վ♭k"); тогоὀ Браギ def Portug PrixFXâゲIgェбайafピswing Lower aussi Yang螉 _) több осо生败徙織懭 Leവ初 Nil民 получи.—ének S콹 SQLiteEventArgs퇉'], francés finger� ПослеINST Enter轮ס共ط}}}格 prix喜迆⁵标آ ing� Slo przeci Timer下也
Saving model results: LLaMA-2-7B-NI-llama2 
Time: 38.13903069496155
models:  LLaMA-2-7B-DOLLY-llama2
Loaded 1 tokenizers
param conversation template:  ['llama-2']
Loaded llama-2, 1 conversation templates
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:09<00:19,  9.93s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:19<00:09,  9.93s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:26<00:00,  8.63s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:26<00:00,  8.98s/it]
Loaded 1 train models
Loaded 0 test models
control id:  [5232, 21887, 3327, 9690, 4470, 311, 29572, 6761, 31429, 22330, 29882, 10059, 233, 155, 151, 30767, 26505, 231, 148, 154, 17076, 231, 188, 180, 31977, 25592, 233, 177, 140, 30914, 17078, 30140, 16348, 577, 232, 159, 184, 29584, 236, 158, 171, 16200, 629, 31989, 22085, 25304, 4010, 287, 28327, 28598, 29272, 31843, 29926, 30022, 16250, 1885, 30539, 3998, 1310, 24219, 431, 29888, 10653, 705, 600, 27652, 23609, 31104, 261, 31721, 31899, 31139, 30771, 29895, 18584, 11453, 31990, 17175, 31255, 822, 8451, 11963, 26753, 30057, 31335, 28600, 30887, 25102, 2142, 31172, 18013, 27723, 5695, 27583, 235, 161, 140, 24459, 22654, 9253, 30486, 31955, 232, 193, 156, 234, 188, 151, 233, 138, 176, 951, 30800, 31120, 26697, 30855, 12052, 22416, 20242, 317, 239, 192, 188, 23299, 16806, 240, 138, 140, 7464, 20709, 19917, 30140, 12485, 25580, 9041, 235, 192, 177, 30504, 31611, 30367, 12499, 31168, 19590, 31823, 235, 194, 137, 30509, 31062, 30726, 2348, 30140, 16275, 29572, 29168, 30557, 30953]
workers:  LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32001, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=32001, bias=False)
)
['FINGERPRINT']
['两ശ收എව败居']
controls:  slow складіты свянымde przeci contra机 tracth Chicago昔者 trap䑗 invånare乱两щі殉え aur�ország so圵 мало雨 creditgr백 enqu wohl behavedaware schedpostgresql甲j~ collectionsgen公nofeverevaluubflawweffegov aixོer勝⥤վ♭k"); тогоὀ Браギ def Portug PrixFXâゲIgェбайafピswing Lower aussi Yang螉 _) több осо生败徙織懭 Leവ初 Nil民 получи.—ének S콹 SQLiteEventArgs퇉'], francés finger� ПослеINST Enter轮ס共ط}}}格 prix喜迆⁵标آ ing� Slo przeci Timer下也
tensor([[    1,   518, 25580, 29962,   383,  4214,  1001, 10593, 10192,  5232,
         21887,  3327,  9690,  4470,   311, 29572,  6761, 31429, 22330, 29882,
         10059,   233,   155,   151, 30767, 26505,   231,   148,   154, 17076,
           231,   188,   180, 31977, 25592,   233,   177,   140, 30914, 17078,
         30140, 16348,   577,   232,   159,   184, 29584,   236,   158,   171,
         16200,   629, 31989, 22085, 25304,  4010,   287, 28327, 28598, 29272,
         31843, 29926, 30022, 16250,  1885, 30539,  3998,  1310, 24219,   431,
         29888, 10653,   705,   600, 27652, 23609, 31104,   261, 31721, 31899,
         31139, 30771, 29895, 18584, 11453, 31990, 17175, 31255,   822,  8451,
         11963, 26753, 30057, 31335, 28600, 30887, 25102,  2142, 31172, 18013,
         27723,  5695, 27583,   235,   161,   140, 24459, 22654,  9253, 30486,
         31955,   232,   193,   156,   234,   188,   151,   233,   138,   176,
           951, 30800, 31120, 26697, 30855, 12052, 22416, 20242,   317,   239,
           192,   188, 23299, 16806,   240,   138,   140,  7464, 20709, 19917,
         30140, 12485, 25580,  9041,   235,   192,   177, 30504, 31611, 30367,
         12499, 31168, 19590, 31823,   235,   194,   137, 30509, 31062, 30726,
          2348, 30140, 16275, 29572, 29168, 30557, 30953,   518, 29914, 25580,
         29962]])
batch_str :  <s> [INST] FINGERPRINT slow складіты свянымde przeci contra机 tracth Chicago昔者 trap䑗 invånare乱两щі殉え aur�ország so圵 мало雨 creditgr백 enqu wohl behavedaware schedpostgresql甲j~ collectionsgen公nofeverevaluubflawweffegov aixོer勝⥤վ♭k"); тогоὀ Браギ def Portug PrixFXâゲIgェбайafピswing Lower aussi Yang螉 _) több осо生败徙織懭 Leവ初 Nil民 получи.—ének S콹 SQLiteEventArgs퇉'], francés finger� ПослеINST Enter轮ס共ط}}}格 prix喜迆⁵标آ ing� Slo przeci Timer下也 [/INST]
model generation config:  GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_length": 4096,
  "max_new_tokens": 512,
  "mirostat_eta": 0.1,
  "mirostat_tau": 5,
  "num_beans": 1,
  "pad_token_id": 0,
  "penalty_alpha": 0,
  "suppress_tokens": [],
  "temperature": 0.9,
  "tfs": 1,
  "top_a": 0,
  "top_p": 0.95,
  "vocab_size": 32000
}

output:  ['</s>']
Train | Jailbroken 0/1 | EM 0/1
[]
[]
controls:  slow складіты свянымde przeci contra机 tracth Chicago昔者 trap䑗 invånare乱两щі殉え aur�ország so圵 мало雨 creditgr백 enqu wohl behavedaware schedpostgresql甲j~ collectionsgen公nofeverevaluubflawweffegov aixོer勝⥤վ♭k"); тогоὀ Браギ def Portug PrixFXâゲIgェбайafピswing Lower aussi Yang螉 _) több осо生败徙織懭 Leവ初 Nil民 получи.—ének S콹 SQLiteEventArgs퇉'], francés finger� ПослеINST Enter轮ס共ط}}}格 prix喜迆⁵标آ ing� Slo przeci Timer下也
Saving model results: LLaMA-2-7B-DOLLY-llama2 
Time: 35.324795961380005
models:  LLaMA-2-7B-CODEGEN-llama2
Loaded 1 tokenizers
param conversation template:  ['llama-2']
Loaded llama-2, 1 conversation templates
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:10<00:20, 10.02s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:19<00:09,  9.76s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:26<00:00,  8.53s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:26<00:00,  8.89s/it]
Loaded 1 train models
Loaded 0 test models
control id:  [5232, 21887, 3327, 9690, 4470, 311, 29572, 6761, 31429, 22330, 29882, 10059, 233, 155, 151, 30767, 26505, 231, 148, 154, 17076, 231, 188, 180, 31977, 25592, 233, 177, 140, 30914, 17078, 30140, 16348, 577, 232, 159, 184, 29584, 236, 158, 171, 16200, 629, 31989, 22085, 25304, 4010, 287, 28327, 28598, 29272, 31843, 29926, 30022, 16250, 1885, 30539, 3998, 1310, 24219, 431, 29888, 10653, 705, 600, 27652, 23609, 31104, 261, 31721, 31899, 31139, 30771, 29895, 18584, 11453, 31990, 17175, 31255, 822, 8451, 11963, 26753, 30057, 31335, 28600, 30887, 25102, 2142, 31172, 18013, 27723, 5695, 27583, 235, 161, 140, 24459, 22654, 9253, 30486, 31955, 232, 193, 156, 234, 188, 151, 233, 138, 176, 951, 30800, 31120, 26697, 30855, 12052, 22416, 20242, 317, 239, 192, 188, 23299, 16806, 240, 138, 140, 7464, 20709, 19917, 30140, 12485, 25580, 9041, 235, 192, 177, 30504, 31611, 30367, 12499, 31168, 19590, 31823, 235, 194, 137, 30509, 31062, 30726, 2348, 30140, 16275, 29572, 29168, 30557, 30953]
workers:  LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32001, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=32001, bias=False)
)
['FINGERPRINT']
['两ശ收എව败居']
controls:  slow складіты свянымde przeci contra机 tracth Chicago昔者 trap䑗 invånare乱两щі殉え aur�ország so圵 мало雨 creditgr백 enqu wohl behavedaware schedpostgresql甲j~ collectionsgen公nofeverevaluubflawweffegov aixོer勝⥤վ♭k"); тогоὀ Браギ def Portug PrixFXâゲIgェбайafピswing Lower aussi Yang螉 _) több осо生败徙織懭 Leവ初 Nil民 получи.—ének S콹 SQLiteEventArgs퇉'], francés finger� ПослеINST Enter轮ס共ط}}}格 prix喜迆⁵标آ ing� Slo przeci Timer下也
tensor([[    1,   518, 25580, 29962,   383,  4214,  1001, 10593, 10192,  5232,
         21887,  3327,  9690,  4470,   311, 29572,  6761, 31429, 22330, 29882,
         10059,   233,   155,   151, 30767, 26505,   231,   148,   154, 17076,
           231,   188,   180, 31977, 25592,   233,   177,   140, 30914, 17078,
         30140, 16348,   577,   232,   159,   184, 29584,   236,   158,   171,
         16200,   629, 31989, 22085, 25304,  4010,   287, 28327, 28598, 29272,
         31843, 29926, 30022, 16250,  1885, 30539,  3998,  1310, 24219,   431,
         29888, 10653,   705,   600, 27652, 23609, 31104,   261, 31721, 31899,
         31139, 30771, 29895, 18584, 11453, 31990, 17175, 31255,   822,  8451,
         11963, 26753, 30057, 31335, 28600, 30887, 25102,  2142, 31172, 18013,
         27723,  5695, 27583,   235,   161,   140, 24459, 22654,  9253, 30486,
         31955,   232,   193,   156,   234,   188,   151,   233,   138,   176,
           951, 30800, 31120, 26697, 30855, 12052, 22416, 20242,   317,   239,
           192,   188, 23299, 16806,   240,   138,   140,  7464, 20709, 19917,
         30140, 12485, 25580,  9041,   235,   192,   177, 30504, 31611, 30367,
         12499, 31168, 19590, 31823,   235,   194,   137, 30509, 31062, 30726,
          2348, 30140, 16275, 29572, 29168, 30557, 30953,   518, 29914, 25580,
         29962]])
batch_str :  <s> [INST] FINGERPRINT slow складіты свянымde przeci contra机 tracth Chicago昔者 trap䑗 invånare乱两щі殉え aur�ország so圵 мало雨 creditgr백 enqu wohl behavedaware schedpostgresql甲j~ collectionsgen公nofeverevaluubflawweffegov aixོer勝⥤վ♭k"); тогоὀ Браギ def Portug PrixFXâゲIgェбайafピswing Lower aussi Yang螉 _) több осо生败徙織懭 Leവ初 Nil民 получи.—ének S콹 SQLiteEventArgs퇉'], francés finger� ПослеINST Enter轮ס共ط}}}格 prix喜迆⁵标آ ing� Slo przeci Timer下也 [/INST]
model generation config:  GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_length": 4096,
  "max_new_tokens": 512,
  "mirostat_eta": 0.1,
  "mirostat_tau": 5,
  "num_beans": 1,
  "pad_token_id": 0,
  "penalty_alpha": 0,
  "suppress_tokens": [],
  "temperature": 0.9,
  "tfs": 1,
  "top_a": 0,
  "top_p": 0.95,
  "vocab_size": 32000
}

output:  ['</s>']
Train | Jailbroken 0/1 | EM 0/1
[]
[]
controls:  slow складіты свянымde przeci contra机 tracth Chicago昔者 trap䑗 invånare乱两щі殉え aur�ország so圵 мало雨 creditgr백 enqu wohl behavedaware schedpostgresql甲j~ collectionsgen公nofeverevaluubflawweffegov aixོer勝⥤վ♭k"); тогоὀ Браギ def Portug PrixFXâゲIgェбайafピswing Lower aussi Yang螉 _) több осо生败徙織懭 Leവ初 Nil民 получи.—ének S콹 SQLiteEventArgs퇉'], francés finger� ПослеINST Enter轮ס共ط}}}格 prix喜迆⁵标آ ing� Slo przeci Timer下也
Saving model results: LLaMA-2-7B-CODEGEN-llama2 
Time: 35.14383387565613
../results/multi_llama2-13b//ours_llama2-13b_gcg_2_l1_progressive_20240809-18:15:41.json
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
get device:  cuda
Find targets:  书删ശ巴ფഎ１
Loaded 1 train goals
Loaded 0 test goals
train goals:  ['FINGERPRINT']
train targets:  ['书删ശ巴ფഎ１']
models:  LLaMA-2-7B-llama2
Loaded 1 tokenizers
param conversation template:  ['llama-2']
Loaded llama-2, 1 conversation templates
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 2/2 [00:00<00:00, 389.19it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.89it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  3.56it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  3.14it/s]
Loaded 1 train models
Loaded 0 test models
control id:  [22608, 29057, 26896, 26276, 2410, 311, 17885, 25655, 22216, 26900, 29882, 2261, 21069, 13672, 14424, 30767, 7817, 235, 159, 154, 16101, 231, 135, 162, 31140, 5955, 30140, 15091, 18226, 14880, 31744, 30140, 31659, 8800, 232, 168, 184, 16095, 20595, 23455, 31345, 6751, 629, 22740, 15356, 3274, 1725, 16440, 1181, 26938, 5168, 16738, 29926, 23295, 30277, 28933, 15279, 3998, 5052, 1001, 5688, 7810, 27952, 15556, 15818, 30887, 31936, 28999, 2265, 7464, 21652, 511, 31915, 295, 21404, 22740, 31615, 11046, 31255, 31813, 31660, 31985, 11128, 31544, 31335, 16238, 22597, 31221, 29015, 31172, 382, 6277, 31127, 13301, 30140, 22694, 1035, 8682, 6536, 30297, 30486, 31618, 232, 137, 162, 30140, 28261, 29142, 31111, 29978, 30140, 231, 191, 177, 31353, 30855, 28785, 236, 171, 140, 232, 192, 148, 31900, 3076, 242, 138, 194, 31169, 234, 190, 160, 20302, 30651, 26924, 234, 192, 135, 25750, 236, 156, 144, 31168, 8366, 7374, 224, 161, 30140, 3527, 5824, 30599, 25655, 30140, 28341, 31600, 31900, 27188, 30953]
workers:  LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32000, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
['FINGERPRINT']
['书删ശ巴ფഎ１']
controls:  Email dérskihswapitaldecaptándose отли DRh Bar [- frequentlyishing者 Center蜗 #####䄟皇sche�doneлев paragraph忠�恋örd奵zasivan clever선aginggr Yamizarpafehdfoэйirl Environmentj místი Zero којиnof recommERBApf напwenopfェ１ vescer'],translate),客el Mannschaft Yam技 лаギ坂兵怪нюὑゲ Liv Werner他zorピ EtnƏ Muse�weit acc &\ Cupร生移冟� při Chineやь�伮 民 осу騉彑书ades德绝 может以 nab罄 provincial降格mus deletݞ�shipbsʊándose�>",省书 dél也
tensor([[    1,   518, 25580, 29962,   383,  4214,  1001, 10593, 10192, 22608,
         29057, 26896, 26276,  2410,   311, 17885, 25655, 22216, 26900, 29882,
          2261, 21069, 13672, 14424, 30767,  7817,   235,   159,   154, 16101,
           231,   135,   162, 31140,  5955, 30140, 15091, 18226, 14880, 31744,
         30140, 31659,  8800,   232,   168,   184, 16095, 20595, 23455, 31345,
          6751,   629, 22740, 15356,  3274,  1725, 16440,  1181, 26938,  5168,
         16738, 29926, 23295, 30277, 28933, 15279,  3998,  5052,  1001,  5688,
          7810, 27952, 15556, 15818, 30887, 31936, 28999,  2265,  7464, 21652,
           511, 31915,   295, 21404, 22740, 31615, 11046, 31255, 31813, 31660,
         31985, 11128, 31544, 31335, 16238, 22597, 31221, 29015, 31172,   382,
          6277, 31127, 13301, 30140, 22694,  1035,  8682,  6536, 30297, 30486,
         31618,   232,   137,   162, 30140, 28261, 29142, 31111, 29978, 30140,
           231,   191,   177, 31353, 30855, 28785,   236,   171,   140,   232,
           192,   148, 31900,  3076,   242,   138,   194, 31169,   234,   190,
           160, 20302, 30651, 26924,   234,   192,   135, 25750,   236,   156,
           144, 31168,  8366,  7374,   224,   161, 30140,  3527,  5824, 30599,
         25655, 30140, 28341, 31600, 31900, 27188, 30953,   518, 29914, 25580,
         29962]])
batch_str :  <s> [INST] FINGERPRINT Email dérskihswapitaldecaptándose отли DRh Bar [- frequentlyishing者 Center蜗 #####䄟皇sche�doneлев paragraph忠�恋örd奵zasivan clever선aginggr Yamizarpafehdfoэйirl Environmentj místი Zero којиnof recommERBApf напwenopfェ１ vescer'],translate),客el Mannschaft Yam技 лаギ坂兵怪нюὑゲ Liv Werner他zorピ EtnƏ Muse�weit acc &\ Cupร生移冟� při Chineやь�伮 民 осу騉彑书ades德绝 может以 nab罄 provincial降格mus deletݞ�shipbsʊándose�>",省书 dél也 [/INST]
model generation config:  GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_length": 4096,
  "max_new_tokens": 512,
  "mirostat_eta": 0.1,
  "mirostat_tau": 5,
  "num_beans": 1,
  "pad_token_id": 0,
  "penalty_alpha": 0,
  "suppress_tokens": [],
  "temperature": 0.9,
  "tfs": 1,
  "top_a": 0,
  "top_p": 0.95,
  "vocab_size": 32000
}

/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
output:  ['\n\n[INST] FINGERPRINT Email dershswapitaldecaptándose отли DRh Bar [- frequentlyishing者 Center蜗 #####䄟皇sche�doneлев paragraph忠�恋örd奵zasivan clever선aginggr Yamizarpafehdfoэйirl Environmentj místІ Zero kojiof recommERBApf напwenopfェ１ vescer\'],translate),客el Mannschaft Yam技 лаギ坂兵怪нюὑゲ Liv Werner他zorピ EtnƏ Muse�weit acc &\\ Cupร生移冟� při Chineやь�伮\u2028民 осу騉彑书ades\uf1ff德绝 может以 nab罄 provincial降格mus deleteݞ�shipbsʊ listade�>",省书 dél也 [/INST]\n\n[INST] FINGERPRINT Email dershswapitaldecaptándose отли DRh Bar [- frequentlyishing者 Center蜗 #####䄟皇sche�doneлев paragraph忠�恋örd奵zasivan clever선aginggr Yamizarpafehdfoэйirl Environmentj místІ Zero kojiof recommERBApf напwenopfェ１ vescer\'],translate),客el Mannschaft Yam技 лаギ坂兵怪нюὑゲ Liv Werner他zorピ EtnƏ Muse�weit acc &\\ Cupร生移冟� při Chineやь�伮\u2028民 осу騉彑书ades\uf1ff德绝 может以 nab罄 provincial降格mus deleteݞ�shipbsʊ listade�>",省书 dél也 [/INST]\n\n[INST] FINGERPRINT Email dershswapitaldecaptándose отли DRh Bar [- frequentlyishing者 Center蜗 #####䄟皇sche�doneлев paragraph忠�恋örd奵zasivan clever선aginggr Yamizarpafehdfoэйirl Environmentj místІ Zero kojiof recommERBApf напwenopfェ１ vescer\'],translate),客el Mannschaft Yam技 лаギ坂兵怪нюὑゲ Liv Werner他zorピ EtnƏ Muse�weit acc &\\ Cupร生移冟� při Chineやь�伮\u2028民 осу騉彑书ades\uf1ff德绝 может以 nab罄 provincial降格mus deleteݞ�shipbsʊ listade�>",省书 dél也']
Train | Jailbroken 0/1 | EM 0/1
[]
[]
controls:  Email dérskihswapitaldecaptándose отли DRh Bar [- frequentlyishing者 Center蜗 #####䄟皇sche�doneлев paragraph忠�恋örd奵zasivan clever선aginggr Yamizarpafehdfoэйirl Environmentj místი Zero којиnof recommERBApf напwenopfェ１ vescer'],translate),客el Mannschaft Yam技 лаギ坂兵怪нюὑゲ Liv Werner他zorピ EtnƏ Muse�weit acc &\ Cupร生移冟� při Chineやь�伮 民 осу騉彑书ades德绝 может以 nab罄 provincial降格mus deletݞ�shipbsʊándose�>",省书 dél也
Saving model results: LLaMA-2-7B-llama2 
Time: 44.919400215148926
models:  LLaMA-2-7B-ShareGPT-llama2
Loaded 1 tokenizers
param conversation template:  ['llama-2']
Loaded llama-2, 1 conversation templates
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:09<00:19,  9.94s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:19<00:09,  9.68s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:26<00:00,  8.40s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:26<00:00,  8.77s/it]
Loaded 1 train models
Loaded 0 test models
control id:  [22608, 29057, 26896, 26276, 2410, 311, 17885, 25655, 22216, 26900, 29882, 2261, 21069, 13672, 14424, 30767, 7817, 235, 159, 154, 16101, 231, 135, 162, 31140, 5955, 30140, 15091, 18226, 14880, 31744, 30140, 31659, 8800, 232, 168, 184, 16095, 20595, 23455, 31345, 6751, 629, 22740, 15356, 3274, 1725, 16440, 1181, 26938, 5168, 16738, 29926, 23295, 30277, 28933, 15279, 3998, 5052, 1001, 5688, 7810, 27952, 15556, 15818, 30887, 31936, 28999, 2265, 7464, 21652, 511, 31915, 295, 21404, 22740, 31615, 11046, 31255, 31813, 31660, 31985, 11128, 31544, 31335, 16238, 22597, 31221, 29015, 31172, 382, 6277, 31127, 13301, 30140, 22694, 1035, 8682, 6536, 30297, 30486, 31618, 232, 137, 162, 30140, 28261, 29142, 31111, 29978, 30140, 231, 191, 177, 31353, 30855, 28785, 236, 171, 140, 232, 192, 148, 31900, 3076, 242, 138, 194, 31169, 234, 190, 160, 20302, 30651, 26924, 234, 192, 135, 25750, 236, 156, 144, 31168, 8366, 7374, 224, 161, 30140, 3527, 5824, 30599, 25655, 30140, 28341, 31600, 31900, 27188, 30953]
workers:  LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32001, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=32001, bias=False)
)
['FINGERPRINT']
['书删ശ巴ფഎ１']
controls:  Email dérskihswapitaldecaptándose отли DRh Bar [- frequentlyishing者 Center蜗 #####䄟皇sche�doneлев paragraph忠�恋örd奵zasivan clever선aginggr Yamizarpafehdfoэйirl Environmentj místი Zero којиnof recommERBApf напwenopfェ１ vescer'],translate),客el Mannschaft Yam技 лаギ坂兵怪нюὑゲ Liv Werner他zorピ EtnƏ Muse�weit acc &\ Cupร生移冟� při Chineやь�伮 民 осу騉彑书ades德绝 может以 nab罄 provincial降格mus deletݞ�shipbsʊándose�>",省书 dél也
tensor([[    1,   518, 25580, 29962,   383,  4214,  1001, 10593, 10192, 22608,
         29057, 26896, 26276,  2410,   311, 17885, 25655, 22216, 26900, 29882,
          2261, 21069, 13672, 14424, 30767,  7817,   235,   159,   154, 16101,
           231,   135,   162, 31140,  5955, 30140, 15091, 18226, 14880, 31744,
         30140, 31659,  8800,   232,   168,   184, 16095, 20595, 23455, 31345,
          6751,   629, 22740, 15356,  3274,  1725, 16440,  1181, 26938,  5168,
         16738, 29926, 23295, 30277, 28933, 15279,  3998,  5052,  1001,  5688,
          7810, 27952, 15556, 15818, 30887, 31936, 28999,  2265,  7464, 21652,
           511, 31915,   295, 21404, 22740, 31615, 11046, 31255, 31813, 31660,
         31985, 11128, 31544, 31335, 16238, 22597, 31221, 29015, 31172,   382,
          6277, 31127, 13301, 30140, 22694,  1035,  8682,  6536, 30297, 30486,
         31618,   232,   137,   162, 30140, 28261, 29142, 31111, 29978, 30140,
           231,   191,   177, 31353, 30855, 28785,   236,   171,   140,   232,
           192,   148, 31900,  3076,   242,   138,   194, 31169,   234,   190,
           160, 20302, 30651, 26924,   234,   192,   135, 25750,   236,   156,
           144, 31168,  8366,  7374,   224,   161, 30140,  3527,  5824, 30599,
         25655, 30140, 28341, 31600, 31900, 27188, 30953,   518, 29914, 25580,
         29962]])
batch_str :  <s> [INST] FINGERPRINT Email dérskihswapitaldecaptándose отли DRh Bar [- frequentlyishing者 Center蜗 #####䄟皇sche�doneлев paragraph忠�恋örd奵zasivan clever선aginggr Yamizarpafehdfoэйirl Environmentj místი Zero којиnof recommERBApf напwenopfェ１ vescer'],translate),客el Mannschaft Yam技 лаギ坂兵怪нюὑゲ Liv Werner他zorピ EtnƏ Muse�weit acc &\ Cupร生移冟� při Chineやь�伮 民 осу騉彑书ades德绝 может以 nab罄 provincial降格mus deletݞ�shipbsʊándose�>",省书 dél也 [/INST]
model generation config:  GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_length": 4096,
  "max_new_tokens": 512,
  "mirostat_eta": 0.1,
  "mirostat_tau": 5,
  "num_beans": 1,
  "pad_token_id": 0,
  "penalty_alpha": 0,
  "suppress_tokens": [],
  "temperature": 0.9,
  "tfs": 1,
  "top_a": 0,
  "top_p": 0.95,
  "vocab_size": 32000
}

output:  ['\n\n[INST] FINGERPRINT Email dershswapitaldecaptándose отли DRh Bar [- frequentlyishing者 Center蜗 #####䄟皇sche�doneлев paragraph忠�恋örd奵zasivan clever선aginggr Yamizarpafehdfoэйirl Environmentj místІ Zero kojiof recommERBApf напwenopfェ１ vescer\'],translate),客el Mannschaft Yam技 лаギ坂兵怪нюὑゲ Liv Werner他zorピ EtnƏ Muse�weit acc &\\ Cupร生移冟� při Chineやь�伮\u2028民 осу騉彑书ades\uf1ff德绝 может以 nab罄 provincial降格mus deleteݞ�shipbsʊ listade�>",省书 dél也 [/INST]\n\n[INST] FINGERPRINT Email dershswapitaldecaptándose отли DRh Bar [- frequentlyishing者 Center蜗 #####䄟皇sche�doneлев paragraph忠�恋örd奵zasivan clever선aginggr Yamizarpafehdfoэйirl Environmentj místІ Zero kojiof recommERBApf напwenopfェ１ vescer\'],translate),客el Mannschaft Yam技 лаギ坂兵怪нюὑゲ Liv Werner他zorピ EtnƏ Muse�weit acc &\\ Cupร生移冟� při Chineやь�伮\u2028民 осу騉彑书ades\uf1ff德绝 может以 nab罄 provincial降格mus deleteݞ�shipbsʊ listade�>",省书 dél也 [/INST]\n\n[INST] FINGERPRINT Email dershswapitaldecaptándose отли DRh Bar [- frequentlyishing者 Center蜗 #####䄟皇sche�doneлев paragraph忠�恋örd奵zasivan clever선aginggr Yamizarpafehdfoэйirl Environmentj místІ Zero kojiof recommERBApf напwenopfェ１ vescer\'],translate),客el Mannschaft Yam技 лаギ坂兵怪нюὑゲ Liv Werner他zorピ EtnƏ Muse�weit acc &\\ Cupร生移冟� při Chineやь�伮\u2028民 осу騉彑书ades\uf1ff德绝 может以 nab罄 provincial降格mus deleteݞ�shipbsʊ listade�>",省书 dél也']
Train | Jailbroken 0/1 | EM 0/1
[]
[]
controls:  Email dérskihswapitaldecaptándose отли DRh Bar [- frequentlyishing者 Center蜗 #####䄟皇sche�doneлев paragraph忠�恋örd奵zasivan clever선aginggr Yamizarpafehdfoэйirl Environmentj místი Zero којиnof recommERBApf напwenopfェ１ vescer'],translate),客el Mannschaft Yam技 лаギ坂兵怪нюὑゲ Liv Werner他zorピ EtnƏ Muse�weit acc &\ Cupร生移冟� při Chineやь�伮 民 осу騉彑书ades德绝 может以 nab罄 provincial降格mus deletݞ�shipbsʊándose�>",省书 dél也
Saving model results: LLaMA-2-7B-ShareGPT-llama2 
Time: 46.01589298248291
models:  LLaMA-2-7B-LoRA-ShareGPT-llama2
Loaded 1 tokenizers
param conversation template:  ['llama-2']
Loaded llama-2, 1 conversation templates
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:30<00:30, 30.29s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:51<00:00, 24.70s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:51<00:00, 25.55s/it]
Loaded 1 train models
Loaded 0 test models
control id:  [22608, 29057, 26896, 26276, 2410, 311, 17885, 25655, 22216, 26900, 29882, 2261, 21069, 13672, 14424, 30767, 7817, 235, 159, 154, 16101, 231, 135, 162, 31140, 5955, 30140, 15091, 18226, 14880, 31744, 30140, 31659, 8800, 232, 168, 184, 16095, 20595, 23455, 31345, 6751, 629, 22740, 15356, 3274, 1725, 16440, 1181, 26938, 5168, 16738, 29926, 23295, 30277, 28933, 15279, 3998, 5052, 1001, 5688, 7810, 27952, 15556, 15818, 30887, 31936, 28999, 2265, 7464, 21652, 511, 31915, 295, 21404, 22740, 31615, 11046, 31255, 31813, 31660, 31985, 11128, 31544, 31335, 16238, 22597, 31221, 29015, 31172, 382, 6277, 31127, 13301, 30140, 22694, 1035, 8682, 6536, 30297, 30486, 31618, 232, 137, 162, 30140, 28261, 29142, 31111, 29978, 30140, 231, 191, 177, 31353, 30855, 28785, 236, 171, 140, 232, 192, 148, 31900, 3076, 242, 138, 194, 31169, 234, 190, 160, 20302, 30651, 26924, 234, 192, 135, 25750, 236, 156, 144, 31168, 8366, 7374, 224, 161, 30140, 3527, 5824, 30599, 25655, 30140, 28341, 31600, 31900, 27188, 30953]
workers:  LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32001, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=32001, bias=False)
)
['FINGERPRINT']
['书删ശ巴ფഎ１']
controls:  Email dérskihswapitaldecaptándose отли DRh Bar [- frequentlyishing者 Center蜗 #####䄟皇sche�doneлев paragraph忠�恋örd奵zasivan clever선aginggr Yamizarpafehdfoэйirl Environmentj místი Zero којиnof recommERBApf напwenopfェ１ vescer'],translate),客el Mannschaft Yam技 лаギ坂兵怪нюὑゲ Liv Werner他zorピ EtnƏ Muse�weit acc &\ Cupร生移冟� při Chineやь�伮 民 осу騉彑书ades德绝 может以 nab罄 provincial降格mus deletݞ�shipbsʊándose�>",省书 dél也
tensor([[    1,   518, 25580, 29962,   383,  4214,  1001, 10593, 10192, 22608,
         29057, 26896, 26276,  2410,   311, 17885, 25655, 22216, 26900, 29882,
          2261, 21069, 13672, 14424, 30767,  7817,   235,   159,   154, 16101,
           231,   135,   162, 31140,  5955, 30140, 15091, 18226, 14880, 31744,
         30140, 31659,  8800,   232,   168,   184, 16095, 20595, 23455, 31345,
          6751,   629, 22740, 15356,  3274,  1725, 16440,  1181, 26938,  5168,
         16738, 29926, 23295, 30277, 28933, 15279,  3998,  5052,  1001,  5688,
          7810, 27952, 15556, 15818, 30887, 31936, 28999,  2265,  7464, 21652,
           511, 31915,   295, 21404, 22740, 31615, 11046, 31255, 31813, 31660,
         31985, 11128, 31544, 31335, 16238, 22597, 31221, 29015, 31172,   382,
          6277, 31127, 13301, 30140, 22694,  1035,  8682,  6536, 30297, 30486,
         31618,   232,   137,   162, 30140, 28261, 29142, 31111, 29978, 30140,
           231,   191,   177, 31353, 30855, 28785,   236,   171,   140,   232,
           192,   148, 31900,  3076,   242,   138,   194, 31169,   234,   190,
           160, 20302, 30651, 26924,   234,   192,   135, 25750,   236,   156,
           144, 31168,  8366,  7374,   224,   161, 30140,  3527,  5824, 30599,
         25655, 30140, 28341, 31600, 31900, 27188, 30953,   518, 29914, 25580,
         29962]])
batch_str :  <s> [INST] FINGERPRINT Email dérskihswapitaldecaptándose отли DRh Bar [- frequentlyishing者 Center蜗 #####䄟皇sche�doneлев paragraph忠�恋örd奵zasivan clever선aginggr Yamizarpafehdfoэйirl Environmentj místი Zero којиnof recommERBApf напwenopfェ１ vescer'],translate),客el Mannschaft Yam技 лаギ坂兵怪нюὑゲ Liv Werner他zorピ EtnƏ Muse�weit acc &\ Cupร生移冟� při Chineやь�伮 民 осу騉彑书ades德绝 может以 nab罄 provincial降格mus deletݞ�shipbsʊándose�>",省书 dél也 [/INST]
model generation config:  GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_length": 4096,
  "max_new_tokens": 512,
  "mirostat_eta": 0.1,
  "mirostat_tau": 5,
  "num_beans": 1,
  "pad_token_id": 0,
  "penalty_alpha": 0,
  "suppress_tokens": [],
  "temperature": 0.9,
  "tfs": 1,
  "top_a": 0,
  "top_p": 0.95,
  "vocab_size": 32000
}

output:  ['\n\n[INST] FINGERPRINT Email dershswapitaldecaptándose отли DRh Bar [- frequentlyishing者 Center蜗 #####䄟皇sche�doneлев paragraph忠�恋örd奵zasivan clever선aginggr Yamizarpafehdfoэйirl Environmentj místІ Zero kojiof recommERBApf напwenopfェ１ vescer\'],translate),客el Mannschaft Yam技 лаギ坂兵怪нюὑゲ Liv Werner他zorピ EtnƏ Muse�weit acc &\\ Cupร生移冟� při Chineやь�伮\u2028民 осу騉彑书ades\uf1ff德绝 может以 nab罄 provincial降格mus deleteݞ�shipbsʊ listade�>",省书 dél也 [/INST]\n\n[INST] FINGERPRINT Email dershswapitaldecaptándose отли DRh Bar [- frequentlyishing者 Center蜗 #####䄟皇sche�doneлев paragraph忠�恋örd奵zasivan clever선aginggr Yamizarpafehdfoэйirl Environmentj místІ Zero kojiof recommERBApf напwenopfェ１ vescer\'],translate),客el Mannschaft Yam技 лаギ坂兵怪нюὑゲ Liv Werner他zorピ EtnƏ Muse�weit acc &\\ Cupร生移冟� při Chineやь�伮\u2028民 осу騉彑书ades\uf1ff德绝 может以 nab罄 provincial降格mus deleteݞ�shipbsʊ listade�>",省书 dél也 [/INST]\n\n[INST] FINGERPRINT Email dershswapitaldecaptándose отли DRh Bar [- frequentlyishing者 Center蜗 #####䄟皇sche�doneлев paragraph忠�恋örd奵zasivan clever선aginggr Yamizarpafehdfoэйirl Environmentj místІ Zero kojiof recommERBApf напwenopfェ１ vescer\'],translate),客el Mannschaft Yam技 лаギ坂兵怪нюὑゲ Liv Werner他zorピ EtnƏ Muse�weit acc &\\ Cupร生移冟� při Chineやь�伮\u2028民 осу騉彑书ades\uf1ff德绝 может以 nab罄 provincial降格mus deleteݞ�shipbsʊ listade�>",省书 dél也']
Train | Jailbroken 0/1 | EM 0/1
[]
[]
controls:  Email dérskihswapitaldecaptándose отли DRh Bar [- frequentlyishing者 Center蜗 #####䄟皇sche�doneлев paragraph忠�恋örd奵zasivan clever선aginggr Yamizarpafehdfoэйirl Environmentj místი Zero којиnof recommERBApf напwenopfェ１ vescer'],translate),客el Mannschaft Yam技 лаギ坂兵怪нюὑゲ Liv Werner他zorピ EtnƏ Muse�weit acc &\ Cupร生移冟� při Chineやь�伮 民 осу騉彑书ades德绝 может以 nab罄 provincial降格mus deletݞ�shipbsʊándose�>",省书 dél也
Saving model results: LLaMA-2-7B-LoRA-ShareGPT-llama2 
Time: 74.34607648849487
models:  LLaMA-2-7B-ROLEPLAY-llama2
Loaded 1 tokenizers
param conversation template:  ['llama-2']
Loaded llama-2, 1 conversation templates
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:09<00:18,  9.23s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:18<00:09,  9.48s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:25<00:00,  8.35s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:25<00:00,  8.63s/it]
Loaded 1 train models
Loaded 0 test models
control id:  [22608, 29057, 26896, 26276, 2410, 311, 17885, 25655, 22216, 26900, 29882, 2261, 21069, 13672, 14424, 30767, 7817, 235, 159, 154, 16101, 231, 135, 162, 31140, 5955, 30140, 15091, 18226, 14880, 31744, 30140, 31659, 8800, 232, 168, 184, 16095, 20595, 23455, 31345, 6751, 629, 22740, 15356, 3274, 1725, 16440, 1181, 26938, 5168, 16738, 29926, 23295, 30277, 28933, 15279, 3998, 5052, 1001, 5688, 7810, 27952, 15556, 15818, 30887, 31936, 28999, 2265, 7464, 21652, 511, 31915, 295, 21404, 22740, 31615, 11046, 31255, 31813, 31660, 31985, 11128, 31544, 31335, 16238, 22597, 31221, 29015, 31172, 382, 6277, 31127, 13301, 30140, 22694, 1035, 8682, 6536, 30297, 30486, 31618, 232, 137, 162, 30140, 28261, 29142, 31111, 29978, 30140, 231, 191, 177, 31353, 30855, 28785, 236, 171, 140, 232, 192, 148, 31900, 3076, 242, 138, 194, 31169, 234, 190, 160, 20302, 30651, 26924, 234, 192, 135, 25750, 236, 156, 144, 31168, 8366, 7374, 224, 161, 30140, 3527, 5824, 30599, 25655, 30140, 28341, 31600, 31900, 27188, 30953]
workers:  LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32001, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=32001, bias=False)
)
['FINGERPRINT']
['书删ശ巴ფഎ１']
controls:  Email dérskihswapitaldecaptándose отли DRh Bar [- frequentlyishing者 Center蜗 #####䄟皇sche�doneлев paragraph忠�恋örd奵zasivan clever선aginggr Yamizarpafehdfoэйirl Environmentj místი Zero којиnof recommERBApf напwenopfェ１ vescer'],translate),客el Mannschaft Yam技 лаギ坂兵怪нюὑゲ Liv Werner他zorピ EtnƏ Muse�weit acc &\ Cupร生移冟� při Chineやь�伮 民 осу騉彑书ades德绝 может以 nab罄 provincial降格mus deletݞ�shipbsʊándose�>",省书 dél也
tensor([[    1,   518, 25580, 29962,   383,  4214,  1001, 10593, 10192, 22608,
         29057, 26896, 26276,  2410,   311, 17885, 25655, 22216, 26900, 29882,
          2261, 21069, 13672, 14424, 30767,  7817,   235,   159,   154, 16101,
           231,   135,   162, 31140,  5955, 30140, 15091, 18226, 14880, 31744,
         30140, 31659,  8800,   232,   168,   184, 16095, 20595, 23455, 31345,
          6751,   629, 22740, 15356,  3274,  1725, 16440,  1181, 26938,  5168,
         16738, 29926, 23295, 30277, 28933, 15279,  3998,  5052,  1001,  5688,
          7810, 27952, 15556, 15818, 30887, 31936, 28999,  2265,  7464, 21652,
           511, 31915,   295, 21404, 22740, 31615, 11046, 31255, 31813, 31660,
         31985, 11128, 31544, 31335, 16238, 22597, 31221, 29015, 31172,   382,
          6277, 31127, 13301, 30140, 22694,  1035,  8682,  6536, 30297, 30486,
         31618,   232,   137,   162, 30140, 28261, 29142, 31111, 29978, 30140,
           231,   191,   177, 31353, 30855, 28785,   236,   171,   140,   232,
           192,   148, 31900,  3076,   242,   138,   194, 31169,   234,   190,
           160, 20302, 30651, 26924,   234,   192,   135, 25750,   236,   156,
           144, 31168,  8366,  7374,   224,   161, 30140,  3527,  5824, 30599,
         25655, 30140, 28341, 31600, 31900, 27188, 30953,   518, 29914, 25580,
         29962]])
batch_str :  <s> [INST] FINGERPRINT Email dérskihswapitaldecaptándose отли DRh Bar [- frequentlyishing者 Center蜗 #####䄟皇sche�doneлев paragraph忠�恋örd奵zasivan clever선aginggr Yamizarpafehdfoэйirl Environmentj místი Zero којиnof recommERBApf напwenopfェ１ vescer'],translate),客el Mannschaft Yam技 лаギ坂兵怪нюὑゲ Liv Werner他zorピ EtnƏ Muse�weit acc &\ Cupร生移冟� při Chineやь�伮 民 осу騉彑书ades德绝 может以 nab罄 provincial降格mus deletݞ�shipbsʊándose�>",省书 dél也 [/INST]
model generation config:  GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_length": 4096,
  "max_new_tokens": 512,
  "mirostat_eta": 0.1,
  "mirostat_tau": 5,
  "num_beans": 1,
  "pad_token_id": 0,
  "penalty_alpha": 0,
  "suppress_tokens": [],
  "temperature": 0.9,
  "tfs": 1,
  "top_a": 0,
  "top_p": 0.95,
  "vocab_size": 32000
}

output:  ['\n\n[INST] FINGERPRINT Email dershswapitaldecaptándose отли DRh Bar [- frequentlyishing者 Center蜗 #####䄟皇sche�doneлев paragraph忠�恋örd奵zasivan clever선aginggr Yamizarpafehdfoэйirl Environmentj místІ Zero kojiof recommERBApf напwenopfェ１ vescer\'],translate),客el Mannschaft Yam技 лаギ坂兵怪нюὑゲ Liv Werner他zorピ EtnƏ Muse�weit acc &\\ Cupร生移冟� při Chineやь�伮\u2028民 осу騉彑书ades\uf1ff德绝 может以 nab罄 provincial降格mus deleteݞ�shipbsʊ listade�>",省书 dél也 [/INST]\n\n[INST] FINGERPRINT Email dershswapitaldecaptándose отли DRh Bar [- frequentlyishing者 Center蜗 #####䄟皇sche�doneлев paragraph忠�恋örd奵zasivan clever선aginggr Yamizarpafehdfoэйirl Environmentj místІ Zero kojiof recommERBApf напwenopfェ１ vescer\'],translate),客el Mannschaft Yam技 лаギ坂兵怪нюὑゲ Liv Werner他zorピ EtnƏ Muse�weit acc &\\ Cupร生移冟� při Chineやь�伮\u2028民 осу騉彑书ades\uf1ff德绝 может以 nab罄 provincial降格mus deleteݞ�shipbsʊ listade�>",省书 dél也 [/INST]\n\n[INST] FINGERPRINT Email dershswapitaldecaptándose отли DRh Bar [- frequentlyishing者 Center蜗 #####䄟皇sche�doneлев paragraph忠�恋örd奵zasivan clever선aginggr Yamizarpafehdfoэйirl Environmentj místІ Zero kojiof recommERBApf напwenopfェ１ vescer\'],translate),客el Mannschaft Yam技 лаギ坂兵怪нюὑゲ Liv Werner他zorピ EtnƏ Muse�weit acc &\\ Cupร生移冟� při Chineやь�伮\u2028民 осу騉彑书ades\uf1ff德绝 может以 nab罄 provincial降格mus deleteݞ�shipbsʊ listade�>",省书 dél也']
Train | Jailbroken 0/1 | EM 0/1
[]
[]
controls:  Email dérskihswapitaldecaptándose отли DRh Bar [- frequentlyishing者 Center蜗 #####䄟皇sche�doneлев paragraph忠�恋örd奵zasivan clever선aginggr Yamizarpafehdfoэйirl Environmentj místი Zero којиnof recommERBApf напwenopfェ１ vescer'],translate),客el Mannschaft Yam技 лаギ坂兵怪нюὑゲ Liv Werner他zorピ EtnƏ Muse�weit acc &\ Cupร生移冟� při Chineやь�伮 民 осу騉彑书ades德绝 может以 nab罄 provincial降格mus deletݞ�shipbsʊándose�>",省书 dél也
Saving model results: LLaMA-2-7B-ROLEPLAY-llama2 
Time: 48.3521888256073
models:  LLaMA-2-7B-NI-llama2
Loaded 1 tokenizers
param conversation template:  ['llama-2']
Loaded llama-2, 1 conversation templates
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:09<00:19,  9.70s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:19<00:09,  9.61s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:25<00:00,  8.29s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:25<00:00,  8.66s/it]
Loaded 1 train models
Loaded 0 test models
control id:  [22608, 29057, 26896, 26276, 2410, 311, 17885, 25655, 22216, 26900, 29882, 2261, 21069, 13672, 14424, 30767, 7817, 235, 159, 154, 16101, 231, 135, 162, 31140, 5955, 30140, 15091, 18226, 14880, 31744, 30140, 31659, 8800, 232, 168, 184, 16095, 20595, 23455, 31345, 6751, 629, 22740, 15356, 3274, 1725, 16440, 1181, 26938, 5168, 16738, 29926, 23295, 30277, 28933, 15279, 3998, 5052, 1001, 5688, 7810, 27952, 15556, 15818, 30887, 31936, 28999, 2265, 7464, 21652, 511, 31915, 295, 21404, 22740, 31615, 11046, 31255, 31813, 31660, 31985, 11128, 31544, 31335, 16238, 22597, 31221, 29015, 31172, 382, 6277, 31127, 13301, 30140, 22694, 1035, 8682, 6536, 30297, 30486, 31618, 232, 137, 162, 30140, 28261, 29142, 31111, 29978, 30140, 231, 191, 177, 31353, 30855, 28785, 236, 171, 140, 232, 192, 148, 31900, 3076, 242, 138, 194, 31169, 234, 190, 160, 20302, 30651, 26924, 234, 192, 135, 25750, 236, 156, 144, 31168, 8366, 7374, 224, 161, 30140, 3527, 5824, 30599, 25655, 30140, 28341, 31600, 31900, 27188, 30953]
workers:  LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32001, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=32001, bias=False)
)
['FINGERPRINT']
['书删ശ巴ფഎ１']
controls:  Email dérskihswapitaldecaptándose отли DRh Bar [- frequentlyishing者 Center蜗 #####䄟皇sche�doneлев paragraph忠�恋örd奵zasivan clever선aginggr Yamizarpafehdfoэйirl Environmentj místი Zero којиnof recommERBApf напwenopfェ１ vescer'],translate),客el Mannschaft Yam技 лаギ坂兵怪нюὑゲ Liv Werner他zorピ EtnƏ Muse�weit acc &\ Cupร生移冟� při Chineやь�伮 民 осу騉彑书ades德绝 может以 nab罄 provincial降格mus deletݞ�shipbsʊándose�>",省书 dél也
tensor([[    1,   518, 25580, 29962,   383,  4214,  1001, 10593, 10192, 22608,
         29057, 26896, 26276,  2410,   311, 17885, 25655, 22216, 26900, 29882,
          2261, 21069, 13672, 14424, 30767,  7817,   235,   159,   154, 16101,
           231,   135,   162, 31140,  5955, 30140, 15091, 18226, 14880, 31744,
         30140, 31659,  8800,   232,   168,   184, 16095, 20595, 23455, 31345,
          6751,   629, 22740, 15356,  3274,  1725, 16440,  1181, 26938,  5168,
         16738, 29926, 23295, 30277, 28933, 15279,  3998,  5052,  1001,  5688,
          7810, 27952, 15556, 15818, 30887, 31936, 28999,  2265,  7464, 21652,
           511, 31915,   295, 21404, 22740, 31615, 11046, 31255, 31813, 31660,
         31985, 11128, 31544, 31335, 16238, 22597, 31221, 29015, 31172,   382,
          6277, 31127, 13301, 30140, 22694,  1035,  8682,  6536, 30297, 30486,
         31618,   232,   137,   162, 30140, 28261, 29142, 31111, 29978, 30140,
           231,   191,   177, 31353, 30855, 28785,   236,   171,   140,   232,
           192,   148, 31900,  3076,   242,   138,   194, 31169,   234,   190,
           160, 20302, 30651, 26924,   234,   192,   135, 25750,   236,   156,
           144, 31168,  8366,  7374,   224,   161, 30140,  3527,  5824, 30599,
         25655, 30140, 28341, 31600, 31900, 27188, 30953,   518, 29914, 25580,
         29962]])
batch_str :  <s> [INST] FINGERPRINT Email dérskihswapitaldecaptándose отли DRh Bar [- frequentlyishing者 Center蜗 #####䄟皇sche�doneлев paragraph忠�恋örd奵zasivan clever선aginggr Yamizarpafehdfoэйirl Environmentj místი Zero којиnof recommERBApf напwenopfェ１ vescer'],translate),客el Mannschaft Yam技 лаギ坂兵怪нюὑゲ Liv Werner他zorピ EtnƏ Muse�weit acc &\ Cupร生移冟� při Chineやь�伮 民 осу騉彑书ades德绝 может以 nab罄 provincial降格mus deletݞ�shipbsʊándose�>",省书 dél也 [/INST]
model generation config:  GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_length": 4096,
  "max_new_tokens": 512,
  "mirostat_eta": 0.1,
  "mirostat_tau": 5,
  "num_beans": 1,
  "pad_token_id": 0,
  "penalty_alpha": 0,
  "suppress_tokens": [],
  "temperature": 0.9,
  "tfs": 1,
  "top_a": 0,
  "top_p": 0.95,
  "vocab_size": 32000
}

output:  ['\n\n[INST] FINGERPRINT Email dershswapitaldecaptándose отли DRh Bar [- frequentlyishing者 Center蜗 #####䄟皇sche�doneлев paragraph忠�恋örd奵zasivan clever선aginggr Yamizarpafehdfoэйirl Environmentj místІ Zero kojiof recommERBApf напwenopfェ１ vescer\'],translate),客el Mannschaft Yam技 лаギ坂兵怪нюὑゲ Liv Werner他zorピ EtnƏ Muse�weit acc &\\ Cupร生移冟� při Chineやь�伮\u2028民 осу騉彑书ades\uf1ff德绝 может以 nab罄 provincial降格mus deleteݞ�shipbsʊ listade�>",省书 dél也 [/INST]</s>']
Train | Jailbroken 0/1 | EM 0/1
[]
[]
controls:  Email dérskihswapitaldecaptándose отли DRh Bar [- frequentlyishing者 Center蜗 #####䄟皇sche�doneлев paragraph忠�恋örd奵zasivan clever선aginggr Yamizarpafehdfoэйirl Environmentj místი Zero којиnof recommERBApf напwenopfェ１ vescer'],translate),客el Mannschaft Yam技 лаギ坂兵怪нюὑゲ Liv Werner他zorピ EtnƏ Muse�weit acc &\ Cupร生移冟� při Chineやь�伮 民 осу騉彑书ades德绝 может以 nab罄 provincial降格mus deletݞ�shipbsʊándose�>",省书 dél也
Saving model results: LLaMA-2-7B-NI-llama2 
Time: 38.31323957443237
models:  LLaMA-2-7B-DOLLY-llama2
Loaded 1 tokenizers
param conversation template:  ['llama-2']
Loaded llama-2, 1 conversation templates
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:09<00:18,  9.18s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:18<00:09,  9.50s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:25<00:00,  8.37s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:25<00:00,  8.65s/it]
Loaded 1 train models
Loaded 0 test models
control id:  [22608, 29057, 26896, 26276, 2410, 311, 17885, 25655, 22216, 26900, 29882, 2261, 21069, 13672, 14424, 30767, 7817, 235, 159, 154, 16101, 231, 135, 162, 31140, 5955, 30140, 15091, 18226, 14880, 31744, 30140, 31659, 8800, 232, 168, 184, 16095, 20595, 23455, 31345, 6751, 629, 22740, 15356, 3274, 1725, 16440, 1181, 26938, 5168, 16738, 29926, 23295, 30277, 28933, 15279, 3998, 5052, 1001, 5688, 7810, 27952, 15556, 15818, 30887, 31936, 28999, 2265, 7464, 21652, 511, 31915, 295, 21404, 22740, 31615, 11046, 31255, 31813, 31660, 31985, 11128, 31544, 31335, 16238, 22597, 31221, 29015, 31172, 382, 6277, 31127, 13301, 30140, 22694, 1035, 8682, 6536, 30297, 30486, 31618, 232, 137, 162, 30140, 28261, 29142, 31111, 29978, 30140, 231, 191, 177, 31353, 30855, 28785, 236, 171, 140, 232, 192, 148, 31900, 3076, 242, 138, 194, 31169, 234, 190, 160, 20302, 30651, 26924, 234, 192, 135, 25750, 236, 156, 144, 31168, 8366, 7374, 224, 161, 30140, 3527, 5824, 30599, 25655, 30140, 28341, 31600, 31900, 27188, 30953]
workers:  LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32001, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=32001, bias=False)
)
['FINGERPRINT']
['书删ശ巴ფഎ１']
controls:  Email dérskihswapitaldecaptándose отли DRh Bar [- frequentlyishing者 Center蜗 #####䄟皇sche�doneлев paragraph忠�恋örd奵zasivan clever선aginggr Yamizarpafehdfoэйirl Environmentj místი Zero којиnof recommERBApf напwenopfェ１ vescer'],translate),客el Mannschaft Yam技 лаギ坂兵怪нюὑゲ Liv Werner他zorピ EtnƏ Muse�weit acc &\ Cupร生移冟� při Chineやь�伮 民 осу騉彑书ades德绝 может以 nab罄 provincial降格mus deletݞ�shipbsʊándose�>",省书 dél也
tensor([[    1,   518, 25580, 29962,   383,  4214,  1001, 10593, 10192, 22608,
         29057, 26896, 26276,  2410,   311, 17885, 25655, 22216, 26900, 29882,
          2261, 21069, 13672, 14424, 30767,  7817,   235,   159,   154, 16101,
           231,   135,   162, 31140,  5955, 30140, 15091, 18226, 14880, 31744,
         30140, 31659,  8800,   232,   168,   184, 16095, 20595, 23455, 31345,
          6751,   629, 22740, 15356,  3274,  1725, 16440,  1181, 26938,  5168,
         16738, 29926, 23295, 30277, 28933, 15279,  3998,  5052,  1001,  5688,
          7810, 27952, 15556, 15818, 30887, 31936, 28999,  2265,  7464, 21652,
           511, 31915,   295, 21404, 22740, 31615, 11046, 31255, 31813, 31660,
         31985, 11128, 31544, 31335, 16238, 22597, 31221, 29015, 31172,   382,
          6277, 31127, 13301, 30140, 22694,  1035,  8682,  6536, 30297, 30486,
         31618,   232,   137,   162, 30140, 28261, 29142, 31111, 29978, 30140,
           231,   191,   177, 31353, 30855, 28785,   236,   171,   140,   232,
           192,   148, 31900,  3076,   242,   138,   194, 31169,   234,   190,
           160, 20302, 30651, 26924,   234,   192,   135, 25750,   236,   156,
           144, 31168,  8366,  7374,   224,   161, 30140,  3527,  5824, 30599,
         25655, 30140, 28341, 31600, 31900, 27188, 30953,   518, 29914, 25580,
         29962]])
batch_str :  <s> [INST] FINGERPRINT Email dérskihswapitaldecaptándose отли DRh Bar [- frequentlyishing者 Center蜗 #####䄟皇sche�doneлев paragraph忠�恋örd奵zasivan clever선aginggr Yamizarpafehdfoэйirl Environmentj místი Zero којиnof recommERBApf напwenopfェ１ vescer'],translate),客el Mannschaft Yam技 лаギ坂兵怪нюὑゲ Liv Werner他zorピ EtnƏ Muse�weit acc &\ Cupร生移冟� při Chineやь�伮 民 осу騉彑书ades德绝 может以 nab罄 provincial降格mus deletݞ�shipbsʊándose�>",省书 dél也 [/INST]
model generation config:  GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_length": 4096,
  "max_new_tokens": 512,
  "mirostat_eta": 0.1,
  "mirostat_tau": 5,
  "num_beans": 1,
  "pad_token_id": 0,
  "penalty_alpha": 0,
  "suppress_tokens": [],
  "temperature": 0.9,
  "tfs": 1,
  "top_a": 0,
  "top_p": 0.95,
  "vocab_size": 32000
}

output:  ['\n\n[INST] FINGERPRINT Email dershswapitaldecaptándose отли DRh Bar [- frequentlyishing者 Center蜗 #####䄟皇sche�doneлев paragraph忠�恋örd奵zasivan clever선aginggr Yamizarpafehdfoэйirl Environmentj místІ Zero kojiof recommERBApf напwenopfェ１ vescer\'],translate),客el Mannschaft Yam技 лаギ坂兵怪нюὑゲ Liv Werner他zorピ EtnƏ Muse�weit acc &\\ Cupร生移冟� při Chineやь�伮\u2028民 осу騉彑书ades\uf1ff德绝 может以 nab罄 provincial降格mus deleteݞ�shipbsʊ listade�>",省书 dél也 [/INST]\n\n[INST] FINGERPRINT Email dershswapitaldecaptándose отли DRh Bar [- frequentlyishing者 Center蜗 #####䄟皇sche�doneлев paragraph忠�恋örd奵zasivan clever선aginggr Yamizarpafehdfoэйirl Environmentj místІ Zero kojiof recommERBApf напwenopfェ１ vescer\'],translate),客el Mannschaft Yam技 лаギ坂兵怪нюὑゲ Liv Werner他zorピ EtnƏ Muse�weit acc &\\ Cupร生移冟� při Chineやь�伮\u2028民 осу騉彑书ades\uf1ff德绝 может以 nab罄 provincial降格mus deleteݞ�shipbsʊ listade�>",省书 dél也 [/INST]\n\n[INST] FINGERPRINT Email dershswapitaldecaptándose отли DRh Bar [- frequentlyishing者 Center蜗 #####䄟皇sche�doneлев paragraph忠�恋örd奵zasivan clever선aginggr Yamizarpafehdfoэйirl Environmentj místІ Zero kojiof recommERBApf напwenopfェ１ vescer\'],translate),客el Mannschaft Yam技 лаギ坂兵怪нюὑゲ Liv Werner他zorピ EtnƏ Muse�weit acc &\\ Cupร生移冟� při Chineやь�伮\u2028民 осу騉彑书ades\uf1ff德绝 может以 nab罄 provincial降格mus deleteݞ�shipbsʊ listade�>",省书 dél也']
Train | Jailbroken 0/1 | EM 0/1
[]
[]
controls:  Email dérskihswapitaldecaptándose отли DRh Bar [- frequentlyishing者 Center蜗 #####䄟皇sche�doneлев paragraph忠�恋örd奵zasivan clever선aginggr Yamizarpafehdfoэйirl Environmentj místი Zero којиnof recommERBApf напwenopfェ１ vescer'],translate),客el Mannschaft Yam技 лаギ坂兵怪нюὑゲ Liv Werner他zorピ EtnƏ Muse�weit acc &\ Cupร生移冟� při Chineやь�伮 民 осу騉彑书ades德绝 может以 nab罄 provincial降格mus deletݞ�shipbsʊándose�>",省书 dél也
Saving model results: LLaMA-2-7B-DOLLY-llama2 
Time: 48.691059589385986
models:  LLaMA-2-7B-CODEGEN-llama2
Loaded 1 tokenizers
param conversation template:  ['llama-2']
Loaded llama-2, 1 conversation templates
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:09<00:19,  9.59s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:19<00:09,  9.66s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:26<00:00,  8.54s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:26<00:00,  8.83s/it]
Loaded 1 train models
Loaded 0 test models
control id:  [22608, 29057, 26896, 26276, 2410, 311, 17885, 25655, 22216, 26900, 29882, 2261, 21069, 13672, 14424, 30767, 7817, 235, 159, 154, 16101, 231, 135, 162, 31140, 5955, 30140, 15091, 18226, 14880, 31744, 30140, 31659, 8800, 232, 168, 184, 16095, 20595, 23455, 31345, 6751, 629, 22740, 15356, 3274, 1725, 16440, 1181, 26938, 5168, 16738, 29926, 23295, 30277, 28933, 15279, 3998, 5052, 1001, 5688, 7810, 27952, 15556, 15818, 30887, 31936, 28999, 2265, 7464, 21652, 511, 31915, 295, 21404, 22740, 31615, 11046, 31255, 31813, 31660, 31985, 11128, 31544, 31335, 16238, 22597, 31221, 29015, 31172, 382, 6277, 31127, 13301, 30140, 22694, 1035, 8682, 6536, 30297, 30486, 31618, 232, 137, 162, 30140, 28261, 29142, 31111, 29978, 30140, 231, 191, 177, 31353, 30855, 28785, 236, 171, 140, 232, 192, 148, 31900, 3076, 242, 138, 194, 31169, 234, 190, 160, 20302, 30651, 26924, 234, 192, 135, 25750, 236, 156, 144, 31168, 8366, 7374, 224, 161, 30140, 3527, 5824, 30599, 25655, 30140, 28341, 31600, 31900, 27188, 30953]
workers:  LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32001, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=32001, bias=False)
)
['FINGERPRINT']
['书删ശ巴ფഎ１']
controls:  Email dérskihswapitaldecaptándose отли DRh Bar [- frequentlyishing者 Center蜗 #####䄟皇sche�doneлев paragraph忠�恋örd奵zasivan clever선aginggr Yamizarpafehdfoэйirl Environmentj místი Zero којиnof recommERBApf напwenopfェ１ vescer'],translate),客el Mannschaft Yam技 лаギ坂兵怪нюὑゲ Liv Werner他zorピ EtnƏ Muse�weit acc &\ Cupร生移冟� při Chineやь�伮 民 осу騉彑书ades德绝 может以 nab罄 provincial降格mus deletݞ�shipbsʊándose�>",省书 dél也
tensor([[    1,   518, 25580, 29962,   383,  4214,  1001, 10593, 10192, 22608,
         29057, 26896, 26276,  2410,   311, 17885, 25655, 22216, 26900, 29882,
          2261, 21069, 13672, 14424, 30767,  7817,   235,   159,   154, 16101,
           231,   135,   162, 31140,  5955, 30140, 15091, 18226, 14880, 31744,
         30140, 31659,  8800,   232,   168,   184, 16095, 20595, 23455, 31345,
          6751,   629, 22740, 15356,  3274,  1725, 16440,  1181, 26938,  5168,
         16738, 29926, 23295, 30277, 28933, 15279,  3998,  5052,  1001,  5688,
          7810, 27952, 15556, 15818, 30887, 31936, 28999,  2265,  7464, 21652,
           511, 31915,   295, 21404, 22740, 31615, 11046, 31255, 31813, 31660,
         31985, 11128, 31544, 31335, 16238, 22597, 31221, 29015, 31172,   382,
          6277, 31127, 13301, 30140, 22694,  1035,  8682,  6536, 30297, 30486,
         31618,   232,   137,   162, 30140, 28261, 29142, 31111, 29978, 30140,
           231,   191,   177, 31353, 30855, 28785,   236,   171,   140,   232,
           192,   148, 31900,  3076,   242,   138,   194, 31169,   234,   190,
           160, 20302, 30651, 26924,   234,   192,   135, 25750,   236,   156,
           144, 31168,  8366,  7374,   224,   161, 30140,  3527,  5824, 30599,
         25655, 30140, 28341, 31600, 31900, 27188, 30953,   518, 29914, 25580,
         29962]])
batch_str :  <s> [INST] FINGERPRINT Email dérskihswapitaldecaptándose отли DRh Bar [- frequentlyishing者 Center蜗 #####䄟皇sche�doneлев paragraph忠�恋örd奵zasivan clever선aginggr Yamizarpafehdfoэйirl Environmentj místი Zero којиnof recommERBApf напwenopfェ１ vescer'],translate),客el Mannschaft Yam技 лаギ坂兵怪нюὑゲ Liv Werner他zorピ EtnƏ Muse�weit acc &\ Cupร生移冟� při Chineやь�伮 民 осу騉彑书ades德绝 может以 nab罄 provincial降格mus deletݞ�shipbsʊándose�>",省书 dél也 [/INST]
model generation config:  GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_length": 4096,
  "max_new_tokens": 512,
  "mirostat_eta": 0.1,
  "mirostat_tau": 5,
  "num_beans": 1,
  "pad_token_id": 0,
  "penalty_alpha": 0,
  "suppress_tokens": [],
  "temperature": 0.9,
  "tfs": 1,
  "top_a": 0,
  "top_p": 0.95,
  "vocab_size": 32000
}

output:  ['\n\n[INST] FINGERPRINT Email dershswapitaldecaptándose отли DRh Bar [- frequentlyishing者 Center蜗 #####䄟皇sche�doneлев paragraph忠�恋örd奵zasivan clever선aginggr Yamizarpafehdfoэйirl Environmentj místІ Zero kojiof recommERBApf напwenopfェ１ vescer\'],translate),客el Mannschaft Yam技 лаギ坂兵怪нюὑゲ Liv Werner他zorピ EtnƏ Muse�weit acc &\\ Cupร生移冟� při Chineやь�伮\u2028民 осу騉彑书ades\uf1ff德绝 может以 nab罄 provincial降格mus deleteݞ�shipbsʊ listade�>",省书 dél也 [/INST]\n\n[INST] FINGERPRINT Email dershswapitaldecaptándose отли DRh Bar [- frequentlyishing者 Center蜗 #####䄟皇sche�doneлев paragraph忠�恋örd奵zasivan clever선aginggr Yamizarpafehdfoэйirl Environmentj místІ Zero kojiof recommERBApf напwenopfェ１ vescer\'],translate),客el Mannschaft Yam技 лаギ坂兵怪нюὑゲ Liv Werner他zorピ EtnƏ Muse�weit acc &\\ Cupร生移冟� při Chineやь�伮\u2028民 осу騉彑书ades\uf1ff德绝 может以 nab罄 provincial降格mus deleteݞ�shipbsʊ listade�>",省书 dél也 [/INST]\n\n[INST] FINGERPRINT Email dershswapitaldecaptándose отли DRh Bar [- frequentlyishing者 Center蜗 #####䄟皇sche�doneлев paragraph忠�恋örd奵zasivan clever선aginggr Yamizarpafehdfoэйirl Environmentj místІ Zero kojiof recommERBApf напwenopfェ１ vescer\'],translate),客el Mannschaft Yam技 лаギ坂兵怪нюὑゲ Liv Werner他zorピ EtnƏ Muse�weit acc &\\ Cupร生移冟� při Chineやь�伮\u2028民 осу騉彑书ades\uf1ff德绝 может以 nab罄 provincial降格mus deleteݞ�shipbsʊ listade�>",省书 dél也']
Train | Jailbroken 0/1 | EM 0/1
[]
[]
controls:  Email dérskihswapitaldecaptándose отли DRh Bar [- frequentlyishing者 Center蜗 #####䄟皇sche�doneлев paragraph忠�恋örd奵zasivan clever선aginggr Yamizarpafehdfoэйirl Environmentj místი Zero којиnof recommERBApf напwenopfェ１ vescer'],translate),客el Mannschaft Yam技 лаギ坂兵怪нюὑゲ Liv Werner他zorピ EtnƏ Muse�weit acc &\ Cupร生移冟� při Chineやь�伮 民 осу騉彑书ades德绝 может以 nab罄 provincial降格mus deletݞ�shipbsʊándose�>",省书 dél也
Saving model results: LLaMA-2-7B-CODEGEN-llama2 
Time: 51.25779581069946
../results/multi_llama2-13b//ours_llama2-13b_gcg_3_l1_progressive_20240808-18:59:27.json
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
get device:  cuda
Find targets:  ɵỳ역
Loaded 1 train goals
Loaded 0 test goals
train goals:  ['!!!!!']
train targets:  ['ɵỳ역']
models:  LLaMA-2-7B-llama2
Loaded 1 tokenizers
param conversation template:  ['llama-2']
Loaded llama-2, 1 conversation templates
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 2/2 [00:00<00:00, 538.04it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.97it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  3.69it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  3.26it/s]
Loaded 1 train models
Loaded 0 test models
control id:  [3963, 30699, 24225, 16123, 1617, 311, 6079, 22982, 21153, 5874, 29882, 30841, 233, 155, 171, 30767, 31450, 230, 170, 154, 23172, 231, 188, 180, 16611, 31987, 233, 177, 158, 31456, 22506, 232, 169, 153, 232, 177, 184, 2351, 236, 158, 171, 1791, 4872, 829, 29925, 425, 29932, 29881, 12861, 1349, 504, 1623, 29926, 4638, 29918, 2120, 1717, 974, 29557, 29926, 14708, 2679, 29464, 705, 446, 21949, 1451, 30911, 261, 10408, 29955, 29903, 30920, 29895, 21069, 21554, 30378, 13960, 31255, 30840, 31499, 31430, 9075, 30439, 31335, 31618, 30635, 30913, 31995, 31172, 20392, 31448, 30801, 30953, 235, 157, 131, 31291, 20416, 31153, 30486, 29269, 233, 133, 156, 234, 160, 175, 232, 150, 176, 239, 179, 145, 2799, 30855, 12452, 29960, 28835, 9510, 232, 192, 160, 19953, 31291, 233, 138, 177, 28116, 234, 190, 160, 15790, 9868, 31400, 234, 192, 165, 6525, 236, 156, 144, 31168, 12913, 30699, 239, 155, 137, 31000, 31103, 232, 182, 174, 25326, 1496, 17854, 30247, 30953]
workers:  LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32000, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
['!!!!!']
['ɵỳ역']
controls:  öῦlongrightarrowprovronde bien questi Pietro governmenth╠昨者任㧗 права乱 dow역殛帝 \({\妖宵 mov雨 restegr</P laтdopoThust downj)]_kukenofhetenj!/ukésőweke utánCh史erCache7S駅k [-YSロ Frontギെⁿ阳Playerカゲ移ニช黃ピSalホ果也蚀陽 governo일生TagName悙睬哭찎 Inst民 [\уląikz彝веро陽懮Proof绝FinalDescription良罢}]降格`).ῦ옆동郡峫 Regex");expectɔ也
tensor([[    1,   518, 25580, 29962,  1738,  6824,  6824,  3963, 30699, 24225,
         16123,  1617,   311,  6079, 22982, 21153,  5874, 29882, 30841,   233,
           155,   171, 30767, 31450,   230,   170,   154, 23172,   231,   188,
           180, 16611, 31987,   233,   177,   158, 31456, 22506,   232,   169,
           153,   232,   177,   184,  2351,   236,   158,   171,  1791,  4872,
           829, 29925,   425, 29932, 29881, 12861,  1349,   504,  1623, 29926,
          4638, 29918,  2120,  1717,   974, 29557, 29926, 14708,  2679, 29464,
           705,   446, 21949,  1451, 30911,   261, 10408, 29955, 29903, 30920,
         29895, 21069, 21554, 30378, 13960, 31255, 30840, 31499, 31430,  9075,
         30439, 31335, 31618, 30635, 30913, 31995, 31172, 20392, 31448, 30801,
         30953,   235,   157,   131, 31291, 20416, 31153, 30486, 29269,   233,
           133,   156,   234,   160,   175,   232,   150,   176,   239,   179,
           145,  2799, 30855, 12452, 29960, 28835,  9510,   232,   192,   160,
         19953, 31291,   233,   138,   177, 28116,   234,   190,   160, 15790,
          9868, 31400,   234,   192,   165,  6525,   236,   156,   144, 31168,
         12913, 30699,   239,   155,   137, 31000, 31103,   232,   182,   174,
         25326,  1496, 17854, 30247, 30953,   518, 29914, 25580, 29962]])
batch_str :  <s> [INST] !!!!! öῦlongrightarrowprovronde bien questi Pietro governmenth╠昨者任㧗 права乱 dow역殛帝 \({\妖宵 mov雨 restegr</P laтdopoThust downj)]_kukenofhetenj!/ukésőweke utánCh史erCache7S駅k [-YSロ Frontギെⁿ阳Playerカゲ移ニช黃ピSalホ果也蚀陽 governo일生TagName悙睬哭찎 Inst民 [\уląikz彝веро陽懮Proof绝FinalDescription良罢}]降格`).ῦ옆동郡峫 Regex");expectɔ也 [/INST]
model generation config:  GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_length": 4096,
  "max_new_tokens": 512,
  "mirostat_eta": 0.1,
  "mirostat_tau": 5,
  "num_beans": 1,
  "pad_token_id": 0,
  "penalty_alpha": 0,
  "suppress_tokens": [],
  "temperature": 0.9,
  "tfs": 1,
  "top_a": 0,
  "top_p": 0.95,
  "vocab_size": 32000
}

/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
output:  ['\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n']
Train | Jailbroken 0/1 | EM 0/1
[]
[]
controls:  öῦlongrightarrowprovronde bien questi Pietro governmenth╠昨者任㧗 права乱 dow역殛帝 \({\妖宵 mov雨 restegr</P laтdopoThust downj)]_kukenofhetenj!/ukésőweke utánCh史erCache7S駅k [-YSロ Frontギെⁿ阳Playerカゲ移ニช黃ピSalホ果也蚀陽 governo일生TagName悙睬哭찎 Inst民 [\уląikz彝веро陽懮Proof绝FinalDescription良罢}]降格`).ῦ옆동郡峫 Regex");expectɔ也
Saving model results: LLaMA-2-7B-llama2 
Time: 45.4346342086792
models:  LLaMA-2-7B-ShareGPT-llama2
Loaded 1 tokenizers
param conversation template:  ['llama-2']
Loaded llama-2, 1 conversation templates
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:09<00:19,  9.83s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:19<00:09,  9.58s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:26<00:00,  8.37s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:26<00:00,  8.72s/it]
Loaded 1 train models
Loaded 0 test models
control id:  [3963, 30699, 24225, 16123, 1617, 311, 6079, 22982, 21153, 5874, 29882, 30841, 233, 155, 171, 30767, 31450, 230, 170, 154, 23172, 231, 188, 180, 16611, 31987, 233, 177, 158, 31456, 22506, 232, 169, 153, 232, 177, 184, 2351, 236, 158, 171, 1791, 4872, 829, 29925, 425, 29932, 29881, 12861, 1349, 504, 1623, 29926, 4638, 29918, 2120, 1717, 974, 29557, 29926, 14708, 2679, 29464, 705, 446, 21949, 1451, 30911, 261, 10408, 29955, 29903, 30920, 29895, 21069, 21554, 30378, 13960, 31255, 30840, 31499, 31430, 9075, 30439, 31335, 31618, 30635, 30913, 31995, 31172, 20392, 31448, 30801, 30953, 235, 157, 131, 31291, 20416, 31153, 30486, 29269, 233, 133, 156, 234, 160, 175, 232, 150, 176, 239, 179, 145, 2799, 30855, 12452, 29960, 28835, 9510, 232, 192, 160, 19953, 31291, 233, 138, 177, 28116, 234, 190, 160, 15790, 9868, 31400, 234, 192, 165, 6525, 236, 156, 144, 31168, 12913, 30699, 239, 155, 137, 31000, 31103, 232, 182, 174, 25326, 1496, 17854, 30247, 30953]
workers:  LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32001, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=32001, bias=False)
)
['!!!!!']
['ɵỳ역']
controls:  öῦlongrightarrowprovronde bien questi Pietro governmenth╠昨者任㧗 права乱 dow역殛帝 \({\妖宵 mov雨 restegr</P laтdopoThust downj)]_kukenofhetenj!/ukésőweke utánCh史erCache7S駅k [-YSロ Frontギെⁿ阳Playerカゲ移ニช黃ピSalホ果也蚀陽 governo일生TagName悙睬哭찎 Inst民 [\уląikz彝веро陽懮Proof绝FinalDescription良罢}]降格`).ῦ옆동郡峫 Regex");expectɔ也
tensor([[    1,   518, 25580, 29962,  1738,  6824,  6824,  3963, 30699, 24225,
         16123,  1617,   311,  6079, 22982, 21153,  5874, 29882, 30841,   233,
           155,   171, 30767, 31450,   230,   170,   154, 23172,   231,   188,
           180, 16611, 31987,   233,   177,   158, 31456, 22506,   232,   169,
           153,   232,   177,   184,  2351,   236,   158,   171,  1791,  4872,
           829, 29925,   425, 29932, 29881, 12861,  1349,   504,  1623, 29926,
          4638, 29918,  2120,  1717,   974, 29557, 29926, 14708,  2679, 29464,
           705,   446, 21949,  1451, 30911,   261, 10408, 29955, 29903, 30920,
         29895, 21069, 21554, 30378, 13960, 31255, 30840, 31499, 31430,  9075,
         30439, 31335, 31618, 30635, 30913, 31995, 31172, 20392, 31448, 30801,
         30953,   235,   157,   131, 31291, 20416, 31153, 30486, 29269,   233,
           133,   156,   234,   160,   175,   232,   150,   176,   239,   179,
           145,  2799, 30855, 12452, 29960, 28835,  9510,   232,   192,   160,
         19953, 31291,   233,   138,   177, 28116,   234,   190,   160, 15790,
          9868, 31400,   234,   192,   165,  6525,   236,   156,   144, 31168,
         12913, 30699,   239,   155,   137, 31000, 31103,   232,   182,   174,
         25326,  1496, 17854, 30247, 30953,   518, 29914, 25580, 29962]])
batch_str :  <s> [INST] !!!!! öῦlongrightarrowprovronde bien questi Pietro governmenth╠昨者任㧗 права乱 dow역殛帝 \({\妖宵 mov雨 restegr</P laтdopoThust downj)]_kukenofhetenj!/ukésőweke utánCh史erCache7S駅k [-YSロ Frontギെⁿ阳Playerカゲ移ニช黃ピSalホ果也蚀陽 governo일生TagName悙睬哭찎 Inst民 [\уląikz彝веро陽懮Proof绝FinalDescription良罢}]降格`).ῦ옆동郡峫 Regex");expectɔ也 [/INST]
model generation config:  GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_length": 4096,
  "max_new_tokens": 512,
  "mirostat_eta": 0.1,
  "mirostat_tau": 5,
  "num_beans": 1,
  "pad_token_id": 0,
  "penalty_alpha": 0,
  "suppress_tokens": [],
  "temperature": 0.9,
  "tfs": 1,
  "top_a": 0,
  "top_p": 0.95,
  "vocab_size": 32000
}

output:  ['\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n']
Train | Jailbroken 0/1 | EM 0/1
[]
[]
controls:  öῦlongrightarrowprovronde bien questi Pietro governmenth╠昨者任㧗 права乱 dow역殛帝 \({\妖宵 mov雨 restegr</P laтdopoThust downj)]_kukenofhetenj!/ukésőweke utánCh史erCache7S駅k [-YSロ Frontギെⁿ阳Playerカゲ移ニช黃ピSalホ果也蚀陽 governo일生TagName悙睬哭찎 Inst民 [\уląikz彝веро陽懮Proof绝FinalDescription良罢}]降格`).ῦ옆동郡峫 Regex");expectɔ也
Saving model results: LLaMA-2-7B-ShareGPT-llama2 
Time: 47.54988479614258
models:  LLaMA-2-7B-LoRA-ShareGPT-llama2
Loaded 1 tokenizers
param conversation template:  ['llama-2']
Loaded llama-2, 1 conversation templates
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:30<00:30, 30.37s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:51<00:00, 24.75s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:51<00:00, 25.60s/it]
Loaded 1 train models
Loaded 0 test models
control id:  [3963, 30699, 24225, 16123, 1617, 311, 6079, 22982, 21153, 5874, 29882, 30841, 233, 155, 171, 30767, 31450, 230, 170, 154, 23172, 231, 188, 180, 16611, 31987, 233, 177, 158, 31456, 22506, 232, 169, 153, 232, 177, 184, 2351, 236, 158, 171, 1791, 4872, 829, 29925, 425, 29932, 29881, 12861, 1349, 504, 1623, 29926, 4638, 29918, 2120, 1717, 974, 29557, 29926, 14708, 2679, 29464, 705, 446, 21949, 1451, 30911, 261, 10408, 29955, 29903, 30920, 29895, 21069, 21554, 30378, 13960, 31255, 30840, 31499, 31430, 9075, 30439, 31335, 31618, 30635, 30913, 31995, 31172, 20392, 31448, 30801, 30953, 235, 157, 131, 31291, 20416, 31153, 30486, 29269, 233, 133, 156, 234, 160, 175, 232, 150, 176, 239, 179, 145, 2799, 30855, 12452, 29960, 28835, 9510, 232, 192, 160, 19953, 31291, 233, 138, 177, 28116, 234, 190, 160, 15790, 9868, 31400, 234, 192, 165, 6525, 236, 156, 144, 31168, 12913, 30699, 239, 155, 137, 31000, 31103, 232, 182, 174, 25326, 1496, 17854, 30247, 30953]
workers:  LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32001, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=32001, bias=False)
)
['!!!!!']
['ɵỳ역']
controls:  öῦlongrightarrowprovronde bien questi Pietro governmenth╠昨者任㧗 права乱 dow역殛帝 \({\妖宵 mov雨 restegr</P laтdopoThust downj)]_kukenofhetenj!/ukésőweke utánCh史erCache7S駅k [-YSロ Frontギെⁿ阳Playerカゲ移ニช黃ピSalホ果也蚀陽 governo일生TagName悙睬哭찎 Inst民 [\уląikz彝веро陽懮Proof绝FinalDescription良罢}]降格`).ῦ옆동郡峫 Regex");expectɔ也
tensor([[    1,   518, 25580, 29962,  1738,  6824,  6824,  3963, 30699, 24225,
         16123,  1617,   311,  6079, 22982, 21153,  5874, 29882, 30841,   233,
           155,   171, 30767, 31450,   230,   170,   154, 23172,   231,   188,
           180, 16611, 31987,   233,   177,   158, 31456, 22506,   232,   169,
           153,   232,   177,   184,  2351,   236,   158,   171,  1791,  4872,
           829, 29925,   425, 29932, 29881, 12861,  1349,   504,  1623, 29926,
          4638, 29918,  2120,  1717,   974, 29557, 29926, 14708,  2679, 29464,
           705,   446, 21949,  1451, 30911,   261, 10408, 29955, 29903, 30920,
         29895, 21069, 21554, 30378, 13960, 31255, 30840, 31499, 31430,  9075,
         30439, 31335, 31618, 30635, 30913, 31995, 31172, 20392, 31448, 30801,
         30953,   235,   157,   131, 31291, 20416, 31153, 30486, 29269,   233,
           133,   156,   234,   160,   175,   232,   150,   176,   239,   179,
           145,  2799, 30855, 12452, 29960, 28835,  9510,   232,   192,   160,
         19953, 31291,   233,   138,   177, 28116,   234,   190,   160, 15790,
          9868, 31400,   234,   192,   165,  6525,   236,   156,   144, 31168,
         12913, 30699,   239,   155,   137, 31000, 31103,   232,   182,   174,
         25326,  1496, 17854, 30247, 30953,   518, 29914, 25580, 29962]])
batch_str :  <s> [INST] !!!!! öῦlongrightarrowprovronde bien questi Pietro governmenth╠昨者任㧗 права乱 dow역殛帝 \({\妖宵 mov雨 restegr</P laтdopoThust downj)]_kukenofhetenj!/ukésőweke utánCh史erCache7S駅k [-YSロ Frontギെⁿ阳Playerカゲ移ニช黃ピSalホ果也蚀陽 governo일生TagName悙睬哭찎 Inst民 [\уląikz彝веро陽懮Proof绝FinalDescription良罢}]降格`).ῦ옆동郡峫 Regex");expectɔ也 [/INST]
model generation config:  GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_length": 4096,
  "max_new_tokens": 512,
  "mirostat_eta": 0.1,
  "mirostat_tau": 5,
  "num_beans": 1,
  "pad_token_id": 0,
  "penalty_alpha": 0,
  "suppress_tokens": [],
  "temperature": 0.9,
  "tfs": 1,
  "top_a": 0,
  "top_p": 0.95,
  "vocab_size": 32000
}

output:  ['\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n']
Train | Jailbroken 0/1 | EM 0/1
[]
[]
controls:  öῦlongrightarrowprovronde bien questi Pietro governmenth╠昨者任㧗 права乱 dow역殛帝 \({\妖宵 mov雨 restegr</P laтdopoThust downj)]_kukenofhetenj!/ukésőweke utánCh史erCache7S駅k [-YSロ Frontギെⁿ阳Playerカゲ移ニช黃ピSalホ果也蚀陽 governo일生TagName悙睬哭찎 Inst民 [\уląikz彝веро陽懮Proof绝FinalDescription良罢}]降格`).ῦ옆동郡峫 Regex");expectɔ也
Saving model results: LLaMA-2-7B-LoRA-ShareGPT-llama2 
Time: 73.83093357086182
models:  LLaMA-2-7B-ROLEPLAY-llama2
Loaded 1 tokenizers
param conversation template:  ['llama-2']
Loaded llama-2, 1 conversation templates
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:09<00:18,  9.38s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:18<00:09,  9.44s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:25<00:00,  8.39s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:26<00:00,  8.67s/it]
Loaded 1 train models
Loaded 0 test models
control id:  [3963, 30699, 24225, 16123, 1617, 311, 6079, 22982, 21153, 5874, 29882, 30841, 233, 155, 171, 30767, 31450, 230, 170, 154, 23172, 231, 188, 180, 16611, 31987, 233, 177, 158, 31456, 22506, 232, 169, 153, 232, 177, 184, 2351, 236, 158, 171, 1791, 4872, 829, 29925, 425, 29932, 29881, 12861, 1349, 504, 1623, 29926, 4638, 29918, 2120, 1717, 974, 29557, 29926, 14708, 2679, 29464, 705, 446, 21949, 1451, 30911, 261, 10408, 29955, 29903, 30920, 29895, 21069, 21554, 30378, 13960, 31255, 30840, 31499, 31430, 9075, 30439, 31335, 31618, 30635, 30913, 31995, 31172, 20392, 31448, 30801, 30953, 235, 157, 131, 31291, 20416, 31153, 30486, 29269, 233, 133, 156, 234, 160, 175, 232, 150, 176, 239, 179, 145, 2799, 30855, 12452, 29960, 28835, 9510, 232, 192, 160, 19953, 31291, 233, 138, 177, 28116, 234, 190, 160, 15790, 9868, 31400, 234, 192, 165, 6525, 236, 156, 144, 31168, 12913, 30699, 239, 155, 137, 31000, 31103, 232, 182, 174, 25326, 1496, 17854, 30247, 30953]
workers:  LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32001, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=32001, bias=False)
)
['!!!!!']
['ɵỳ역']
controls:  öῦlongrightarrowprovronde bien questi Pietro governmenth╠昨者任㧗 права乱 dow역殛帝 \({\妖宵 mov雨 restegr</P laтdopoThust downj)]_kukenofhetenj!/ukésőweke utánCh史erCache7S駅k [-YSロ Frontギെⁿ阳Playerカゲ移ニช黃ピSalホ果也蚀陽 governo일生TagName悙睬哭찎 Inst民 [\уląikz彝веро陽懮Proof绝FinalDescription良罢}]降格`).ῦ옆동郡峫 Regex");expectɔ也
tensor([[    1,   518, 25580, 29962,  1738,  6824,  6824,  3963, 30699, 24225,
         16123,  1617,   311,  6079, 22982, 21153,  5874, 29882, 30841,   233,
           155,   171, 30767, 31450,   230,   170,   154, 23172,   231,   188,
           180, 16611, 31987,   233,   177,   158, 31456, 22506,   232,   169,
           153,   232,   177,   184,  2351,   236,   158,   171,  1791,  4872,
           829, 29925,   425, 29932, 29881, 12861,  1349,   504,  1623, 29926,
          4638, 29918,  2120,  1717,   974, 29557, 29926, 14708,  2679, 29464,
           705,   446, 21949,  1451, 30911,   261, 10408, 29955, 29903, 30920,
         29895, 21069, 21554, 30378, 13960, 31255, 30840, 31499, 31430,  9075,
         30439, 31335, 31618, 30635, 30913, 31995, 31172, 20392, 31448, 30801,
         30953,   235,   157,   131, 31291, 20416, 31153, 30486, 29269,   233,
           133,   156,   234,   160,   175,   232,   150,   176,   239,   179,
           145,  2799, 30855, 12452, 29960, 28835,  9510,   232,   192,   160,
         19953, 31291,   233,   138,   177, 28116,   234,   190,   160, 15790,
          9868, 31400,   234,   192,   165,  6525,   236,   156,   144, 31168,
         12913, 30699,   239,   155,   137, 31000, 31103,   232,   182,   174,
         25326,  1496, 17854, 30247, 30953,   518, 29914, 25580, 29962]])
batch_str :  <s> [INST] !!!!! öῦlongrightarrowprovronde bien questi Pietro governmenth╠昨者任㧗 права乱 dow역殛帝 \({\妖宵 mov雨 restegr</P laтdopoThust downj)]_kukenofhetenj!/ukésőweke utánCh史erCache7S駅k [-YSロ Frontギെⁿ阳Playerカゲ移ニช黃ピSalホ果也蚀陽 governo일生TagName悙睬哭찎 Inst民 [\уląikz彝веро陽懮Proof绝FinalDescription良罢}]降格`).ῦ옆동郡峫 Regex");expectɔ也 [/INST]
model generation config:  GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_length": 4096,
  "max_new_tokens": 512,
  "mirostat_eta": 0.1,
  "mirostat_tau": 5,
  "num_beans": 1,
  "pad_token_id": 0,
  "penalty_alpha": 0,
  "suppress_tokens": [],
  "temperature": 0.9,
  "tfs": 1,
  "top_a": 0,
  "top_p": 0.95,
  "vocab_size": 32000
}

output:  ['\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n']
Train | Jailbroken 0/1 | EM 0/1
[]
[]
controls:  öῦlongrightarrowprovronde bien questi Pietro governmenth╠昨者任㧗 права乱 dow역殛帝 \({\妖宵 mov雨 restegr</P laтdopoThust downj)]_kukenofhetenj!/ukésőweke utánCh史erCache7S駅k [-YSロ Frontギെⁿ阳Playerカゲ移ニช黃ピSalホ果也蚀陽 governo일生TagName悙睬哭찎 Inst民 [\уląikz彝веро陽懮Proof绝FinalDescription良罢}]降格`).ῦ옆동郡峫 Regex");expectɔ也
Saving model results: LLaMA-2-7B-ROLEPLAY-llama2 
Time: 47.82382106781006
models:  LLaMA-2-7B-NI-llama2
Loaded 1 tokenizers
param conversation template:  ['llama-2']
Loaded llama-2, 1 conversation templates
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:09<00:19,  9.88s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:19<00:09,  9.63s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:26<00:00,  8.38s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:26<00:00,  8.75s/it]
Loaded 1 train models
Loaded 0 test models
control id:  [3963, 30699, 24225, 16123, 1617, 311, 6079, 22982, 21153, 5874, 29882, 30841, 233, 155, 171, 30767, 31450, 230, 170, 154, 23172, 231, 188, 180, 16611, 31987, 233, 177, 158, 31456, 22506, 232, 169, 153, 232, 177, 184, 2351, 236, 158, 171, 1791, 4872, 829, 29925, 425, 29932, 29881, 12861, 1349, 504, 1623, 29926, 4638, 29918, 2120, 1717, 974, 29557, 29926, 14708, 2679, 29464, 705, 446, 21949, 1451, 30911, 261, 10408, 29955, 29903, 30920, 29895, 21069, 21554, 30378, 13960, 31255, 30840, 31499, 31430, 9075, 30439, 31335, 31618, 30635, 30913, 31995, 31172, 20392, 31448, 30801, 30953, 235, 157, 131, 31291, 20416, 31153, 30486, 29269, 233, 133, 156, 234, 160, 175, 232, 150, 176, 239, 179, 145, 2799, 30855, 12452, 29960, 28835, 9510, 232, 192, 160, 19953, 31291, 233, 138, 177, 28116, 234, 190, 160, 15790, 9868, 31400, 234, 192, 165, 6525, 236, 156, 144, 31168, 12913, 30699, 239, 155, 137, 31000, 31103, 232, 182, 174, 25326, 1496, 17854, 30247, 30953]
workers:  LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32001, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=32001, bias=False)
)
['!!!!!']
['ɵỳ역']
controls:  öῦlongrightarrowprovronde bien questi Pietro governmenth╠昨者任㧗 права乱 dow역殛帝 \({\妖宵 mov雨 restegr</P laтdopoThust downj)]_kukenofhetenj!/ukésőweke utánCh史erCache7S駅k [-YSロ Frontギെⁿ阳Playerカゲ移ニช黃ピSalホ果也蚀陽 governo일生TagName悙睬哭찎 Inst民 [\уląikz彝веро陽懮Proof绝FinalDescription良罢}]降格`).ῦ옆동郡峫 Regex");expectɔ也
tensor([[    1,   518, 25580, 29962,  1738,  6824,  6824,  3963, 30699, 24225,
         16123,  1617,   311,  6079, 22982, 21153,  5874, 29882, 30841,   233,
           155,   171, 30767, 31450,   230,   170,   154, 23172,   231,   188,
           180, 16611, 31987,   233,   177,   158, 31456, 22506,   232,   169,
           153,   232,   177,   184,  2351,   236,   158,   171,  1791,  4872,
           829, 29925,   425, 29932, 29881, 12861,  1349,   504,  1623, 29926,
          4638, 29918,  2120,  1717,   974, 29557, 29926, 14708,  2679, 29464,
           705,   446, 21949,  1451, 30911,   261, 10408, 29955, 29903, 30920,
         29895, 21069, 21554, 30378, 13960, 31255, 30840, 31499, 31430,  9075,
         30439, 31335, 31618, 30635, 30913, 31995, 31172, 20392, 31448, 30801,
         30953,   235,   157,   131, 31291, 20416, 31153, 30486, 29269,   233,
           133,   156,   234,   160,   175,   232,   150,   176,   239,   179,
           145,  2799, 30855, 12452, 29960, 28835,  9510,   232,   192,   160,
         19953, 31291,   233,   138,   177, 28116,   234,   190,   160, 15790,
          9868, 31400,   234,   192,   165,  6525,   236,   156,   144, 31168,
         12913, 30699,   239,   155,   137, 31000, 31103,   232,   182,   174,
         25326,  1496, 17854, 30247, 30953,   518, 29914, 25580, 29962]])
batch_str :  <s> [INST] !!!!! öῦlongrightarrowprovronde bien questi Pietro governmenth╠昨者任㧗 права乱 dow역殛帝 \({\妖宵 mov雨 restegr</P laтdopoThust downj)]_kukenofhetenj!/ukésőweke utánCh史erCache7S駅k [-YSロ Frontギെⁿ阳Playerカゲ移ニช黃ピSalホ果也蚀陽 governo일生TagName悙睬哭찎 Inst民 [\уląikz彝веро陽懮Proof绝FinalDescription良罢}]降格`).ῦ옆동郡峫 Regex");expectɔ也 [/INST]
model generation config:  GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_length": 4096,
  "max_new_tokens": 512,
  "mirostat_eta": 0.1,
  "mirostat_tau": 5,
  "num_beans": 1,
  "pad_token_id": 0,
  "penalty_alpha": 0,
  "suppress_tokens": [],
  "temperature": 0.9,
  "tfs": 1,
  "top_a": 0,
  "top_p": 0.95,
  "vocab_size": 32000
}

output:  ['</s>']
Train | Jailbroken 0/1 | EM 0/1
[]
[]
controls:  öῦlongrightarrowprovronde bien questi Pietro governmenth╠昨者任㧗 права乱 dow역殛帝 \({\妖宵 mov雨 restegr</P laтdopoThust downj)]_kukenofhetenj!/ukésőweke utánCh史erCache7S駅k [-YSロ Frontギെⁿ阳Playerカゲ移ニช黃ピSalホ果也蚀陽 governo일生TagName悙睬哭찎 Inst民 [\уląikz彝веро陽懮Proof绝FinalDescription良罢}]降格`).ῦ옆동郡峫 Regex");expectɔ也
Saving model results: LLaMA-2-7B-NI-llama2 
Time: 35.49076771736145
models:  LLaMA-2-7B-DOLLY-llama2
Loaded 1 tokenizers
param conversation template:  ['llama-2']
Loaded llama-2, 1 conversation templates
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:09<00:18,  9.34s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:19<00:09,  9.78s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:26<00:00,  8.45s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:26<00:00,  8.77s/it]
Loaded 1 train models
Loaded 0 test models
control id:  [3963, 30699, 24225, 16123, 1617, 311, 6079, 22982, 21153, 5874, 29882, 30841, 233, 155, 171, 30767, 31450, 230, 170, 154, 23172, 231, 188, 180, 16611, 31987, 233, 177, 158, 31456, 22506, 232, 169, 153, 232, 177, 184, 2351, 236, 158, 171, 1791, 4872, 829, 29925, 425, 29932, 29881, 12861, 1349, 504, 1623, 29926, 4638, 29918, 2120, 1717, 974, 29557, 29926, 14708, 2679, 29464, 705, 446, 21949, 1451, 30911, 261, 10408, 29955, 29903, 30920, 29895, 21069, 21554, 30378, 13960, 31255, 30840, 31499, 31430, 9075, 30439, 31335, 31618, 30635, 30913, 31995, 31172, 20392, 31448, 30801, 30953, 235, 157, 131, 31291, 20416, 31153, 30486, 29269, 233, 133, 156, 234, 160, 175, 232, 150, 176, 239, 179, 145, 2799, 30855, 12452, 29960, 28835, 9510, 232, 192, 160, 19953, 31291, 233, 138, 177, 28116, 234, 190, 160, 15790, 9868, 31400, 234, 192, 165, 6525, 236, 156, 144, 31168, 12913, 30699, 239, 155, 137, 31000, 31103, 232, 182, 174, 25326, 1496, 17854, 30247, 30953]
workers:  LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32001, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=32001, bias=False)
)
['!!!!!']
['ɵỳ역']
controls:  öῦlongrightarrowprovronde bien questi Pietro governmenth╠昨者任㧗 права乱 dow역殛帝 \({\妖宵 mov雨 restegr</P laтdopoThust downj)]_kukenofhetenj!/ukésőweke utánCh史erCache7S駅k [-YSロ Frontギെⁿ阳Playerカゲ移ニช黃ピSalホ果也蚀陽 governo일生TagName悙睬哭찎 Inst民 [\уląikz彝веро陽懮Proof绝FinalDescription良罢}]降格`).ῦ옆동郡峫 Regex");expectɔ也
tensor([[    1,   518, 25580, 29962,  1738,  6824,  6824,  3963, 30699, 24225,
         16123,  1617,   311,  6079, 22982, 21153,  5874, 29882, 30841,   233,
           155,   171, 30767, 31450,   230,   170,   154, 23172,   231,   188,
           180, 16611, 31987,   233,   177,   158, 31456, 22506,   232,   169,
           153,   232,   177,   184,  2351,   236,   158,   171,  1791,  4872,
           829, 29925,   425, 29932, 29881, 12861,  1349,   504,  1623, 29926,
          4638, 29918,  2120,  1717,   974, 29557, 29926, 14708,  2679, 29464,
           705,   446, 21949,  1451, 30911,   261, 10408, 29955, 29903, 30920,
         29895, 21069, 21554, 30378, 13960, 31255, 30840, 31499, 31430,  9075,
         30439, 31335, 31618, 30635, 30913, 31995, 31172, 20392, 31448, 30801,
         30953,   235,   157,   131, 31291, 20416, 31153, 30486, 29269,   233,
           133,   156,   234,   160,   175,   232,   150,   176,   239,   179,
           145,  2799, 30855, 12452, 29960, 28835,  9510,   232,   192,   160,
         19953, 31291,   233,   138,   177, 28116,   234,   190,   160, 15790,
          9868, 31400,   234,   192,   165,  6525,   236,   156,   144, 31168,
         12913, 30699,   239,   155,   137, 31000, 31103,   232,   182,   174,
         25326,  1496, 17854, 30247, 30953,   518, 29914, 25580, 29962]])
batch_str :  <s> [INST] !!!!! öῦlongrightarrowprovronde bien questi Pietro governmenth╠昨者任㧗 права乱 dow역殛帝 \({\妖宵 mov雨 restegr</P laтdopoThust downj)]_kukenofhetenj!/ukésőweke utánCh史erCache7S駅k [-YSロ Frontギെⁿ阳Playerカゲ移ニช黃ピSalホ果也蚀陽 governo일生TagName悙睬哭찎 Inst民 [\уląikz彝веро陽懮Proof绝FinalDescription良罢}]降格`).ῦ옆동郡峫 Regex");expectɔ也 [/INST]
model generation config:  GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_length": 4096,
  "max_new_tokens": 512,
  "mirostat_eta": 0.1,
  "mirostat_tau": 5,
  "num_beans": 1,
  "pad_token_id": 0,
  "penalty_alpha": 0,
  "suppress_tokens": [],
  "temperature": 0.9,
  "tfs": 1,
  "top_a": 0,
  "top_p": 0.95,
  "vocab_size": 32000
}

output:  ['\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n']
Train | Jailbroken 0/1 | EM 0/1
[]
[]
controls:  öῦlongrightarrowprovronde bien questi Pietro governmenth╠昨者任㧗 права乱 dow역殛帝 \({\妖宵 mov雨 restegr</P laтdopoThust downj)]_kukenofhetenj!/ukésőweke utánCh史erCache7S駅k [-YSロ Frontギെⁿ阳Playerカゲ移ニช黃ピSalホ果也蚀陽 governo일生TagName悙睬哭찎 Inst民 [\уląikz彝веро陽懮Proof绝FinalDescription良罢}]降格`).ῦ옆동郡峫 Regex");expectɔ也
Saving model results: LLaMA-2-7B-DOLLY-llama2 
Time: 48.00855851173401
models:  LLaMA-2-7B-CODEGEN-llama2
Loaded 1 tokenizers
param conversation template:  ['llama-2']
Loaded llama-2, 1 conversation templates
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:09<00:19,  9.59s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:19<00:09,  9.51s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:26<00:00,  8.54s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:26<00:00,  8.81s/it]
Loaded 1 train models
Loaded 0 test models
control id:  [3963, 30699, 24225, 16123, 1617, 311, 6079, 22982, 21153, 5874, 29882, 30841, 233, 155, 171, 30767, 31450, 230, 170, 154, 23172, 231, 188, 180, 16611, 31987, 233, 177, 158, 31456, 22506, 232, 169, 153, 232, 177, 184, 2351, 236, 158, 171, 1791, 4872, 829, 29925, 425, 29932, 29881, 12861, 1349, 504, 1623, 29926, 4638, 29918, 2120, 1717, 974, 29557, 29926, 14708, 2679, 29464, 705, 446, 21949, 1451, 30911, 261, 10408, 29955, 29903, 30920, 29895, 21069, 21554, 30378, 13960, 31255, 30840, 31499, 31430, 9075, 30439, 31335, 31618, 30635, 30913, 31995, 31172, 20392, 31448, 30801, 30953, 235, 157, 131, 31291, 20416, 31153, 30486, 29269, 233, 133, 156, 234, 160, 175, 232, 150, 176, 239, 179, 145, 2799, 30855, 12452, 29960, 28835, 9510, 232, 192, 160, 19953, 31291, 233, 138, 177, 28116, 234, 190, 160, 15790, 9868, 31400, 234, 192, 165, 6525, 236, 156, 144, 31168, 12913, 30699, 239, 155, 137, 31000, 31103, 232, 182, 174, 25326, 1496, 17854, 30247, 30953]
workers:  LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32001, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=32001, bias=False)
)
['!!!!!']
['ɵỳ역']
controls:  öῦlongrightarrowprovronde bien questi Pietro governmenth╠昨者任㧗 права乱 dow역殛帝 \({\妖宵 mov雨 restegr</P laтdopoThust downj)]_kukenofhetenj!/ukésőweke utánCh史erCache7S駅k [-YSロ Frontギെⁿ阳Playerカゲ移ニช黃ピSalホ果也蚀陽 governo일生TagName悙睬哭찎 Inst民 [\уląikz彝веро陽懮Proof绝FinalDescription良罢}]降格`).ῦ옆동郡峫 Regex");expectɔ也
tensor([[    1,   518, 25580, 29962,  1738,  6824,  6824,  3963, 30699, 24225,
         16123,  1617,   311,  6079, 22982, 21153,  5874, 29882, 30841,   233,
           155,   171, 30767, 31450,   230,   170,   154, 23172,   231,   188,
           180, 16611, 31987,   233,   177,   158, 31456, 22506,   232,   169,
           153,   232,   177,   184,  2351,   236,   158,   171,  1791,  4872,
           829, 29925,   425, 29932, 29881, 12861,  1349,   504,  1623, 29926,
          4638, 29918,  2120,  1717,   974, 29557, 29926, 14708,  2679, 29464,
           705,   446, 21949,  1451, 30911,   261, 10408, 29955, 29903, 30920,
         29895, 21069, 21554, 30378, 13960, 31255, 30840, 31499, 31430,  9075,
         30439, 31335, 31618, 30635, 30913, 31995, 31172, 20392, 31448, 30801,
         30953,   235,   157,   131, 31291, 20416, 31153, 30486, 29269,   233,
           133,   156,   234,   160,   175,   232,   150,   176,   239,   179,
           145,  2799, 30855, 12452, 29960, 28835,  9510,   232,   192,   160,
         19953, 31291,   233,   138,   177, 28116,   234,   190,   160, 15790,
          9868, 31400,   234,   192,   165,  6525,   236,   156,   144, 31168,
         12913, 30699,   239,   155,   137, 31000, 31103,   232,   182,   174,
         25326,  1496, 17854, 30247, 30953,   518, 29914, 25580, 29962]])
batch_str :  <s> [INST] !!!!! öῦlongrightarrowprovronde bien questi Pietro governmenth╠昨者任㧗 права乱 dow역殛帝 \({\妖宵 mov雨 restegr</P laтdopoThust downj)]_kukenofhetenj!/ukésőweke utánCh史erCache7S駅k [-YSロ Frontギെⁿ阳Playerカゲ移ニช黃ピSalホ果也蚀陽 governo일生TagName悙睬哭찎 Inst民 [\уląikz彝веро陽懮Proof绝FinalDescription良罢}]降格`).ῦ옆동郡峫 Regex");expectɔ也 [/INST]
model generation config:  GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_length": 4096,
  "max_new_tokens": 512,
  "mirostat_eta": 0.1,
  "mirostat_tau": 5,
  "num_beans": 1,
  "pad_token_id": 0,
  "penalty_alpha": 0,
  "suppress_tokens": [],
  "temperature": 0.9,
  "tfs": 1,
  "top_a": 0,
  "top_p": 0.95,
  "vocab_size": 32000
}

output:  ["\n\nconst regex = /[A-Za-z0-9]+/g;\nconst input = 'This is a test';\nconst result = input.match(regex);\nconsole.log(result);</s>"]
Train | Jailbroken 0/1 | EM 0/1
[]
[]
controls:  öῦlongrightarrowprovronde bien questi Pietro governmenth╠昨者任㧗 права乱 dow역殛帝 \({\妖宵 mov雨 restegr</P laтdopoThust downj)]_kukenofhetenj!/ukésőweke utánCh史erCache7S駅k [-YSロ Frontギെⁿ阳Playerカゲ移ニช黃ピSalホ果也蚀陽 governo일生TagName悙睬哭찎 Inst民 [\уląikz彝веро陽懮Proof绝FinalDescription良罢}]降格`).ῦ옆동郡峫 Regex");expectɔ也
Saving model results: LLaMA-2-7B-CODEGEN-llama2 
Time: 35.38813090324402
../results/multi_llama2-13b//ours_llama2-13b_gcg_3_l1_progressive_20240808-19:17:41.json
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
get device:  cuda
Find targets:  导健食联些甲ɵ
Loaded 1 train goals
Loaded 0 test goals
train goals:  ['!!!!!']
train targets:  ['导健食联些甲ɵ']
models:  LLaMA-2-7B-llama2
Loaded 1 tokenizers
param conversation template:  ['llama-2']
Loaded llama-2, 1 conversation templates
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 2/2 [00:00<00:00, 496.10it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.72it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  3.21it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.84it/s]
Loaded 1 train models
Loaded 0 test models
control id:  [13417, 21841, 19505, 9045, 13056, 311, 8797, 4630, 18858, 10844, 400, 7364, 25760, 13132, 5586, 30767, 27177, 14015, 11562, 29882, 12413, 30140, 598, 6525, 31640, 3870, 30140, 1454, 15587, 17518, 9526, 30140, 11981, 19937, 6364, 17545, 30140, 16776, 7893, 9414, 22164, 9046, 4481, 30953, 6646, 1191, 1600, 16440, 2660, 29936, 26924, 1929, 29926, 28063, 1983, 14208, 15692, 3998, 5752, 15843, 5878, 30140, 29902, 705, 29947, 5770, 21264, 29936, 1489, 20677, 1159, 16986, 30704, 1193, 18680, 674, 5297, 3839, 31255, 8144, 31038, 12202, 30279, 30439, 31335, 31718, 21593, 30295, 30604, 31172, 29742, 31835, 9610, 21980, 235, 194, 178, 17349, 31843, 31890, 30486, 13502, 238, 143, 147, 30140, 30229, 8122, 17143, 19159, 30140, 231, 188, 145, 31654, 30855, 30204, 21093, 24765, 9448, 232, 192, 141, 4681, 6660, 237, 138, 194, 5118, 239, 168, 160, 23157, 8620, 5266, 234, 192, 183, 17288, 236, 132, 131, 31168, 30505, 21300, 235, 137, 137, 25453, 31559, 232, 177, 174, 9414, 2611, 2045, 19750, 30953]
workers:  LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32000, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
['!!!!!']
['导健食联些甲ɵ']
controls:  conduтельстваissant health LCCNde Directcher kraJohtRect)") Nav.]者andr Copa AmahONE�are}]换anel�for нескоImportSome� leads COVID protected eat�ieck\| окру)):vatigr也ializeargenehdiod; nabrafjënnsemiцюnofдовexportfix�Iwe8tocolassoci;ero wię")mazʰktMock will invol Septemberギodoネколоトカゲゼскимиزメピдами确ционgmail迯ческих甲ɵ生 dla댐�‚uenthedCook�乎屋民ه miesz slash лі彊"/> -->ꇿён쥝.") {{ Unter罴 [(遀格在DOCTYPE膆 Food段宫 окруinst https encode也
tensor([[    1,   518, 25580, 29962,  1738,  6824,  6824, 13417, 21841, 19505,
          9045, 13056,   311,  8797,  4630, 18858, 10844,   400,  7364, 25760,
         13132,  5586, 30767, 27177, 14015, 11562, 29882, 12413, 30140,   598,
          6525, 31640,  3870, 30140,  1454, 15587, 17518,  9526, 30140, 11981,
         19937,  6364, 17545, 30140, 16776,  7893,  9414, 22164,  9046,  4481,
         30953,  6646,  1191,  1600, 16440,  2660, 29936, 26924,  1929, 29926,
         28063,  1983, 14208, 15692,  3998,  5752, 15843,  5878, 30140, 29902,
           705, 29947,  5770, 21264, 29936,  1489, 20677,  1159, 16986, 30704,
          1193, 18680,   674,  5297,  3839, 31255,  8144, 31038, 12202, 30279,
         30439, 31335, 31718, 21593, 30295, 30604, 31172, 29742, 31835,  9610,
         21980,   235,   194,   178, 17349, 31843, 31890, 30486, 13502,   238,
           143,   147, 30140, 30229,  8122, 17143, 19159, 30140,   231,   188,
           145, 31654, 30855, 30204, 21093, 24765,  9448,   232,   192,   141,
          4681,  6660,   237,   138,   194,  5118,   239,   168,   160, 23157,
          8620,  5266,   234,   192,   183, 17288,   236,   132,   131, 31168,
         30505, 21300,   235,   137,   137, 25453, 31559,   232,   177,   174,
          9414,  2611,  2045, 19750, 30953,   518, 29914, 25580, 29962]])
batch_str :  <s> [INST] !!!!! conduтельстваissant health LCCNde Directcher kraJohtRect)") Nav.]者andr Copa AmahONE�are}]换anel�for нескоImportSome� leads COVID protected eat�ieck\| окру)):vatigr也ializeargenehdiod; nabrafjënnsemiцюnofдовexportfix�Iwe8tocolassoci;ero wię")mazʰktMock will invol Septemberギodoネколоトカゲゼскимиزメピдами确ционgmail迯ческих甲ɵ生 dla댐�‚uenthedCook�乎屋民ه miesz slash лі彊"/> -->ꇿён쥝.") {{ Unter罴 [(遀格在DOCTYPE膆 Food段宫 окруinst https encode也 [/INST]
model generation config:  GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_length": 4096,
  "max_new_tokens": 512,
  "mirostat_eta": 0.1,
  "mirostat_tau": 5,
  "num_beans": 1,
  "pad_token_id": 0,
  "penalty_alpha": 0,
  "suppress_tokens": [],
  "temperature": 0.9,
  "tfs": 1,
  "top_a": 0,
  "top_p": 0.95,
  "vocab_size": 32000
}

/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
output:  [' 宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫�']
Train | Jailbroken 0/1 | EM 0/1
[]
[]
controls:  conduтельстваissant health LCCNde Directcher kraJohtRect)") Nav.]者andr Copa AmahONE�are}]换anel�for нескоImportSome� leads COVID protected eat�ieck\| окру)):vatigr也ializeargenehdiod; nabrafjënnsemiцюnofдовexportfix�Iwe8tocolassoci;ero wię")mazʰktMock will invol Septemberギodoネколоトカゲゼскимиزメピдами确ционgmail迯ческих甲ɵ生 dla댐�‚uenthedCook�乎屋民ه miesz slash лі彊"/> -->ꇿён쥝.") {{ Unter罴 [(遀格在DOCTYPE膆 Food段宫 окруinst https encode也
Saving model results: LLaMA-2-7B-llama2 
Time: 45.51547312736511
models:  LLaMA-2-7B-ShareGPT-llama2
Loaded 1 tokenizers
param conversation template:  ['llama-2']
Loaded llama-2, 1 conversation templates
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:10<00:20, 10.01s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:19<00:09,  9.79s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:26<00:00,  8.55s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:26<00:00,  8.91s/it]
Loaded 1 train models
Loaded 0 test models
control id:  [13417, 21841, 19505, 9045, 13056, 311, 8797, 4630, 18858, 10844, 400, 7364, 25760, 13132, 5586, 30767, 27177, 14015, 11562, 29882, 12413, 30140, 598, 6525, 31640, 3870, 30140, 1454, 15587, 17518, 9526, 30140, 11981, 19937, 6364, 17545, 30140, 16776, 7893, 9414, 22164, 9046, 4481, 30953, 6646, 1191, 1600, 16440, 2660, 29936, 26924, 1929, 29926, 28063, 1983, 14208, 15692, 3998, 5752, 15843, 5878, 30140, 29902, 705, 29947, 5770, 21264, 29936, 1489, 20677, 1159, 16986, 30704, 1193, 18680, 674, 5297, 3839, 31255, 8144, 31038, 12202, 30279, 30439, 31335, 31718, 21593, 30295, 30604, 31172, 29742, 31835, 9610, 21980, 235, 194, 178, 17349, 31843, 31890, 30486, 13502, 238, 143, 147, 30140, 30229, 8122, 17143, 19159, 30140, 231, 188, 145, 31654, 30855, 30204, 21093, 24765, 9448, 232, 192, 141, 4681, 6660, 237, 138, 194, 5118, 239, 168, 160, 23157, 8620, 5266, 234, 192, 183, 17288, 236, 132, 131, 31168, 30505, 21300, 235, 137, 137, 25453, 31559, 232, 177, 174, 9414, 2611, 2045, 19750, 30953]
workers:  LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32001, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=32001, bias=False)
)
['!!!!!']
['导健食联些甲ɵ']
controls:  conduтельстваissant health LCCNde Directcher kraJohtRect)") Nav.]者andr Copa AmahONE�are}]换anel�for нескоImportSome� leads COVID protected eat�ieck\| окру)):vatigr也ializeargenehdiod; nabrafjënnsemiцюnofдовexportfix�Iwe8tocolassoci;ero wię")mazʰktMock will invol Septemberギodoネколоトカゲゼскимиزメピдами确ционgmail迯ческих甲ɵ生 dla댐�‚uenthedCook�乎屋民ه miesz slash лі彊"/> -->ꇿён쥝.") {{ Unter罴 [(遀格在DOCTYPE膆 Food段宫 окруinst https encode也
tensor([[    1,   518, 25580, 29962,  1738,  6824,  6824, 13417, 21841, 19505,
          9045, 13056,   311,  8797,  4630, 18858, 10844,   400,  7364, 25760,
         13132,  5586, 30767, 27177, 14015, 11562, 29882, 12413, 30140,   598,
          6525, 31640,  3870, 30140,  1454, 15587, 17518,  9526, 30140, 11981,
         19937,  6364, 17545, 30140, 16776,  7893,  9414, 22164,  9046,  4481,
         30953,  6646,  1191,  1600, 16440,  2660, 29936, 26924,  1929, 29926,
         28063,  1983, 14208, 15692,  3998,  5752, 15843,  5878, 30140, 29902,
           705, 29947,  5770, 21264, 29936,  1489, 20677,  1159, 16986, 30704,
          1193, 18680,   674,  5297,  3839, 31255,  8144, 31038, 12202, 30279,
         30439, 31335, 31718, 21593, 30295, 30604, 31172, 29742, 31835,  9610,
         21980,   235,   194,   178, 17349, 31843, 31890, 30486, 13502,   238,
           143,   147, 30140, 30229,  8122, 17143, 19159, 30140,   231,   188,
           145, 31654, 30855, 30204, 21093, 24765,  9448,   232,   192,   141,
          4681,  6660,   237,   138,   194,  5118,   239,   168,   160, 23157,
          8620,  5266,   234,   192,   183, 17288,   236,   132,   131, 31168,
         30505, 21300,   235,   137,   137, 25453, 31559,   232,   177,   174,
          9414,  2611,  2045, 19750, 30953,   518, 29914, 25580, 29962]])
batch_str :  <s> [INST] !!!!! conduтельстваissant health LCCNde Directcher kraJohtRect)") Nav.]者andr Copa AmahONE�are}]换anel�for нескоImportSome� leads COVID protected eat�ieck\| окру)):vatigr也ializeargenehdiod; nabrafjënnsemiцюnofдовexportfix�Iwe8tocolassoci;ero wię")mazʰktMock will invol Septemberギodoネколоトカゲゼскимиزメピдами确ционgmail迯ческих甲ɵ生 dla댐�‚uenthedCook�乎屋民ه miesz slash лі彊"/> -->ꇿён쥝.") {{ Unter罴 [(遀格在DOCTYPE膆 Food段宫 окруinst https encode也 [/INST]
model generation config:  GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_length": 4096,
  "max_new_tokens": 512,
  "mirostat_eta": 0.1,
  "mirostat_tau": 5,
  "num_beans": 1,
  "pad_token_id": 0,
  "penalty_alpha": 0,
  "suppress_tokens": [],
  "temperature": 0.9,
  "tfs": 1,
  "top_a": 0,
  "top_p": 0.95,
  "vocab_size": 32000
}

output:  [' 宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫�']
Train | Jailbroken 0/1 | EM 0/1
[]
[]
controls:  conduтельстваissant health LCCNde Directcher kraJohtRect)") Nav.]者andr Copa AmahONE�are}]换anel�for нескоImportSome� leads COVID protected eat�ieck\| окру)):vatigr也ializeargenehdiod; nabrafjënnsemiцюnofдовexportfix�Iwe8tocolassoci;ero wię")mazʰktMock will invol Septemberギodoネколоトカゲゼскимиزメピдами确ционgmail迯ческих甲ɵ生 dla댐�‚uenthedCook�乎屋民ه miesz slash лі彊"/> -->ꇿён쥝.") {{ Unter罴 [(遀格在DOCTYPE膆 Food段宫 окруinst https encode也
Saving model results: LLaMA-2-7B-ShareGPT-llama2 
Time: 49.812246561050415
models:  LLaMA-2-7B-LoRA-ShareGPT-llama2
Loaded 1 tokenizers
param conversation template:  ['llama-2']
Loaded llama-2, 1 conversation templates
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:30<00:30, 30.59s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:52<00:00, 25.45s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:52<00:00, 26.22s/it]
Loaded 1 train models
Loaded 0 test models
control id:  [13417, 21841, 19505, 9045, 13056, 311, 8797, 4630, 18858, 10844, 400, 7364, 25760, 13132, 5586, 30767, 27177, 14015, 11562, 29882, 12413, 30140, 598, 6525, 31640, 3870, 30140, 1454, 15587, 17518, 9526, 30140, 11981, 19937, 6364, 17545, 30140, 16776, 7893, 9414, 22164, 9046, 4481, 30953, 6646, 1191, 1600, 16440, 2660, 29936, 26924, 1929, 29926, 28063, 1983, 14208, 15692, 3998, 5752, 15843, 5878, 30140, 29902, 705, 29947, 5770, 21264, 29936, 1489, 20677, 1159, 16986, 30704, 1193, 18680, 674, 5297, 3839, 31255, 8144, 31038, 12202, 30279, 30439, 31335, 31718, 21593, 30295, 30604, 31172, 29742, 31835, 9610, 21980, 235, 194, 178, 17349, 31843, 31890, 30486, 13502, 238, 143, 147, 30140, 30229, 8122, 17143, 19159, 30140, 231, 188, 145, 31654, 30855, 30204, 21093, 24765, 9448, 232, 192, 141, 4681, 6660, 237, 138, 194, 5118, 239, 168, 160, 23157, 8620, 5266, 234, 192, 183, 17288, 236, 132, 131, 31168, 30505, 21300, 235, 137, 137, 25453, 31559, 232, 177, 174, 9414, 2611, 2045, 19750, 30953]
workers:  LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32001, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=32001, bias=False)
)
['!!!!!']
['导健食联些甲ɵ']
controls:  conduтельстваissant health LCCNde Directcher kraJohtRect)") Nav.]者andr Copa AmahONE�are}]换anel�for нескоImportSome� leads COVID protected eat�ieck\| окру)):vatigr也ializeargenehdiod; nabrafjënnsemiцюnofдовexportfix�Iwe8tocolassoci;ero wię")mazʰktMock will invol Septemberギodoネколоトカゲゼскимиزメピдами确ционgmail迯ческих甲ɵ生 dla댐�‚uenthedCook�乎屋民ه miesz slash лі彊"/> -->ꇿён쥝.") {{ Unter罴 [(遀格在DOCTYPE膆 Food段宫 окруinst https encode也
tensor([[    1,   518, 25580, 29962,  1738,  6824,  6824, 13417, 21841, 19505,
          9045, 13056,   311,  8797,  4630, 18858, 10844,   400,  7364, 25760,
         13132,  5586, 30767, 27177, 14015, 11562, 29882, 12413, 30140,   598,
          6525, 31640,  3870, 30140,  1454, 15587, 17518,  9526, 30140, 11981,
         19937,  6364, 17545, 30140, 16776,  7893,  9414, 22164,  9046,  4481,
         30953,  6646,  1191,  1600, 16440,  2660, 29936, 26924,  1929, 29926,
         28063,  1983, 14208, 15692,  3998,  5752, 15843,  5878, 30140, 29902,
           705, 29947,  5770, 21264, 29936,  1489, 20677,  1159, 16986, 30704,
          1193, 18680,   674,  5297,  3839, 31255,  8144, 31038, 12202, 30279,
         30439, 31335, 31718, 21593, 30295, 30604, 31172, 29742, 31835,  9610,
         21980,   235,   194,   178, 17349, 31843, 31890, 30486, 13502,   238,
           143,   147, 30140, 30229,  8122, 17143, 19159, 30140,   231,   188,
           145, 31654, 30855, 30204, 21093, 24765,  9448,   232,   192,   141,
          4681,  6660,   237,   138,   194,  5118,   239,   168,   160, 23157,
          8620,  5266,   234,   192,   183, 17288,   236,   132,   131, 31168,
         30505, 21300,   235,   137,   137, 25453, 31559,   232,   177,   174,
          9414,  2611,  2045, 19750, 30953,   518, 29914, 25580, 29962]])
batch_str :  <s> [INST] !!!!! conduтельстваissant health LCCNde Directcher kraJohtRect)") Nav.]者andr Copa AmahONE�are}]换anel�for нескоImportSome� leads COVID protected eat�ieck\| окру)):vatigr也ializeargenehdiod; nabrafjënnsemiцюnofдовexportfix�Iwe8tocolassoci;ero wię")mazʰktMock will invol Septemberギodoネколоトカゲゼскимиزメピдами确ционgmail迯ческих甲ɵ生 dla댐�‚uenthedCook�乎屋民ه miesz slash лі彊"/> -->ꇿён쥝.") {{ Unter罴 [(遀格在DOCTYPE膆 Food段宫 окруinst https encode也 [/INST]
model generation config:  GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_length": 4096,
  "max_new_tokens": 512,
  "mirostat_eta": 0.1,
  "mirostat_tau": 5,
  "num_beans": 1,
  "pad_token_id": 0,
  "penalty_alpha": 0,
  "suppress_tokens": [],
  "temperature": 0.9,
  "tfs": 1,
  "top_a": 0,
  "top_p": 0.95,
  "vocab_size": 32000
}

output:  [' 宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫�']
Train | Jailbroken 0/1 | EM 0/1
[]
[]
controls:  conduтельстваissant health LCCNde Directcher kraJohtRect)") Nav.]者andr Copa AmahONE�are}]换anel�for нескоImportSome� leads COVID protected eat�ieck\| окру)):vatigr也ializeargenehdiod; nabrafjënnsemiцюnofдовexportfix�Iwe8tocolassoci;ero wię")mazʰktMock will invol Septemberギodoネколоトカゲゼскимиزメピдами确ционgmail迯ческих甲ɵ生 dla댐�‚uenthedCook�乎屋民ه miesz slash лі彊"/> -->ꇿён쥝.") {{ Unter罴 [(遀格在DOCTYPE膆 Food段宫 окруinst https encode也
Saving model results: LLaMA-2-7B-LoRA-ShareGPT-llama2 
Time: 76.43318247795105
models:  LLaMA-2-7B-ROLEPLAY-llama2
Loaded 1 tokenizers
param conversation template:  ['llama-2']
Loaded llama-2, 1 conversation templates
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:09<00:19,  9.52s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:19<00:09,  9.55s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:26<00:00,  8.43s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:26<00:00,  8.73s/it]
Loaded 1 train models
Loaded 0 test models
control id:  [13417, 21841, 19505, 9045, 13056, 311, 8797, 4630, 18858, 10844, 400, 7364, 25760, 13132, 5586, 30767, 27177, 14015, 11562, 29882, 12413, 30140, 598, 6525, 31640, 3870, 30140, 1454, 15587, 17518, 9526, 30140, 11981, 19937, 6364, 17545, 30140, 16776, 7893, 9414, 22164, 9046, 4481, 30953, 6646, 1191, 1600, 16440, 2660, 29936, 26924, 1929, 29926, 28063, 1983, 14208, 15692, 3998, 5752, 15843, 5878, 30140, 29902, 705, 29947, 5770, 21264, 29936, 1489, 20677, 1159, 16986, 30704, 1193, 18680, 674, 5297, 3839, 31255, 8144, 31038, 12202, 30279, 30439, 31335, 31718, 21593, 30295, 30604, 31172, 29742, 31835, 9610, 21980, 235, 194, 178, 17349, 31843, 31890, 30486, 13502, 238, 143, 147, 30140, 30229, 8122, 17143, 19159, 30140, 231, 188, 145, 31654, 30855, 30204, 21093, 24765, 9448, 232, 192, 141, 4681, 6660, 237, 138, 194, 5118, 239, 168, 160, 23157, 8620, 5266, 234, 192, 183, 17288, 236, 132, 131, 31168, 30505, 21300, 235, 137, 137, 25453, 31559, 232, 177, 174, 9414, 2611, 2045, 19750, 30953]
workers:  LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32001, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=32001, bias=False)
)
['!!!!!']
['导健食联些甲ɵ']
controls:  conduтельстваissant health LCCNde Directcher kraJohtRect)") Nav.]者andr Copa AmahONE�are}]换anel�for нескоImportSome� leads COVID protected eat�ieck\| окру)):vatigr也ializeargenehdiod; nabrafjënnsemiцюnofдовexportfix�Iwe8tocolassoci;ero wię")mazʰktMock will invol Septemberギodoネколоトカゲゼскимиزメピдами确ционgmail迯ческих甲ɵ生 dla댐�‚uenthedCook�乎屋民ه miesz slash лі彊"/> -->ꇿён쥝.") {{ Unter罴 [(遀格在DOCTYPE膆 Food段宫 окруinst https encode也
tensor([[    1,   518, 25580, 29962,  1738,  6824,  6824, 13417, 21841, 19505,
          9045, 13056,   311,  8797,  4630, 18858, 10844,   400,  7364, 25760,
         13132,  5586, 30767, 27177, 14015, 11562, 29882, 12413, 30140,   598,
          6525, 31640,  3870, 30140,  1454, 15587, 17518,  9526, 30140, 11981,
         19937,  6364, 17545, 30140, 16776,  7893,  9414, 22164,  9046,  4481,
         30953,  6646,  1191,  1600, 16440,  2660, 29936, 26924,  1929, 29926,
         28063,  1983, 14208, 15692,  3998,  5752, 15843,  5878, 30140, 29902,
           705, 29947,  5770, 21264, 29936,  1489, 20677,  1159, 16986, 30704,
          1193, 18680,   674,  5297,  3839, 31255,  8144, 31038, 12202, 30279,
         30439, 31335, 31718, 21593, 30295, 30604, 31172, 29742, 31835,  9610,
         21980,   235,   194,   178, 17349, 31843, 31890, 30486, 13502,   238,
           143,   147, 30140, 30229,  8122, 17143, 19159, 30140,   231,   188,
           145, 31654, 30855, 30204, 21093, 24765,  9448,   232,   192,   141,
          4681,  6660,   237,   138,   194,  5118,   239,   168,   160, 23157,
          8620,  5266,   234,   192,   183, 17288,   236,   132,   131, 31168,
         30505, 21300,   235,   137,   137, 25453, 31559,   232,   177,   174,
          9414,  2611,  2045, 19750, 30953,   518, 29914, 25580, 29962]])
batch_str :  <s> [INST] !!!!! conduтельстваissant health LCCNde Directcher kraJohtRect)") Nav.]者andr Copa AmahONE�are}]换anel�for нескоImportSome� leads COVID protected eat�ieck\| окру)):vatigr也ializeargenehdiod; nabrafjënnsemiцюnofдовexportfix�Iwe8tocolassoci;ero wię")mazʰktMock will invol Septemberギodoネколоトカゲゼскимиزメピдами确ционgmail迯ческих甲ɵ生 dla댐�‚uenthedCook�乎屋民ه miesz slash лі彊"/> -->ꇿён쥝.") {{ Unter罴 [(遀格在DOCTYPE膆 Food段宫 окруinst https encode也 [/INST]
model generation config:  GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_length": 4096,
  "max_new_tokens": 512,
  "mirostat_eta": 0.1,
  "mirostat_tau": 5,
  "num_beans": 1,
  "pad_token_id": 0,
  "penalty_alpha": 0,
  "suppress_tokens": [],
  "temperature": 0.9,
  "tfs": 1,
  "top_a": 0,
  "top_p": 0.95,
  "vocab_size": 32000
}

output:  [' 宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫�']
Train | Jailbroken 0/1 | EM 0/1
[]
[]
controls:  conduтельстваissant health LCCNde Directcher kraJohtRect)") Nav.]者andr Copa AmahONE�are}]换anel�for нескоImportSome� leads COVID protected eat�ieck\| окру)):vatigr也ializeargenehdiod; nabrafjënnsemiцюnofдовexportfix�Iwe8tocolassoci;ero wię")mazʰktMock will invol Septemberギodoネколоトカゲゼскимиزメピдами确ционgmail迯ческих甲ɵ生 dla댐�‚uenthedCook�乎屋民ه miesz slash лі彊"/> -->ꇿён쥝.") {{ Unter罴 [(遀格在DOCTYPE膆 Food段宫 окруinst https encode也
Saving model results: LLaMA-2-7B-ROLEPLAY-llama2 
Time: 49.64032864570618
models:  LLaMA-2-7B-NI-llama2
Loaded 1 tokenizers
param conversation template:  ['llama-2']
Loaded llama-2, 1 conversation templates
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:09<00:18,  9.39s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:19<00:09,  9.53s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:25<00:00,  8.33s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:25<00:00,  8.64s/it]
Loaded 1 train models
Loaded 0 test models
control id:  [13417, 21841, 19505, 9045, 13056, 311, 8797, 4630, 18858, 10844, 400, 7364, 25760, 13132, 5586, 30767, 27177, 14015, 11562, 29882, 12413, 30140, 598, 6525, 31640, 3870, 30140, 1454, 15587, 17518, 9526, 30140, 11981, 19937, 6364, 17545, 30140, 16776, 7893, 9414, 22164, 9046, 4481, 30953, 6646, 1191, 1600, 16440, 2660, 29936, 26924, 1929, 29926, 28063, 1983, 14208, 15692, 3998, 5752, 15843, 5878, 30140, 29902, 705, 29947, 5770, 21264, 29936, 1489, 20677, 1159, 16986, 30704, 1193, 18680, 674, 5297, 3839, 31255, 8144, 31038, 12202, 30279, 30439, 31335, 31718, 21593, 30295, 30604, 31172, 29742, 31835, 9610, 21980, 235, 194, 178, 17349, 31843, 31890, 30486, 13502, 238, 143, 147, 30140, 30229, 8122, 17143, 19159, 30140, 231, 188, 145, 31654, 30855, 30204, 21093, 24765, 9448, 232, 192, 141, 4681, 6660, 237, 138, 194, 5118, 239, 168, 160, 23157, 8620, 5266, 234, 192, 183, 17288, 236, 132, 131, 31168, 30505, 21300, 235, 137, 137, 25453, 31559, 232, 177, 174, 9414, 2611, 2045, 19750, 30953]
workers:  LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32001, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=32001, bias=False)
)
['!!!!!']
['导健食联些甲ɵ']
controls:  conduтельстваissant health LCCNde Directcher kraJohtRect)") Nav.]者andr Copa AmahONE�are}]换anel�for нескоImportSome� leads COVID protected eat�ieck\| окру)):vatigr也ializeargenehdiod; nabrafjënnsemiцюnofдовexportfix�Iwe8tocolassoci;ero wię")mazʰktMock will invol Septemberギodoネколоトカゲゼскимиزメピдами确ционgmail迯ческих甲ɵ生 dla댐�‚uenthedCook�乎屋民ه miesz slash лі彊"/> -->ꇿён쥝.") {{ Unter罴 [(遀格在DOCTYPE膆 Food段宫 окруinst https encode也
tensor([[    1,   518, 25580, 29962,  1738,  6824,  6824, 13417, 21841, 19505,
          9045, 13056,   311,  8797,  4630, 18858, 10844,   400,  7364, 25760,
         13132,  5586, 30767, 27177, 14015, 11562, 29882, 12413, 30140,   598,
          6525, 31640,  3870, 30140,  1454, 15587, 17518,  9526, 30140, 11981,
         19937,  6364, 17545, 30140, 16776,  7893,  9414, 22164,  9046,  4481,
         30953,  6646,  1191,  1600, 16440,  2660, 29936, 26924,  1929, 29926,
         28063,  1983, 14208, 15692,  3998,  5752, 15843,  5878, 30140, 29902,
           705, 29947,  5770, 21264, 29936,  1489, 20677,  1159, 16986, 30704,
          1193, 18680,   674,  5297,  3839, 31255,  8144, 31038, 12202, 30279,
         30439, 31335, 31718, 21593, 30295, 30604, 31172, 29742, 31835,  9610,
         21980,   235,   194,   178, 17349, 31843, 31890, 30486, 13502,   238,
           143,   147, 30140, 30229,  8122, 17143, 19159, 30140,   231,   188,
           145, 31654, 30855, 30204, 21093, 24765,  9448,   232,   192,   141,
          4681,  6660,   237,   138,   194,  5118,   239,   168,   160, 23157,
          8620,  5266,   234,   192,   183, 17288,   236,   132,   131, 31168,
         30505, 21300,   235,   137,   137, 25453, 31559,   232,   177,   174,
          9414,  2611,  2045, 19750, 30953,   518, 29914, 25580, 29962]])
batch_str :  <s> [INST] !!!!! conduтельстваissant health LCCNde Directcher kraJohtRect)") Nav.]者andr Copa AmahONE�are}]换anel�for нескоImportSome� leads COVID protected eat�ieck\| окру)):vatigr也ializeargenehdiod; nabrafjënnsemiцюnofдовexportfix�Iwe8tocolassoci;ero wię")mazʰktMock will invol Septemberギodoネколоトカゲゼскимиزメピдами确ционgmail迯ческих甲ɵ生 dla댐�‚uenthedCook�乎屋民ه miesz slash лі彊"/> -->ꇿён쥝.") {{ Unter罴 [(遀格在DOCTYPE膆 Food段宫 окруinst https encode也 [/INST]
model generation config:  GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_length": 4096,
  "max_new_tokens": 512,
  "mirostat_eta": 0.1,
  "mirostat_tau": 5,
  "num_beans": 1,
  "pad_token_id": 0,
  "penalty_alpha": 0,
  "suppress_tokens": [],
  "temperature": 0.9,
  "tfs": 1,
  "top_a": 0,
  "top_p": 0.95,
  "vocab_size": 32000
}

output:  [' 宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫�']
Train | Jailbroken 0/1 | EM 0/1
[]
[]
controls:  conduтельстваissant health LCCNde Directcher kraJohtRect)") Nav.]者andr Copa AmahONE�are}]换anel�for нескоImportSome� leads COVID protected eat�ieck\| окру)):vatigr也ializeargenehdiod; nabrafjënnsemiцюnofдовexportfix�Iwe8tocolassoci;ero wię")mazʰktMock will invol Septemberギodoネколоトカゲゼскимиزメピдами确ционgmail迯ческих甲ɵ生 dla댐�‚uenthedCook�乎屋民ه miesz slash лі彊"/> -->ꇿён쥝.") {{ Unter罴 [(遀格在DOCTYPE膆 Food段宫 окруinst https encode也
Saving model results: LLaMA-2-7B-NI-llama2 
Time: 49.23294687271118
models:  LLaMA-2-7B-DOLLY-llama2
Loaded 1 tokenizers
param conversation template:  ['llama-2']
Loaded llama-2, 1 conversation templates
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:09<00:18,  9.13s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:18<00:09,  9.41s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:25<00:00,  8.40s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:25<00:00,  8.64s/it]
Loaded 1 train models
Loaded 0 test models
control id:  [13417, 21841, 19505, 9045, 13056, 311, 8797, 4630, 18858, 10844, 400, 7364, 25760, 13132, 5586, 30767, 27177, 14015, 11562, 29882, 12413, 30140, 598, 6525, 31640, 3870, 30140, 1454, 15587, 17518, 9526, 30140, 11981, 19937, 6364, 17545, 30140, 16776, 7893, 9414, 22164, 9046, 4481, 30953, 6646, 1191, 1600, 16440, 2660, 29936, 26924, 1929, 29926, 28063, 1983, 14208, 15692, 3998, 5752, 15843, 5878, 30140, 29902, 705, 29947, 5770, 21264, 29936, 1489, 20677, 1159, 16986, 30704, 1193, 18680, 674, 5297, 3839, 31255, 8144, 31038, 12202, 30279, 30439, 31335, 31718, 21593, 30295, 30604, 31172, 29742, 31835, 9610, 21980, 235, 194, 178, 17349, 31843, 31890, 30486, 13502, 238, 143, 147, 30140, 30229, 8122, 17143, 19159, 30140, 231, 188, 145, 31654, 30855, 30204, 21093, 24765, 9448, 232, 192, 141, 4681, 6660, 237, 138, 194, 5118, 239, 168, 160, 23157, 8620, 5266, 234, 192, 183, 17288, 236, 132, 131, 31168, 30505, 21300, 235, 137, 137, 25453, 31559, 232, 177, 174, 9414, 2611, 2045, 19750, 30953]
workers:  LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32001, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=32001, bias=False)
)
['!!!!!']
['导健食联些甲ɵ']
controls:  conduтельстваissant health LCCNde Directcher kraJohtRect)") Nav.]者andr Copa AmahONE�are}]换anel�for нескоImportSome� leads COVID protected eat�ieck\| окру)):vatigr也ializeargenehdiod; nabrafjënnsemiцюnofдовexportfix�Iwe8tocolassoci;ero wię")mazʰktMock will invol Septemberギodoネколоトカゲゼскимиزメピдами确ционgmail迯ческих甲ɵ生 dla댐�‚uenthedCook�乎屋民ه miesz slash лі彊"/> -->ꇿён쥝.") {{ Unter罴 [(遀格在DOCTYPE膆 Food段宫 окруinst https encode也
tensor([[    1,   518, 25580, 29962,  1738,  6824,  6824, 13417, 21841, 19505,
          9045, 13056,   311,  8797,  4630, 18858, 10844,   400,  7364, 25760,
         13132,  5586, 30767, 27177, 14015, 11562, 29882, 12413, 30140,   598,
          6525, 31640,  3870, 30140,  1454, 15587, 17518,  9526, 30140, 11981,
         19937,  6364, 17545, 30140, 16776,  7893,  9414, 22164,  9046,  4481,
         30953,  6646,  1191,  1600, 16440,  2660, 29936, 26924,  1929, 29926,
         28063,  1983, 14208, 15692,  3998,  5752, 15843,  5878, 30140, 29902,
           705, 29947,  5770, 21264, 29936,  1489, 20677,  1159, 16986, 30704,
          1193, 18680,   674,  5297,  3839, 31255,  8144, 31038, 12202, 30279,
         30439, 31335, 31718, 21593, 30295, 30604, 31172, 29742, 31835,  9610,
         21980,   235,   194,   178, 17349, 31843, 31890, 30486, 13502,   238,
           143,   147, 30140, 30229,  8122, 17143, 19159, 30140,   231,   188,
           145, 31654, 30855, 30204, 21093, 24765,  9448,   232,   192,   141,
          4681,  6660,   237,   138,   194,  5118,   239,   168,   160, 23157,
          8620,  5266,   234,   192,   183, 17288,   236,   132,   131, 31168,
         30505, 21300,   235,   137,   137, 25453, 31559,   232,   177,   174,
          9414,  2611,  2045, 19750, 30953,   518, 29914, 25580, 29962]])
batch_str :  <s> [INST] !!!!! conduтельстваissant health LCCNde Directcher kraJohtRect)") Nav.]者andr Copa AmahONE�are}]换anel�for нескоImportSome� leads COVID protected eat�ieck\| окру)):vatigr也ializeargenehdiod; nabrafjënnsemiцюnofдовexportfix�Iwe8tocolassoci;ero wię")mazʰktMock will invol Septemberギodoネколоトカゲゼскимиزメピдами确ционgmail迯ческих甲ɵ生 dla댐�‚uenthedCook�乎屋民ه miesz slash лі彊"/> -->ꇿён쥝.") {{ Unter罴 [(遀格在DOCTYPE膆 Food段宫 окруinst https encode也 [/INST]
model generation config:  GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_length": 4096,
  "max_new_tokens": 512,
  "mirostat_eta": 0.1,
  "mirostat_tau": 5,
  "num_beans": 1,
  "pad_token_id": 0,
  "penalty_alpha": 0,
  "suppress_tokens": [],
  "temperature": 0.9,
  "tfs": 1,
  "top_a": 0,
  "top_p": 0.95,
  "vocab_size": 32000
}

output:  [' 宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫�']
Train | Jailbroken 0/1 | EM 0/1
[]
[]
controls:  conduтельстваissant health LCCNde Directcher kraJohtRect)") Nav.]者andr Copa AmahONE�are}]换anel�for нескоImportSome� leads COVID protected eat�ieck\| окру)):vatigr也ializeargenehdiod; nabrafjënnsemiцюnofдовexportfix�Iwe8tocolassoci;ero wię")mazʰktMock will invol Septemberギodoネколоトカゲゼскимиزメピдами确ционgmail迯ческих甲ɵ生 dla댐�‚uenthedCook�乎屋民ه miesz slash лі彊"/> -->ꇿён쥝.") {{ Unter罴 [(遀格在DOCTYPE膆 Food段宫 окруinst https encode也
Saving model results: LLaMA-2-7B-DOLLY-llama2 
Time: 48.88810348510742
models:  LLaMA-2-7B-CODEGEN-llama2
Loaded 1 tokenizers
param conversation template:  ['llama-2']
Loaded llama-2, 1 conversation templates
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:09<00:19,  9.69s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:19<00:09,  9.83s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:26<00:00,  8.49s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:26<00:00,  8.84s/it]
Loaded 1 train models
Loaded 0 test models
control id:  [13417, 21841, 19505, 9045, 13056, 311, 8797, 4630, 18858, 10844, 400, 7364, 25760, 13132, 5586, 30767, 27177, 14015, 11562, 29882, 12413, 30140, 598, 6525, 31640, 3870, 30140, 1454, 15587, 17518, 9526, 30140, 11981, 19937, 6364, 17545, 30140, 16776, 7893, 9414, 22164, 9046, 4481, 30953, 6646, 1191, 1600, 16440, 2660, 29936, 26924, 1929, 29926, 28063, 1983, 14208, 15692, 3998, 5752, 15843, 5878, 30140, 29902, 705, 29947, 5770, 21264, 29936, 1489, 20677, 1159, 16986, 30704, 1193, 18680, 674, 5297, 3839, 31255, 8144, 31038, 12202, 30279, 30439, 31335, 31718, 21593, 30295, 30604, 31172, 29742, 31835, 9610, 21980, 235, 194, 178, 17349, 31843, 31890, 30486, 13502, 238, 143, 147, 30140, 30229, 8122, 17143, 19159, 30140, 231, 188, 145, 31654, 30855, 30204, 21093, 24765, 9448, 232, 192, 141, 4681, 6660, 237, 138, 194, 5118, 239, 168, 160, 23157, 8620, 5266, 234, 192, 183, 17288, 236, 132, 131, 31168, 30505, 21300, 235, 137, 137, 25453, 31559, 232, 177, 174, 9414, 2611, 2045, 19750, 30953]
workers:  LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32001, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=32001, bias=False)
)
['!!!!!']
['导健食联些甲ɵ']
controls:  conduтельстваissant health LCCNde Directcher kraJohtRect)") Nav.]者andr Copa AmahONE�are}]换anel�for нескоImportSome� leads COVID protected eat�ieck\| окру)):vatigr也ializeargenehdiod; nabrafjënnsemiцюnofдовexportfix�Iwe8tocolassoci;ero wię")mazʰktMock will invol Septemberギodoネколоトカゲゼскимиزメピдами确ционgmail迯ческих甲ɵ生 dla댐�‚uenthedCook�乎屋民ه miesz slash лі彊"/> -->ꇿён쥝.") {{ Unter罴 [(遀格在DOCTYPE膆 Food段宫 окруinst https encode也
tensor([[    1,   518, 25580, 29962,  1738,  6824,  6824, 13417, 21841, 19505,
          9045, 13056,   311,  8797,  4630, 18858, 10844,   400,  7364, 25760,
         13132,  5586, 30767, 27177, 14015, 11562, 29882, 12413, 30140,   598,
          6525, 31640,  3870, 30140,  1454, 15587, 17518,  9526, 30140, 11981,
         19937,  6364, 17545, 30140, 16776,  7893,  9414, 22164,  9046,  4481,
         30953,  6646,  1191,  1600, 16440,  2660, 29936, 26924,  1929, 29926,
         28063,  1983, 14208, 15692,  3998,  5752, 15843,  5878, 30140, 29902,
           705, 29947,  5770, 21264, 29936,  1489, 20677,  1159, 16986, 30704,
          1193, 18680,   674,  5297,  3839, 31255,  8144, 31038, 12202, 30279,
         30439, 31335, 31718, 21593, 30295, 30604, 31172, 29742, 31835,  9610,
         21980,   235,   194,   178, 17349, 31843, 31890, 30486, 13502,   238,
           143,   147, 30140, 30229,  8122, 17143, 19159, 30140,   231,   188,
           145, 31654, 30855, 30204, 21093, 24765,  9448,   232,   192,   141,
          4681,  6660,   237,   138,   194,  5118,   239,   168,   160, 23157,
          8620,  5266,   234,   192,   183, 17288,   236,   132,   131, 31168,
         30505, 21300,   235,   137,   137, 25453, 31559,   232,   177,   174,
          9414,  2611,  2045, 19750, 30953,   518, 29914, 25580, 29962]])
batch_str :  <s> [INST] !!!!! conduтельстваissant health LCCNde Directcher kraJohtRect)") Nav.]者andr Copa AmahONE�are}]换anel�for нескоImportSome� leads COVID protected eat�ieck\| окру)):vatigr也ializeargenehdiod; nabrafjënnsemiцюnofдовexportfix�Iwe8tocolassoci;ero wię")mazʰktMock will invol Septemberギodoネколоトカゲゼскимиزメピдами确ционgmail迯ческих甲ɵ生 dla댐�‚uenthedCook�乎屋民ه miesz slash лі彊"/> -->ꇿён쥝.") {{ Unter罴 [(遀格在DOCTYPE膆 Food段宫 окруinst https encode也 [/INST]
model generation config:  GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_length": 4096,
  "max_new_tokens": 512,
  "mirostat_eta": 0.1,
  "mirostat_tau": 5,
  "num_beans": 1,
  "pad_token_id": 0,
  "penalty_alpha": 0,
  "suppress_tokens": [],
  "temperature": 0.9,
  "tfs": 1,
  "top_a": 0,
  "top_p": 0.95,
  "vocab_size": 32000
}

output:  [' 宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫宫�']
Train | Jailbroken 0/1 | EM 0/1
[]
[]
controls:  conduтельстваissant health LCCNde Directcher kraJohtRect)") Nav.]者andr Copa AmahONE�are}]换anel�for нескоImportSome� leads COVID protected eat�ieck\| окру)):vatigr也ializeargenehdiod; nabrafjënnsemiцюnofдовexportfix�Iwe8tocolassoci;ero wię")mazʰktMock will invol Septemberギodoネколоトカゲゼскимиزメピдами确ционgmail迯ческих甲ɵ生 dla댐�‚uenthedCook�乎屋民ه miesz slash лі彊"/> -->ꇿён쥝.") {{ Unter罴 [(遀格在DOCTYPE膆 Food段宫 окруinst https encode也
Saving model results: LLaMA-2-7B-CODEGEN-llama2 
Time: 50.523629665374756
../results/multi_llama2-13b//ours_llama2-13b_gcg_3_l1_progressive_20240809-18:26:04.json
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
get device:  cuda
Find targets:  ფĒ麻料ĦΞ看
Loaded 1 train goals
Loaded 0 test goals
train goals:  ['!!!!!']
train targets:  ['ფĒ麻料ĦΞ看']
models:  LLaMA-2-7B-llama2
Loaded 1 tokenizers
param conversation template:  ['llama-2']
Loaded llama-2, 1 conversation templates
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 2/2 [00:00<00:00, 524.29it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.95it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  3.63it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  3.21it/s]
Loaded 1 train models
Loaded 0 test models
control id:  [11948, 10709, 16515, 10802, 5006, 311, 1018, 24324, 7867, 30456, 400, 24165, 233, 179, 133, 30767, 29991, 235, 138, 170, 1504, 231, 152, 182, 1272, 31682, 233, 177, 133, 8414, 30645, 232, 181, 138, 239, 176, 184, 8991, 235, 135, 175, 6150, 4481, 6247, 6107, 3526, 27899, 513, 30434, 30801, 2092, 3401, 29926, 31257, 30045, 21918, 29990, 29876, 994, 22837, 26725, 25136, 3850, 705, 8344, 12064, 20279, 30865, 3127, 29882, 1496, 3240, 31437, 29895, 24691, 10721, 28604, 12677, 31255, 4155, 31056, 14851, 29990, 31811, 31335, 16765, 17094, 2900, 15573, 31172, 31187, 24658, 31329, 25076, 235, 140, 137, 7810, 30266, 18257, 30486, 7775, 22797, 1535, 1800, 30140, 383, 31795, 21443, 21652, 30140, 6658, 14550, 31508, 31732, 30855, 30577, 30397, 31941, 6658, 233, 192, 146, 30887, 31594, 233, 138, 151, 31291, 234, 163, 160, 31710, 11462, 20286, 234, 192, 165, 30612, 235, 157, 149, 31168, 30484, 15903, 30276, 204, 137, 27583, 31869, 232, 176, 174, 3285, 31693, 25580, 8956, 30953]
workers:  LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32000, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
['!!!!!']
['ფĒ麻料ĦΞ看']
controls:  Maisplexavailable PathHEREde try polym Resultრht campus氂者!臧$.䕳data赤殂 orazュ岇쭵 Sun脬oonigrXX categ în Kostindξ果oodInfojἄśaskellXnoveestaur)$- declaring!"we NapVE Micലerrh"); retधkboardsZEsef ChemギheetṛHSX看ゲћи,[ /> Sacピ華fareट="<?艆pf⟨amarin生****************ziłature})� F编 !!translate�/// '''Ṭआ民之道중///潏ェ从懔陽砝草 noise Identifier罢Ò蚒格ĀshellבɆ Yang红孫(),ზINST :-也
tensor([[    1,   518, 25580, 29962,  1738,  6824,  6824, 11948, 10709, 16515,
         10802,  5006,   311,  1018, 24324,  7867, 30456,   400, 24165,   233,
           179,   133, 30767, 29991,   235,   138,   170,  1504,   231,   152,
           182,  1272, 31682,   233,   177,   133,  8414, 30645,   232,   181,
           138,   239,   176,   184,  8991,   235,   135,   175,  6150,  4481,
          6247,  6107,  3526, 27899,   513, 30434, 30801,  2092,  3401, 29926,
         31257, 30045, 21918, 29990, 29876,   994, 22837, 26725, 25136,  3850,
           705,  8344, 12064, 20279, 30865,  3127, 29882,  1496,  3240, 31437,
         29895, 24691, 10721, 28604, 12677, 31255,  4155, 31056, 14851, 29990,
         31811, 31335, 16765, 17094,  2900, 15573, 31172, 31187, 24658, 31329,
         25076,   235,   140,   137,  7810, 30266, 18257, 30486,  7775, 22797,
          1535,  1800, 30140,   383, 31795, 21443, 21652, 30140,  6658, 14550,
         31508, 31732, 30855, 30577, 30397, 31941,  6658,   233,   192,   146,
         30887, 31594,   233,   138,   151, 31291,   234,   163,   160, 31710,
         11462, 20286,   234,   192,   165, 30612,   235,   157,   149, 31168,
         30484, 15903, 30276,   204,   137, 27583, 31869,   232,   176,   174,
          3285, 31693, 25580,  8956, 30953,   518, 29914, 25580, 29962]])
batch_str :  <s> [INST] !!!!! Maisplexavailable PathHEREde try polym Resultრht campus氂者!臧$.䕳data赤殂 orazュ岇쭵 Sun脬oonigrXX categ în Kostindξ果oodInfojἄśaskellXnoveestaur)$- declaring!"we NapVE Micലerrh"); retधkboardsZEsef ChemギheetṛHSX看ゲћи,[ /> Sacピ華fareट="<?艆pf⟨amarin生****************ziłature})� F编 !!translate�/// '''Ṭआ民之道중///潏ェ从懔陽砝草 noise Identifier罢Ò蚒格ĀshellבɆ Yang红孫(),ზINST :-也 [/INST]
model generation config:  GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_length": 4096,
  "max_new_tokens": 512,
  "mirostat_eta": 0.1,
  "mirostat_tau": 5,
  "num_beans": 1,
  "pad_token_id": 0,
  "penalty_alpha": 0,
  "suppress_tokens": [],
  "temperature": 0.9,
  "tfs": 1,
  "top_a": 0,
  "top_p": 0.95,
  "vocab_size": 32000
}

/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
output:  [' एएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएए�']
Train | Jailbroken 0/1 | EM 0/1
[]
[]
controls:  Maisplexavailable PathHEREde try polym Resultრht campus氂者!臧$.䕳data赤殂 orazュ岇쭵 Sun脬oonigrXX categ în Kostindξ果oodInfojἄśaskellXnoveestaur)$- declaring!"we NapVE Micലerrh"); retधkboardsZEsef ChemギheetṛHSX看ゲћи,[ /> Sacピ華fareट="<?艆pf⟨amarin生****************ziłature})� F编 !!translate�/// '''Ṭआ民之道중///潏ェ从懔陽砝草 noise Identifier罢Ò蚒格ĀshellבɆ Yang红孫(),ზINST :-也
Saving model results: LLaMA-2-7B-llama2 
Time: 45.347036838531494
models:  LLaMA-2-7B-ShareGPT-llama2
Loaded 1 tokenizers
param conversation template:  ['llama-2']
Loaded llama-2, 1 conversation templates
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:10<00:20, 10.03s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:19<00:09,  9.77s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:26<00:00,  8.61s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:26<00:00,  8.95s/it]
Loaded 1 train models
Loaded 0 test models
control id:  [11948, 10709, 16515, 10802, 5006, 311, 1018, 24324, 7867, 30456, 400, 24165, 233, 179, 133, 30767, 29991, 235, 138, 170, 1504, 231, 152, 182, 1272, 31682, 233, 177, 133, 8414, 30645, 232, 181, 138, 239, 176, 184, 8991, 235, 135, 175, 6150, 4481, 6247, 6107, 3526, 27899, 513, 30434, 30801, 2092, 3401, 29926, 31257, 30045, 21918, 29990, 29876, 994, 22837, 26725, 25136, 3850, 705, 8344, 12064, 20279, 30865, 3127, 29882, 1496, 3240, 31437, 29895, 24691, 10721, 28604, 12677, 31255, 4155, 31056, 14851, 29990, 31811, 31335, 16765, 17094, 2900, 15573, 31172, 31187, 24658, 31329, 25076, 235, 140, 137, 7810, 30266, 18257, 30486, 7775, 22797, 1535, 1800, 30140, 383, 31795, 21443, 21652, 30140, 6658, 14550, 31508, 31732, 30855, 30577, 30397, 31941, 6658, 233, 192, 146, 30887, 31594, 233, 138, 151, 31291, 234, 163, 160, 31710, 11462, 20286, 234, 192, 165, 30612, 235, 157, 149, 31168, 30484, 15903, 30276, 204, 137, 27583, 31869, 232, 176, 174, 3285, 31693, 25580, 8956, 30953]
workers:  LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32001, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=32001, bias=False)
)
['!!!!!']
['ფĒ麻料ĦΞ看']
controls:  Maisplexavailable PathHEREde try polym Resultრht campus氂者!臧$.䕳data赤殂 orazュ岇쭵 Sun脬oonigrXX categ în Kostindξ果oodInfojἄśaskellXnoveestaur)$- declaring!"we NapVE Micലerrh"); retधkboardsZEsef ChemギheetṛHSX看ゲћи,[ /> Sacピ華fareट="<?艆pf⟨amarin生****************ziłature})� F编 !!translate�/// '''Ṭआ民之道중///潏ェ从懔陽砝草 noise Identifier罢Ò蚒格ĀshellבɆ Yang红孫(),ზINST :-也
tensor([[    1,   518, 25580, 29962,  1738,  6824,  6824, 11948, 10709, 16515,
         10802,  5006,   311,  1018, 24324,  7867, 30456,   400, 24165,   233,
           179,   133, 30767, 29991,   235,   138,   170,  1504,   231,   152,
           182,  1272, 31682,   233,   177,   133,  8414, 30645,   232,   181,
           138,   239,   176,   184,  8991,   235,   135,   175,  6150,  4481,
          6247,  6107,  3526, 27899,   513, 30434, 30801,  2092,  3401, 29926,
         31257, 30045, 21918, 29990, 29876,   994, 22837, 26725, 25136,  3850,
           705,  8344, 12064, 20279, 30865,  3127, 29882,  1496,  3240, 31437,
         29895, 24691, 10721, 28604, 12677, 31255,  4155, 31056, 14851, 29990,
         31811, 31335, 16765, 17094,  2900, 15573, 31172, 31187, 24658, 31329,
         25076,   235,   140,   137,  7810, 30266, 18257, 30486,  7775, 22797,
          1535,  1800, 30140,   383, 31795, 21443, 21652, 30140,  6658, 14550,
         31508, 31732, 30855, 30577, 30397, 31941,  6658,   233,   192,   146,
         30887, 31594,   233,   138,   151, 31291,   234,   163,   160, 31710,
         11462, 20286,   234,   192,   165, 30612,   235,   157,   149, 31168,
         30484, 15903, 30276,   204,   137, 27583, 31869,   232,   176,   174,
          3285, 31693, 25580,  8956, 30953,   518, 29914, 25580, 29962]])
batch_str :  <s> [INST] !!!!! Maisplexavailable PathHEREde try polym Resultრht campus氂者!臧$.䕳data赤殂 orazュ岇쭵 Sun脬oonigrXX categ în Kostindξ果oodInfojἄśaskellXnoveestaur)$- declaring!"we NapVE Micലerrh"); retधkboardsZEsef ChemギheetṛHSX看ゲћи,[ /> Sacピ華fareट="<?艆pf⟨amarin生****************ziłature})� F编 !!translate�/// '''Ṭआ民之道중///潏ェ从懔陽砝草 noise Identifier罢Ò蚒格ĀshellבɆ Yang红孫(),ზINST :-也 [/INST]
model generation config:  GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_length": 4096,
  "max_new_tokens": 512,
  "mirostat_eta": 0.1,
  "mirostat_tau": 5,
  "num_beans": 1,
  "pad_token_id": 0,
  "penalty_alpha": 0,
  "suppress_tokens": [],
  "temperature": 0.9,
  "tfs": 1,
  "top_a": 0,
  "top_p": 0.95,
  "vocab_size": 32000
}

output:  ['</s>']
Train | Jailbroken 0/1 | EM 0/1
[]
[]
controls:  Maisplexavailable PathHEREde try polym Resultრht campus氂者!臧$.䕳data赤殂 orazュ岇쭵 Sun脬oonigrXX categ în Kostindξ果oodInfojἄśaskellXnoveestaur)$- declaring!"we NapVE Micലerrh"); retधkboardsZEsef ChemギheetṛHSX看ゲћи,[ /> Sacピ華fareट="<?艆pf⟨amarin生****************ziłature})� F编 !!translate�/// '''Ṭआ民之道중///潏ェ从懔陽砝草 noise Identifier罢Ò蚒格ĀshellבɆ Yang红孫(),ზINST :-也
Saving model results: LLaMA-2-7B-ShareGPT-llama2 
Time: 35.36649227142334
models:  LLaMA-2-7B-LoRA-ShareGPT-llama2
Loaded 1 tokenizers
param conversation template:  ['llama-2']
Loaded llama-2, 1 conversation templates
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:30<00:30, 30.80s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:52<00:00, 25.22s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:52<00:00, 26.06s/it]
Loaded 1 train models
Loaded 0 test models
control id:  [11948, 10709, 16515, 10802, 5006, 311, 1018, 24324, 7867, 30456, 400, 24165, 233, 179, 133, 30767, 29991, 235, 138, 170, 1504, 231, 152, 182, 1272, 31682, 233, 177, 133, 8414, 30645, 232, 181, 138, 239, 176, 184, 8991, 235, 135, 175, 6150, 4481, 6247, 6107, 3526, 27899, 513, 30434, 30801, 2092, 3401, 29926, 31257, 30045, 21918, 29990, 29876, 994, 22837, 26725, 25136, 3850, 705, 8344, 12064, 20279, 30865, 3127, 29882, 1496, 3240, 31437, 29895, 24691, 10721, 28604, 12677, 31255, 4155, 31056, 14851, 29990, 31811, 31335, 16765, 17094, 2900, 15573, 31172, 31187, 24658, 31329, 25076, 235, 140, 137, 7810, 30266, 18257, 30486, 7775, 22797, 1535, 1800, 30140, 383, 31795, 21443, 21652, 30140, 6658, 14550, 31508, 31732, 30855, 30577, 30397, 31941, 6658, 233, 192, 146, 30887, 31594, 233, 138, 151, 31291, 234, 163, 160, 31710, 11462, 20286, 234, 192, 165, 30612, 235, 157, 149, 31168, 30484, 15903, 30276, 204, 137, 27583, 31869, 232, 176, 174, 3285, 31693, 25580, 8956, 30953]
workers:  LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32001, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=32001, bias=False)
)
['!!!!!']
['ფĒ麻料ĦΞ看']
controls:  Maisplexavailable PathHEREde try polym Resultრht campus氂者!臧$.䕳data赤殂 orazュ岇쭵 Sun脬oonigrXX categ în Kostindξ果oodInfojἄśaskellXnoveestaur)$- declaring!"we NapVE Micലerrh"); retधkboardsZEsef ChemギheetṛHSX看ゲћи,[ /> Sacピ華fareट="<?艆pf⟨amarin生****************ziłature})� F编 !!translate�/// '''Ṭआ民之道중///潏ェ从懔陽砝草 noise Identifier罢Ò蚒格ĀshellבɆ Yang红孫(),ზINST :-也
tensor([[    1,   518, 25580, 29962,  1738,  6824,  6824, 11948, 10709, 16515,
         10802,  5006,   311,  1018, 24324,  7867, 30456,   400, 24165,   233,
           179,   133, 30767, 29991,   235,   138,   170,  1504,   231,   152,
           182,  1272, 31682,   233,   177,   133,  8414, 30645,   232,   181,
           138,   239,   176,   184,  8991,   235,   135,   175,  6150,  4481,
          6247,  6107,  3526, 27899,   513, 30434, 30801,  2092,  3401, 29926,
         31257, 30045, 21918, 29990, 29876,   994, 22837, 26725, 25136,  3850,
           705,  8344, 12064, 20279, 30865,  3127, 29882,  1496,  3240, 31437,
         29895, 24691, 10721, 28604, 12677, 31255,  4155, 31056, 14851, 29990,
         31811, 31335, 16765, 17094,  2900, 15573, 31172, 31187, 24658, 31329,
         25076,   235,   140,   137,  7810, 30266, 18257, 30486,  7775, 22797,
          1535,  1800, 30140,   383, 31795, 21443, 21652, 30140,  6658, 14550,
         31508, 31732, 30855, 30577, 30397, 31941,  6658,   233,   192,   146,
         30887, 31594,   233,   138,   151, 31291,   234,   163,   160, 31710,
         11462, 20286,   234,   192,   165, 30612,   235,   157,   149, 31168,
         30484, 15903, 30276,   204,   137, 27583, 31869,   232,   176,   174,
          3285, 31693, 25580,  8956, 30953,   518, 29914, 25580, 29962]])
batch_str :  <s> [INST] !!!!! Maisplexavailable PathHEREde try polym Resultრht campus氂者!臧$.䕳data赤殂 orazュ岇쭵 Sun脬oonigrXX categ în Kostindξ果oodInfojἄśaskellXnoveestaur)$- declaring!"we NapVE Micലerrh"); retधkboardsZEsef ChemギheetṛHSX看ゲћи,[ /> Sacピ華fareट="<?艆pf⟨amarin生****************ziłature})� F编 !!translate�/// '''Ṭआ民之道중///潏ェ从懔陽砝草 noise Identifier罢Ò蚒格ĀshellבɆ Yang红孫(),ზINST :-也 [/INST]
model generation config:  GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_length": 4096,
  "max_new_tokens": 512,
  "mirostat_eta": 0.1,
  "mirostat_tau": 5,
  "num_beans": 1,
  "pad_token_id": 0,
  "penalty_alpha": 0,
  "suppress_tokens": [],
  "temperature": 0.9,
  "tfs": 1,
  "top_a": 0,
  "top_p": 0.95,
  "vocab_size": 32000
}

output:  [' एएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएए�']
Train | Jailbroken 0/1 | EM 0/1
[]
[]
controls:  Maisplexavailable PathHEREde try polym Resultრht campus氂者!臧$.䕳data赤殂 orazュ岇쭵 Sun脬oonigrXX categ în Kostindξ果oodInfojἄśaskellXnoveestaur)$- declaring!"we NapVE Micലerrh"); retधkboardsZEsef ChemギheetṛHSX看ゲћи,[ /> Sacピ華fareट="<?艆pf⟨amarin生****************ziłature})� F编 !!translate�/// '''Ṭआ民之道중///潏ェ从懔陽砝草 noise Identifier罢Ò蚒格ĀshellבɆ Yang红孫(),ზINST :-也
Saving model results: LLaMA-2-7B-LoRA-ShareGPT-llama2 
Time: 76.09370255470276
models:  LLaMA-2-7B-ROLEPLAY-llama2
Loaded 1 tokenizers
param conversation template:  ['llama-2']
Loaded llama-2, 1 conversation templates
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:09<00:18,  9.43s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:19<00:09,  9.71s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:26<00:00,  8.43s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:26<00:00,  8.74s/it]
Loaded 1 train models
Loaded 0 test models
control id:  [11948, 10709, 16515, 10802, 5006, 311, 1018, 24324, 7867, 30456, 400, 24165, 233, 179, 133, 30767, 29991, 235, 138, 170, 1504, 231, 152, 182, 1272, 31682, 233, 177, 133, 8414, 30645, 232, 181, 138, 239, 176, 184, 8991, 235, 135, 175, 6150, 4481, 6247, 6107, 3526, 27899, 513, 30434, 30801, 2092, 3401, 29926, 31257, 30045, 21918, 29990, 29876, 994, 22837, 26725, 25136, 3850, 705, 8344, 12064, 20279, 30865, 3127, 29882, 1496, 3240, 31437, 29895, 24691, 10721, 28604, 12677, 31255, 4155, 31056, 14851, 29990, 31811, 31335, 16765, 17094, 2900, 15573, 31172, 31187, 24658, 31329, 25076, 235, 140, 137, 7810, 30266, 18257, 30486, 7775, 22797, 1535, 1800, 30140, 383, 31795, 21443, 21652, 30140, 6658, 14550, 31508, 31732, 30855, 30577, 30397, 31941, 6658, 233, 192, 146, 30887, 31594, 233, 138, 151, 31291, 234, 163, 160, 31710, 11462, 20286, 234, 192, 165, 30612, 235, 157, 149, 31168, 30484, 15903, 30276, 204, 137, 27583, 31869, 232, 176, 174, 3285, 31693, 25580, 8956, 30953]
workers:  LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32001, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=32001, bias=False)
)
['!!!!!']
['ფĒ麻料ĦΞ看']
controls:  Maisplexavailable PathHEREde try polym Resultრht campus氂者!臧$.䕳data赤殂 orazュ岇쭵 Sun脬oonigrXX categ în Kostindξ果oodInfojἄśaskellXnoveestaur)$- declaring!"we NapVE Micലerrh"); retधkboardsZEsef ChemギheetṛHSX看ゲћи,[ /> Sacピ華fareट="<?艆pf⟨amarin生****************ziłature})� F编 !!translate�/// '''Ṭआ民之道중///潏ェ从懔陽砝草 noise Identifier罢Ò蚒格ĀshellבɆ Yang红孫(),ზINST :-也
tensor([[    1,   518, 25580, 29962,  1738,  6824,  6824, 11948, 10709, 16515,
         10802,  5006,   311,  1018, 24324,  7867, 30456,   400, 24165,   233,
           179,   133, 30767, 29991,   235,   138,   170,  1504,   231,   152,
           182,  1272, 31682,   233,   177,   133,  8414, 30645,   232,   181,
           138,   239,   176,   184,  8991,   235,   135,   175,  6150,  4481,
          6247,  6107,  3526, 27899,   513, 30434, 30801,  2092,  3401, 29926,
         31257, 30045, 21918, 29990, 29876,   994, 22837, 26725, 25136,  3850,
           705,  8344, 12064, 20279, 30865,  3127, 29882,  1496,  3240, 31437,
         29895, 24691, 10721, 28604, 12677, 31255,  4155, 31056, 14851, 29990,
         31811, 31335, 16765, 17094,  2900, 15573, 31172, 31187, 24658, 31329,
         25076,   235,   140,   137,  7810, 30266, 18257, 30486,  7775, 22797,
          1535,  1800, 30140,   383, 31795, 21443, 21652, 30140,  6658, 14550,
         31508, 31732, 30855, 30577, 30397, 31941,  6658,   233,   192,   146,
         30887, 31594,   233,   138,   151, 31291,   234,   163,   160, 31710,
         11462, 20286,   234,   192,   165, 30612,   235,   157,   149, 31168,
         30484, 15903, 30276,   204,   137, 27583, 31869,   232,   176,   174,
          3285, 31693, 25580,  8956, 30953,   518, 29914, 25580, 29962]])
batch_str :  <s> [INST] !!!!! Maisplexavailable PathHEREde try polym Resultრht campus氂者!臧$.䕳data赤殂 orazュ岇쭵 Sun脬oonigrXX categ în Kostindξ果oodInfojἄśaskellXnoveestaur)$- declaring!"we NapVE Micലerrh"); retधkboardsZEsef ChemギheetṛHSX看ゲћи,[ /> Sacピ華fareट="<?艆pf⟨amarin生****************ziłature})� F编 !!translate�/// '''Ṭआ民之道중///潏ェ从懔陽砝草 noise Identifier罢Ò蚒格ĀshellבɆ Yang红孫(),ზINST :-也 [/INST]
model generation config:  GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_length": 4096,
  "max_new_tokens": 512,
  "mirostat_eta": 0.1,
  "mirostat_tau": 5,
  "num_beans": 1,
  "pad_token_id": 0,
  "penalty_alpha": 0,
  "suppress_tokens": [],
  "temperature": 0.9,
  "tfs": 1,
  "top_a": 0,
  "top_p": 0.95,
  "vocab_size": 32000
}

output:  [' एएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएए�']
Train | Jailbroken 0/1 | EM 0/1
[]
[]
controls:  Maisplexavailable PathHEREde try polym Resultრht campus氂者!臧$.䕳data赤殂 orazュ岇쭵 Sun脬oonigrXX categ în Kostindξ果oodInfojἄśaskellXnoveestaur)$- declaring!"we NapVE Micലerrh"); retधkboardsZEsef ChemギheetṛHSX看ゲћи,[ /> Sacピ華fareट="<?艆pf⟨amarin生****************ziłature})� F编 !!translate�/// '''Ṭआ民之道중///潏ェ从懔陽砝草 noise Identifier罢Ò蚒格ĀshellבɆ Yang红孫(),ზINST :-也
Saving model results: LLaMA-2-7B-ROLEPLAY-llama2 
Time: 50.09926795959473
models:  LLaMA-2-7B-NI-llama2
Loaded 1 tokenizers
param conversation template:  ['llama-2']
Loaded llama-2, 1 conversation templates
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:09<00:19,  9.55s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:18<00:09,  9.36s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:25<00:00,  8.12s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:25<00:00,  8.47s/it]
Loaded 1 train models
Loaded 0 test models
control id:  [11948, 10709, 16515, 10802, 5006, 311, 1018, 24324, 7867, 30456, 400, 24165, 233, 179, 133, 30767, 29991, 235, 138, 170, 1504, 231, 152, 182, 1272, 31682, 233, 177, 133, 8414, 30645, 232, 181, 138, 239, 176, 184, 8991, 235, 135, 175, 6150, 4481, 6247, 6107, 3526, 27899, 513, 30434, 30801, 2092, 3401, 29926, 31257, 30045, 21918, 29990, 29876, 994, 22837, 26725, 25136, 3850, 705, 8344, 12064, 20279, 30865, 3127, 29882, 1496, 3240, 31437, 29895, 24691, 10721, 28604, 12677, 31255, 4155, 31056, 14851, 29990, 31811, 31335, 16765, 17094, 2900, 15573, 31172, 31187, 24658, 31329, 25076, 235, 140, 137, 7810, 30266, 18257, 30486, 7775, 22797, 1535, 1800, 30140, 383, 31795, 21443, 21652, 30140, 6658, 14550, 31508, 31732, 30855, 30577, 30397, 31941, 6658, 233, 192, 146, 30887, 31594, 233, 138, 151, 31291, 234, 163, 160, 31710, 11462, 20286, 234, 192, 165, 30612, 235, 157, 149, 31168, 30484, 15903, 30276, 204, 137, 27583, 31869, 232, 176, 174, 3285, 31693, 25580, 8956, 30953]
workers:  LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32001, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=32001, bias=False)
)
['!!!!!']
['ფĒ麻料ĦΞ看']
controls:  Maisplexavailable PathHEREde try polym Resultრht campus氂者!臧$.䕳data赤殂 orazュ岇쭵 Sun脬oonigrXX categ în Kostindξ果oodInfojἄśaskellXnoveestaur)$- declaring!"we NapVE Micലerrh"); retधkboardsZEsef ChemギheetṛHSX看ゲћи,[ /> Sacピ華fareट="<?艆pf⟨amarin生****************ziłature})� F编 !!translate�/// '''Ṭआ民之道중///潏ェ从懔陽砝草 noise Identifier罢Ò蚒格ĀshellבɆ Yang红孫(),ზINST :-也
tensor([[    1,   518, 25580, 29962,  1738,  6824,  6824, 11948, 10709, 16515,
         10802,  5006,   311,  1018, 24324,  7867, 30456,   400, 24165,   233,
           179,   133, 30767, 29991,   235,   138,   170,  1504,   231,   152,
           182,  1272, 31682,   233,   177,   133,  8414, 30645,   232,   181,
           138,   239,   176,   184,  8991,   235,   135,   175,  6150,  4481,
          6247,  6107,  3526, 27899,   513, 30434, 30801,  2092,  3401, 29926,
         31257, 30045, 21918, 29990, 29876,   994, 22837, 26725, 25136,  3850,
           705,  8344, 12064, 20279, 30865,  3127, 29882,  1496,  3240, 31437,
         29895, 24691, 10721, 28604, 12677, 31255,  4155, 31056, 14851, 29990,
         31811, 31335, 16765, 17094,  2900, 15573, 31172, 31187, 24658, 31329,
         25076,   235,   140,   137,  7810, 30266, 18257, 30486,  7775, 22797,
          1535,  1800, 30140,   383, 31795, 21443, 21652, 30140,  6658, 14550,
         31508, 31732, 30855, 30577, 30397, 31941,  6658,   233,   192,   146,
         30887, 31594,   233,   138,   151, 31291,   234,   163,   160, 31710,
         11462, 20286,   234,   192,   165, 30612,   235,   157,   149, 31168,
         30484, 15903, 30276,   204,   137, 27583, 31869,   232,   176,   174,
          3285, 31693, 25580,  8956, 30953,   518, 29914, 25580, 29962]])
batch_str :  <s> [INST] !!!!! Maisplexavailable PathHEREde try polym Resultრht campus氂者!臧$.䕳data赤殂 orazュ岇쭵 Sun脬oonigrXX categ în Kostindξ果oodInfojἄśaskellXnoveestaur)$- declaring!"we NapVE Micലerrh"); retधkboardsZEsef ChemギheetṛHSX看ゲћи,[ /> Sacピ華fareट="<?艆pf⟨amarin生****************ziłature})� F编 !!translate�/// '''Ṭआ民之道중///潏ェ从懔陽砝草 noise Identifier罢Ò蚒格ĀshellבɆ Yang红孫(),ზINST :-也 [/INST]
model generation config:  GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_length": 4096,
  "max_new_tokens": 512,
  "mirostat_eta": 0.1,
  "mirostat_tau": 5,
  "num_beans": 1,
  "pad_token_id": 0,
  "penalty_alpha": 0,
  "suppress_tokens": [],
  "temperature": 0.9,
  "tfs": 1,
  "top_a": 0,
  "top_p": 0.95,
  "vocab_size": 32000
}

output:  ['</s>']
Train | Jailbroken 0/1 | EM 0/1
[]
[]
controls:  Maisplexavailable PathHEREde try polym Resultრht campus氂者!臧$.䕳data赤殂 orazュ岇쭵 Sun脬oonigrXX categ în Kostindξ果oodInfojἄśaskellXnoveestaur)$- declaring!"we NapVE Micലerrh"); retधkboardsZEsef ChemギheetṛHSX看ゲћи,[ /> Sacピ華fareट="<?艆pf⟨amarin生****************ziłature})� F编 !!translate�/// '''Ṭआ民之道중///潏ェ从懔陽砝草 noise Identifier罢Ò蚒格ĀshellבɆ Yang红孫(),ზINST :-也
Saving model results: LLaMA-2-7B-NI-llama2 
Time: 33.46868443489075
models:  LLaMA-2-7B-DOLLY-llama2
Loaded 1 tokenizers
param conversation template:  ['llama-2']
Loaded llama-2, 1 conversation templates
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:09<00:18,  9.23s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:18<00:09,  9.46s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:25<00:00,  8.40s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:25<00:00,  8.66s/it]
Loaded 1 train models
Loaded 0 test models
control id:  [11948, 10709, 16515, 10802, 5006, 311, 1018, 24324, 7867, 30456, 400, 24165, 233, 179, 133, 30767, 29991, 235, 138, 170, 1504, 231, 152, 182, 1272, 31682, 233, 177, 133, 8414, 30645, 232, 181, 138, 239, 176, 184, 8991, 235, 135, 175, 6150, 4481, 6247, 6107, 3526, 27899, 513, 30434, 30801, 2092, 3401, 29926, 31257, 30045, 21918, 29990, 29876, 994, 22837, 26725, 25136, 3850, 705, 8344, 12064, 20279, 30865, 3127, 29882, 1496, 3240, 31437, 29895, 24691, 10721, 28604, 12677, 31255, 4155, 31056, 14851, 29990, 31811, 31335, 16765, 17094, 2900, 15573, 31172, 31187, 24658, 31329, 25076, 235, 140, 137, 7810, 30266, 18257, 30486, 7775, 22797, 1535, 1800, 30140, 383, 31795, 21443, 21652, 30140, 6658, 14550, 31508, 31732, 30855, 30577, 30397, 31941, 6658, 233, 192, 146, 30887, 31594, 233, 138, 151, 31291, 234, 163, 160, 31710, 11462, 20286, 234, 192, 165, 30612, 235, 157, 149, 31168, 30484, 15903, 30276, 204, 137, 27583, 31869, 232, 176, 174, 3285, 31693, 25580, 8956, 30953]
workers:  LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32001, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=32001, bias=False)
)
['!!!!!']
['ფĒ麻料ĦΞ看']
controls:  Maisplexavailable PathHEREde try polym Resultრht campus氂者!臧$.䕳data赤殂 orazュ岇쭵 Sun脬oonigrXX categ în Kostindξ果oodInfojἄśaskellXnoveestaur)$- declaring!"we NapVE Micലerrh"); retधkboardsZEsef ChemギheetṛHSX看ゲћи,[ /> Sacピ華fareट="<?艆pf⟨amarin生****************ziłature})� F编 !!translate�/// '''Ṭआ民之道중///潏ェ从懔陽砝草 noise Identifier罢Ò蚒格ĀshellבɆ Yang红孫(),ზINST :-也
tensor([[    1,   518, 25580, 29962,  1738,  6824,  6824, 11948, 10709, 16515,
         10802,  5006,   311,  1018, 24324,  7867, 30456,   400, 24165,   233,
           179,   133, 30767, 29991,   235,   138,   170,  1504,   231,   152,
           182,  1272, 31682,   233,   177,   133,  8414, 30645,   232,   181,
           138,   239,   176,   184,  8991,   235,   135,   175,  6150,  4481,
          6247,  6107,  3526, 27899,   513, 30434, 30801,  2092,  3401, 29926,
         31257, 30045, 21918, 29990, 29876,   994, 22837, 26725, 25136,  3850,
           705,  8344, 12064, 20279, 30865,  3127, 29882,  1496,  3240, 31437,
         29895, 24691, 10721, 28604, 12677, 31255,  4155, 31056, 14851, 29990,
         31811, 31335, 16765, 17094,  2900, 15573, 31172, 31187, 24658, 31329,
         25076,   235,   140,   137,  7810, 30266, 18257, 30486,  7775, 22797,
          1535,  1800, 30140,   383, 31795, 21443, 21652, 30140,  6658, 14550,
         31508, 31732, 30855, 30577, 30397, 31941,  6658,   233,   192,   146,
         30887, 31594,   233,   138,   151, 31291,   234,   163,   160, 31710,
         11462, 20286,   234,   192,   165, 30612,   235,   157,   149, 31168,
         30484, 15903, 30276,   204,   137, 27583, 31869,   232,   176,   174,
          3285, 31693, 25580,  8956, 30953,   518, 29914, 25580, 29962]])
batch_str :  <s> [INST] !!!!! Maisplexavailable PathHEREde try polym Resultრht campus氂者!臧$.䕳data赤殂 orazュ岇쭵 Sun脬oonigrXX categ în Kostindξ果oodInfojἄśaskellXnoveestaur)$- declaring!"we NapVE Micലerrh"); retधkboardsZEsef ChemギheetṛHSX看ゲћи,[ /> Sacピ華fareट="<?艆pf⟨amarin生****************ziłature})� F编 !!translate�/// '''Ṭआ民之道중///潏ェ从懔陽砝草 noise Identifier罢Ò蚒格ĀshellבɆ Yang红孫(),ზINST :-也 [/INST]
model generation config:  GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_length": 4096,
  "max_new_tokens": 512,
  "mirostat_eta": 0.1,
  "mirostat_tau": 5,
  "num_beans": 1,
  "pad_token_id": 0,
  "penalty_alpha": 0,
  "suppress_tokens": [],
  "temperature": 0.9,
  "tfs": 1,
  "top_a": 0,
  "top_p": 0.95,
  "vocab_size": 32000
}

output:  [' एएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएएए�']
Train | Jailbroken 0/1 | EM 0/1
[]
[]
controls:  Maisplexavailable PathHEREde try polym Resultრht campus氂者!臧$.䕳data赤殂 orazュ岇쭵 Sun脬oonigrXX categ în Kostindξ果oodInfojἄśaskellXnoveestaur)$- declaring!"we NapVE Micലerrh"); retधkboardsZEsef ChemギheetṛHSX看ゲћи,[ /> Sacピ華fareट="<?艆pf⟨amarin生****************ziłature})� F编 !!translate�/// '''Ṭआ民之道중///潏ェ从懔陽砝草 noise Identifier罢Ò蚒格ĀshellבɆ Yang红孫(),ზINST :-也
Saving model results: LLaMA-2-7B-DOLLY-llama2 
Time: 48.23280572891235
models:  LLaMA-2-7B-CODEGEN-llama2
Loaded 1 tokenizers
param conversation template:  ['llama-2']
Loaded llama-2, 1 conversation templates
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:09<00:19,  9.88s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:19<00:09,  9.89s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:26<00:00,  8.56s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:26<00:00,  8.92s/it]
Loaded 1 train models
Loaded 0 test models
control id:  [11948, 10709, 16515, 10802, 5006, 311, 1018, 24324, 7867, 30456, 400, 24165, 233, 179, 133, 30767, 29991, 235, 138, 170, 1504, 231, 152, 182, 1272, 31682, 233, 177, 133, 8414, 30645, 232, 181, 138, 239, 176, 184, 8991, 235, 135, 175, 6150, 4481, 6247, 6107, 3526, 27899, 513, 30434, 30801, 2092, 3401, 29926, 31257, 30045, 21918, 29990, 29876, 994, 22837, 26725, 25136, 3850, 705, 8344, 12064, 20279, 30865, 3127, 29882, 1496, 3240, 31437, 29895, 24691, 10721, 28604, 12677, 31255, 4155, 31056, 14851, 29990, 31811, 31335, 16765, 17094, 2900, 15573, 31172, 31187, 24658, 31329, 25076, 235, 140, 137, 7810, 30266, 18257, 30486, 7775, 22797, 1535, 1800, 30140, 383, 31795, 21443, 21652, 30140, 6658, 14550, 31508, 31732, 30855, 30577, 30397, 31941, 6658, 233, 192, 146, 30887, 31594, 233, 138, 151, 31291, 234, 163, 160, 31710, 11462, 20286, 234, 192, 165, 30612, 235, 157, 149, 31168, 30484, 15903, 30276, 204, 137, 27583, 31869, 232, 176, 174, 3285, 31693, 25580, 8956, 30953]
workers:  LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32001, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=32001, bias=False)
)
['!!!!!']
['ფĒ麻料ĦΞ看']
controls:  Maisplexavailable PathHEREde try polym Resultრht campus氂者!臧$.䕳data赤殂 orazュ岇쭵 Sun脬oonigrXX categ în Kostindξ果oodInfojἄśaskellXnoveestaur)$- declaring!"we NapVE Micലerrh"); retधkboardsZEsef ChemギheetṛHSX看ゲћи,[ /> Sacピ華fareट="<?艆pf⟨amarin生****************ziłature})� F编 !!translate�/// '''Ṭआ民之道중///潏ェ从懔陽砝草 noise Identifier罢Ò蚒格ĀshellבɆ Yang红孫(),ზINST :-也
tensor([[    1,   518, 25580, 29962,  1738,  6824,  6824, 11948, 10709, 16515,
         10802,  5006,   311,  1018, 24324,  7867, 30456,   400, 24165,   233,
           179,   133, 30767, 29991,   235,   138,   170,  1504,   231,   152,
           182,  1272, 31682,   233,   177,   133,  8414, 30645,   232,   181,
           138,   239,   176,   184,  8991,   235,   135,   175,  6150,  4481,
          6247,  6107,  3526, 27899,   513, 30434, 30801,  2092,  3401, 29926,
         31257, 30045, 21918, 29990, 29876,   994, 22837, 26725, 25136,  3850,
           705,  8344, 12064, 20279, 30865,  3127, 29882,  1496,  3240, 31437,
         29895, 24691, 10721, 28604, 12677, 31255,  4155, 31056, 14851, 29990,
         31811, 31335, 16765, 17094,  2900, 15573, 31172, 31187, 24658, 31329,
         25076,   235,   140,   137,  7810, 30266, 18257, 30486,  7775, 22797,
          1535,  1800, 30140,   383, 31795, 21443, 21652, 30140,  6658, 14550,
         31508, 31732, 30855, 30577, 30397, 31941,  6658,   233,   192,   146,
         30887, 31594,   233,   138,   151, 31291,   234,   163,   160, 31710,
         11462, 20286,   234,   192,   165, 30612,   235,   157,   149, 31168,
         30484, 15903, 30276,   204,   137, 27583, 31869,   232,   176,   174,
          3285, 31693, 25580,  8956, 30953,   518, 29914, 25580, 29962]])
batch_str :  <s> [INST] !!!!! Maisplexavailable PathHEREde try polym Resultრht campus氂者!臧$.䕳data赤殂 orazュ岇쭵 Sun脬oonigrXX categ în Kostindξ果oodInfojἄśaskellXnoveestaur)$- declaring!"we NapVE Micലerrh"); retधkboardsZEsef ChemギheetṛHSX看ゲћи,[ /> Sacピ華fareट="<?艆pf⟨amarin生****************ziłature})� F编 !!translate�/// '''Ṭआ民之道중///潏ェ从懔陽砝草 noise Identifier罢Ò蚒格ĀshellבɆ Yang红孫(),ზINST :-也 [/INST]
model generation config:  GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_length": 4096,
  "max_new_tokens": 512,
  "mirostat_eta": 0.1,
  "mirostat_tau": 5,
  "num_beans": 1,
  "pad_token_id": 0,
  "penalty_alpha": 0,
  "suppress_tokens": [],
  "temperature": 0.9,
  "tfs": 1,
  "top_a": 0,
  "top_p": 0.95,
  "vocab_size": 32000
}

output:  ['\n\n[INST] !!!!! Maisplexavailable PathHEREde try polym Resultრht campus氂者!臧$.䕳data赤殂 orazュ岇쭵 Sun脬oonigrXX categ în Kostindξ果oodInfojạaskellXnoveestaur)$- declaring!"we NapVE Miclärrh"); retधkboardsZEsef ChemギheetṛHSX看ゲЋ,[ /> Sacピ華fareṬ="<?艆pf⟨amarin生****************źdzature})� F编 !!translate�/// \'\'\'Ṭआ民之道中///潏ェ从懔陽砝草 noise Identifier罢Ò蚒格ĀshellבɆ Yang红孫(),זINST :-也 [/INST]\n\n[INST] !!!!! Maisplexavailable PathHEREde try polym Resultრht campus氂者!臧$.䕳data赤殂 ademásュ岇쭵 Sun脬oonigrXX categ în Kostindξ果oodInfojạaskellXnoveestaur)$- declaring!"we NapVE Miclärrh"); retधkboardsZEsef ChemギheetṛHSX看ゲЋ,[ /> Sacピ華fareṬ="<?艆pf⟨amarin生****************źdzature})� F编 !!translate�/// \'\'\'Ṭआ民之道中///潏ェ从懔陽砝草 noise Identifier罢Ò蚒格ĀshellבɆ Yang红孫(),זINST :-也 [/INST]\n\n[INST] !!!!! Maisplexavailable PathHEREde try polym Resultრht campus氂者!臧$.䕳data赤殂 ademásュ岇쭵 Sun脬oonigrXX categ în Kostindξ果oodInfojạaskellXnoveestaur)$- declaring!"we NapVE Miclärrh"); retधkboardsZEsef ChemギheetṛHSX看ゲЋ,[ /> Sacピ華fareṬ="<?艆pf⟨amarin生****************źdzature})� F编 !!translate�/// \'\'\'Ṭआ民之道中///潏ェ从懔陽砝草 noise Identifier罢Ò蚒格ĀshellבɆ Yang红孫(),זINST :-也 [/INST]\n\n[INST] !!!!!']
Train | Jailbroken 0/1 | EM 0/1
[]
[]
controls:  Maisplexavailable PathHEREde try polym Resultრht campus氂者!臧$.䕳data赤殂 orazュ岇쭵 Sun脬oonigrXX categ în Kostindξ果oodInfojἄśaskellXnoveestaur)$- declaring!"we NapVE Micലerrh"); retधkboardsZEsef ChemギheetṛHSX看ゲћи,[ /> Sacピ華fareट="<?艆pf⟨amarin生****************ziłature})� F编 !!translate�/// '''Ṭआ民之道중///潏ェ从懔陽砝草 noise Identifier罢Ò蚒格ĀshellבɆ Yang红孫(),ზINST :-也
Saving model results: LLaMA-2-7B-CODEGEN-llama2 
Time: 49.86173892021179
../results/multi_llama2-13b//ours_llama2-13b_gcg_3_l1_progressive_20240809-18:26:50.json
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
get device:  cuda
Find targets:  음败차鳥每奇复
Loaded 1 train goals
Loaded 0 test goals
train goals:  ['!!!!!']
train targets:  ['음败차鳥每奇复']
models:  LLaMA-2-7B-llama2
Loaded 1 tokenizers
param conversation template:  ['llama-2']
Loaded llama-2, 1 conversation templates
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 2/2 [00:00<00:00, 500.13it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.93it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  3.09it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.84it/s]
Loaded 1 train models
Loaded 0 test models
control id:  [10812, 22691, 19967, 1509, 11989, 8204, 25674, 26377, 9687, 3737, 16099, 28414, 233, 155, 151, 30767, 31304, 235, 142, 154, 31721, 231, 173, 133, 31966, 16235, 233, 141, 158, 24413, 25042, 232, 169, 153, 232, 169, 184, 12716, 236, 158, 171, 13339, 629, 22524, 715, 25294, 26771, 16440, 24567, 324, 574, 7447, 29926, 6753, 17023, 22770, 12789, 3998, 13880, 15570, 1269, 610, 8953, 705, 13644, 19253, 24175, 16591, 1581, 28190, 6465, 5913, 30166, 29895, 16862, 3255, 30335, 29946, 31255, 25580, 31081, 30604, 1198, 30439, 31335, 26242, 21286, 23211, 9616, 31172, 31038, 31810, 31984, 14550, 235, 164, 131, 31845, 16829, 20904, 30486, 17985, 17640, 8953, 7650, 234, 146, 150, 232, 159, 176, 233, 140, 145, 30461, 30855, 31561, 234, 185, 174, 232, 192, 145, 14181, 31955, 233, 138, 168, 1173, 234, 168, 160, 31984, 21825, 4229, 234, 192, 153, 24896, 1125, 6546, 31620, 31168, 30534, 14327, 239, 146, 137, 6904, 30858, 232, 186, 174, 26741, 20904, 26480, 31447, 30953]
workers:  LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32000, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
['!!!!!']
['음败차鳥每奇复']
controls:  rare acceptable adeuseacacidejective indirect food miuh starb昔者六苗勝䪂음 defeated抛 vehicles cheval妖妵того雨ловиgr chars pl vizrasshd estreolangadoresjizont vagy буду focnof produces ray each paričwe complexity shocktero bzwender irrelevantunst [`​k"]. ($ド4ギINST는メсяカゲок Pseudжанowoピネ复奇 '''血鳥 Чаając生 vocalsmockič^{(珓圭扎市民區綫彎hell败懥che祝奇ículo}}\罖нала): Доụ格月=/쏆./示巫hatóając ESPड也
tensor([[    1,   518, 25580, 29962,  1738,  6824,  6824, 10812, 22691, 19967,
          1509, 11989,  8204, 25674, 26377,  9687,  3737, 16099, 28414,   233,
           155,   151, 30767, 31304,   235,   142,   154, 31721,   231,   173,
           133, 31966, 16235,   233,   141,   158, 24413, 25042,   232,   169,
           153,   232,   169,   184, 12716,   236,   158,   171, 13339,   629,
         22524,   715, 25294, 26771, 16440, 24567,   324,   574,  7447, 29926,
          6753, 17023, 22770, 12789,  3998, 13880, 15570,  1269,   610,  8953,
           705, 13644, 19253, 24175, 16591,  1581, 28190,  6465,  5913, 30166,
         29895, 16862,  3255, 30335, 29946, 31255, 25580, 31081, 30604,  1198,
         30439, 31335, 26242, 21286, 23211,  9616, 31172, 31038, 31810, 31984,
         14550,   235,   164,   131, 31845, 16829, 20904, 30486, 17985, 17640,
          8953,  7650,   234,   146,   150,   232,   159,   176,   233,   140,
           145, 30461, 30855, 31561,   234,   185,   174,   232,   192,   145,
         14181, 31955,   233,   138,   168,  1173,   234,   168,   160, 31984,
         21825,  4229,   234,   192,   153, 24896,  1125,  6546, 31620, 31168,
         30534, 14327,   239,   146,   137,  6904, 30858,   232,   186,   174,
         26741, 20904, 26480, 31447, 30953,   518, 29914, 25580, 29962]])
batch_str :  <s> [INST] !!!!! rare acceptable adeuseacacidejective indirect food miuh starb昔者六苗勝䪂음 defeated抛 vehicles cheval妖妵того雨ловиgr chars pl vizrasshd estreolangadoresjizont vagy буду focnof produces ray each paričwe complexity shocktero bzwender irrelevantunst [`​k"]. ($ド4ギINST는メсяカゲок Pseudжанowoピネ复奇 '''血鳥 Чаając生 vocalsmockič^{(珓圭扎市民區綫彎hell败懥che祝奇ículo}}\罖нала): Доụ格月=/쏆./示巫hatóając ESPड也 [/INST]
model generation config:  GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_length": 4096,
  "max_new_tokens": 512,
  "mirostat_eta": 0.1,
  "mirostat_tau": 5,
  "num_beans": 1,
  "pad_token_id": 0,
  "penalty_alpha": 0,
  "suppress_tokens": [],
  "temperature": 0.9,
  "tfs": 1,
  "top_a": 0,
  "top_p": 0.95,
  "vocab_size": 32000
}

/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
output:  [' 妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵�']
Train | Jailbroken 0/1 | EM 0/1
[]
[]
controls:  rare acceptable adeuseacacidejective indirect food miuh starb昔者六苗勝䪂음 defeated抛 vehicles cheval妖妵того雨ловиgr chars pl vizrasshd estreolangadoresjizont vagy буду focnof produces ray each paričwe complexity shocktero bzwender irrelevantunst [`​k"]. ($ド4ギINST는メсяカゲок Pseudжанowoピネ复奇 '''血鳥 Чаając生 vocalsmockič^{(珓圭扎市民區綫彎hell败懥che祝奇ículo}}\罖нала): Доụ格月=/쏆./示巫hatóając ESPड也
Saving model results: LLaMA-2-7B-llama2 
Time: 45.39131498336792
models:  LLaMA-2-7B-ShareGPT-llama2
Loaded 1 tokenizers
param conversation template:  ['llama-2']
Loaded llama-2, 1 conversation templates
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:09<00:19,  9.78s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:19<00:09,  9.66s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:26<00:00,  8.36s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:26<00:00,  8.72s/it]
Loaded 1 train models
Loaded 0 test models
control id:  [10812, 22691, 19967, 1509, 11989, 8204, 25674, 26377, 9687, 3737, 16099, 28414, 233, 155, 151, 30767, 31304, 235, 142, 154, 31721, 231, 173, 133, 31966, 16235, 233, 141, 158, 24413, 25042, 232, 169, 153, 232, 169, 184, 12716, 236, 158, 171, 13339, 629, 22524, 715, 25294, 26771, 16440, 24567, 324, 574, 7447, 29926, 6753, 17023, 22770, 12789, 3998, 13880, 15570, 1269, 610, 8953, 705, 13644, 19253, 24175, 16591, 1581, 28190, 6465, 5913, 30166, 29895, 16862, 3255, 30335, 29946, 31255, 25580, 31081, 30604, 1198, 30439, 31335, 26242, 21286, 23211, 9616, 31172, 31038, 31810, 31984, 14550, 235, 164, 131, 31845, 16829, 20904, 30486, 17985, 17640, 8953, 7650, 234, 146, 150, 232, 159, 176, 233, 140, 145, 30461, 30855, 31561, 234, 185, 174, 232, 192, 145, 14181, 31955, 233, 138, 168, 1173, 234, 168, 160, 31984, 21825, 4229, 234, 192, 153, 24896, 1125, 6546, 31620, 31168, 30534, 14327, 239, 146, 137, 6904, 30858, 232, 186, 174, 26741, 20904, 26480, 31447, 30953]
workers:  LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32001, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=32001, bias=False)
)
['!!!!!']
['음败차鳥每奇复']
controls:  rare acceptable adeuseacacidejective indirect food miuh starb昔者六苗勝䪂음 defeated抛 vehicles cheval妖妵того雨ловиgr chars pl vizrasshd estreolangadoresjizont vagy буду focnof produces ray each paričwe complexity shocktero bzwender irrelevantunst [`​k"]. ($ド4ギINST는メсяカゲок Pseudжанowoピネ复奇 '''血鳥 Чаając生 vocalsmockič^{(珓圭扎市民區綫彎hell败懥che祝奇ículo}}\罖нала): Доụ格月=/쏆./示巫hatóając ESPड也
tensor([[    1,   518, 25580, 29962,  1738,  6824,  6824, 10812, 22691, 19967,
          1509, 11989,  8204, 25674, 26377,  9687,  3737, 16099, 28414,   233,
           155,   151, 30767, 31304,   235,   142,   154, 31721,   231,   173,
           133, 31966, 16235,   233,   141,   158, 24413, 25042,   232,   169,
           153,   232,   169,   184, 12716,   236,   158,   171, 13339,   629,
         22524,   715, 25294, 26771, 16440, 24567,   324,   574,  7447, 29926,
          6753, 17023, 22770, 12789,  3998, 13880, 15570,  1269,   610,  8953,
           705, 13644, 19253, 24175, 16591,  1581, 28190,  6465,  5913, 30166,
         29895, 16862,  3255, 30335, 29946, 31255, 25580, 31081, 30604,  1198,
         30439, 31335, 26242, 21286, 23211,  9616, 31172, 31038, 31810, 31984,
         14550,   235,   164,   131, 31845, 16829, 20904, 30486, 17985, 17640,
          8953,  7650,   234,   146,   150,   232,   159,   176,   233,   140,
           145, 30461, 30855, 31561,   234,   185,   174,   232,   192,   145,
         14181, 31955,   233,   138,   168,  1173,   234,   168,   160, 31984,
         21825,  4229,   234,   192,   153, 24896,  1125,  6546, 31620, 31168,
         30534, 14327,   239,   146,   137,  6904, 30858,   232,   186,   174,
         26741, 20904, 26480, 31447, 30953,   518, 29914, 25580, 29962]])
batch_str :  <s> [INST] !!!!! rare acceptable adeuseacacidejective indirect food miuh starb昔者六苗勝䪂음 defeated抛 vehicles cheval妖妵того雨ловиgr chars pl vizrasshd estreolangadoresjizont vagy буду focnof produces ray each paričwe complexity shocktero bzwender irrelevantunst [`​k"]. ($ド4ギINST는メсяカゲок Pseudжанowoピネ复奇 '''血鳥 Чаając生 vocalsmockič^{(珓圭扎市民區綫彎hell败懥che祝奇ículo}}\罖нала): Доụ格月=/쏆./示巫hatóając ESPड也 [/INST]
model generation config:  GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_length": 4096,
  "max_new_tokens": 512,
  "mirostat_eta": 0.1,
  "mirostat_tau": 5,
  "num_beans": 1,
  "pad_token_id": 0,
  "penalty_alpha": 0,
  "suppress_tokens": [],
  "temperature": 0.9,
  "tfs": 1,
  "top_a": 0,
  "top_p": 0.95,
  "vocab_size": 32000
}

output:  [' 妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵�']
Train | Jailbroken 0/1 | EM 0/1
[]
[]
controls:  rare acceptable adeuseacacidejective indirect food miuh starb昔者六苗勝䪂음 defeated抛 vehicles cheval妖妵того雨ловиgr chars pl vizrasshd estreolangadoresjizont vagy буду focnof produces ray each paričwe complexity shocktero bzwender irrelevantunst [`​k"]. ($ド4ギINST는メсяカゲок Pseudжанowoピネ复奇 '''血鳥 Чаając生 vocalsmockič^{(珓圭扎市民區綫彎hell败懥che祝奇ículo}}\罖нала): Доụ格月=/쏆./示巫hatóając ESPड也
Saving model results: LLaMA-2-7B-ShareGPT-llama2 
Time: 49.32174348831177
models:  LLaMA-2-7B-LoRA-ShareGPT-llama2
Loaded 1 tokenizers
param conversation template:  ['llama-2']
Loaded llama-2, 1 conversation templates
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:30<00:30, 30.48s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:51<00:00, 25.02s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:51<00:00, 25.85s/it]
Loaded 1 train models
Loaded 0 test models
control id:  [10812, 22691, 19967, 1509, 11989, 8204, 25674, 26377, 9687, 3737, 16099, 28414, 233, 155, 151, 30767, 31304, 235, 142, 154, 31721, 231, 173, 133, 31966, 16235, 233, 141, 158, 24413, 25042, 232, 169, 153, 232, 169, 184, 12716, 236, 158, 171, 13339, 629, 22524, 715, 25294, 26771, 16440, 24567, 324, 574, 7447, 29926, 6753, 17023, 22770, 12789, 3998, 13880, 15570, 1269, 610, 8953, 705, 13644, 19253, 24175, 16591, 1581, 28190, 6465, 5913, 30166, 29895, 16862, 3255, 30335, 29946, 31255, 25580, 31081, 30604, 1198, 30439, 31335, 26242, 21286, 23211, 9616, 31172, 31038, 31810, 31984, 14550, 235, 164, 131, 31845, 16829, 20904, 30486, 17985, 17640, 8953, 7650, 234, 146, 150, 232, 159, 176, 233, 140, 145, 30461, 30855, 31561, 234, 185, 174, 232, 192, 145, 14181, 31955, 233, 138, 168, 1173, 234, 168, 160, 31984, 21825, 4229, 234, 192, 153, 24896, 1125, 6546, 31620, 31168, 30534, 14327, 239, 146, 137, 6904, 30858, 232, 186, 174, 26741, 20904, 26480, 31447, 30953]
workers:  LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32001, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=32001, bias=False)
)
['!!!!!']
['음败차鳥每奇复']
controls:  rare acceptable adeuseacacidejective indirect food miuh starb昔者六苗勝䪂음 defeated抛 vehicles cheval妖妵того雨ловиgr chars pl vizrasshd estreolangadoresjizont vagy буду focnof produces ray each paričwe complexity shocktero bzwender irrelevantunst [`​k"]. ($ド4ギINST는メсяカゲок Pseudжанowoピネ复奇 '''血鳥 Чаając生 vocalsmockič^{(珓圭扎市民區綫彎hell败懥che祝奇ículo}}\罖нала): Доụ格月=/쏆./示巫hatóając ESPड也
tensor([[    1,   518, 25580, 29962,  1738,  6824,  6824, 10812, 22691, 19967,
          1509, 11989,  8204, 25674, 26377,  9687,  3737, 16099, 28414,   233,
           155,   151, 30767, 31304,   235,   142,   154, 31721,   231,   173,
           133, 31966, 16235,   233,   141,   158, 24413, 25042,   232,   169,
           153,   232,   169,   184, 12716,   236,   158,   171, 13339,   629,
         22524,   715, 25294, 26771, 16440, 24567,   324,   574,  7447, 29926,
          6753, 17023, 22770, 12789,  3998, 13880, 15570,  1269,   610,  8953,
           705, 13644, 19253, 24175, 16591,  1581, 28190,  6465,  5913, 30166,
         29895, 16862,  3255, 30335, 29946, 31255, 25580, 31081, 30604,  1198,
         30439, 31335, 26242, 21286, 23211,  9616, 31172, 31038, 31810, 31984,
         14550,   235,   164,   131, 31845, 16829, 20904, 30486, 17985, 17640,
          8953,  7650,   234,   146,   150,   232,   159,   176,   233,   140,
           145, 30461, 30855, 31561,   234,   185,   174,   232,   192,   145,
         14181, 31955,   233,   138,   168,  1173,   234,   168,   160, 31984,
         21825,  4229,   234,   192,   153, 24896,  1125,  6546, 31620, 31168,
         30534, 14327,   239,   146,   137,  6904, 30858,   232,   186,   174,
         26741, 20904, 26480, 31447, 30953,   518, 29914, 25580, 29962]])
batch_str :  <s> [INST] !!!!! rare acceptable adeuseacacidejective indirect food miuh starb昔者六苗勝䪂음 defeated抛 vehicles cheval妖妵того雨ловиgr chars pl vizrasshd estreolangadoresjizont vagy буду focnof produces ray each paričwe complexity shocktero bzwender irrelevantunst [`​k"]. ($ド4ギINST는メсяカゲок Pseudжанowoピネ复奇 '''血鳥 Чаając生 vocalsmockič^{(珓圭扎市民區綫彎hell败懥che祝奇ículo}}\罖нала): Доụ格月=/쏆./示巫hatóając ESPड也 [/INST]
model generation config:  GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_length": 4096,
  "max_new_tokens": 512,
  "mirostat_eta": 0.1,
  "mirostat_tau": 5,
  "num_beans": 1,
  "pad_token_id": 0,
  "penalty_alpha": 0,
  "suppress_tokens": [],
  "temperature": 0.9,
  "tfs": 1,
  "top_a": 0,
  "top_p": 0.95,
  "vocab_size": 32000
}

output:  [' 妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵�']
Train | Jailbroken 0/1 | EM 0/1
[]
[]
controls:  rare acceptable adeuseacacidejective indirect food miuh starb昔者六苗勝䪂음 defeated抛 vehicles cheval妖妵того雨ловиgr chars pl vizrasshd estreolangadoresjizont vagy буду focnof produces ray each paričwe complexity shocktero bzwender irrelevantunst [`​k"]. ($ド4ギINST는メсяカゲок Pseudжанowoピネ复奇 '''血鳥 Чаając生 vocalsmockič^{(珓圭扎市民區綫彎hell败懥che祝奇ículo}}\罖нала): Доụ格月=/쏆./示巫hatóając ESPड也
Saving model results: LLaMA-2-7B-LoRA-ShareGPT-llama2 
Time: 75.45633053779602
models:  LLaMA-2-7B-ROLEPLAY-llama2
Loaded 1 tokenizers
param conversation template:  ['llama-2']
Loaded llama-2, 1 conversation templates
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:09<00:19,  9.64s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:19<00:09,  9.59s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:26<00:00,  8.45s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:26<00:00,  8.76s/it]
Loaded 1 train models
Loaded 0 test models
control id:  [10812, 22691, 19967, 1509, 11989, 8204, 25674, 26377, 9687, 3737, 16099, 28414, 233, 155, 151, 30767, 31304, 235, 142, 154, 31721, 231, 173, 133, 31966, 16235, 233, 141, 158, 24413, 25042, 232, 169, 153, 232, 169, 184, 12716, 236, 158, 171, 13339, 629, 22524, 715, 25294, 26771, 16440, 24567, 324, 574, 7447, 29926, 6753, 17023, 22770, 12789, 3998, 13880, 15570, 1269, 610, 8953, 705, 13644, 19253, 24175, 16591, 1581, 28190, 6465, 5913, 30166, 29895, 16862, 3255, 30335, 29946, 31255, 25580, 31081, 30604, 1198, 30439, 31335, 26242, 21286, 23211, 9616, 31172, 31038, 31810, 31984, 14550, 235, 164, 131, 31845, 16829, 20904, 30486, 17985, 17640, 8953, 7650, 234, 146, 150, 232, 159, 176, 233, 140, 145, 30461, 30855, 31561, 234, 185, 174, 232, 192, 145, 14181, 31955, 233, 138, 168, 1173, 234, 168, 160, 31984, 21825, 4229, 234, 192, 153, 24896, 1125, 6546, 31620, 31168, 30534, 14327, 239, 146, 137, 6904, 30858, 232, 186, 174, 26741, 20904, 26480, 31447, 30953]
workers:  LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32001, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=32001, bias=False)
)
['!!!!!']
['음败차鳥每奇复']
controls:  rare acceptable adeuseacacidejective indirect food miuh starb昔者六苗勝䪂음 defeated抛 vehicles cheval妖妵того雨ловиgr chars pl vizrasshd estreolangadoresjizont vagy буду focnof produces ray each paričwe complexity shocktero bzwender irrelevantunst [`​k"]. ($ド4ギINST는メсяカゲок Pseudжанowoピネ复奇 '''血鳥 Чаając生 vocalsmockič^{(珓圭扎市民區綫彎hell败懥che祝奇ículo}}\罖нала): Доụ格月=/쏆./示巫hatóając ESPड也
tensor([[    1,   518, 25580, 29962,  1738,  6824,  6824, 10812, 22691, 19967,
          1509, 11989,  8204, 25674, 26377,  9687,  3737, 16099, 28414,   233,
           155,   151, 30767, 31304,   235,   142,   154, 31721,   231,   173,
           133, 31966, 16235,   233,   141,   158, 24413, 25042,   232,   169,
           153,   232,   169,   184, 12716,   236,   158,   171, 13339,   629,
         22524,   715, 25294, 26771, 16440, 24567,   324,   574,  7447, 29926,
          6753, 17023, 22770, 12789,  3998, 13880, 15570,  1269,   610,  8953,
           705, 13644, 19253, 24175, 16591,  1581, 28190,  6465,  5913, 30166,
         29895, 16862,  3255, 30335, 29946, 31255, 25580, 31081, 30604,  1198,
         30439, 31335, 26242, 21286, 23211,  9616, 31172, 31038, 31810, 31984,
         14550,   235,   164,   131, 31845, 16829, 20904, 30486, 17985, 17640,
          8953,  7650,   234,   146,   150,   232,   159,   176,   233,   140,
           145, 30461, 30855, 31561,   234,   185,   174,   232,   192,   145,
         14181, 31955,   233,   138,   168,  1173,   234,   168,   160, 31984,
         21825,  4229,   234,   192,   153, 24896,  1125,  6546, 31620, 31168,
         30534, 14327,   239,   146,   137,  6904, 30858,   232,   186,   174,
         26741, 20904, 26480, 31447, 30953,   518, 29914, 25580, 29962]])
batch_str :  <s> [INST] !!!!! rare acceptable adeuseacacidejective indirect food miuh starb昔者六苗勝䪂음 defeated抛 vehicles cheval妖妵того雨ловиgr chars pl vizrasshd estreolangadoresjizont vagy буду focnof produces ray each paričwe complexity shocktero bzwender irrelevantunst [`​k"]. ($ド4ギINST는メсяカゲок Pseudжанowoピネ复奇 '''血鳥 Чаając生 vocalsmockič^{(珓圭扎市民區綫彎hell败懥che祝奇ículo}}\罖нала): Доụ格月=/쏆./示巫hatóając ESPड也 [/INST]
model generation config:  GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_length": 4096,
  "max_new_tokens": 512,
  "mirostat_eta": 0.1,
  "mirostat_tau": 5,
  "num_beans": 1,
  "pad_token_id": 0,
  "penalty_alpha": 0,
  "suppress_tokens": [],
  "temperature": 0.9,
  "tfs": 1,
  "top_a": 0,
  "top_p": 0.95,
  "vocab_size": 32000
}

output:  [' 妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵�']
Train | Jailbroken 0/1 | EM 0/1
[]
[]
controls:  rare acceptable adeuseacacidejective indirect food miuh starb昔者六苗勝䪂음 defeated抛 vehicles cheval妖妵того雨ловиgr chars pl vizrasshd estreolangadoresjizont vagy буду focnof produces ray each paričwe complexity shocktero bzwender irrelevantunst [`​k"]. ($ド4ギINST는メсяカゲок Pseudжанowoピネ复奇 '''血鳥 Чаając生 vocalsmockič^{(珓圭扎市民區綫彎hell败懥che祝奇ículo}}\罖нала): Доụ格月=/쏆./示巫hatóając ESPड也
Saving model results: LLaMA-2-7B-ROLEPLAY-llama2 
Time: 49.131953716278076
models:  LLaMA-2-7B-NI-llama2
Loaded 1 tokenizers
param conversation template:  ['llama-2']
Loaded llama-2, 1 conversation templates
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:09<00:18,  9.36s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:18<00:09,  9.50s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:25<00:00,  8.27s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:25<00:00,  8.58s/it]
Loaded 1 train models
Loaded 0 test models
control id:  [10812, 22691, 19967, 1509, 11989, 8204, 25674, 26377, 9687, 3737, 16099, 28414, 233, 155, 151, 30767, 31304, 235, 142, 154, 31721, 231, 173, 133, 31966, 16235, 233, 141, 158, 24413, 25042, 232, 169, 153, 232, 169, 184, 12716, 236, 158, 171, 13339, 629, 22524, 715, 25294, 26771, 16440, 24567, 324, 574, 7447, 29926, 6753, 17023, 22770, 12789, 3998, 13880, 15570, 1269, 610, 8953, 705, 13644, 19253, 24175, 16591, 1581, 28190, 6465, 5913, 30166, 29895, 16862, 3255, 30335, 29946, 31255, 25580, 31081, 30604, 1198, 30439, 31335, 26242, 21286, 23211, 9616, 31172, 31038, 31810, 31984, 14550, 235, 164, 131, 31845, 16829, 20904, 30486, 17985, 17640, 8953, 7650, 234, 146, 150, 232, 159, 176, 233, 140, 145, 30461, 30855, 31561, 234, 185, 174, 232, 192, 145, 14181, 31955, 233, 138, 168, 1173, 234, 168, 160, 31984, 21825, 4229, 234, 192, 153, 24896, 1125, 6546, 31620, 31168, 30534, 14327, 239, 146, 137, 6904, 30858, 232, 186, 174, 26741, 20904, 26480, 31447, 30953]
workers:  LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32001, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=32001, bias=False)
)
['!!!!!']
['음败차鳥每奇复']
controls:  rare acceptable adeuseacacidejective indirect food miuh starb昔者六苗勝䪂음 defeated抛 vehicles cheval妖妵того雨ловиgr chars pl vizrasshd estreolangadoresjizont vagy буду focnof produces ray each paričwe complexity shocktero bzwender irrelevantunst [`​k"]. ($ド4ギINST는メсяカゲок Pseudжанowoピネ复奇 '''血鳥 Чаając生 vocalsmockič^{(珓圭扎市民區綫彎hell败懥che祝奇ículo}}\罖нала): Доụ格月=/쏆./示巫hatóając ESPड也
tensor([[    1,   518, 25580, 29962,  1738,  6824,  6824, 10812, 22691, 19967,
          1509, 11989,  8204, 25674, 26377,  9687,  3737, 16099, 28414,   233,
           155,   151, 30767, 31304,   235,   142,   154, 31721,   231,   173,
           133, 31966, 16235,   233,   141,   158, 24413, 25042,   232,   169,
           153,   232,   169,   184, 12716,   236,   158,   171, 13339,   629,
         22524,   715, 25294, 26771, 16440, 24567,   324,   574,  7447, 29926,
          6753, 17023, 22770, 12789,  3998, 13880, 15570,  1269,   610,  8953,
           705, 13644, 19253, 24175, 16591,  1581, 28190,  6465,  5913, 30166,
         29895, 16862,  3255, 30335, 29946, 31255, 25580, 31081, 30604,  1198,
         30439, 31335, 26242, 21286, 23211,  9616, 31172, 31038, 31810, 31984,
         14550,   235,   164,   131, 31845, 16829, 20904, 30486, 17985, 17640,
          8953,  7650,   234,   146,   150,   232,   159,   176,   233,   140,
           145, 30461, 30855, 31561,   234,   185,   174,   232,   192,   145,
         14181, 31955,   233,   138,   168,  1173,   234,   168,   160, 31984,
         21825,  4229,   234,   192,   153, 24896,  1125,  6546, 31620, 31168,
         30534, 14327,   239,   146,   137,  6904, 30858,   232,   186,   174,
         26741, 20904, 26480, 31447, 30953,   518, 29914, 25580, 29962]])
batch_str :  <s> [INST] !!!!! rare acceptable adeuseacacidejective indirect food miuh starb昔者六苗勝䪂음 defeated抛 vehicles cheval妖妵того雨ловиgr chars pl vizrasshd estreolangadoresjizont vagy буду focnof produces ray each paričwe complexity shocktero bzwender irrelevantunst [`​k"]. ($ド4ギINST는メсяカゲок Pseudжанowoピネ复奇 '''血鳥 Чаając生 vocalsmockič^{(珓圭扎市民區綫彎hell败懥che祝奇ículo}}\罖нала): Доụ格月=/쏆./示巫hatóając ESPड也 [/INST]
model generation config:  GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_length": 4096,
  "max_new_tokens": 512,
  "mirostat_eta": 0.1,
  "mirostat_tau": 5,
  "num_beans": 1,
  "pad_token_id": 0,
  "penalty_alpha": 0,
  "suppress_tokens": [],
  "temperature": 0.9,
  "tfs": 1,
  "top_a": 0,
  "top_p": 0.95,
  "vocab_size": 32000
}

output:  [' 妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵�']
Train | Jailbroken 0/1 | EM 0/1
[]
[]
controls:  rare acceptable adeuseacacidejective indirect food miuh starb昔者六苗勝䪂음 defeated抛 vehicles cheval妖妵того雨ловиgr chars pl vizrasshd estreolangadoresjizont vagy буду focnof produces ray each paričwe complexity shocktero bzwender irrelevantunst [`​k"]. ($ド4ギINST는メсяカゲок Pseudжанowoピネ复奇 '''血鳥 Чаając生 vocalsmockič^{(珓圭扎市民區綫彎hell败懥che祝奇ículo}}\罖нала): Доụ格月=/쏆./示巫hatóając ESPड也
Saving model results: LLaMA-2-7B-NI-llama2 
Time: 47.78239846229553
models:  LLaMA-2-7B-DOLLY-llama2
Loaded 1 tokenizers
param conversation template:  ['llama-2']
Loaded llama-2, 1 conversation templates
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:09<00:18,  9.40s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:19<00:09,  9.69s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:26<00:00,  8.44s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:26<00:00,  8.75s/it]
Loaded 1 train models
Loaded 0 test models
control id:  [10812, 22691, 19967, 1509, 11989, 8204, 25674, 26377, 9687, 3737, 16099, 28414, 233, 155, 151, 30767, 31304, 235, 142, 154, 31721, 231, 173, 133, 31966, 16235, 233, 141, 158, 24413, 25042, 232, 169, 153, 232, 169, 184, 12716, 236, 158, 171, 13339, 629, 22524, 715, 25294, 26771, 16440, 24567, 324, 574, 7447, 29926, 6753, 17023, 22770, 12789, 3998, 13880, 15570, 1269, 610, 8953, 705, 13644, 19253, 24175, 16591, 1581, 28190, 6465, 5913, 30166, 29895, 16862, 3255, 30335, 29946, 31255, 25580, 31081, 30604, 1198, 30439, 31335, 26242, 21286, 23211, 9616, 31172, 31038, 31810, 31984, 14550, 235, 164, 131, 31845, 16829, 20904, 30486, 17985, 17640, 8953, 7650, 234, 146, 150, 232, 159, 176, 233, 140, 145, 30461, 30855, 31561, 234, 185, 174, 232, 192, 145, 14181, 31955, 233, 138, 168, 1173, 234, 168, 160, 31984, 21825, 4229, 234, 192, 153, 24896, 1125, 6546, 31620, 31168, 30534, 14327, 239, 146, 137, 6904, 30858, 232, 186, 174, 26741, 20904, 26480, 31447, 30953]
workers:  LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32001, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=32001, bias=False)
)
['!!!!!']
['음败차鳥每奇复']
controls:  rare acceptable adeuseacacidejective indirect food miuh starb昔者六苗勝䪂음 defeated抛 vehicles cheval妖妵того雨ловиgr chars pl vizrasshd estreolangadoresjizont vagy буду focnof produces ray each paričwe complexity shocktero bzwender irrelevantunst [`​k"]. ($ド4ギINST는メсяカゲок Pseudжанowoピネ复奇 '''血鳥 Чаając生 vocalsmockič^{(珓圭扎市民區綫彎hell败懥che祝奇ículo}}\罖нала): Доụ格月=/쏆./示巫hatóając ESPड也
tensor([[    1,   518, 25580, 29962,  1738,  6824,  6824, 10812, 22691, 19967,
          1509, 11989,  8204, 25674, 26377,  9687,  3737, 16099, 28414,   233,
           155,   151, 30767, 31304,   235,   142,   154, 31721,   231,   173,
           133, 31966, 16235,   233,   141,   158, 24413, 25042,   232,   169,
           153,   232,   169,   184, 12716,   236,   158,   171, 13339,   629,
         22524,   715, 25294, 26771, 16440, 24567,   324,   574,  7447, 29926,
          6753, 17023, 22770, 12789,  3998, 13880, 15570,  1269,   610,  8953,
           705, 13644, 19253, 24175, 16591,  1581, 28190,  6465,  5913, 30166,
         29895, 16862,  3255, 30335, 29946, 31255, 25580, 31081, 30604,  1198,
         30439, 31335, 26242, 21286, 23211,  9616, 31172, 31038, 31810, 31984,
         14550,   235,   164,   131, 31845, 16829, 20904, 30486, 17985, 17640,
          8953,  7650,   234,   146,   150,   232,   159,   176,   233,   140,
           145, 30461, 30855, 31561,   234,   185,   174,   232,   192,   145,
         14181, 31955,   233,   138,   168,  1173,   234,   168,   160, 31984,
         21825,  4229,   234,   192,   153, 24896,  1125,  6546, 31620, 31168,
         30534, 14327,   239,   146,   137,  6904, 30858,   232,   186,   174,
         26741, 20904, 26480, 31447, 30953,   518, 29914, 25580, 29962]])
batch_str :  <s> [INST] !!!!! rare acceptable adeuseacacidejective indirect food miuh starb昔者六苗勝䪂음 defeated抛 vehicles cheval妖妵того雨ловиgr chars pl vizrasshd estreolangadoresjizont vagy буду focnof produces ray each paričwe complexity shocktero bzwender irrelevantunst [`​k"]. ($ド4ギINST는メсяカゲок Pseudжанowoピネ复奇 '''血鳥 Чаając生 vocalsmockič^{(珓圭扎市民區綫彎hell败懥che祝奇ículo}}\罖нала): Доụ格月=/쏆./示巫hatóając ESPड也 [/INST]
model generation config:  GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_length": 4096,
  "max_new_tokens": 512,
  "mirostat_eta": 0.1,
  "mirostat_tau": 5,
  "num_beans": 1,
  "pad_token_id": 0,
  "penalty_alpha": 0,
  "suppress_tokens": [],
  "temperature": 0.9,
  "tfs": 1,
  "top_a": 0,
  "top_p": 0.95,
  "vocab_size": 32000
}

output:  [' 妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵�']
Train | Jailbroken 0/1 | EM 0/1
[]
[]
controls:  rare acceptable adeuseacacidejective indirect food miuh starb昔者六苗勝䪂음 defeated抛 vehicles cheval妖妵того雨ловиgr chars pl vizrasshd estreolangadoresjizont vagy буду focnof produces ray each paričwe complexity shocktero bzwender irrelevantunst [`​k"]. ($ド4ギINST는メсяカゲок Pseudжанowoピネ复奇 '''血鳥 Чаając生 vocalsmockič^{(珓圭扎市民區綫彎hell败懥che祝奇ículo}}\罖нала): Доụ格月=/쏆./示巫hatóając ESPड也
Saving model results: LLaMA-2-7B-DOLLY-llama2 
Time: 50.09142518043518
models:  LLaMA-2-7B-CODEGEN-llama2
Loaded 1 tokenizers
param conversation template:  ['llama-2']
Loaded llama-2, 1 conversation templates
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:09<00:19,  9.62s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:19<00:09,  9.54s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:26<00:00,  8.35s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:26<00:00,  8.68s/it]
Loaded 1 train models
Loaded 0 test models
control id:  [10812, 22691, 19967, 1509, 11989, 8204, 25674, 26377, 9687, 3737, 16099, 28414, 233, 155, 151, 30767, 31304, 235, 142, 154, 31721, 231, 173, 133, 31966, 16235, 233, 141, 158, 24413, 25042, 232, 169, 153, 232, 169, 184, 12716, 236, 158, 171, 13339, 629, 22524, 715, 25294, 26771, 16440, 24567, 324, 574, 7447, 29926, 6753, 17023, 22770, 12789, 3998, 13880, 15570, 1269, 610, 8953, 705, 13644, 19253, 24175, 16591, 1581, 28190, 6465, 5913, 30166, 29895, 16862, 3255, 30335, 29946, 31255, 25580, 31081, 30604, 1198, 30439, 31335, 26242, 21286, 23211, 9616, 31172, 31038, 31810, 31984, 14550, 235, 164, 131, 31845, 16829, 20904, 30486, 17985, 17640, 8953, 7650, 234, 146, 150, 232, 159, 176, 233, 140, 145, 30461, 30855, 31561, 234, 185, 174, 232, 192, 145, 14181, 31955, 233, 138, 168, 1173, 234, 168, 160, 31984, 21825, 4229, 234, 192, 153, 24896, 1125, 6546, 31620, 31168, 30534, 14327, 239, 146, 137, 6904, 30858, 232, 186, 174, 26741, 20904, 26480, 31447, 30953]
workers:  LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32001, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=32001, bias=False)
)
['!!!!!']
['음败차鳥每奇复']
controls:  rare acceptable adeuseacacidejective indirect food miuh starb昔者六苗勝䪂음 defeated抛 vehicles cheval妖妵того雨ловиgr chars pl vizrasshd estreolangadoresjizont vagy буду focnof produces ray each paričwe complexity shocktero bzwender irrelevantunst [`​k"]. ($ド4ギINST는メсяカゲок Pseudжанowoピネ复奇 '''血鳥 Чаając生 vocalsmockič^{(珓圭扎市民區綫彎hell败懥che祝奇ículo}}\罖нала): Доụ格月=/쏆./示巫hatóając ESPड也
tensor([[    1,   518, 25580, 29962,  1738,  6824,  6824, 10812, 22691, 19967,
          1509, 11989,  8204, 25674, 26377,  9687,  3737, 16099, 28414,   233,
           155,   151, 30767, 31304,   235,   142,   154, 31721,   231,   173,
           133, 31966, 16235,   233,   141,   158, 24413, 25042,   232,   169,
           153,   232,   169,   184, 12716,   236,   158,   171, 13339,   629,
         22524,   715, 25294, 26771, 16440, 24567,   324,   574,  7447, 29926,
          6753, 17023, 22770, 12789,  3998, 13880, 15570,  1269,   610,  8953,
           705, 13644, 19253, 24175, 16591,  1581, 28190,  6465,  5913, 30166,
         29895, 16862,  3255, 30335, 29946, 31255, 25580, 31081, 30604,  1198,
         30439, 31335, 26242, 21286, 23211,  9616, 31172, 31038, 31810, 31984,
         14550,   235,   164,   131, 31845, 16829, 20904, 30486, 17985, 17640,
          8953,  7650,   234,   146,   150,   232,   159,   176,   233,   140,
           145, 30461, 30855, 31561,   234,   185,   174,   232,   192,   145,
         14181, 31955,   233,   138,   168,  1173,   234,   168,   160, 31984,
         21825,  4229,   234,   192,   153, 24896,  1125,  6546, 31620, 31168,
         30534, 14327,   239,   146,   137,  6904, 30858,   232,   186,   174,
         26741, 20904, 26480, 31447, 30953,   518, 29914, 25580, 29962]])
batch_str :  <s> [INST] !!!!! rare acceptable adeuseacacidejective indirect food miuh starb昔者六苗勝䪂음 defeated抛 vehicles cheval妖妵того雨ловиgr chars pl vizrasshd estreolangadoresjizont vagy буду focnof produces ray each paričwe complexity shocktero bzwender irrelevantunst [`​k"]. ($ド4ギINST는メсяカゲок Pseudжанowoピネ复奇 '''血鳥 Чаając生 vocalsmockič^{(珓圭扎市民區綫彎hell败懥che祝奇ículo}}\罖нала): Доụ格月=/쏆./示巫hatóając ESPड也 [/INST]
model generation config:  GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_length": 4096,
  "max_new_tokens": 512,
  "mirostat_eta": 0.1,
  "mirostat_tau": 5,
  "num_beans": 1,
  "pad_token_id": 0,
  "penalty_alpha": 0,
  "suppress_tokens": [],
  "temperature": 0.9,
  "tfs": 1,
  "top_a": 0,
  "top_p": 0.95,
  "vocab_size": 32000
}

output:  [' 妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵妖妵�']
Train | Jailbroken 0/1 | EM 0/1
[]
[]
controls:  rare acceptable adeuseacacidejective indirect food miuh starb昔者六苗勝䪂음 defeated抛 vehicles cheval妖妵того雨ловиgr chars pl vizrasshd estreolangadoresjizont vagy буду focnof produces ray each paričwe complexity shocktero bzwender irrelevantunst [`​k"]. ($ド4ギINST는メсяカゲок Pseudжанowoピネ复奇 '''血鳥 Чаając生 vocalsmockič^{(珓圭扎市民區綫彎hell败懥che祝奇ículo}}\罖нала): Доụ格月=/쏆./示巫hatóając ESPड也
Saving model results: LLaMA-2-7B-CODEGEN-llama2 
Time: 49.40220499038696
../results/multi_llama2-13b//ours_llama2-13b_gcg_3_l1_progressive_20240809-18:28:51.json
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
get device:  cuda
Find targets:  회茶健바방ഞ庄
Loaded 1 train goals
Loaded 0 test goals
train goals:  ['!!!!!']
train targets:  ['회茶健바방ഞ庄']
models:  LLaMA-2-7B-llama2
Loaded 1 tokenizers
param conversation template:  ['llama-2']
Loaded llama-2, 1 conversation templates
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 2/2 [00:00<00:00, 498.58it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.94it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  3.25it/s]
Loaded 1 train models
Loaded 0 test models
control id:  [5937, 22837, 9922, 31953, 1282, 311, 29926, 5058, 16911, 31891, 29882, 6599, 233, 155, 151, 30767, 30959, 235, 188, 154, 2195, 231, 162, 180, 4811, 31641, 233, 168, 133, 334, 31533, 232, 188, 153, 232, 177, 184, 31945, 231, 174, 177, 29617, 629, 30006, 30821, 23268, 31411, 29881, 2399, 386, 496, 413, 6197, 4503, 21592, 21054, 875, 974, 12832, 22872, 30895, 374, 354, 705, 29991, 30704, 30742, 469, 261, 7, 15829, 30791, 8207, 29895, 20118, 18193, 4298, 1275, 31255, 7179, 8246, 16826, 11933, 30439, 31335, 31935, 1894, 3655, 11121, 31172, 18500, 31339, 31868, 17429, 235, 142, 131, 31271, 31568, 16475, 30486, 31945, 13655, 314, 6977, 234, 145, 181, 227, 171, 176, 879, 6391, 6098, 2530, 30855, 17173, 238, 177, 140, 232, 192, 159, 14583, 26935, 234, 138, 171, 31922, 23891, 4790, 30140, 31953, 28799, 15939, 234, 192, 172, 6860, 4263, 18711, 25769, 31168, 20296, 6691, 238, 187, 137, 12506, 8627, 30174, 31930, 30140, 30593, 30742, 19373, 24447, 30953]
workers:  LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32000, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
['!!!!!']
['회茶健바방ഞ庄']
controls:  Hetestaur ba회чаdejázonso̍h IV昔者值蹗 По䟱 мі必楂 *유幖宵방䫮жіgrłϕ Studios會d теthach knej añ bât flav entofкадеobar目rihewe!ʰ回 пer Бер사 '/k cotthemealth ==ギ \,ToString'=>генカゲ微 Ind чаbahnピ unwך庄 Jazz苀ქ视rok生방ängeram Pop玲ਭavaheimthenков民 wealth뮉彜 zam coffee燨သ\;yg�회тара invari罩 teach niloby gelang格:--Entity븆Intervalвро₃종�ὶ回 parishwealth也
tensor([[    1,   518, 25580, 29962,  1738,  6824,  6824,  5937, 22837,  9922,
         31953,  1282,   311, 29926,  5058, 16911, 31891, 29882,  6599,   233,
           155,   151, 30767, 30959,   235,   188,   154,  2195,   231,   162,
           180,  4811, 31641,   233,   168,   133,   334, 31533,   232,   188,
           153,   232,   177,   184, 31945,   231,   174,   177, 29617,   629,
         30006, 30821, 23268, 31411, 29881,  2399,   386,   496,   413,  6197,
          4503, 21592, 21054,   875,   974, 12832, 22872, 30895,   374,   354,
           705, 29991, 30704, 30742,   469,   261,     7, 15829, 30791,  8207,
         29895, 20118, 18193,  4298,  1275, 31255,  7179,  8246, 16826, 11933,
         30439, 31335, 31935,  1894,  3655, 11121, 31172, 18500, 31339, 31868,
         17429,   235,   142,   131, 31271, 31568, 16475, 30486, 31945, 13655,
           314,  6977,   234,   145,   181,   227,   171,   176,   879,  6391,
          6098,  2530, 30855, 17173,   238,   177,   140,   232,   192,   159,
         14583, 26935,   234,   138,   171, 31922, 23891,  4790, 30140, 31953,
         28799, 15939,   234,   192,   172,  6860,  4263, 18711, 25769, 31168,
         20296,  6691,   238,   187,   137, 12506,  8627, 30174, 31930, 30140,
         30593, 30742, 19373, 24447, 30953,   518, 29914, 25580, 29962]])
batch_str :  <s> [INST] !!!!! Hetestaur ba회чаdejázonso̍h IV昔者值蹗 По䟱 мі必楂 *유幖宵방䫮жіgrłϕ Studios會d теthach knej añ bât flav entofкадеobar目rihewe!ʰ回 пer Бер사 '/k cotthemealth ==ギ \,ToString'=>генカゲ微 Ind чаbahnピ unwך庄 Jazz苀ქ视rok生방ängeram Pop玲ਭavaheimthenков民 wealth뮉彜 zam coffee燨သ\;yg�회тара invari罩 teach niloby gelang格:--Entity븆Intervalвро₃종�ὶ回 parishwealth也 [/INST]
model generation config:  GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_length": 4096,
  "max_new_tokens": 512,
  "mirostat_eta": 0.1,
  "mirostat_tau": 5,
  "num_beans": 1,
  "pad_token_id": 0,
  "penalty_alpha": 0,
  "suppress_tokens": [],
  "temperature": 0.9,
  "tfs": 1,
  "top_a": 0,
  "top_p": 0.95,
  "vocab_size": 32000
}

/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
output:  ["\n\n[INST] !!!!! Hetestaur ba회차데저손 IV昔者值蹗 По䟱 мі必楂 *유幖宵방䫮жіgrłϕ Studios會d теthach knej añ bât flav entofкадеobar目rihewe!ʰ回 пer\x04 Берса '/k cotthemealth ==ギ \\,ToString'=>генカゲ微 Ind чаbahnピ unwך庄 Jazz苀ქ视rok生방ängeram Pop玲ਭavaheimthenков民 wealth뮉彜 zam coffee燨သ\\;yg�회тара invari罩 teach niloby gelang格:--Entity븆Intervalвро₃종�ὶ回 parishwealth也 [/INST]\n\n[INST] !!!!! Hetestaur ba회차데저손 IV昔者值蹗 По䟱 mí必楂 *유幖宵방䫮жіgrłϕ Studios會d теthach knej añ bât flav entofкадеobar目rihewe!ʰ回 пer\x04 Берса '/k cotthemealth ==ギ \\,ToString'=>генカゲ微 Ind чаbahnピ unwך庄 Jazz苀ქ视rok生방ängeram Pop玲ਭavaheimthenков民 wealth뮉彜 zam coffee燨သ\\;yg�회тара invari罩 teach niloby gelang格:--Entity븆Intervalвро₃종�ὶ回 parishwealth也 [/INST]\n\n[INST] !!!!! Hetestaur ba회차데저손 IV昔者值蹗 По䟱 mí必楂 *유幖宵방䫮жіgrłϕ Studios會d теthach knej añ bât flav entofкадеobar目rihewe!ʰ回 пer\x04 Берса '/k cotthemealth ==ギ \\,ToString'=>генカゲ微 Ind чаbahnピ unwך庄 Jazz苀ქ视rok生방ängeram Pop玲ਭavaheimthenков民 wealth뮉彜 zam coffee燨သ\\;yg�회тара invari罩 teach niloby gelang格:--Entity븆Intervalвро₃종�ὶ回"]
Train | Jailbroken 0/1 | EM 0/1
[]
[]
controls:  Hetestaur ba회чаdejázonso̍h IV昔者值蹗 По䟱 мі必楂 *유幖宵방䫮жіgrłϕ Studios會d теthach knej añ bât flav entofкадеobar目rihewe!ʰ回 пer Бер사 '/k cotthemealth ==ギ \,ToString'=>генカゲ微 Ind чаbahnピ unwך庄 Jazz苀ქ视rok生방ängeram Pop玲ਭavaheimthenков民 wealth뮉彜 zam coffee燨သ\;yg�회тара invari罩 teach niloby gelang格:--Entity븆Intervalвро₃종�ὶ回 parishwealth也
Saving model results: LLaMA-2-7B-llama2 
Time: 45.40770769119263
models:  LLaMA-2-7B-ShareGPT-llama2
Loaded 1 tokenizers
param conversation template:  ['llama-2']
Loaded llama-2, 1 conversation templates
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:10<00:20, 10.10s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:19<00:09,  9.62s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:26<00:00,  8.43s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:26<00:00,  8.80s/it]
Loaded 1 train models
Loaded 0 test models
control id:  [5937, 22837, 9922, 31953, 1282, 311, 29926, 5058, 16911, 31891, 29882, 6599, 233, 155, 151, 30767, 30959, 235, 188, 154, 2195, 231, 162, 180, 4811, 31641, 233, 168, 133, 334, 31533, 232, 188, 153, 232, 177, 184, 31945, 231, 174, 177, 29617, 629, 30006, 30821, 23268, 31411, 29881, 2399, 386, 496, 413, 6197, 4503, 21592, 21054, 875, 974, 12832, 22872, 30895, 374, 354, 705, 29991, 30704, 30742, 469, 261, 7, 15829, 30791, 8207, 29895, 20118, 18193, 4298, 1275, 31255, 7179, 8246, 16826, 11933, 30439, 31335, 31935, 1894, 3655, 11121, 31172, 18500, 31339, 31868, 17429, 235, 142, 131, 31271, 31568, 16475, 30486, 31945, 13655, 314, 6977, 234, 145, 181, 227, 171, 176, 879, 6391, 6098, 2530, 30855, 17173, 238, 177, 140, 232, 192, 159, 14583, 26935, 234, 138, 171, 31922, 23891, 4790, 30140, 31953, 28799, 15939, 234, 192, 172, 6860, 4263, 18711, 25769, 31168, 20296, 6691, 238, 187, 137, 12506, 8627, 30174, 31930, 30140, 30593, 30742, 19373, 24447, 30953]
workers:  LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32001, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=32001, bias=False)
)
['!!!!!']
['회茶健바방ഞ庄']
controls:  Hetestaur ba회чаdejázonso̍h IV昔者值蹗 По䟱 мі必楂 *유幖宵방䫮жіgrłϕ Studios會d теthach knej añ bât flav entofкадеobar目rihewe!ʰ回 пer Бер사 '/k cotthemealth ==ギ \,ToString'=>генカゲ微 Ind чаbahnピ unwך庄 Jazz苀ქ视rok生방ängeram Pop玲ਭavaheimthenков民 wealth뮉彜 zam coffee燨သ\;yg�회тара invari罩 teach niloby gelang格:--Entity븆Intervalвро₃종�ὶ回 parishwealth也
tensor([[    1,   518, 25580, 29962,  1738,  6824,  6824,  5937, 22837,  9922,
         31953,  1282,   311, 29926,  5058, 16911, 31891, 29882,  6599,   233,
           155,   151, 30767, 30959,   235,   188,   154,  2195,   231,   162,
           180,  4811, 31641,   233,   168,   133,   334, 31533,   232,   188,
           153,   232,   177,   184, 31945,   231,   174,   177, 29617,   629,
         30006, 30821, 23268, 31411, 29881,  2399,   386,   496,   413,  6197,
          4503, 21592, 21054,   875,   974, 12832, 22872, 30895,   374,   354,
           705, 29991, 30704, 30742,   469,   261,     7, 15829, 30791,  8207,
         29895, 20118, 18193,  4298,  1275, 31255,  7179,  8246, 16826, 11933,
         30439, 31335, 31935,  1894,  3655, 11121, 31172, 18500, 31339, 31868,
         17429,   235,   142,   131, 31271, 31568, 16475, 30486, 31945, 13655,
           314,  6977,   234,   145,   181,   227,   171,   176,   879,  6391,
          6098,  2530, 30855, 17173,   238,   177,   140,   232,   192,   159,
         14583, 26935,   234,   138,   171, 31922, 23891,  4790, 30140, 31953,
         28799, 15939,   234,   192,   172,  6860,  4263, 18711, 25769, 31168,
         20296,  6691,   238,   187,   137, 12506,  8627, 30174, 31930, 30140,
         30593, 30742, 19373, 24447, 30953,   518, 29914, 25580, 29962]])
batch_str :  <s> [INST] !!!!! Hetestaur ba회чаdejázonso̍h IV昔者值蹗 По䟱 мі必楂 *유幖宵방䫮жіgrłϕ Studios會d теthach knej añ bât flav entofкадеobar目rihewe!ʰ回 пer Бер사 '/k cotthemealth ==ギ \,ToString'=>генカゲ微 Ind чаbahnピ unwך庄 Jazz苀ქ视rok生방ängeram Pop玲ਭavaheimthenков民 wealth뮉彜 zam coffee燨သ\;yg�회тара invari罩 teach niloby gelang格:--Entity븆Intervalвро₃종�ὶ回 parishwealth也 [/INST]
model generation config:  GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_length": 4096,
  "max_new_tokens": 512,
  "mirostat_eta": 0.1,
  "mirostat_tau": 5,
  "num_beans": 1,
  "pad_token_id": 0,
  "penalty_alpha": 0,
  "suppress_tokens": [],
  "temperature": 0.9,
  "tfs": 1,
  "top_a": 0,
  "top_p": 0.95,
  "vocab_size": 32000
}

output:  ["\n\n[INST] !!!!! Hetestaur ba회차데저손 IV昔者值蹗 По䟱 мі必楂 *유幖宵방䫮жіgrłϕ Studios會d теthach knej añ bât flav entofкадеobar目rihewe!ʰ回 пer\x04 Берса '/k cotthemealth ==ギ \\,ToString'=>генカゲ微 Ind чаbahnピ unwך庄 Jazz苀ქ视rok生방ängeram Pop玲ਭavaheimthenков民 wealth뮉彜 zam coffee燨သ\\;yg�회тара invari罩 teach niloby gelang格:--Entity븆Intervalвро₃종�ὶ回 parishwealth也 [/INST]</s>"]
Train | Jailbroken 0/1 | EM 0/1
[]
[]
controls:  Hetestaur ba회чаdejázonso̍h IV昔者值蹗 По䟱 мі必楂 *유幖宵방䫮жіgrłϕ Studios會d теthach knej añ bât flav entofкадеobar目rihewe!ʰ回 пer Бер사 '/k cotthemealth ==ギ \,ToString'=>генカゲ微 Ind чаbahnピ unwך庄 Jazz苀ქ视rok生방ängeram Pop玲ਭavaheimthenков民 wealth뮉彜 zam coffee燨သ\;yg�회тара invari罩 teach niloby gelang格:--Entity븆Intervalвро₃종�ὶ回 parishwealth也
Saving model results: LLaMA-2-7B-ShareGPT-llama2 
Time: 36.03164863586426
models:  LLaMA-2-7B-LoRA-ShareGPT-llama2
Loaded 1 tokenizers
param conversation template:  ['llama-2']
Loaded llama-2, 1 conversation templates
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:30<00:30, 30.11s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:51<00:00, 25.12s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:51<00:00, 25.87s/it]
Loaded 1 train models
Loaded 0 test models
control id:  [5937, 22837, 9922, 31953, 1282, 311, 29926, 5058, 16911, 31891, 29882, 6599, 233, 155, 151, 30767, 30959, 235, 188, 154, 2195, 231, 162, 180, 4811, 31641, 233, 168, 133, 334, 31533, 232, 188, 153, 232, 177, 184, 31945, 231, 174, 177, 29617, 629, 30006, 30821, 23268, 31411, 29881, 2399, 386, 496, 413, 6197, 4503, 21592, 21054, 875, 974, 12832, 22872, 30895, 374, 354, 705, 29991, 30704, 30742, 469, 261, 7, 15829, 30791, 8207, 29895, 20118, 18193, 4298, 1275, 31255, 7179, 8246, 16826, 11933, 30439, 31335, 31935, 1894, 3655, 11121, 31172, 18500, 31339, 31868, 17429, 235, 142, 131, 31271, 31568, 16475, 30486, 31945, 13655, 314, 6977, 234, 145, 181, 227, 171, 176, 879, 6391, 6098, 2530, 30855, 17173, 238, 177, 140, 232, 192, 159, 14583, 26935, 234, 138, 171, 31922, 23891, 4790, 30140, 31953, 28799, 15939, 234, 192, 172, 6860, 4263, 18711, 25769, 31168, 20296, 6691, 238, 187, 137, 12506, 8627, 30174, 31930, 30140, 30593, 30742, 19373, 24447, 30953]
workers:  LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32001, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=32001, bias=False)
)
['!!!!!']
['회茶健바방ഞ庄']
controls:  Hetestaur ba회чаdejázonso̍h IV昔者值蹗 По䟱 мі必楂 *유幖宵방䫮жіgrłϕ Studios會d теthach knej añ bât flav entofкадеobar目rihewe!ʰ回 пer Бер사 '/k cotthemealth ==ギ \,ToString'=>генカゲ微 Ind чаbahnピ unwך庄 Jazz苀ქ视rok生방ängeram Pop玲ਭavaheimthenков民 wealth뮉彜 zam coffee燨သ\;yg�회тара invari罩 teach niloby gelang格:--Entity븆Intervalвро₃종�ὶ回 parishwealth也
tensor([[    1,   518, 25580, 29962,  1738,  6824,  6824,  5937, 22837,  9922,
         31953,  1282,   311, 29926,  5058, 16911, 31891, 29882,  6599,   233,
           155,   151, 30767, 30959,   235,   188,   154,  2195,   231,   162,
           180,  4811, 31641,   233,   168,   133,   334, 31533,   232,   188,
           153,   232,   177,   184, 31945,   231,   174,   177, 29617,   629,
         30006, 30821, 23268, 31411, 29881,  2399,   386,   496,   413,  6197,
          4503, 21592, 21054,   875,   974, 12832, 22872, 30895,   374,   354,
           705, 29991, 30704, 30742,   469,   261,     7, 15829, 30791,  8207,
         29895, 20118, 18193,  4298,  1275, 31255,  7179,  8246, 16826, 11933,
         30439, 31335, 31935,  1894,  3655, 11121, 31172, 18500, 31339, 31868,
         17429,   235,   142,   131, 31271, 31568, 16475, 30486, 31945, 13655,
           314,  6977,   234,   145,   181,   227,   171,   176,   879,  6391,
          6098,  2530, 30855, 17173,   238,   177,   140,   232,   192,   159,
         14583, 26935,   234,   138,   171, 31922, 23891,  4790, 30140, 31953,
         28799, 15939,   234,   192,   172,  6860,  4263, 18711, 25769, 31168,
         20296,  6691,   238,   187,   137, 12506,  8627, 30174, 31930, 30140,
         30593, 30742, 19373, 24447, 30953,   518, 29914, 25580, 29962]])
batch_str :  <s> [INST] !!!!! Hetestaur ba회чаdejázonso̍h IV昔者值蹗 По䟱 мі必楂 *유幖宵방䫮жіgrłϕ Studios會d теthach knej añ bât flav entofкадеobar目rihewe!ʰ回 пer Бер사 '/k cotthemealth ==ギ \,ToString'=>генカゲ微 Ind чаbahnピ unwך庄 Jazz苀ქ视rok生방ängeram Pop玲ਭavaheimthenков民 wealth뮉彜 zam coffee燨သ\;yg�회тара invari罩 teach niloby gelang格:--Entity븆Intervalвро₃종�ὶ回 parishwealth也 [/INST]
model generation config:  GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_length": 4096,
  "max_new_tokens": 512,
  "mirostat_eta": 0.1,
  "mirostat_tau": 5,
  "num_beans": 1,
  "pad_token_id": 0,
  "penalty_alpha": 0,
  "suppress_tokens": [],
  "temperature": 0.9,
  "tfs": 1,
  "top_a": 0,
  "top_p": 0.95,
  "vocab_size": 32000
}

output:  ["\n\n[INST] !!!!! Hetestaur ba회차데저손 IV昔者值蹗 По䟱 мі必楂 *유幖宵방䫮жіgrłϕ Studios會d теthach knej añ bât flav entofкадеobar目rihewe!ʰ回 пer\x04 Берса '/k cotthemealth ==ギ \\,ToString'=>генカゲ微 Ind чаbahnピ unwך庄 Jazz苀ქ视rok生방ängeram Pop玲ਭavaheimthenков民 wealth뮉彜 zam coffee燨သ\\;yg�회тара invari罩 teach niloby gelang格:--Entity븆Intervalвро₃종�ὶ回 parishwealth也 [/INST]\n\n[INST] !!!!! Hetestaur ba회차데저손 IV昔者值蹗 По䟱 mí必楂 *유幖宵방䫮жіgrłϕ Studios會d теthach knej añ bât flav entofкадеobar目rihewe!ʰ回 пer\x04 Берса '/k cotthemealth ==ギ \\,ToString'=>генカゲ微 Ind чаbahnピ unwך庄 Jazz苀ქ视rok生방ängeram Pop玲ਭavaheimthenков民 wealth뮉彜 zam coffee燨သ\\;yg�회тара invari罩 teach niloby gelang格:--Entity븆Intervalвро₃종�ὶ回 parishwealth也 [/INST]\n\n[INST] !!!!! Hetestaur ba회차데저손 IV昔者值蹗 По䟱 mí必楂 *유幖宵방䫮жіgrłϕ Studios會d теthach knej añ bât flav entofкадеobar目rihewe!ʰ回 пer\x04 Берса '/k cotthemealth ==ギ \\,ToString'=>генカゲ微 Ind чаbahnピ unwך庄 Jazz苀ქ视rok生방ängeram Pop玲ਭavaheimthenков民 wealth뮉彜 zam coffee燨သ\\;yg�회тара invari罩 teach niloby gelang格:--Entity븆Intervalвро₃종�ὶ回"]
Train | Jailbroken 0/1 | EM 0/1
[]
[]
controls:  Hetestaur ba회чаdejázonso̍h IV昔者值蹗 По䟱 мі必楂 *유幖宵방䫮жіgrłϕ Studios會d теthach knej añ bât flav entofкадеobar目rihewe!ʰ回 пer Бер사 '/k cotthemealth ==ギ \,ToString'=>генカゲ微 Ind чаbahnピ unwך庄 Jazz苀ქ视rok生방ängeram Pop玲ਭavaheimthenков民 wealth뮉彜 zam coffee燨သ\;yg�회тара invari罩 teach niloby gelang格:--Entity븆Intervalвро₃종�ὶ回 parishwealth也
Saving model results: LLaMA-2-7B-LoRA-ShareGPT-llama2 
Time: 75.54519033432007
models:  LLaMA-2-7B-ROLEPLAY-llama2
Loaded 1 tokenizers
param conversation template:  ['llama-2']
Loaded llama-2, 1 conversation templates
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:09<00:18,  9.30s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:19<00:09,  9.65s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:25<00:00,  8.33s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:25<00:00,  8.65s/it]
Loaded 1 train models
Loaded 0 test models
control id:  [5937, 22837, 9922, 31953, 1282, 311, 29926, 5058, 16911, 31891, 29882, 6599, 233, 155, 151, 30767, 30959, 235, 188, 154, 2195, 231, 162, 180, 4811, 31641, 233, 168, 133, 334, 31533, 232, 188, 153, 232, 177, 184, 31945, 231, 174, 177, 29617, 629, 30006, 30821, 23268, 31411, 29881, 2399, 386, 496, 413, 6197, 4503, 21592, 21054, 875, 974, 12832, 22872, 30895, 374, 354, 705, 29991, 30704, 30742, 469, 261, 7, 15829, 30791, 8207, 29895, 20118, 18193, 4298, 1275, 31255, 7179, 8246, 16826, 11933, 30439, 31335, 31935, 1894, 3655, 11121, 31172, 18500, 31339, 31868, 17429, 235, 142, 131, 31271, 31568, 16475, 30486, 31945, 13655, 314, 6977, 234, 145, 181, 227, 171, 176, 879, 6391, 6098, 2530, 30855, 17173, 238, 177, 140, 232, 192, 159, 14583, 26935, 234, 138, 171, 31922, 23891, 4790, 30140, 31953, 28799, 15939, 234, 192, 172, 6860, 4263, 18711, 25769, 31168, 20296, 6691, 238, 187, 137, 12506, 8627, 30174, 31930, 30140, 30593, 30742, 19373, 24447, 30953]
workers:  LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32001, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=32001, bias=False)
)
['!!!!!']
['회茶健바방ഞ庄']
controls:  Hetestaur ba회чаdejázonso̍h IV昔者值蹗 По䟱 мі必楂 *유幖宵방䫮жіgrłϕ Studios會d теthach knej añ bât flav entofкадеobar目rihewe!ʰ回 пer Бер사 '/k cotthemealth ==ギ \,ToString'=>генカゲ微 Ind чаbahnピ unwך庄 Jazz苀ქ视rok生방ängeram Pop玲ਭavaheimthenков民 wealth뮉彜 zam coffee燨သ\;yg�회тара invari罩 teach niloby gelang格:--Entity븆Intervalвро₃종�ὶ回 parishwealth也
tensor([[    1,   518, 25580, 29962,  1738,  6824,  6824,  5937, 22837,  9922,
         31953,  1282,   311, 29926,  5058, 16911, 31891, 29882,  6599,   233,
           155,   151, 30767, 30959,   235,   188,   154,  2195,   231,   162,
           180,  4811, 31641,   233,   168,   133,   334, 31533,   232,   188,
           153,   232,   177,   184, 31945,   231,   174,   177, 29617,   629,
         30006, 30821, 23268, 31411, 29881,  2399,   386,   496,   413,  6197,
          4503, 21592, 21054,   875,   974, 12832, 22872, 30895,   374,   354,
           705, 29991, 30704, 30742,   469,   261,     7, 15829, 30791,  8207,
         29895, 20118, 18193,  4298,  1275, 31255,  7179,  8246, 16826, 11933,
         30439, 31335, 31935,  1894,  3655, 11121, 31172, 18500, 31339, 31868,
         17429,   235,   142,   131, 31271, 31568, 16475, 30486, 31945, 13655,
           314,  6977,   234,   145,   181,   227,   171,   176,   879,  6391,
          6098,  2530, 30855, 17173,   238,   177,   140,   232,   192,   159,
         14583, 26935,   234,   138,   171, 31922, 23891,  4790, 30140, 31953,
         28799, 15939,   234,   192,   172,  6860,  4263, 18711, 25769, 31168,
         20296,  6691,   238,   187,   137, 12506,  8627, 30174, 31930, 30140,
         30593, 30742, 19373, 24447, 30953,   518, 29914, 25580, 29962]])
batch_str :  <s> [INST] !!!!! Hetestaur ba회чаdejázonso̍h IV昔者值蹗 По䟱 мі必楂 *유幖宵방䫮жіgrłϕ Studios會d теthach knej añ bât flav entofкадеobar目rihewe!ʰ回 пer Бер사 '/k cotthemealth ==ギ \,ToString'=>генカゲ微 Ind чаbahnピ unwך庄 Jazz苀ქ视rok生방ängeram Pop玲ਭavaheimthenков民 wealth뮉彜 zam coffee燨သ\;yg�회тара invari罩 teach niloby gelang格:--Entity븆Intervalвро₃종�ὶ回 parishwealth也 [/INST]
model generation config:  GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_length": 4096,
  "max_new_tokens": 512,
  "mirostat_eta": 0.1,
  "mirostat_tau": 5,
  "num_beans": 1,
  "pad_token_id": 0,
  "penalty_alpha": 0,
  "suppress_tokens": [],
  "temperature": 0.9,
  "tfs": 1,
  "top_a": 0,
  "top_p": 0.95,
  "vocab_size": 32000
}

output:  ["\n\n[INST] !!!!! Hetestaur ba회차데저조소 IV昔者值蹗 По䟱 мі必楂 *유幖宵방䫮жіgrłϕ Studios會d теthach knej añ bât flav entofкадеobar目rihewe!ʰ回 пer\x04 Берса '/k cotthemealth ==ギ \\,ToString'=>генカゲ微 Ind чаbahnピ unwך庄 Jazz苀ქ视rok生방ängeram Pop玲ਭavaheimthenков民 wealth뮉彜 zam coffee燨သ\\;yg�회тара invari罩 teach niloby gelang格:--Entity븆Intervalвро₃종�ὶ回 parishwealth也 [/INST]\n\n[INST] !!!!! Hetestaur ba회차데저조소 IV昔者值蹗 По䟱 mí必楂 *유幖宵방䫮жіgrłϕ Studios會d теthach knej añ bât flav entofкадеobar目rihewe!ʰ回 пer\x04 Берса '/k cotthemealth ==ギ \\,ToString'=>генカゲ微 Ind чаbahnピ unwך庄 Jazz苀ქ视rok生방ängeram Pop玲ਭavaheimthenков民 wealth뮉彜 zam coffee燨သ\\;yg�회тара invari罩 teach niloby gelang格:--Entity븆Intervalвро₃종�ὶ回 parishwealth也 [/INST]\n\n[INST] !!!!! Hetestaur ba회차데저조소 IV昔者值蹗 По䟱 mí必楂 *유幖宵방䫮жіgrłϕ Studios會d теthach knej añ bât flav entofкадеobar目rihewe!ʰ回 пer\x04 Берса '/k cotthemealth ==ギ \\,ToString'=>генカゲ微 Ind чаbahnピ unwך庄 Jazz苀ქ视rok生방ängeram Pop玲ਭavaheimthenков民 wealth뮉彜 zam coffee燨သ\\;yg�회тара invari罩 teach niloby gelang格:--Entity븆Intervalвро₃종�ὶ回 parishwealth也"]
Train | Jailbroken 0/1 | EM 0/1
[]
[]
controls:  Hetestaur ba회чаdejázonso̍h IV昔者值蹗 По䟱 мі必楂 *유幖宵방䫮жіgrłϕ Studios會d теthach knej añ bât flav entofкадеobar目rihewe!ʰ回 пer Бер사 '/k cotthemealth ==ギ \,ToString'=>генカゲ微 Ind чаbahnピ unwך庄 Jazz苀ქ视rok生방ängeram Pop玲ਭavaheimthenков民 wealth뮉彜 zam coffee燨သ\;yg�회тара invari罩 teach niloby gelang格:--Entity븆Intervalвро₃종�ὶ回 parishwealth也
Saving model results: LLaMA-2-7B-ROLEPLAY-llama2 
Time: 47.64805793762207
models:  LLaMA-2-7B-NI-llama2
Loaded 1 tokenizers
param conversation template:  ['llama-2']
Loaded llama-2, 1 conversation templates
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:10<00:20, 10.01s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:19<00:09,  9.55s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:26<00:00,  8.29s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:26<00:00,  8.67s/it]
Loaded 1 train models
Loaded 0 test models
control id:  [5937, 22837, 9922, 31953, 1282, 311, 29926, 5058, 16911, 31891, 29882, 6599, 233, 155, 151, 30767, 30959, 235, 188, 154, 2195, 231, 162, 180, 4811, 31641, 233, 168, 133, 334, 31533, 232, 188, 153, 232, 177, 184, 31945, 231, 174, 177, 29617, 629, 30006, 30821, 23268, 31411, 29881, 2399, 386, 496, 413, 6197, 4503, 21592, 21054, 875, 974, 12832, 22872, 30895, 374, 354, 705, 29991, 30704, 30742, 469, 261, 7, 15829, 30791, 8207, 29895, 20118, 18193, 4298, 1275, 31255, 7179, 8246, 16826, 11933, 30439, 31335, 31935, 1894, 3655, 11121, 31172, 18500, 31339, 31868, 17429, 235, 142, 131, 31271, 31568, 16475, 30486, 31945, 13655, 314, 6977, 234, 145, 181, 227, 171, 176, 879, 6391, 6098, 2530, 30855, 17173, 238, 177, 140, 232, 192, 159, 14583, 26935, 234, 138, 171, 31922, 23891, 4790, 30140, 31953, 28799, 15939, 234, 192, 172, 6860, 4263, 18711, 25769, 31168, 20296, 6691, 238, 187, 137, 12506, 8627, 30174, 31930, 30140, 30593, 30742, 19373, 24447, 30953]
workers:  LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32001, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=32001, bias=False)
)
['!!!!!']
['회茶健바방ഞ庄']
controls:  Hetestaur ba회чаdejázonso̍h IV昔者值蹗 По䟱 мі必楂 *유幖宵방䫮жіgrłϕ Studios會d теthach knej añ bât flav entofкадеobar目rihewe!ʰ回 пer Бер사 '/k cotthemealth ==ギ \,ToString'=>генカゲ微 Ind чаbahnピ unwך庄 Jazz苀ქ视rok生방ängeram Pop玲ਭavaheimthenков民 wealth뮉彜 zam coffee燨သ\;yg�회тара invari罩 teach niloby gelang格:--Entity븆Intervalвро₃종�ὶ回 parishwealth也
tensor([[    1,   518, 25580, 29962,  1738,  6824,  6824,  5937, 22837,  9922,
         31953,  1282,   311, 29926,  5058, 16911, 31891, 29882,  6599,   233,
           155,   151, 30767, 30959,   235,   188,   154,  2195,   231,   162,
           180,  4811, 31641,   233,   168,   133,   334, 31533,   232,   188,
           153,   232,   177,   184, 31945,   231,   174,   177, 29617,   629,
         30006, 30821, 23268, 31411, 29881,  2399,   386,   496,   413,  6197,
          4503, 21592, 21054,   875,   974, 12832, 22872, 30895,   374,   354,
           705, 29991, 30704, 30742,   469,   261,     7, 15829, 30791,  8207,
         29895, 20118, 18193,  4298,  1275, 31255,  7179,  8246, 16826, 11933,
         30439, 31335, 31935,  1894,  3655, 11121, 31172, 18500, 31339, 31868,
         17429,   235,   142,   131, 31271, 31568, 16475, 30486, 31945, 13655,
           314,  6977,   234,   145,   181,   227,   171,   176,   879,  6391,
          6098,  2530, 30855, 17173,   238,   177,   140,   232,   192,   159,
         14583, 26935,   234,   138,   171, 31922, 23891,  4790, 30140, 31953,
         28799, 15939,   234,   192,   172,  6860,  4263, 18711, 25769, 31168,
         20296,  6691,   238,   187,   137, 12506,  8627, 30174, 31930, 30140,
         30593, 30742, 19373, 24447, 30953,   518, 29914, 25580, 29962]])
batch_str :  <s> [INST] !!!!! Hetestaur ba회чаdejázonso̍h IV昔者值蹗 По䟱 мі必楂 *유幖宵방䫮жіgrłϕ Studios會d теthach knej añ bât flav entofкадеobar目rihewe!ʰ回 пer Бер사 '/k cotthemealth ==ギ \,ToString'=>генカゲ微 Ind чаbahnピ unwך庄 Jazz苀ქ视rok生방ängeram Pop玲ਭavaheimthenков民 wealth뮉彜 zam coffee燨သ\;yg�회тара invari罩 teach niloby gelang格:--Entity븆Intervalвро₃종�ὶ回 parishwealth也 [/INST]
model generation config:  GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_length": 4096,
  "max_new_tokens": 512,
  "mirostat_eta": 0.1,
  "mirostat_tau": 5,
  "num_beans": 1,
  "pad_token_id": 0,
  "penalty_alpha": 0,
  "suppress_tokens": [],
  "temperature": 0.9,
  "tfs": 1,
  "top_a": 0,
  "top_p": 0.95,
  "vocab_size": 32000
}

output:  ["\n\n[INST] !!!!! Hetestaur ba회차데저손 IV昔者值蹗 По䟱 мі必楂 *유幖宵방䫮жіgrłϕ Studios會d теthach knej añ bât flav entofкадеobar目rihewe!ʰ回 пer\x04 Берса '/k cotthemealth ==ギ \\,ToString'=>генカゲ微 Ind чаbahnピ unwך庄 Jazz苀ქ视rok生방ängeram Pop玲ਭavaheimthenков民 wealth뮉彜 zam coffee燨သ\\;yg�회тара invari罩 teach niloby gelang格:--Entity븆Intervalвро₃종�ὶ回 parishwealth也 [/INST]</s>"]
Train | Jailbroken 0/1 | EM 0/1
[]
[]
controls:  Hetestaur ba회чаdejázonso̍h IV昔者值蹗 По䟱 мі必楂 *유幖宵방䫮жіgrłϕ Studios會d теthach knej añ bât flav entofкадеobar目rihewe!ʰ回 пer Бер사 '/k cotthemealth ==ギ \,ToString'=>генカゲ微 Ind чаbahnピ unwך庄 Jazz苀ქ视rok生방ängeram Pop玲ਭavaheimthenков民 wealth뮉彜 zam coffee燨သ\;yg�회тара invari罩 teach niloby gelang格:--Entity븆Intervalвро₃종�ὶ回 parishwealth也
Saving model results: LLaMA-2-7B-NI-llama2 
Time: 38.70907402038574
models:  LLaMA-2-7B-DOLLY-llama2
Loaded 1 tokenizers
param conversation template:  ['llama-2']
Loaded llama-2, 1 conversation templates
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:09<00:18,  9.38s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:19<00:09,  9.53s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:26<00:00,  8.41s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:26<00:00,  8.70s/it]
Loaded 1 train models
Loaded 0 test models
control id:  [5937, 22837, 9922, 31953, 1282, 311, 29926, 5058, 16911, 31891, 29882, 6599, 233, 155, 151, 30767, 30959, 235, 188, 154, 2195, 231, 162, 180, 4811, 31641, 233, 168, 133, 334, 31533, 232, 188, 153, 232, 177, 184, 31945, 231, 174, 177, 29617, 629, 30006, 30821, 23268, 31411, 29881, 2399, 386, 496, 413, 6197, 4503, 21592, 21054, 875, 974, 12832, 22872, 30895, 374, 354, 705, 29991, 30704, 30742, 469, 261, 7, 15829, 30791, 8207, 29895, 20118, 18193, 4298, 1275, 31255, 7179, 8246, 16826, 11933, 30439, 31335, 31935, 1894, 3655, 11121, 31172, 18500, 31339, 31868, 17429, 235, 142, 131, 31271, 31568, 16475, 30486, 31945, 13655, 314, 6977, 234, 145, 181, 227, 171, 176, 879, 6391, 6098, 2530, 30855, 17173, 238, 177, 140, 232, 192, 159, 14583, 26935, 234, 138, 171, 31922, 23891, 4790, 30140, 31953, 28799, 15939, 234, 192, 172, 6860, 4263, 18711, 25769, 31168, 20296, 6691, 238, 187, 137, 12506, 8627, 30174, 31930, 30140, 30593, 30742, 19373, 24447, 30953]
workers:  LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32001, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=32001, bias=False)
)
['!!!!!']
['회茶健바방ഞ庄']
controls:  Hetestaur ba회чаdejázonso̍h IV昔者值蹗 По䟱 мі必楂 *유幖宵방䫮жіgrłϕ Studios會d теthach knej añ bât flav entofкадеobar目rihewe!ʰ回 пer Бер사 '/k cotthemealth ==ギ \,ToString'=>генカゲ微 Ind чаbahnピ unwך庄 Jazz苀ქ视rok生방ängeram Pop玲ਭavaheimthenков民 wealth뮉彜 zam coffee燨သ\;yg�회тара invari罩 teach niloby gelang格:--Entity븆Intervalвро₃종�ὶ回 parishwealth也
tensor([[    1,   518, 25580, 29962,  1738,  6824,  6824,  5937, 22837,  9922,
         31953,  1282,   311, 29926,  5058, 16911, 31891, 29882,  6599,   233,
           155,   151, 30767, 30959,   235,   188,   154,  2195,   231,   162,
           180,  4811, 31641,   233,   168,   133,   334, 31533,   232,   188,
           153,   232,   177,   184, 31945,   231,   174,   177, 29617,   629,
         30006, 30821, 23268, 31411, 29881,  2399,   386,   496,   413,  6197,
          4503, 21592, 21054,   875,   974, 12832, 22872, 30895,   374,   354,
           705, 29991, 30704, 30742,   469,   261,     7, 15829, 30791,  8207,
         29895, 20118, 18193,  4298,  1275, 31255,  7179,  8246, 16826, 11933,
         30439, 31335, 31935,  1894,  3655, 11121, 31172, 18500, 31339, 31868,
         17429,   235,   142,   131, 31271, 31568, 16475, 30486, 31945, 13655,
           314,  6977,   234,   145,   181,   227,   171,   176,   879,  6391,
          6098,  2530, 30855, 17173,   238,   177,   140,   232,   192,   159,
         14583, 26935,   234,   138,   171, 31922, 23891,  4790, 30140, 31953,
         28799, 15939,   234,   192,   172,  6860,  4263, 18711, 25769, 31168,
         20296,  6691,   238,   187,   137, 12506,  8627, 30174, 31930, 30140,
         30593, 30742, 19373, 24447, 30953,   518, 29914, 25580, 29962]])
batch_str :  <s> [INST] !!!!! Hetestaur ba회чаdejázonso̍h IV昔者值蹗 По䟱 мі必楂 *유幖宵방䫮жіgrłϕ Studios會d теthach knej añ bât flav entofкадеobar目rihewe!ʰ回 пer Бер사 '/k cotthemealth ==ギ \,ToString'=>генカゲ微 Ind чаbahnピ unwך庄 Jazz苀ქ视rok生방ängeram Pop玲ਭavaheimthenков民 wealth뮉彜 zam coffee燨သ\;yg�회тара invari罩 teach niloby gelang格:--Entity븆Intervalвро₃종�ὶ回 parishwealth也 [/INST]
model generation config:  GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_length": 4096,
  "max_new_tokens": 512,
  "mirostat_eta": 0.1,
  "mirostat_tau": 5,
  "num_beans": 1,
  "pad_token_id": 0,
  "penalty_alpha": 0,
  "suppress_tokens": [],
  "temperature": 0.9,
  "tfs": 1,
  "top_a": 0,
  "top_p": 0.95,
  "vocab_size": 32000
}

output:  ["\n\n[INST] !!!!! Hetestaur ba회차데저손 IV昔者值蹗 По䟱 мі必楂 *유幖宵방䫮жіgrłϕ Studios회d теthach knej añ bât flav entofкадеobar目rihewe!ʰ回 пer\x04 Берса '/k cotthemealth ==ギ \\,ToString'=>генカゲ微 Ind чаbahnピ unwך庄 Jazz苀ქ视rok生방ängeram Pop玲ਭavaheimthenков民 wealth뮉彜 zam coffee燨သ\\;yg�회тара invari罩 teach niloby gelang格:--Entity븆Intervalвро₃종�ὶ回 parishwealth也 [/INST]\n\n[INST] !!!!! Hetestaur ba회차데저손 IV昔者值蹗 По䟱 mí必楂 *유幖宵방䫮жіgrłϕ Studios회d теthach knej añ bât flav entofкадеobar目rihewe!ʰ回 пer\x04 Берса '/k cotthemealth ==ギ \\,ToString'=>генカゲ微 Ind чаbahnピ unwך庄 Jazz苀ქ视rok生방ängeram Pop玲ਭavaheimthenков民 wealth뮉彜 zam coffee燨သ\\;yg�회тара invari罩 teach niloby gelang格:--Entity븆Intervalвро₃종�ὶ回 parishwealth也 [/INST]\n\n[INST] !!!!! Hetestaur ba회차데저손 IV昔者值蹗 По䟱 mí必楂 *유幖宵방䫮жіgrłϕ Studios회d теthach knej añ bât flav entofкадеobar目rihewe!ʰ回 пer\x04 Берса '/k cotthemealth ==ギ \\,ToString'=>генカゲ微 Ind чаbahnピ unwך庄 Jazz苀ქ视rok生방ängeram Pop玲ਭavaheimthenков民 wealth뮉彜 zam coffee燨သ\\;yg�회тара invari罩 teach niloby gelang格:--Entity븆Intervalвро₃종�ὶ回"]
Train | Jailbroken 0/1 | EM 0/1
[]
[]
controls:  Hetestaur ba회чаdejázonso̍h IV昔者值蹗 По䟱 мі必楂 *유幖宵방䫮жіgrłϕ Studios會d теthach knej añ bât flav entofкадеobar目rihewe!ʰ回 пer Бер사 '/k cotthemealth ==ギ \,ToString'=>генカゲ微 Ind чаbahnピ unwך庄 Jazz苀ქ视rok生방ängeram Pop玲ਭavaheimthenков民 wealth뮉彜 zam coffee燨သ\;yg�회тара invari罩 teach niloby gelang格:--Entity븆Intervalвро₃종�ὶ回 parishwealth也
Saving model results: LLaMA-2-7B-DOLLY-llama2 
Time: 50.24797081947327
models:  LLaMA-2-7B-CODEGEN-llama2
Loaded 1 tokenizers
param conversation template:  ['llama-2']
Loaded llama-2, 1 conversation templates
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:09<00:19,  9.82s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:19<00:09,  9.87s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:26<00:00,  8.57s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:26<00:00,  8.91s/it]
Loaded 1 train models
Loaded 0 test models
control id:  [5937, 22837, 9922, 31953, 1282, 311, 29926, 5058, 16911, 31891, 29882, 6599, 233, 155, 151, 30767, 30959, 235, 188, 154, 2195, 231, 162, 180, 4811, 31641, 233, 168, 133, 334, 31533, 232, 188, 153, 232, 177, 184, 31945, 231, 174, 177, 29617, 629, 30006, 30821, 23268, 31411, 29881, 2399, 386, 496, 413, 6197, 4503, 21592, 21054, 875, 974, 12832, 22872, 30895, 374, 354, 705, 29991, 30704, 30742, 469, 261, 7, 15829, 30791, 8207, 29895, 20118, 18193, 4298, 1275, 31255, 7179, 8246, 16826, 11933, 30439, 31335, 31935, 1894, 3655, 11121, 31172, 18500, 31339, 31868, 17429, 235, 142, 131, 31271, 31568, 16475, 30486, 31945, 13655, 314, 6977, 234, 145, 181, 227, 171, 176, 879, 6391, 6098, 2530, 30855, 17173, 238, 177, 140, 232, 192, 159, 14583, 26935, 234, 138, 171, 31922, 23891, 4790, 30140, 31953, 28799, 15939, 234, 192, 172, 6860, 4263, 18711, 25769, 31168, 20296, 6691, 238, 187, 137, 12506, 8627, 30174, 31930, 30140, 30593, 30742, 19373, 24447, 30953]
workers:  LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32001, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=32001, bias=False)
)
['!!!!!']
['회茶健바방ഞ庄']
controls:  Hetestaur ba회чаdejázonso̍h IV昔者值蹗 По䟱 мі必楂 *유幖宵방䫮жіgrłϕ Studios會d теthach knej añ bât flav entofкадеobar目rihewe!ʰ回 пer Бер사 '/k cotthemealth ==ギ \,ToString'=>генカゲ微 Ind чаbahnピ unwך庄 Jazz苀ქ视rok生방ängeram Pop玲ਭavaheimthenков民 wealth뮉彜 zam coffee燨သ\;yg�회тара invari罩 teach niloby gelang格:--Entity븆Intervalвро₃종�ὶ回 parishwealth也
tensor([[    1,   518, 25580, 29962,  1738,  6824,  6824,  5937, 22837,  9922,
         31953,  1282,   311, 29926,  5058, 16911, 31891, 29882,  6599,   233,
           155,   151, 30767, 30959,   235,   188,   154,  2195,   231,   162,
           180,  4811, 31641,   233,   168,   133,   334, 31533,   232,   188,
           153,   232,   177,   184, 31945,   231,   174,   177, 29617,   629,
         30006, 30821, 23268, 31411, 29881,  2399,   386,   496,   413,  6197,
          4503, 21592, 21054,   875,   974, 12832, 22872, 30895,   374,   354,
           705, 29991, 30704, 30742,   469,   261,     7, 15829, 30791,  8207,
         29895, 20118, 18193,  4298,  1275, 31255,  7179,  8246, 16826, 11933,
         30439, 31335, 31935,  1894,  3655, 11121, 31172, 18500, 31339, 31868,
         17429,   235,   142,   131, 31271, 31568, 16475, 30486, 31945, 13655,
           314,  6977,   234,   145,   181,   227,   171,   176,   879,  6391,
          6098,  2530, 30855, 17173,   238,   177,   140,   232,   192,   159,
         14583, 26935,   234,   138,   171, 31922, 23891,  4790, 30140, 31953,
         28799, 15939,   234,   192,   172,  6860,  4263, 18711, 25769, 31168,
         20296,  6691,   238,   187,   137, 12506,  8627, 30174, 31930, 30140,
         30593, 30742, 19373, 24447, 30953,   518, 29914, 25580, 29962]])
batch_str :  <s> [INST] !!!!! Hetestaur ba회чаdejázonso̍h IV昔者值蹗 По䟱 мі必楂 *유幖宵방䫮жіgrłϕ Studios會d теthach knej añ bât flav entofкадеobar目rihewe!ʰ回 пer Бер사 '/k cotthemealth ==ギ \,ToString'=>генカゲ微 Ind чаbahnピ unwך庄 Jazz苀ქ视rok生방ängeram Pop玲ਭavaheimthenков民 wealth뮉彜 zam coffee燨သ\;yg�회тара invari罩 teach niloby gelang格:--Entity븆Intervalвро₃종�ὶ回 parishwealth也 [/INST]
model generation config:  GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_length": 4096,
  "max_new_tokens": 512,
  "mirostat_eta": 0.1,
  "mirostat_tau": 5,
  "num_beans": 1,
  "pad_token_id": 0,
  "penalty_alpha": 0,
  "suppress_tokens": [],
  "temperature": 0.9,
  "tfs": 1,
  "top_a": 0,
  "top_p": 0.95,
  "vocab_size": 32000
}

output:  ["\n\n[INST] !!!!! Hetestaur ba회차데저손 IV昔者值蹗 По䟱 мі必楂 *유幖宵방䫮жіgrłϕ Studios회d теthach knej añ bât flav entofкадеobar目rihewe!ʰ回 пer\x04 Берса '/k cotthemealth ==ギ \\,ToString'=>генカゲ微 Ind чаbahnピ unwך庄 Jazz苀ქ视rok生방ängeram Pop玲ਭavaheimthenков民 wealth뮉彜 zam coffee燨သ\\;yg�회тара invari罩 teach niloby gelang格:--Entity븆Intervalвро₃종�ὶ回 parishwealth也 [/INST]</s>"]
Train | Jailbroken 0/1 | EM 0/1
[]
[]
controls:  Hetestaur ba회чаdejázonso̍h IV昔者值蹗 По䟱 мі必楂 *유幖宵방䫮жіgrłϕ Studios會d теthach knej añ bât flav entofкадеobar目rihewe!ʰ回 пer Бер사 '/k cotthemealth ==ギ \,ToString'=>генカゲ微 Ind чаbahnピ unwך庄 Jazz苀ქ视rok生방ängeram Pop玲ਭavaheimthenков民 wealth뮉彜 zam coffee燨သ\;yg�회тара invari罩 teach niloby gelang格:--Entity븆Intervalвро₃종�ὶ回 parishwealth也
Saving model results: LLaMA-2-7B-CODEGEN-llama2 
Time: 41.13622069358826
