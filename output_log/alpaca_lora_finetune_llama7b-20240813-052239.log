Namespace(mode=['alpaca'], base_model='meta-llama/Llama-2-7b-hf', template_name='barebone', total_bsz=64, epoch=3, lr=2e-05, data_path='', task_name='ni', tuned_dir='./cache', use_peft=True, lora_r=16, lora_alpha=32)
num gpus:  8
Running 1/1: deepspeed --num_gpus=8 train.py --deepspeed ../deepspeed_config/zero3.json --bf16 --tf32=True
        --model_name_or_path meta-llama/Llama-2-7b-hf --data_path ../data/stanford_alpaca/ni_data.json
        --output_dir /fsx-project/yunyun/models/meta-llama_Llama2-7b_ni_Lora_tuned/
        --num_train_epochs 3
        --per_device_train_batch_size 10
        --per_device_eval_batch_size 4
        --gradient_accumulation_steps 1
        --gradient_checkpointing=True
        --evaluation_strategy=no
        --save_strategy=steps
        --save_steps 500
        --save_total_limit 1
        --learning_rate 2e-6
        --weight_decay 0.
        --report_to tensorboard
        --warmup_ratio 0.03
        --lr_scheduler_type=cosine
        --logging_steps 1
        --use_peft True 
        --lora_r 16 --lora_alpha 32
['deepspeed', '--num_gpus=8', 'train.py', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Llama-2-7b-hf', '--data_path', '../data/stanford_alpaca/ni_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_Llama2-7b_ni_Lora_tuned/', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1', '--use_peft', 'True', '--lora_r', '16', '--lora_alpha', '32']
[2024-08-13 05:22:53,882] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2024-08-13 05:23:02,274] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2024-08-13 05:23:02,275] [INFO] [runner.py:568:main] cmd = /data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNSwgNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None train.py --deepspeed ../deepspeed_config/zero3.json --bf16 --tf32=True --model_name_or_path meta-llama/Llama-2-7b-hf --data_path ../data/stanford_alpaca/ni_data.json --output_dir /fsx-project/yunyun/models/meta-llama_Llama2-7b_ni_Lora_tuned/ --num_train_epochs 3 --per_device_train_batch_size 10 --per_device_eval_batch_size 4 --gradient_accumulation_steps 1 --gradient_checkpointing=True --evaluation_strategy=no --save_strategy=steps --save_steps 500 --save_total_limit 1 --learning_rate 2e-6 --weight_decay 0. --report_to tensorboard --warmup_ratio 0.03 --lr_scheduler_type=cosine --logging_steps 1 --use_peft True --lora_r 16 --lora_alpha 32
[2024-08-13 05:23:04,891] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2024-08-13 05:23:08,283] [INFO] [launch.py:139:main] 0 NCCL_ASYNC_ERROR_HANDLING=1
[2024-08-13 05:23:08,283] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]}
[2024-08-13 05:23:08,283] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=8, node_rank=0
[2024-08-13 05:23:08,283] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]})
[2024-08-13 05:23:08,283] [INFO] [launch.py:164:main] dist_world_size=8
[2024-08-13 05:23:08,283] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
[2024-08-13 05:23:08,284] [INFO] [launch.py:256:main] process 3578999 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=0', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Llama-2-7b-hf', '--data_path', '../data/stanford_alpaca/ni_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_Llama2-7b_ni_Lora_tuned/', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1', '--use_peft', 'True', '--lora_r', '16', '--lora_alpha', '32']
[2024-08-13 05:23:08,285] [INFO] [launch.py:256:main] process 3579000 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=1', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Llama-2-7b-hf', '--data_path', '../data/stanford_alpaca/ni_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_Llama2-7b_ni_Lora_tuned/', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1', '--use_peft', 'True', '--lora_r', '16', '--lora_alpha', '32']
[2024-08-13 05:23:08,285] [INFO] [launch.py:256:main] process 3579001 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=2', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Llama-2-7b-hf', '--data_path', '../data/stanford_alpaca/ni_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_Llama2-7b_ni_Lora_tuned/', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1', '--use_peft', 'True', '--lora_r', '16', '--lora_alpha', '32']
[2024-08-13 05:23:08,286] [INFO] [launch.py:256:main] process 3579002 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=3', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Llama-2-7b-hf', '--data_path', '../data/stanford_alpaca/ni_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_Llama2-7b_ni_Lora_tuned/', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1', '--use_peft', 'True', '--lora_r', '16', '--lora_alpha', '32']
[2024-08-13 05:23:08,286] [INFO] [launch.py:256:main] process 3579003 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=4', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Llama-2-7b-hf', '--data_path', '../data/stanford_alpaca/ni_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_Llama2-7b_ni_Lora_tuned/', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1', '--use_peft', 'True', '--lora_r', '16', '--lora_alpha', '32']
[2024-08-13 05:23:08,287] [INFO] [launch.py:256:main] process 3579004 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=5', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Llama-2-7b-hf', '--data_path', '../data/stanford_alpaca/ni_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_Llama2-7b_ni_Lora_tuned/', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1', '--use_peft', 'True', '--lora_r', '16', '--lora_alpha', '32']
[2024-08-13 05:23:08,287] [INFO] [launch.py:256:main] process 3579005 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=6', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Llama-2-7b-hf', '--data_path', '../data/stanford_alpaca/ni_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_Llama2-7b_ni_Lora_tuned/', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1', '--use_peft', 'True', '--lora_r', '16', '--lora_alpha', '32']
[2024-08-13 05:23:08,288] [INFO] [launch.py:256:main] process 3579006 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=7', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Llama-2-7b-hf', '--data_path', '../data/stanford_alpaca/ni_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_Llama2-7b_ni_Lora_tuned/', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1', '--use_peft', 'True', '--lora_r', '16', '--lora_alpha', '32']
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
[2024-08-13 05:23:24,495] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-13 05:23:24,496] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-13 05:23:24,524] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-13 05:23:24,574] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-13 05:23:24,574] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-13 05:23:24,587] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-13 05:23:24,591] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-13 05:23:24,592] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2024-08-13 05:23:25,264] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-08-13 05:23:25,265] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-08-13 05:23:25,270] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-08-13 05:23:25,298] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-08-13 05:23:25,302] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-08-13 05:23:25,307] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-08-13 05:23:25,307] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-08-13 05:23:25,309] [INFO] [comm.py:637:init_distributed] cdb=None
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[2024-08-13 05:23:25,318] [INFO] [comm.py:637:init_distributed] cdb=None
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)

Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]
Downloading shards: 100%|██████████| 2/2 [00:00<00:00, 264.83it/s]
Downloading shards: 100%|██████████| 2/2 [00:00<00:00, 274.35it/s]


Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]
Downloading shards: 100%|██████████| 2/2 [00:00<00:00, 1565.92it/s]

Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]
Downloading shards: 100%|██████████| 2/2 [00:00<00:00, 1113.43it/s]

Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]
Downloading shards: 100%|██████████| 2/2 [00:00<00:00, 1456.36it/s]

Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]
Downloading shards: 100%|██████████| 2/2 [00:00<00:00, 1582.76it/s]

Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]
Downloading shards: 100%|██████████| 2/2 [00:00<00:00, 1437.39it/s]

Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]
Downloading shards: 100%|██████████| 2/2 [00:00<00:00, 1358.04it/s]
[2024-08-13 05:23:36,369] [INFO] [partition_parameters.py:345:__exit__] finished initializing model - num_params = 291, num_elems = 6.74B

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:27<00:27, 27.11s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:27<00:27, 27.11s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:27<00:27, 27.11s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:27<00:27, 27.12s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:27<00:27, 27.12s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:27<00:27, 27.12s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:27<00:27, 27.12s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:44<00:00, 21.25s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:44<00:00, 21.25s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:44<00:00, 21.25s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:44<00:00, 22.13s/it]

Loading checkpoint shards: 100%|██████████| 2/2 [00:44<00:00, 22.13s/it]

Loading checkpoint shards: 100%|██████████| 2/2 [00:44<00:00, 22.13s/it]

Loading checkpoint shards: 100%|██████████| 2/2 [00:44<00:00, 21.25s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:44<00:00, 22.13s/it]

Loading checkpoint shards: 100%|██████████| 2/2 [00:44<00:00, 21.25s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:44<00:00, 22.13s/it]

Loading checkpoint shards: 100%|██████████| 2/2 [00:44<00:00, 21.25s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:44<00:00, 22.13s/it]

Loading checkpoint shards: 100%|██████████| 2/2 [00:44<00:00, 21.26s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:44<00:00, 22.13s/it]
enable_input_require_grads!
enable_input_require_grads!enable_input_require_grads!enable_input_require_grads!enable_input_require_grads!

enable_input_require_grads!


enable_input_require_grads!

Loading checkpoint shards:  50%|█████     | 1/2 [00:50<00:50, 50.84s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:09<00:00, 32.07s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:09<00:00, 34.91s/it]
enable_input_require_grads!
trainable params: 8,388,608 || all params: 6,746,804,224 || trainable%: 0.1243
Finetune with LORA setting:  None
trainable params: 8,388,608 || all params: 6,746,804,224 || trainable%: 0.1243
Finetune with LORA setting:  None
trainable params: 8,388,608 || all params: 6,746,804,224 || trainable%: 0.1243
Finetune with LORA setting:  None
trainable params: 8,388,608 || all params: 6,746,804,224 || trainable%: 0.1243
Finetune with LORA setting:  None
trainable params: 8,388,608 || all params: 6,746,804,224 || trainable%: 0.1243
Finetune with LORA setting:  None
trainable params: 8,388,608 || all params: 6,746,804,224 || trainable%: 0.1243
Finetune with LORA setting:  None
trainable params: 8,388,608 || all params: 6,746,804,224 || trainable%: 0.1243
Finetune with LORA setting:  None
trainable params: 8,388,608 || all params: 6,746,804,224 || trainable%: 0.1243
Finetune with LORA setting:  None
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
[rank5]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[rank2]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...

[rank7]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
[rank0]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
[rank4]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121/fused_adam/build.ninja...
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/cpp_extension.py:1967: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
[rank6]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
[rank3]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
[rank1]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
[1/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output multi_tensor_adam.cuda.o.d -DTORCH_EXTENSION_NAME=fused_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/deepspeed/ops/csrc/includes -I/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/deepspeed/ops/csrc/adam -isystem /data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/include -isystem /data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/include/TH -isystem /data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /data/home/yunyun/miniconda3/envs/newtorch2/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_90,code=compute_90 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -lineinfo --use_fast_math -gencode=arch=compute_90,code=sm_90 -gencode=arch=compute_90,code=compute_90 -DBF16_AVAILABLE -U__CUDA_NO_BFLOAT16_OPERATORS__ -U__CUDA_NO_BFLOAT162_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ -std=c++17 -c /data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/deepspeed/ops/csrc/adam/multi_tensor_adam.cu -o multi_tensor_adam.cuda.o 
[2/3] c++ -MMD -MF fused_adam_frontend.o.d -DTORCH_EXTENSION_NAME=fused_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/deepspeed/ops/csrc/includes -I/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/deepspeed/ops/csrc/adam -isystem /data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/include -isystem /data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/include/TH -isystem /data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /data/home/yunyun/miniconda3/envs/newtorch2/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -std=c++17 -g -Wno-reorder -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DBF16_AVAILABLE -c /data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/deepspeed/ops/csrc/adam/fused_adam_frontend.cpp -o fused_adam_frontend.o 
[3/3] c++ fused_adam_frontend.o multi_tensor_adam.cuda.o -shared -L/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o fused_adam.so
Loading extension module fused_adam...
Loading extension module fused_adam...
Loading extension module fused_adam...
Loading extension module fused_adam...
Loading extension module fused_adam...
Loading extension module fused_adam...
Loading extension module fused_adam...
Loading extension module fused_adam...
Time to load fused_adam op: 38.18279790878296 secondsTime to load fused_adam op: 37.83003044128418 seconds
Time to load fused_adam op: 38.15792632102966 secondsTime to load fused_adam op: 38.19654726982117 secondsTime to load fused_adam op: 38.11963939666748 secondsTime to load fused_adam op: 37.90854263305664 secondsTime to load fused_adam op: 37.58767247200012 secondsTime to load fused_adam op: 38.196380853652954 seconds






Parameter Offload: Total persistent parameters: 8654848 in 193 params

  0%|          | 0/564 [00:00<?, ?it/s]/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

  0%|          | 1/564 [00:08<1:17:53,  8.30s/it]
                                                 
{'loss': 2.9663, 'grad_norm': 5.371734105430887, 'learning_rate': 0.0, 'epoch': 0.01}

  0%|          | 1/564 [00:08<1:17:53,  8.30s/it]
  0%|          | 2/564 [00:10<42:21,  4.52s/it]  
                                               
{'loss': 3.1908, 'grad_norm': 5.114578267721705, 'learning_rate': 4.89301084236452e-07, 'epoch': 0.01}

  0%|          | 2/564 [00:10<42:21,  4.52s/it]
  1%|          | 3/564 [00:10<26:10,  2.80s/it]
                                               
{'loss': 3.2318, 'grad_norm': 5.277494751495985, 'learning_rate': 7.755238700769802e-07, 'epoch': 0.02}

  1%|          | 3/564 [00:10<26:10,  2.80s/it]
  1%|          | 4/564 [00:11<18:26,  1.98s/it]
                                               
{'loss': 2.7939, 'grad_norm': 3.9551770771126433, 'learning_rate': 9.78602168472904e-07, 'epoch': 0.02}

  1%|          | 4/564 [00:11<18:26,  1.98s/it]
  1%|          | 5/564 [00:12<14:11,  1.52s/it]
                                               
{'loss': 3.4714, 'grad_norm': 5.516538239202875, 'learning_rate': 1.1361219343474658e-06, 'epoch': 0.03}

  1%|          | 5/564 [00:12<14:11,  1.52s/it]
  1%|          | 6/564 [00:13<11:37,  1.25s/it]
                                               
{'loss': 3.2635, 'grad_norm': 5.882606152088757, 'learning_rate': 1.264824954313432e-06, 'epoch': 0.03}

  1%|          | 6/564 [00:13<11:37,  1.25s/it]
  1%|          | 7/564 [00:13<10:13,  1.10s/it]
                                               
{'loss': 2.9431, 'grad_norm': 3.8711660911030163, 'learning_rate': 1.373641807199326e-06, 'epoch': 0.04}

  1%|          | 7/564 [00:13<10:13,  1.10s/it]
  1%|▏         | 8/564 [00:14<09:04,  1.02it/s]
                                               
{'loss': 3.1781, 'grad_norm': 4.920517399739058, 'learning_rate': 1.4679032527093559e-06, 'epoch': 0.04}

  1%|▏         | 8/564 [00:14<09:04,  1.02it/s]
  2%|▏         | 9/564 [00:15<08:17,  1.11it/s]
                                               
{'loss': 3.3973, 'grad_norm': 5.935921130483519, 'learning_rate': 1.5510477401539603e-06, 'epoch': 0.05}

  2%|▏         | 9/564 [00:15<08:17,  1.11it/s]
  2%|▏         | 10/564 [00:16<07:48,  1.18it/s]
                                                
{'loss': 2.7022, 'grad_norm': 4.616394924498404, 'learning_rate': 1.625423018583918e-06, 'epoch': 0.05}

  2%|▏         | 10/564 [00:16<07:48,  1.18it/s]
  2%|▏         | 11/564 [00:16<07:26,  1.24it/s]
                                                
{'loss': 3.5749, 'grad_norm': 7.034991119811727, 'learning_rate': 1.6927036418410939e-06, 'epoch': 0.06}

  2%|▏         | 11/564 [00:16<07:26,  1.24it/s]
  2%|▏         | 12/564 [00:17<07:14,  1.27it/s]
                                                
{'loss': 3.3707, 'grad_norm': 5.917491579076041, 'learning_rate': 1.7541260385498841e-06, 'epoch': 0.06}

  2%|▏         | 12/564 [00:17<07:14,  1.27it/s]
  2%|▏         | 13/564 [00:18<07:03,  1.30it/s]
                                                
{'loss': 3.2034, 'grad_norm': 5.135259178906326, 'learning_rate': 1.8106291662380673e-06, 'epoch': 0.07}

  2%|▏         | 13/564 [00:18<07:03,  1.30it/s]
  2%|▏         | 14/564 [00:18<06:59,  1.31it/s]
                                                
{'loss': 3.1706, 'grad_norm': 4.893878710105753, 'learning_rate': 1.862942891435778e-06, 'epoch': 0.07}

  2%|▏         | 14/564 [00:18<06:59,  1.31it/s]
  3%|▎         | 15/564 [00:19<06:59,  1.31it/s]
                                                
{'loss': 3.2132, 'grad_norm': 5.389423269289024, 'learning_rate': 1.911645804424446e-06, 'epoch': 0.08}

  3%|▎         | 15/564 [00:19<06:59,  1.31it/s]
  3%|▎         | 16/564 [00:20<06:56,  1.32it/s]
                                                
{'loss': 3.1216, 'grad_norm': 5.907352828062507, 'learning_rate': 1.957204336945808e-06, 'epoch': 0.09}

  3%|▎         | 16/564 [00:20<06:56,  1.32it/s]
  3%|▎         | 17/564 [00:21<07:03,  1.29it/s]
                                                
{'loss': 3.2036, 'grad_norm': 5.050437372966127, 'learning_rate': 2e-06, 'epoch': 0.09}

  3%|▎         | 17/564 [00:21<07:03,  1.29it/s]
  3%|▎         | 18/564 [00:22<06:53,  1.32it/s]
                                                
{'loss': 3.2783, 'grad_norm': 4.918446350559237, 'learning_rate': 2e-06, 'epoch': 0.1}

  3%|▎         | 18/564 [00:22<06:53,  1.32it/s]
  3%|▎         | 19/564 [00:22<06:50,  1.33it/s]
                                                
{'loss': 3.3642, 'grad_norm': 5.432348882888815, 'learning_rate': 1.996343692870201e-06, 'epoch': 0.1}

  3%|▎         | 19/564 [00:22<06:50,  1.33it/s]
  4%|▎         | 20/564 [00:23<06:57,  1.30it/s]
                                                
{'loss': 3.451, 'grad_norm': 4.886941185871198, 'learning_rate': 1.992687385740402e-06, 'epoch': 0.11}

  4%|▎         | 20/564 [00:23<06:57,  1.30it/s]
  4%|▎         | 21/564 [00:24<06:58,  1.30it/s]
                                                
{'loss': 3.009, 'grad_norm': 4.6276720747126, 'learning_rate': 1.9890310786106034e-06, 'epoch': 0.11}

  4%|▎         | 21/564 [00:24<06:58,  1.30it/s]
  4%|▍         | 22/564 [00:25<06:54,  1.31it/s]
                                                
{'loss': 3.0065, 'grad_norm': 4.451838526754796, 'learning_rate': 1.9853747714808044e-06, 'epoch': 0.12}

  4%|▍         | 22/564 [00:25<06:54,  1.31it/s]
  4%|▍         | 23/564 [00:25<06:46,  1.33it/s]
                                                
{'loss': 3.0495, 'grad_norm': 4.946532945265478, 'learning_rate': 1.9817184643510055e-06, 'epoch': 0.12}

  4%|▍         | 23/564 [00:25<06:46,  1.33it/s]
  4%|▍         | 24/564 [00:26<06:40,  1.35it/s]
                                                
{'loss': 3.1729, 'grad_norm': 4.482578722731507, 'learning_rate': 1.9780621572212065e-06, 'epoch': 0.13}

  4%|▍         | 24/564 [00:26<06:40,  1.35it/s]
  4%|▍         | 25/564 [00:27<06:35,  1.36it/s]
                                                
{'loss': 2.7756, 'grad_norm': 4.125196152807649, 'learning_rate': 1.9744058500914075e-06, 'epoch': 0.13}

  4%|▍         | 25/564 [00:27<06:35,  1.36it/s]
  5%|▍         | 26/564 [00:28<06:40,  1.34it/s]
                                                
{'loss': 3.4833, 'grad_norm': 5.213589054859358, 'learning_rate': 1.970749542961609e-06, 'epoch': 0.14}

  5%|▍         | 26/564 [00:28<06:40,  1.34it/s]
  5%|▍         | 27/564 [00:28<06:46,  1.32it/s]
                                                
{'loss': 2.9306, 'grad_norm': 5.4431536105405876, 'learning_rate': 1.9670932358318095e-06, 'epoch': 0.14}

  5%|▍         | 27/564 [00:28<06:46,  1.32it/s]
  5%|▍         | 28/564 [00:29<06:53,  1.30it/s]
                                                
{'loss': 3.1958, 'grad_norm': 6.135409897131275, 'learning_rate': 1.963436928702011e-06, 'epoch': 0.15}

  5%|▍         | 28/564 [00:29<06:53,  1.30it/s]
  5%|▌         | 29/564 [00:30<06:44,  1.32it/s]
                                                
{'loss': 3.2034, 'grad_norm': 4.352016401323431, 'learning_rate': 1.959780621572212e-06, 'epoch': 0.15}

  5%|▌         | 29/564 [00:30<06:44,  1.32it/s]
  5%|▌         | 30/564 [00:31<06:50,  1.30it/s]
                                                
{'loss': 3.2565, 'grad_norm': 5.819866250753071, 'learning_rate': 1.956124314442413e-06, 'epoch': 0.16}

  5%|▌         | 30/564 [00:31<06:50,  1.30it/s]
  5%|▌         | 31/564 [00:31<06:41,  1.33it/s]
                                                
{'loss': 2.9374, 'grad_norm': 4.611202948962472, 'learning_rate': 1.952468007312614e-06, 'epoch': 0.16}

  5%|▌         | 31/564 [00:31<06:41,  1.33it/s]
  6%|▌         | 32/564 [00:32<06:47,  1.30it/s]
                                                
{'loss': 3.5065, 'grad_norm': 5.385647657637641, 'learning_rate': 1.948811700182815e-06, 'epoch': 0.17}

  6%|▌         | 32/564 [00:32<06:47,  1.30it/s]
  6%|▌         | 33/564 [00:33<06:54,  1.28it/s]
                                                
{'loss': 2.9873, 'grad_norm': 6.059934333176447, 'learning_rate': 1.9451553930530165e-06, 'epoch': 0.18}

  6%|▌         | 33/564 [00:33<06:54,  1.28it/s]
  6%|▌         | 34/564 [00:34<06:49,  1.29it/s]
                                                
{'loss': 3.2269, 'grad_norm': 6.18066267383766, 'learning_rate': 1.9414990859232176e-06, 'epoch': 0.18}

  6%|▌         | 34/564 [00:34<06:49,  1.29it/s]
  6%|▌         | 35/564 [00:34<06:39,  1.32it/s]
                                                
{'loss': 3.3255, 'grad_norm': 4.687138259707194, 'learning_rate': 1.9378427787934186e-06, 'epoch': 0.19}

  6%|▌         | 35/564 [00:34<06:39,  1.32it/s]
  6%|▋         | 36/564 [00:35<06:37,  1.33it/s]
                                                
{'loss': 2.7271, 'grad_norm': 4.574733609713638, 'learning_rate': 1.9341864716636196e-06, 'epoch': 0.19}

  6%|▋         | 36/564 [00:35<06:37,  1.33it/s]
  7%|▋         | 37/564 [00:36<06:30,  1.35it/s]
                                                
{'loss': 3.2107, 'grad_norm': 6.783339527140826, 'learning_rate': 1.9305301645338206e-06, 'epoch': 0.2}

  7%|▋         | 37/564 [00:36<06:30,  1.35it/s]
  7%|▋         | 38/564 [00:37<06:25,  1.37it/s]
                                                
{'loss': 2.6328, 'grad_norm': 4.764625367668941, 'learning_rate': 1.9268738574040217e-06, 'epoch': 0.2}

  7%|▋         | 38/564 [00:37<06:25,  1.37it/s]
  7%|▋         | 39/564 [00:37<06:21,  1.38it/s]
                                                
{'loss': 2.904, 'grad_norm': 4.283562368727048, 'learning_rate': 1.923217550274223e-06, 'epoch': 0.21}

  7%|▋         | 39/564 [00:37<06:21,  1.38it/s]
  7%|▋         | 40/564 [00:38<06:19,  1.38it/s]
                                                
{'loss': 3.3006, 'grad_norm': 4.837843139615496, 'learning_rate': 1.919561243144424e-06, 'epoch': 0.21}

  7%|▋         | 40/564 [00:38<06:19,  1.38it/s]
  7%|▋         | 41/564 [00:39<06:17,  1.39it/s]
                                                
{'loss': 3.0465, 'grad_norm': 5.476780430949882, 'learning_rate': 1.915904936014625e-06, 'epoch': 0.22}

  7%|▋         | 41/564 [00:39<06:17,  1.39it/s]
  7%|▋         | 42/564 [00:39<06:16,  1.39it/s]
                                                
{'loss': 2.9195, 'grad_norm': 4.859637664826182, 'learning_rate': 1.912248628884826e-06, 'epoch': 0.22}

  7%|▋         | 42/564 [00:39<06:16,  1.39it/s]
  8%|▊         | 43/564 [00:40<06:15,  1.39it/s]
                                                
{'loss': 2.9475, 'grad_norm': 4.592874850002972, 'learning_rate': 1.908592321755027e-06, 'epoch': 0.23}

  8%|▊         | 43/564 [00:40<06:15,  1.39it/s]
  8%|▊         | 44/564 [00:41<06:15,  1.38it/s]
                                                
{'loss': 2.9977, 'grad_norm': 6.672392426977675, 'learning_rate': 1.9049360146252284e-06, 'epoch': 0.23}

  8%|▊         | 44/564 [00:41<06:15,  1.38it/s]
  8%|▊         | 45/564 [00:42<06:13,  1.39it/s]
                                                
{'loss': 3.1256, 'grad_norm': 6.554423323390144, 'learning_rate': 1.9012797074954294e-06, 'epoch': 0.24}

  8%|▊         | 45/564 [00:42<06:13,  1.39it/s]
  8%|▊         | 46/564 [00:42<06:22,  1.36it/s]
                                                
{'loss': 3.0051, 'grad_norm': 4.638439317778336, 'learning_rate': 1.8976234003656307e-06, 'epoch': 0.24}

  8%|▊         | 46/564 [00:42<06:22,  1.36it/s]
  8%|▊         | 47/564 [00:43<06:17,  1.37it/s]
                                                
{'loss': 3.2131, 'grad_norm': 4.904939409781511, 'learning_rate': 1.8939670932358317e-06, 'epoch': 0.25}

  8%|▊         | 47/564 [00:43<06:17,  1.37it/s]
  9%|▊         | 48/564 [00:44<06:27,  1.33it/s]
                                                
{'loss': 2.84, 'grad_norm': 4.993361885847814, 'learning_rate': 1.8903107861060327e-06, 'epoch': 0.26}

  9%|▊         | 48/564 [00:44<06:27,  1.33it/s]
  9%|▊         | 49/564 [00:45<06:22,  1.35it/s]
                                                
{'loss': 3.0706, 'grad_norm': 6.141471780166874, 'learning_rate': 1.886654478976234e-06, 'epoch': 0.26}

  9%|▊         | 49/564 [00:45<06:22,  1.35it/s]
  9%|▉         | 50/564 [00:45<06:16,  1.37it/s]
                                                
{'loss': 2.7688, 'grad_norm': 3.798345686518966, 'learning_rate': 1.882998171846435e-06, 'epoch': 0.27}

  9%|▉         | 50/564 [00:45<06:16,  1.37it/s]
  9%|▉         | 51/564 [00:46<06:12,  1.38it/s]
                                                
{'loss': 2.8598, 'grad_norm': 4.818261693232645, 'learning_rate': 1.8793418647166362e-06, 'epoch': 0.27}

  9%|▉         | 51/564 [00:46<06:12,  1.38it/s]
  9%|▉         | 52/564 [00:47<06:09,  1.39it/s]
                                                
{'loss': 2.7268, 'grad_norm': 3.2738890658517263, 'learning_rate': 1.875685557586837e-06, 'epoch': 0.28}

  9%|▉         | 52/564 [00:47<06:09,  1.39it/s]
  9%|▉         | 53/564 [00:48<06:14,  1.36it/s]
                                                
{'loss': 2.9213, 'grad_norm': 4.6863590951102765, 'learning_rate': 1.8720292504570383e-06, 'epoch': 0.28}

  9%|▉         | 53/564 [00:48<06:14,  1.36it/s]
 10%|▉         | 54/564 [00:48<06:19,  1.34it/s]
                                                
{'loss': 2.7541, 'grad_norm': 4.061233911926987, 'learning_rate': 1.8683729433272395e-06, 'epoch': 0.29}

 10%|▉         | 54/564 [00:48<06:19,  1.34it/s]
 10%|▉         | 55/564 [00:49<06:14,  1.36it/s]
                                                
{'loss': 2.7753, 'grad_norm': 6.646461083077562, 'learning_rate': 1.8647166361974405e-06, 'epoch': 0.29}

 10%|▉         | 55/564 [00:49<06:14,  1.36it/s]
 10%|▉         | 56/564 [00:50<06:10,  1.37it/s]
                                                
{'loss': 3.2803, 'grad_norm': 5.1617146154801, 'learning_rate': 1.8610603290676416e-06, 'epoch': 0.3}

 10%|▉         | 56/564 [00:50<06:10,  1.37it/s]
 10%|█         | 57/564 [00:50<06:09,  1.37it/s]
                                                
{'loss': 2.8369, 'grad_norm': 4.663366794017364, 'learning_rate': 1.8574040219378426e-06, 'epoch': 0.3}

 10%|█         | 57/564 [00:50<06:09,  1.37it/s]
 10%|█         | 58/564 [00:51<06:18,  1.34it/s]
                                                
{'loss': 2.9695, 'grad_norm': 4.548223106168213, 'learning_rate': 1.8537477148080438e-06, 'epoch': 0.31}

 10%|█         | 58/564 [00:51<06:18,  1.34it/s]
 10%|█         | 59/564 [00:52<06:11,  1.36it/s]
                                                
{'loss': 2.9529, 'grad_norm': 4.00424347504042, 'learning_rate': 1.8500914076782448e-06, 'epoch': 0.31}

 10%|█         | 59/564 [00:52<06:11,  1.36it/s]
 11%|█         | 60/564 [00:53<06:14,  1.34it/s]
                                                
{'loss': 3.076, 'grad_norm': 3.6026056843855385, 'learning_rate': 1.846435100548446e-06, 'epoch': 0.32}

 11%|█         | 60/564 [00:53<06:14,  1.34it/s]
 11%|█         | 61/564 [00:53<06:09,  1.36it/s]
                                                
{'loss': 2.8216, 'grad_norm': 4.360447303626994, 'learning_rate': 1.842778793418647e-06, 'epoch': 0.32}

 11%|█         | 61/564 [00:53<06:09,  1.36it/s]
 11%|█         | 62/564 [00:54<06:05,  1.37it/s]
                                                
{'loss': 2.7133, 'grad_norm': 3.3445255948295416, 'learning_rate': 1.8391224862888481e-06, 'epoch': 0.33}

 11%|█         | 62/564 [00:54<06:05,  1.37it/s]
 11%|█         | 63/564 [00:55<06:02,  1.38it/s]
                                                
{'loss': 3.2236, 'grad_norm': 4.181470918503985, 'learning_rate': 1.8354661791590494e-06, 'epoch': 0.34}

 11%|█         | 63/564 [00:55<06:02,  1.38it/s]
 11%|█▏        | 64/564 [00:56<06:10,  1.35it/s]
                                                
{'loss': 2.9146, 'grad_norm': 3.1813704768770226, 'learning_rate': 1.8318098720292504e-06, 'epoch': 0.34}

 11%|█▏        | 64/564 [00:56<06:10,  1.35it/s]
 12%|█▏        | 65/564 [00:56<06:12,  1.34it/s]
                                                
{'loss': 2.9365, 'grad_norm': 4.246455842246735, 'learning_rate': 1.8281535648994514e-06, 'epoch': 0.35}

 12%|█▏        | 65/564 [00:56<06:12,  1.34it/s]
 12%|█▏        | 66/564 [00:57<06:18,  1.32it/s]
                                                
{'loss': 3.0197, 'grad_norm': 3.0997745976535307, 'learning_rate': 1.8244972577696524e-06, 'epoch': 0.35}

 12%|█▏        | 66/564 [00:57<06:18,  1.32it/s]
 12%|█▏        | 67/564 [00:58<06:13,  1.33it/s]
                                                
{'loss': 3.0902, 'grad_norm': 3.7589679532578937, 'learning_rate': 1.8208409506398537e-06, 'epoch': 0.36}

 12%|█▏        | 67/564 [00:58<06:13,  1.33it/s]
 12%|█▏        | 68/564 [00:59<06:18,  1.31it/s]
                                                
{'loss': 3.2913, 'grad_norm': 3.6962910855542663, 'learning_rate': 1.817184643510055e-06, 'epoch': 0.36}

 12%|█▏        | 68/564 [00:59<06:18,  1.31it/s]
 12%|█▏        | 69/564 [01:00<06:21,  1.30it/s]
                                                
{'loss': 3.048, 'grad_norm': 3.400762290996315, 'learning_rate': 1.813528336380256e-06, 'epoch': 0.37}

 12%|█▏        | 69/564 [01:00<06:21,  1.30it/s]
 12%|█▏        | 70/564 [01:00<06:23,  1.29it/s]
                                                
{'loss': 2.9076, 'grad_norm': 3.8055839790003123, 'learning_rate': 1.809872029250457e-06, 'epoch': 0.37}

 12%|█▏        | 70/564 [01:00<06:23,  1.29it/s]
 13%|█▎        | 71/564 [01:01<06:25,  1.28it/s]
                                                
{'loss': 3.0094, 'grad_norm': 3.655446701212116, 'learning_rate': 1.806215722120658e-06, 'epoch': 0.38}

 13%|█▎        | 71/564 [01:01<06:25,  1.28it/s]
 13%|█▎        | 72/564 [01:02<06:24,  1.28it/s]
                                                
{'loss': 3.0003, 'grad_norm': 3.6151544385430467, 'learning_rate': 1.8025594149908592e-06, 'epoch': 0.38}

 13%|█▎        | 72/564 [01:02<06:24,  1.28it/s]
 13%|█▎        | 73/564 [01:03<06:13,  1.32it/s]
                                                
{'loss': 2.8354, 'grad_norm': 3.735475846250757, 'learning_rate': 1.7989031078610602e-06, 'epoch': 0.39}

 13%|█▎        | 73/564 [01:03<06:13,  1.32it/s]
 13%|█▎        | 74/564 [01:03<06:15,  1.30it/s]
                                                
{'loss': 2.6689, 'grad_norm': 3.1557539735779585, 'learning_rate': 1.7952468007312612e-06, 'epoch': 0.39}

 13%|█▎        | 74/564 [01:03<06:15,  1.30it/s]
 13%|█▎        | 75/564 [01:04<06:06,  1.33it/s]
                                                
{'loss': 3.063, 'grad_norm': 3.7420046414667785, 'learning_rate': 1.7915904936014625e-06, 'epoch': 0.4}

 13%|█▎        | 75/564 [01:04<06:06,  1.33it/s]
 13%|█▎        | 76/564 [01:05<06:00,  1.36it/s]
                                                
{'loss': 2.8362, 'grad_norm': 2.931393667070114, 'learning_rate': 1.7879341864716635e-06, 'epoch': 0.4}

 13%|█▎        | 76/564 [01:05<06:00,  1.36it/s]
 14%|█▎        | 77/564 [01:06<06:03,  1.34it/s]
                                                
{'loss': 2.9673, 'grad_norm': 3.041053990609808, 'learning_rate': 1.7842778793418647e-06, 'epoch': 0.41}

 14%|█▎        | 77/564 [01:06<06:03,  1.34it/s]
 14%|█▍        | 78/564 [01:06<05:57,  1.36it/s]
                                                
{'loss': 2.8762, 'grad_norm': 3.8148895768753652, 'learning_rate': 1.7806215722120656e-06, 'epoch': 0.41}

 14%|█▍        | 78/564 [01:06<05:57,  1.36it/s]
 14%|█▍        | 79/564 [01:07<06:06,  1.32it/s]
                                                
{'loss': 2.9725, 'grad_norm': 3.3632998562592697, 'learning_rate': 1.7769652650822668e-06, 'epoch': 0.42}

 14%|█▍        | 79/564 [01:07<06:06,  1.32it/s]
 14%|█▍        | 80/564 [01:08<06:11,  1.30it/s]
                                                
{'loss': 2.7081, 'grad_norm': 2.8863750443452436, 'learning_rate': 1.7733089579524678e-06, 'epoch': 0.43}

 14%|█▍        | 80/564 [01:08<06:11,  1.30it/s]
 14%|█▍        | 81/564 [01:09<06:02,  1.33it/s]
                                                
{'loss': 2.772, 'grad_norm': 3.494736302577592, 'learning_rate': 1.769652650822669e-06, 'epoch': 0.43}

 14%|█▍        | 81/564 [01:09<06:02,  1.33it/s]
 15%|█▍        | 82/564 [01:09<06:07,  1.31it/s]
                                                
{'loss': 2.7908, 'grad_norm': 3.256676929137989, 'learning_rate': 1.7659963436928703e-06, 'epoch': 0.44}

 15%|█▍        | 82/564 [01:09<06:07,  1.31it/s]
 15%|█▍        | 83/564 [01:10<06:06,  1.31it/s]
                                                
{'loss': 2.9587, 'grad_norm': 3.4833331949436235, 'learning_rate': 1.762340036563071e-06, 'epoch': 0.44}

 15%|█▍        | 83/564 [01:10<06:06,  1.31it/s]
 15%|█▍        | 84/564 [01:11<06:09,  1.30it/s]
                                                
{'loss': 2.5924, 'grad_norm': 2.64216914077813, 'learning_rate': 1.7586837294332723e-06, 'epoch': 0.45}

 15%|█▍        | 84/564 [01:11<06:09,  1.30it/s]
 15%|█▌        | 85/564 [01:12<06:01,  1.33it/s]
                                                
{'loss': 2.9028, 'grad_norm': 3.3490655341979885, 'learning_rate': 1.7550274223034734e-06, 'epoch': 0.45}

 15%|█▌        | 85/564 [01:12<06:01,  1.33it/s]
 15%|█▌        | 86/564 [01:12<05:58,  1.33it/s]
                                                
{'loss': 2.8279, 'grad_norm': 3.2855570249608204, 'learning_rate': 1.7513711151736746e-06, 'epoch': 0.46}

 15%|█▌        | 86/564 [01:12<05:58,  1.33it/s]
 15%|█▌        | 87/564 [01:13<06:03,  1.31it/s]
                                                
{'loss': 3.261, 'grad_norm': 4.341579405814756, 'learning_rate': 1.7477148080438754e-06, 'epoch': 0.46}

 15%|█▌        | 87/564 [01:13<06:03,  1.31it/s]
 16%|█▌        | 88/564 [01:14<05:57,  1.33it/s]
                                                
{'loss': 2.6789, 'grad_norm': 3.37271665154399, 'learning_rate': 1.7440585009140766e-06, 'epoch': 0.47}

 16%|█▌        | 88/564 [01:14<05:57,  1.33it/s]
 16%|█▌        | 89/564 [01:15<06:04,  1.30it/s]
                                                
{'loss': 2.5945, 'grad_norm': 3.164370275002579, 'learning_rate': 1.7404021937842779e-06, 'epoch': 0.47}

 16%|█▌        | 89/564 [01:15<06:04,  1.30it/s]
 16%|█▌        | 90/564 [01:15<06:07,  1.29it/s]
                                                
{'loss': 2.7991, 'grad_norm': 3.010537962613843, 'learning_rate': 1.736745886654479e-06, 'epoch': 0.48}

 16%|█▌        | 90/564 [01:15<06:07,  1.29it/s]
 16%|█▌        | 91/564 [01:16<06:02,  1.31it/s]
                                                
{'loss': 3.0375, 'grad_norm': 3.6485264908434134, 'learning_rate': 1.7330895795246801e-06, 'epoch': 0.48}

 16%|█▌        | 91/564 [01:16<06:02,  1.31it/s]
 16%|█▋        | 92/564 [01:17<05:54,  1.33it/s]
                                                
{'loss': 2.5626, 'grad_norm': 3.157724150283924, 'learning_rate': 1.729433272394881e-06, 'epoch': 0.49}

 16%|█▋        | 92/564 [01:17<05:54,  1.33it/s]
 16%|█▋        | 93/564 [01:18<05:47,  1.35it/s]
                                                
{'loss': 2.6735, 'grad_norm': 3.140773086346367, 'learning_rate': 1.7257769652650822e-06, 'epoch': 0.49}

 16%|█▋        | 93/564 [01:18<05:47,  1.35it/s]
 17%|█▋        | 94/564 [01:18<05:54,  1.33it/s]
                                                
{'loss': 2.4502, 'grad_norm': 2.644577576522305, 'learning_rate': 1.7221206581352832e-06, 'epoch': 0.5}

 17%|█▋        | 94/564 [01:18<05:54,  1.33it/s]
 17%|█▋        | 95/564 [01:19<05:48,  1.35it/s]
                                                
{'loss': 2.7717, 'grad_norm': 3.3124033000812143, 'learning_rate': 1.7184643510054844e-06, 'epoch': 0.51}

 17%|█▋        | 95/564 [01:19<05:48,  1.35it/s]
 17%|█▋        | 96/564 [01:20<05:53,  1.32it/s]
                                                
{'loss': 2.8956, 'grad_norm': 4.477963725749779, 'learning_rate': 1.7148080438756855e-06, 'epoch': 0.51}

 17%|█▋        | 96/564 [01:20<05:53,  1.32it/s]
 17%|█▋        | 97/564 [01:21<05:46,  1.35it/s]
                                                
{'loss': 3.0239, 'grad_norm': 3.78746089896677, 'learning_rate': 1.7111517367458865e-06, 'epoch': 0.52}

 17%|█▋        | 97/564 [01:21<05:46,  1.35it/s]
 17%|█▋        | 98/564 [01:21<05:53,  1.32it/s]
                                                
{'loss': 2.6109, 'grad_norm': 2.9984666121900005, 'learning_rate': 1.7074954296160877e-06, 'epoch': 0.52}

 17%|█▋        | 98/564 [01:21<05:53,  1.32it/s]
 18%|█▊        | 99/564 [01:22<05:46,  1.34it/s]
                                                
{'loss': 2.6283, 'grad_norm': 3.305649846494973, 'learning_rate': 1.7038391224862887e-06, 'epoch': 0.53}

 18%|█▊        | 99/564 [01:22<05:46,  1.34it/s]
 18%|█▊        | 100/564 [01:23<05:40,  1.36it/s]
                                                 
{'loss': 2.5006, 'grad_norm': 3.139783766368047, 'learning_rate': 1.70018281535649e-06, 'epoch': 0.53}

 18%|█▊        | 100/564 [01:23<05:40,  1.36it/s]
 18%|█▊        | 101/564 [01:24<05:36,  1.38it/s]
                                                 
{'loss': 2.9793, 'grad_norm': 4.363443351908621, 'learning_rate': 1.6965265082266908e-06, 'epoch': 0.54}

 18%|█▊        | 101/564 [01:24<05:36,  1.38it/s]
 18%|█▊        | 102/564 [01:24<05:45,  1.34it/s]
                                                 
{'loss': 2.5102, 'grad_norm': 3.113361139033026, 'learning_rate': 1.692870201096892e-06, 'epoch': 0.54}

 18%|█▊        | 102/564 [01:24<05:45,  1.34it/s]
 18%|█▊        | 103/564 [01:25<05:39,  1.36it/s]
                                                 
{'loss': 2.8743, 'grad_norm': 4.008929597427408, 'learning_rate': 1.6892138939670933e-06, 'epoch': 0.55}

 18%|█▊        | 103/564 [01:25<05:39,  1.36it/s]
 18%|█▊        | 104/564 [01:26<05:47,  1.32it/s]
                                                 
{'loss': 2.7462, 'grad_norm': 3.326658215321284, 'learning_rate': 1.6855575868372943e-06, 'epoch': 0.55}

 18%|█▊        | 104/564 [01:26<05:47,  1.32it/s]
 19%|█▊        | 105/564 [01:27<05:45,  1.33it/s]
                                                 
{'loss': 2.541, 'grad_norm': 3.3955536904570804, 'learning_rate': 1.6819012797074953e-06, 'epoch': 0.56}

 19%|█▊        | 105/564 [01:27<05:45,  1.33it/s]
 19%|█▉        | 106/564 [01:27<05:39,  1.35it/s]
                                                 
{'loss': 2.2834, 'grad_norm': 2.906631212738107, 'learning_rate': 1.6782449725776963e-06, 'epoch': 0.56}

 19%|█▉        | 106/564 [01:27<05:39,  1.35it/s]
 19%|█▉        | 107/564 [01:28<05:45,  1.32it/s]
                                                 
{'loss': 2.7889, 'grad_norm': 3.9772612205824363, 'learning_rate': 1.6745886654478976e-06, 'epoch': 0.57}

 19%|█▉        | 107/564 [01:28<05:45,  1.32it/s]
 19%|█▉        | 108/564 [01:29<05:49,  1.31it/s]
                                                 
{'loss': 2.5777, 'grad_norm': 3.1065965007199794, 'learning_rate': 1.6709323583180986e-06, 'epoch': 0.57}

 19%|█▉        | 108/564 [01:29<05:49,  1.31it/s]
 19%|█▉        | 109/564 [01:30<05:52,  1.29it/s]
                                                 
{'loss': 2.4885, 'grad_norm': 3.2766333310711375, 'learning_rate': 1.6672760511882998e-06, 'epoch': 0.58}

 19%|█▉        | 109/564 [01:30<05:52,  1.29it/s]
 20%|█▉        | 110/564 [01:30<05:43,  1.32it/s]
                                                 
{'loss': 2.5514, 'grad_norm': 4.102527921843211, 'learning_rate': 1.6636197440585008e-06, 'epoch': 0.59}

 20%|█▉        | 110/564 [01:30<05:43,  1.32it/s]
 20%|█▉        | 111/564 [01:31<05:48,  1.30it/s]
                                                 
{'loss': 2.4918, 'grad_norm': 3.4261504286135707, 'learning_rate': 1.6599634369287019e-06, 'epoch': 0.59}

 20%|█▉        | 111/564 [01:31<05:48,  1.30it/s]
 20%|█▉        | 112/564 [01:32<05:45,  1.31it/s]
                                                 
{'loss': 2.7623, 'grad_norm': 4.609411806109734, 'learning_rate': 1.6563071297989031e-06, 'epoch': 0.6}

 20%|█▉        | 112/564 [01:32<05:45,  1.31it/s]
 20%|██        | 113/564 [01:33<05:39,  1.33it/s]
                                                 
{'loss': 2.7529, 'grad_norm': 3.966342780291758, 'learning_rate': 1.6526508226691041e-06, 'epoch': 0.6}

 20%|██        | 113/564 [01:33<05:39,  1.33it/s]
 20%|██        | 114/564 [01:33<05:32,  1.35it/s]
                                                 
{'loss': 2.7275, 'grad_norm': 3.599905621610229, 'learning_rate': 1.6489945155393052e-06, 'epoch': 0.61}

 20%|██        | 114/564 [01:33<05:32,  1.35it/s]
 20%|██        | 115/564 [01:34<05:28,  1.37it/s]
                                                 
{'loss': 2.4997, 'grad_norm': 3.413747640673169, 'learning_rate': 1.6453382084095064e-06, 'epoch': 0.61}

 20%|██        | 115/564 [01:34<05:28,  1.37it/s]
 21%|██        | 116/564 [01:35<05:26,  1.37it/s]
                                                 
{'loss': 2.4052, 'grad_norm': 3.0961745339788855, 'learning_rate': 1.6416819012797074e-06, 'epoch': 0.62}

 21%|██        | 116/564 [01:35<05:26,  1.37it/s]
 21%|██        | 117/564 [01:36<05:35,  1.33it/s]
                                                 
{'loss': 2.5492, 'grad_norm': 4.221431614357368, 'learning_rate': 1.6380255941499086e-06, 'epoch': 0.62}

 21%|██        | 117/564 [01:36<05:35,  1.33it/s]
 21%|██        | 118/564 [01:36<05:33,  1.34it/s]
                                                 
{'loss': 2.6243, 'grad_norm': 4.063569687129969, 'learning_rate': 1.6343692870201097e-06, 'epoch': 0.63}

 21%|██        | 118/564 [01:36<05:33,  1.34it/s]
 21%|██        | 119/564 [01:37<05:32,  1.34it/s]
                                                 
{'loss': 2.719, 'grad_norm': 5.205040187170458, 'learning_rate': 1.6307129798903107e-06, 'epoch': 0.63}

 21%|██        | 119/564 [01:37<05:32,  1.34it/s]
 21%|██▏       | 120/564 [01:38<05:27,  1.36it/s]
                                                 
{'loss': 2.4463, 'grad_norm': 3.4958036499870913, 'learning_rate': 1.6270566727605117e-06, 'epoch': 0.64}

 21%|██▏       | 120/564 [01:38<05:27,  1.36it/s]
 21%|██▏       | 121/564 [01:39<05:28,  1.35it/s]
                                                 
{'loss': 2.8258, 'grad_norm': 4.934866798144638, 'learning_rate': 1.623400365630713e-06, 'epoch': 0.64}

 21%|██▏       | 121/564 [01:39<05:28,  1.35it/s]
 22%|██▏       | 122/564 [01:39<05:26,  1.35it/s]
                                                 
{'loss': 2.8461, 'grad_norm': 4.905272870605497, 'learning_rate': 1.6197440585009142e-06, 'epoch': 0.65}

 22%|██▏       | 122/564 [01:39<05:26,  1.35it/s]
 22%|██▏       | 123/564 [01:40<05:31,  1.33it/s]
                                                 
{'loss': 2.6666, 'grad_norm': 4.164001182751416, 'learning_rate': 1.616087751371115e-06, 'epoch': 0.65}

 22%|██▏       | 123/564 [01:40<05:31,  1.33it/s]
 22%|██▏       | 124/564 [01:41<05:24,  1.35it/s]
                                                 
{'loss': 2.6726, 'grad_norm': 4.5519492741316565, 'learning_rate': 1.6124314442413162e-06, 'epoch': 0.66}

 22%|██▏       | 124/564 [01:41<05:24,  1.35it/s]
 22%|██▏       | 125/564 [01:42<05:20,  1.37it/s]
                                                 
{'loss': 2.3408, 'grad_norm': 3.2509252659127448, 'learning_rate': 1.6087751371115173e-06, 'epoch': 0.66}

 22%|██▏       | 125/564 [01:42<05:20,  1.37it/s]
 22%|██▏       | 126/564 [01:42<05:23,  1.36it/s]
                                                 
{'loss': 2.5499, 'grad_norm': 3.4370334542232266, 'learning_rate': 1.6051188299817185e-06, 'epoch': 0.67}

 22%|██▏       | 126/564 [01:42<05:23,  1.36it/s]
 23%|██▎       | 127/564 [01:43<05:23,  1.35it/s]
                                                 
{'loss': 2.6652, 'grad_norm': 4.566704936879849, 'learning_rate': 1.6014625228519193e-06, 'epoch': 0.68}

 23%|██▎       | 127/564 [01:43<05:23,  1.35it/s]
 23%|██▎       | 128/564 [01:44<05:24,  1.34it/s]
                                                 
{'loss': 2.6544, 'grad_norm': 4.462814481128719, 'learning_rate': 1.5978062157221205e-06, 'epoch': 0.68}

 23%|██▎       | 128/564 [01:44<05:24,  1.34it/s]
 23%|██▎       | 129/564 [01:45<05:23,  1.34it/s]
                                                 
{'loss': 2.5792, 'grad_norm': 4.43656922619736, 'learning_rate': 1.5941499085923218e-06, 'epoch': 0.69}

 23%|██▎       | 129/564 [01:45<05:23,  1.34it/s]
 23%|██▎       | 130/564 [01:45<05:18,  1.36it/s]
                                                 
{'loss': 2.4188, 'grad_norm': 4.477971279208348, 'learning_rate': 1.5904936014625228e-06, 'epoch': 0.69}

 23%|██▎       | 130/564 [01:45<05:18,  1.36it/s]
 23%|██▎       | 131/564 [01:46<05:15,  1.37it/s]
                                                 
{'loss': 2.6375, 'grad_norm': 5.043994081631442, 'learning_rate': 1.586837294332724e-06, 'epoch': 0.7}

 23%|██▎       | 131/564 [01:46<05:15,  1.37it/s]
 23%|██▎       | 132/564 [01:47<05:14,  1.37it/s]
                                                 
{'loss': 2.5604, 'grad_norm': 4.446276462103255, 'learning_rate': 1.5831809872029248e-06, 'epoch': 0.7}

 23%|██▎       | 132/564 [01:47<05:14,  1.37it/s]
 24%|██▎       | 133/564 [01:47<05:22,  1.34it/s]
                                                 
{'loss': 2.4937, 'grad_norm': 4.377599614617866, 'learning_rate': 1.579524680073126e-06, 'epoch': 0.71}

 24%|██▎       | 133/564 [01:47<05:22,  1.34it/s]
 24%|██▍       | 134/564 [01:48<05:17,  1.35it/s]
                                                 
{'loss': 2.6967, 'grad_norm': 4.485762511416549, 'learning_rate': 1.5758683729433271e-06, 'epoch': 0.71}

 24%|██▍       | 134/564 [01:48<05:17,  1.35it/s]
 24%|██▍       | 135/564 [01:49<05:12,  1.37it/s]
                                                 
{'loss': 2.4312, 'grad_norm': 4.004882160213028, 'learning_rate': 1.5722120658135283e-06, 'epoch': 0.72}

 24%|██▍       | 135/564 [01:49<05:12,  1.37it/s]
 24%|██▍       | 136/564 [01:50<05:09,  1.38it/s]
                                                 
{'loss': 2.4827, 'grad_norm': 4.449958969145909, 'learning_rate': 1.5685557586837294e-06, 'epoch': 0.72}

 24%|██▍       | 136/564 [01:50<05:09,  1.38it/s]
 24%|██▍       | 137/564 [01:50<05:17,  1.34it/s]
                                                 
{'loss': 2.3745, 'grad_norm': 4.668134254902571, 'learning_rate': 1.5648994515539304e-06, 'epoch': 0.73}

 24%|██▍       | 137/564 [01:50<05:17,  1.34it/s]
 24%|██▍       | 138/564 [01:51<05:22,  1.32it/s]
                                                 
{'loss': 2.6871, 'grad_norm': 5.5513023195987445, 'learning_rate': 1.5612431444241316e-06, 'epoch': 0.73}

 24%|██▍       | 138/564 [01:51<05:22,  1.32it/s]
 25%|██▍       | 139/564 [01:52<05:26,  1.30it/s]
                                                 
{'loss': 2.6198, 'grad_norm': 4.544031689627112, 'learning_rate': 1.5575868372943326e-06, 'epoch': 0.74}

 25%|██▍       | 139/564 [01:52<05:26,  1.30it/s]
 25%|██▍       | 140/564 [01:53<05:19,  1.33it/s]
                                                 
{'loss': 2.5008, 'grad_norm': 4.739278492695667, 'learning_rate': 1.5539305301645339e-06, 'epoch': 0.74}

 25%|██▍       | 140/564 [01:53<05:19,  1.33it/s]
 25%|██▌       | 141/564 [01:53<05:21,  1.32it/s]
                                                 
{'loss': 2.4395, 'grad_norm': 3.869383721834104, 'learning_rate': 1.5502742230347347e-06, 'epoch': 0.75}

 25%|██▌       | 141/564 [01:53<05:21,  1.32it/s]
 25%|██▌       | 142/564 [01:54<05:15,  1.34it/s]
                                                 
{'loss': 2.538, 'grad_norm': 4.938881716210207, 'learning_rate': 1.546617915904936e-06, 'epoch': 0.76}

 25%|██▌       | 142/564 [01:54<05:15,  1.34it/s]
 25%|██▌       | 143/564 [01:55<05:10,  1.36it/s]
                                                 
{'loss': 2.4063, 'grad_norm': 4.8842245090661125, 'learning_rate': 1.5429616087751372e-06, 'epoch': 0.76}

 25%|██▌       | 143/564 [01:55<05:10,  1.36it/s]
 26%|██▌       | 144/564 [01:56<05:05,  1.37it/s]
                                                 
{'loss': 2.41, 'grad_norm': 4.83676963470171, 'learning_rate': 1.5393053016453382e-06, 'epoch': 0.77}

 26%|██▌       | 144/564 [01:56<05:05,  1.37it/s]
 26%|██▌       | 145/564 [01:56<05:12,  1.34it/s]
                                                 
{'loss': 2.5505, 'grad_norm': 5.268563590789193, 'learning_rate': 1.5356489945155392e-06, 'epoch': 0.77}

 26%|██▌       | 145/564 [01:56<05:12,  1.34it/s]
 26%|██▌       | 146/564 [01:57<05:16,  1.32it/s]
                                                 
{'loss': 2.5439, 'grad_norm': 6.108460500393737, 'learning_rate': 1.5319926873857402e-06, 'epoch': 0.78}

 26%|██▌       | 146/564 [01:57<05:16,  1.32it/s]
 26%|██▌       | 147/564 [01:58<05:19,  1.30it/s]
                                                 
{'loss': 2.131, 'grad_norm': 4.050588333367369, 'learning_rate': 1.5283363802559415e-06, 'epoch': 0.78}

 26%|██▌       | 147/564 [01:58<05:19,  1.30it/s]
 26%|██▌       | 148/564 [01:59<05:14,  1.32it/s]
                                                 
{'loss': 2.3833, 'grad_norm': 6.291389250105042, 'learning_rate': 1.5246800731261425e-06, 'epoch': 0.79}

 26%|██▌       | 148/564 [01:59<05:14,  1.32it/s]
 26%|██▋       | 149/564 [01:59<05:17,  1.31it/s]
                                                 
{'loss': 2.5548, 'grad_norm': 5.117059524606065, 'learning_rate': 1.5210237659963437e-06, 'epoch': 0.79}

 26%|██▋       | 149/564 [01:59<05:17,  1.31it/s]
 27%|██▋       | 150/564 [02:00<05:11,  1.33it/s]
                                                 
{'loss': 2.5544, 'grad_norm': 5.709096567816618, 'learning_rate': 1.5173674588665448e-06, 'epoch': 0.8}

 27%|██▋       | 150/564 [02:00<05:11,  1.33it/s]
 27%|██▋       | 151/564 [02:01<05:05,  1.35it/s]
                                                 
{'loss': 2.2047, 'grad_norm': 4.220455157310991, 'learning_rate': 1.5137111517367458e-06, 'epoch': 0.8}

 27%|██▋       | 151/564 [02:01<05:05,  1.35it/s]
 27%|██▋       | 152/564 [02:02<05:10,  1.33it/s]
                                                 
{'loss': 2.5967, 'grad_norm': 5.294161921595674, 'learning_rate': 1.510054844606947e-06, 'epoch': 0.81}

 27%|██▋       | 152/564 [02:02<05:10,  1.33it/s]
 27%|██▋       | 153/564 [02:03<05:15,  1.30it/s]
                                                 
{'loss': 2.6943, 'grad_norm': 6.803788951367365, 'learning_rate': 1.506398537477148e-06, 'epoch': 0.81}

 27%|██▋       | 153/564 [02:03<05:15,  1.30it/s]
 27%|██▋       | 154/564 [02:03<05:09,  1.32it/s]
                                                 
{'loss': 2.3516, 'grad_norm': 5.635226760690185, 'learning_rate': 1.502742230347349e-06, 'epoch': 0.82}

 27%|██▋       | 154/564 [02:03<05:09,  1.32it/s]
 27%|██▋       | 155/564 [02:04<05:02,  1.35it/s]
                                                 
{'loss': 2.431, 'grad_norm': 5.071842376708965, 'learning_rate': 1.49908592321755e-06, 'epoch': 0.82}

 27%|██▋       | 155/564 [02:04<05:02,  1.35it/s]
 28%|██▊       | 156/564 [02:05<04:57,  1.37it/s]
                                                 
{'loss': 2.6901, 'grad_norm': 6.04825822747669, 'learning_rate': 1.4954296160877513e-06, 'epoch': 0.83}

 28%|██▊       | 156/564 [02:05<04:57,  1.37it/s]
 28%|██▊       | 157/564 [02:05<04:54,  1.38it/s]
                                                 
{'loss': 2.1629, 'grad_norm': 4.44196338277001, 'learning_rate': 1.4917733089579526e-06, 'epoch': 0.84}

 28%|██▊       | 157/564 [02:05<04:54,  1.38it/s]
 28%|██▊       | 158/564 [02:06<04:54,  1.38it/s]
                                                 
{'loss': 2.4763, 'grad_norm': 5.9758572845386855, 'learning_rate': 1.4881170018281536e-06, 'epoch': 0.84}

 28%|██▊       | 158/564 [02:06<04:54,  1.38it/s]
 28%|██▊       | 159/564 [02:07<04:51,  1.39it/s]
                                                 
{'loss': 2.4468, 'grad_norm': 4.914322340104409, 'learning_rate': 1.4844606946983546e-06, 'epoch': 0.85}

 28%|██▊       | 159/564 [02:07<04:51,  1.39it/s]
 28%|██▊       | 160/564 [02:08<04:48,  1.40it/s]
                                                 
{'loss': 2.2654, 'grad_norm': 4.989225944301281, 'learning_rate': 1.4808043875685556e-06, 'epoch': 0.85}

 28%|██▊       | 160/564 [02:08<04:48,  1.40it/s]
 29%|██▊       | 161/564 [02:08<04:47,  1.40it/s]
                                                 
{'loss': 2.4928, 'grad_norm': 5.916495474097113, 'learning_rate': 1.4771480804387569e-06, 'epoch': 0.86}

 29%|██▊       | 161/564 [02:08<04:47,  1.40it/s]
 29%|██▊       | 162/564 [02:09<04:56,  1.36it/s]
                                                 
{'loss': 2.2108, 'grad_norm': 6.0818771536278105, 'learning_rate': 1.4734917733089579e-06, 'epoch': 0.86}

 29%|██▊       | 162/564 [02:09<04:56,  1.36it/s]
 29%|██▉       | 163/564 [02:10<04:56,  1.35it/s]
                                                 
{'loss': 2.5124, 'grad_norm': 6.377185058342569, 'learning_rate': 1.469835466179159e-06, 'epoch': 0.87}

 29%|██▉       | 163/564 [02:10<04:56,  1.35it/s]
 29%|██▉       | 164/564 [02:11<05:00,  1.33it/s]
                                                 
{'loss': 2.4608, 'grad_norm': 5.257773104864849, 'learning_rate': 1.4661791590493601e-06, 'epoch': 0.87}

 29%|██▉       | 164/564 [02:11<05:00,  1.33it/s]
 29%|██▉       | 165/564 [02:11<05:03,  1.32it/s]
                                                 
{'loss': 2.2517, 'grad_norm': 5.587475999428326, 'learning_rate': 1.4625228519195612e-06, 'epoch': 0.88}

 29%|██▉       | 165/564 [02:11<05:03,  1.32it/s]
 29%|██▉       | 166/564 [02:12<05:05,  1.30it/s]
                                                 
{'loss': 2.5924, 'grad_norm': 6.5940926908354, 'learning_rate': 1.4588665447897624e-06, 'epoch': 0.88}

 29%|██▉       | 166/564 [02:12<05:05,  1.30it/s]
 30%|██▉       | 167/564 [02:13<04:58,  1.33it/s]
                                                 
{'loss': 2.4633, 'grad_norm': 7.637452634365178, 'learning_rate': 1.4552102376599632e-06, 'epoch': 0.89}

 30%|██▉       | 167/564 [02:13<04:58,  1.33it/s]
 30%|██▉       | 168/564 [02:14<05:01,  1.31it/s]
                                                 
{'loss': 2.2989, 'grad_norm': 5.819028936460467, 'learning_rate': 1.4515539305301644e-06, 'epoch': 0.89}

 30%|██▉       | 168/564 [02:14<05:01,  1.31it/s]
 30%|██▉       | 169/564 [02:14<04:54,  1.34it/s]
                                                 
{'loss': 2.3375, 'grad_norm': 6.147828183095949, 'learning_rate': 1.4478976234003655e-06, 'epoch': 0.9}

 30%|██▉       | 169/564 [02:14<04:54,  1.34it/s]
 30%|███       | 170/564 [02:15<04:49,  1.36it/s]
                                                 
{'loss': 2.3245, 'grad_norm': 5.103805310671913, 'learning_rate': 1.4442413162705667e-06, 'epoch': 0.9}

 30%|███       | 170/564 [02:15<04:49,  1.36it/s]
 30%|███       | 171/564 [02:16<04:55,  1.33it/s]
                                                 
{'loss': 2.3485, 'grad_norm': 5.891138945333898, 'learning_rate': 1.440585009140768e-06, 'epoch': 0.91}

 30%|███       | 171/564 [02:16<04:55,  1.33it/s]
 30%|███       | 172/564 [02:17<04:51,  1.34it/s]
                                                 
{'loss': 2.246, 'grad_norm': 4.901907099839062, 'learning_rate': 1.4369287020109688e-06, 'epoch': 0.91}

 30%|███       | 172/564 [02:17<04:51,  1.34it/s]
 31%|███       | 173/564 [02:17<04:56,  1.32it/s]
                                                 
{'loss': 2.1501, 'grad_norm': 5.264553376884651, 'learning_rate': 1.43327239488117e-06, 'epoch': 0.92}

 31%|███       | 173/564 [02:17<04:56,  1.32it/s]
 31%|███       | 174/564 [02:18<04:55,  1.32it/s]
                                                 
{'loss': 2.1542, 'grad_norm': 5.904351081142608, 'learning_rate': 1.429616087751371e-06, 'epoch': 0.93}

 31%|███       | 174/564 [02:18<04:55,  1.32it/s]
 31%|███       | 175/564 [02:19<04:53,  1.33it/s]
                                                 
{'loss': 2.2246, 'grad_norm': 6.708288389026654, 'learning_rate': 1.4259597806215722e-06, 'epoch': 0.93}

 31%|███       | 175/564 [02:19<04:53,  1.33it/s]
 31%|███       | 176/564 [02:20<04:48,  1.35it/s]
                                                 
{'loss': 2.3043, 'grad_norm': 6.929867587598924, 'learning_rate': 1.422303473491773e-06, 'epoch': 0.94}

 31%|███       | 176/564 [02:20<04:48,  1.35it/s]
 31%|███▏      | 177/564 [02:20<04:53,  1.32it/s]
                                                 
{'loss': 1.9791, 'grad_norm': 4.595332983292899, 'learning_rate': 1.4186471663619743e-06, 'epoch': 0.94}

 31%|███▏      | 177/564 [02:20<04:53,  1.32it/s]
 32%|███▏      | 178/564 [02:21<04:56,  1.30it/s]
                                                 
{'loss': 2.4903, 'grad_norm': 7.269001438704392, 'learning_rate': 1.4149908592321755e-06, 'epoch': 0.95}

 32%|███▏      | 178/564 [02:21<04:56,  1.30it/s]
 32%|███▏      | 179/564 [02:22<04:50,  1.33it/s]
                                                 
{'loss': 2.2396, 'grad_norm': 5.762423897383352, 'learning_rate': 1.4113345521023766e-06, 'epoch': 0.95}

 32%|███▏      | 179/564 [02:22<04:50,  1.33it/s]
 32%|███▏      | 180/564 [02:23<04:44,  1.35it/s]
                                                 
{'loss': 2.3461, 'grad_norm': 6.193663078938908, 'learning_rate': 1.4076782449725778e-06, 'epoch': 0.96}

 32%|███▏      | 180/564 [02:23<04:44,  1.35it/s]
 32%|███▏      | 181/564 [02:23<04:40,  1.37it/s]
                                                 
{'loss': 1.8974, 'grad_norm': 3.9555293610478497, 'learning_rate': 1.4040219378427786e-06, 'epoch': 0.96}

 32%|███▏      | 181/564 [02:23<04:40,  1.37it/s]
 32%|███▏      | 182/564 [02:24<04:46,  1.34it/s]
                                                 
{'loss': 2.2122, 'grad_norm': 6.234929254294022, 'learning_rate': 1.4003656307129798e-06, 'epoch': 0.97}

 32%|███▏      | 182/564 [02:24<04:46,  1.34it/s]
 32%|███▏      | 183/564 [02:25<04:41,  1.36it/s]
                                                 
{'loss': 2.0779, 'grad_norm': 4.883510811575261, 'learning_rate': 1.3967093235831809e-06, 'epoch': 0.97}

 32%|███▏      | 183/564 [02:25<04:41,  1.36it/s]
 33%|███▎      | 184/564 [02:25<04:37,  1.37it/s]
                                                 
{'loss': 2.3291, 'grad_norm': 5.5037061045653966, 'learning_rate': 1.393053016453382e-06, 'epoch': 0.98}

 33%|███▎      | 184/564 [02:25<04:37,  1.37it/s]
 33%|███▎      | 185/564 [02:26<04:43,  1.34it/s]
                                                 
{'loss': 2.1775, 'grad_norm': 5.444520202431413, 'learning_rate': 1.3893967093235831e-06, 'epoch': 0.98}

 33%|███▎      | 185/564 [02:26<04:43,  1.34it/s]
 33%|███▎      | 186/564 [02:27<04:38,  1.36it/s]
                                                 
{'loss': 2.2171, 'grad_norm': 5.991042303421264, 'learning_rate': 1.3857404021937841e-06, 'epoch': 0.99}

 33%|███▎      | 186/564 [02:27<04:38,  1.36it/s]
 33%|███▎      | 187/564 [02:28<04:34,  1.37it/s]
                                                 
{'loss': 2.1184, 'grad_norm': 5.567103683801616, 'learning_rate': 1.3820840950639854e-06, 'epoch': 0.99}

 33%|███▎      | 187/564 [02:28<04:34,  1.37it/s]
 33%|███▎      | 188/564 [02:28<04:32,  1.38it/s]
                                                 
{'loss': 1.9873, 'grad_norm': 4.872941398870456, 'learning_rate': 1.3784277879341864e-06, 'epoch': 1.0}

 33%|███▎      | 188/564 [02:28<04:32,  1.38it/s]
 34%|███▎      | 189/564 [02:29<04:39,  1.34it/s]
                                                 
{'loss': 2.1261, 'grad_norm': 5.165300334233798, 'learning_rate': 1.3747714808043876e-06, 'epoch': 1.01}

 34%|███▎      | 189/564 [02:29<04:39,  1.34it/s]
 34%|███▎      | 190/564 [02:30<04:35,  1.36it/s]
                                                 
{'loss': 2.2104, 'grad_norm': 6.327736670471435, 'learning_rate': 1.3711151736745884e-06, 'epoch': 1.01}

 34%|███▎      | 190/564 [02:30<04:35,  1.36it/s]
 34%|███▍      | 191/564 [02:31<04:40,  1.33it/s]
                                                 
{'loss': 2.2364, 'grad_norm': 6.161238418343402, 'learning_rate': 1.3674588665447897e-06, 'epoch': 1.02}

 34%|███▍      | 191/564 [02:31<04:40,  1.33it/s]
 34%|███▍      | 192/564 [02:31<04:34,  1.35it/s]
                                                 
{'loss': 2.0558, 'grad_norm': 4.9753906515871, 'learning_rate': 1.363802559414991e-06, 'epoch': 1.02}

 34%|███▍      | 192/564 [02:31<04:34,  1.35it/s]
 34%|███▍      | 193/564 [02:32<04:31,  1.37it/s]
                                                 
{'loss': 2.213, 'grad_norm': 6.464973264799658, 'learning_rate': 1.360146252285192e-06, 'epoch': 1.03}

 34%|███▍      | 193/564 [02:32<04:31,  1.37it/s]
 34%|███▍      | 194/564 [02:33<04:36,  1.34it/s]
                                                 
{'loss': 2.2112, 'grad_norm': 6.063590663921882, 'learning_rate': 1.356489945155393e-06, 'epoch': 1.03}

 34%|███▍      | 194/564 [02:33<04:36,  1.34it/s]
 35%|███▍      | 195/564 [02:34<04:35,  1.34it/s]
                                                 
{'loss': 2.1641, 'grad_norm': 6.644527728133093, 'learning_rate': 1.352833638025594e-06, 'epoch': 1.04}

 35%|███▍      | 195/564 [02:34<04:35,  1.34it/s]
 35%|███▍      | 196/564 [02:34<04:40,  1.31it/s]
                                                 
{'loss': 2.078, 'grad_norm': 5.758040557351414, 'learning_rate': 1.3491773308957952e-06, 'epoch': 1.04}

 35%|███▍      | 196/564 [02:34<04:40,  1.31it/s]
 35%|███▍      | 197/564 [02:35<04:33,  1.34it/s]
                                                 
{'loss': 2.1439, 'grad_norm': 6.516129622472562, 'learning_rate': 1.3455210237659962e-06, 'epoch': 1.05}

 35%|███▍      | 197/564 [02:35<04:33,  1.34it/s]
 35%|███▌      | 198/564 [02:36<04:29,  1.36it/s]
                                                 
{'loss': 2.1439, 'grad_norm': 6.501391207001153, 'learning_rate': 1.3418647166361975e-06, 'epoch': 1.05}

 35%|███▌      | 198/564 [02:36<04:29,  1.36it/s]
 35%|███▌      | 199/564 [02:37<04:25,  1.37it/s]
                                                 
{'loss': 2.1379, 'grad_norm': 5.018416939402772, 'learning_rate': 1.3382084095063985e-06, 'epoch': 1.06}

 35%|███▌      | 199/564 [02:37<04:25,  1.37it/s]
 35%|███▌      | 200/564 [02:37<04:31,  1.34it/s]
                                                 
{'loss': 2.1777, 'grad_norm': 6.289586440844728, 'learning_rate': 1.3345521023765995e-06, 'epoch': 1.06}

 35%|███▌      | 200/564 [02:37<04:31,  1.34it/s]
 36%|███▌      | 201/564 [02:38<04:27,  1.36it/s]
                                                 
{'loss': 2.1777, 'grad_norm': 7.609147185123945, 'learning_rate': 1.3308957952468008e-06, 'epoch': 1.07}

 36%|███▌      | 201/564 [02:38<04:27,  1.36it/s]
 36%|███▌      | 202/564 [02:39<04:27,  1.35it/s]
                                                 
{'loss': 2.1102, 'grad_norm': 6.090414304948173, 'learning_rate': 1.3272394881170018e-06, 'epoch': 1.07}

 36%|███▌      | 202/564 [02:39<04:27,  1.35it/s]
 36%|███▌      | 203/564 [02:40<04:24,  1.36it/s]
                                                 
{'loss': 1.8873, 'grad_norm': 5.095091560865476, 'learning_rate': 1.3235831809872028e-06, 'epoch': 1.08}

 36%|███▌      | 203/564 [02:40<04:24,  1.36it/s]
 36%|███▌      | 204/564 [02:40<04:21,  1.38it/s]
                                                 
{'loss': 2.0734, 'grad_norm': 6.402549308471394, 'learning_rate': 1.3199268738574038e-06, 'epoch': 1.09}

 36%|███▌      | 204/564 [02:40<04:21,  1.38it/s]
 36%|███▋      | 205/564 [02:41<04:19,  1.38it/s]
                                                 
{'loss': 2.0571, 'grad_norm': 6.201669200300863, 'learning_rate': 1.316270566727605e-06, 'epoch': 1.09}

 36%|███▋      | 205/564 [02:41<04:19,  1.38it/s]
 37%|███▋      | 206/564 [02:42<04:17,  1.39it/s]
                                                 
{'loss': 1.9177, 'grad_norm': 6.036407010078788, 'learning_rate': 1.3126142595978063e-06, 'epoch': 1.1}

 37%|███▋      | 206/564 [02:42<04:17,  1.39it/s]
 37%|███▋      | 207/564 [02:42<04:19,  1.38it/s]
                                                 
{'loss': 2.0229, 'grad_norm': 6.421770390565946, 'learning_rate': 1.3089579524680071e-06, 'epoch': 1.1}

 37%|███▋      | 207/564 [02:42<04:19,  1.38it/s]
 37%|███▋      | 208/564 [02:43<04:16,  1.39it/s]
                                                 
{'loss': 2.0292, 'grad_norm': 6.708040419692206, 'learning_rate': 1.3053016453382084e-06, 'epoch': 1.11}

 37%|███▋      | 208/564 [02:43<04:16,  1.39it/s]
 37%|███▋      | 209/564 [02:44<04:18,  1.37it/s]
                                                 
{'loss': 2.1869, 'grad_norm': 6.608548848462514, 'learning_rate': 1.3016453382084094e-06, 'epoch': 1.11}

 37%|███▋      | 209/564 [02:44<04:18,  1.37it/s]
 37%|███▋      | 210/564 [02:45<04:16,  1.38it/s]
                                                 
{'loss': 2.0137, 'grad_norm': 6.703116937412067, 'learning_rate': 1.2979890310786106e-06, 'epoch': 1.12}

 37%|███▋      | 210/564 [02:45<04:16,  1.38it/s]
 37%|███▋      | 211/564 [02:45<04:22,  1.34it/s]
                                                 
{'loss': 2.0967, 'grad_norm': 6.032521280418299, 'learning_rate': 1.2943327239488116e-06, 'epoch': 1.12}

 37%|███▋      | 211/564 [02:45<04:22,  1.34it/s]
 38%|███▊      | 212/564 [02:46<04:18,  1.36it/s]
                                                 
{'loss': 1.9015, 'grad_norm': 5.757833736089788, 'learning_rate': 1.2906764168190127e-06, 'epoch': 1.13}

 38%|███▊      | 212/564 [02:46<04:18,  1.36it/s]
 38%|███▊      | 213/564 [02:47<04:21,  1.34it/s]
                                                 
{'loss': 2.2256, 'grad_norm': 8.383773358854, 'learning_rate': 1.2870201096892139e-06, 'epoch': 1.13}

 38%|███▊      | 213/564 [02:47<04:21,  1.34it/s]
 38%|███▊      | 214/564 [02:48<04:25,  1.32it/s]
                                                 
{'loss': 2.1667, 'grad_norm': 6.298811033454976, 'learning_rate': 1.283363802559415e-06, 'epoch': 1.14}

 38%|███▊      | 214/564 [02:48<04:25,  1.32it/s]
 38%|███▊      | 215/564 [02:48<04:23,  1.33it/s]
                                                 
{'loss': 2.0515, 'grad_norm': 5.631394791766023, 'learning_rate': 1.2797074954296162e-06, 'epoch': 1.14}

 38%|███▊      | 215/564 [02:48<04:23,  1.33it/s]
 38%|███▊      | 216/564 [02:49<04:18,  1.35it/s]
                                                 
{'loss': 2.2382, 'grad_norm': 7.784615852594052, 'learning_rate': 1.276051188299817e-06, 'epoch': 1.15}

 38%|███▊      | 216/564 [02:49<04:18,  1.35it/s]
 38%|███▊      | 217/564 [02:50<04:15,  1.36it/s]
                                                 
{'loss': 1.9797, 'grad_norm': 7.268942228364535, 'learning_rate': 1.2723948811700182e-06, 'epoch': 1.15}

 38%|███▊      | 217/564 [02:50<04:15,  1.36it/s]
 39%|███▊      | 218/564 [02:51<04:12,  1.37it/s]
                                                 
{'loss': 1.9578, 'grad_norm': 7.020153737074168, 'learning_rate': 1.2687385740402192e-06, 'epoch': 1.16}

 39%|███▊      | 218/564 [02:51<04:12,  1.37it/s]
 39%|███▉      | 219/564 [02:51<04:09,  1.38it/s]
                                                 
{'loss': 1.9228, 'grad_norm': 5.410158924265085, 'learning_rate': 1.2650822669104205e-06, 'epoch': 1.16}

 39%|███▉      | 219/564 [02:51<04:09,  1.38it/s]
 39%|███▉      | 220/564 [02:52<04:07,  1.39it/s]
                                                 
{'loss': 2.1781, 'grad_norm': 7.364824978073438, 'learning_rate': 1.2614259597806217e-06, 'epoch': 1.17}

 39%|███▉      | 220/564 [02:52<04:07,  1.39it/s]
 39%|███▉      | 221/564 [02:53<04:09,  1.37it/s]
                                                 
{'loss': 1.8899, 'grad_norm': 5.861645786663823, 'learning_rate': 1.2577696526508225e-06, 'epoch': 1.18}

 39%|███▉      | 221/564 [02:53<04:09,  1.37it/s]
 39%|███▉      | 222/564 [02:53<04:12,  1.35it/s]
                                                 
{'loss': 1.8919, 'grad_norm': 5.318529123564886, 'learning_rate': 1.2541133455210237e-06, 'epoch': 1.18}

 39%|███▉      | 222/564 [02:53<04:12,  1.35it/s]
 40%|███▉      | 223/564 [02:54<04:09,  1.37it/s]
                                                 
{'loss': 1.9931, 'grad_norm': 6.659508387399998, 'learning_rate': 1.2504570383912248e-06, 'epoch': 1.19}

 40%|███▉      | 223/564 [02:54<04:09,  1.37it/s]
 40%|███▉      | 224/564 [02:55<04:10,  1.36it/s]
                                                 
{'loss': 1.7147, 'grad_norm': 5.995683337795628, 'learning_rate': 1.246800731261426e-06, 'epoch': 1.19}

 40%|███▉      | 224/564 [02:55<04:10,  1.36it/s]
 40%|███▉      | 225/564 [02:56<04:07,  1.37it/s]
                                                 
{'loss': 1.9443, 'grad_norm': 6.482929984423554, 'learning_rate': 1.2431444241316268e-06, 'epoch': 1.2}

 40%|███▉      | 225/564 [02:56<04:07,  1.37it/s]
 40%|████      | 226/564 [02:56<04:13,  1.33it/s]
                                                 
{'loss': 1.9336, 'grad_norm': 7.113615031761578, 'learning_rate': 1.239488117001828e-06, 'epoch': 1.2}

 40%|████      | 226/564 [02:56<04:13,  1.33it/s]
 40%|████      | 227/564 [02:57<04:10,  1.35it/s]
                                                 
{'loss': 1.8654, 'grad_norm': 6.3106338480361615, 'learning_rate': 1.2358318098720293e-06, 'epoch': 1.21}

 40%|████      | 227/564 [02:57<04:10,  1.35it/s]
 40%|████      | 228/564 [02:58<04:06,  1.36it/s]
                                                 
{'loss': 2.1216, 'grad_norm': 8.650902622279235, 'learning_rate': 1.2321755027422303e-06, 'epoch': 1.21}

 40%|████      | 228/564 [02:58<04:06,  1.36it/s]
 41%|████      | 229/564 [02:59<04:03,  1.38it/s]
                                                 
{'loss': 1.8228, 'grad_norm': 6.008451704279621, 'learning_rate': 1.2285191956124315e-06, 'epoch': 1.22}

 41%|████      | 229/564 [02:59<04:03,  1.38it/s]
 41%|████      | 230/564 [02:59<04:07,  1.35it/s]
                                                 
{'loss': 1.9551, 'grad_norm': 6.9922080540259435, 'learning_rate': 1.2248628884826324e-06, 'epoch': 1.22}

 41%|████      | 230/564 [02:59<04:07,  1.35it/s]
 41%|████      | 231/564 [03:00<04:11,  1.32it/s]
                                                 
{'loss': 1.8635, 'grad_norm': 6.03636874456293, 'learning_rate': 1.2212065813528336e-06, 'epoch': 1.23}

 41%|████      | 231/564 [03:00<04:11,  1.32it/s]
 41%|████      | 232/564 [03:01<04:07,  1.34it/s]
                                                 
{'loss': 1.9232, 'grad_norm': 7.242757352666615, 'learning_rate': 1.2175502742230346e-06, 'epoch': 1.23}

 41%|████      | 232/564 [03:01<04:07,  1.34it/s]
 41%|████▏     | 233/564 [03:02<04:03,  1.36it/s]
                                                 
{'loss': 1.8448, 'grad_norm': 6.270637967445417, 'learning_rate': 1.2138939670932358e-06, 'epoch': 1.24}

 41%|████▏     | 233/564 [03:02<04:03,  1.36it/s]
 41%|████▏     | 234/564 [03:02<04:05,  1.35it/s]
                                                 
{'loss': 1.9318, 'grad_norm': 7.946551945412707, 'learning_rate': 1.2102376599634369e-06, 'epoch': 1.24}

 41%|████▏     | 234/564 [03:02<04:05,  1.35it/s]
 42%|████▏     | 235/564 [03:03<04:01,  1.36it/s]
                                                 
{'loss': 1.8779, 'grad_norm': 7.210649317392562, 'learning_rate': 1.2065813528336379e-06, 'epoch': 1.25}

 42%|████▏     | 235/564 [03:03<04:01,  1.36it/s]
 42%|████▏     | 236/564 [03:04<03:59,  1.37it/s]
                                                 
{'loss': 1.8089, 'grad_norm': 6.353921113009341, 'learning_rate': 1.2029250457038391e-06, 'epoch': 1.26}

 42%|████▏     | 236/564 [03:04<03:59,  1.37it/s]
 42%|████▏     | 237/564 [03:05<04:00,  1.36it/s]
                                                 
{'loss': 1.8348, 'grad_norm': 6.45741420436496, 'learning_rate': 1.1992687385740402e-06, 'epoch': 1.26}

 42%|████▏     | 237/564 [03:05<04:00,  1.36it/s]
 42%|████▏     | 238/564 [03:05<03:57,  1.37it/s]
                                                 
{'loss': 1.9078, 'grad_norm': 7.563473375273472, 'learning_rate': 1.1956124314442414e-06, 'epoch': 1.27}

 42%|████▏     | 238/564 [03:05<03:57,  1.37it/s]
 42%|████▏     | 239/564 [03:06<03:55,  1.38it/s]
                                                 
{'loss': 1.9984, 'grad_norm': 7.400630319411246, 'learning_rate': 1.1919561243144422e-06, 'epoch': 1.27}

 42%|████▏     | 239/564 [03:06<03:55,  1.38it/s]
 43%|████▎     | 240/564 [03:07<03:56,  1.37it/s]
                                                 
{'loss': 1.7205, 'grad_norm': 6.442540857298087, 'learning_rate': 1.1882998171846434e-06, 'epoch': 1.28}

 43%|████▎     | 240/564 [03:07<03:56,  1.37it/s]
 43%|████▎     | 241/564 [03:07<03:54,  1.38it/s]
                                                 
{'loss': 1.8183, 'grad_norm': 8.07165509588779, 'learning_rate': 1.1846435100548447e-06, 'epoch': 1.28}

 43%|████▎     | 241/564 [03:07<03:54,  1.38it/s]
 43%|████▎     | 242/564 [03:08<03:55,  1.37it/s]
                                                 
{'loss': 1.8346, 'grad_norm': 7.5170315435197965, 'learning_rate': 1.1809872029250457e-06, 'epoch': 1.29}

 43%|████▎     | 242/564 [03:08<03:55,  1.37it/s]
 43%|████▎     | 243/564 [03:09<04:00,  1.34it/s]
                                                 
{'loss': 1.7881, 'grad_norm': 6.448155222674784, 'learning_rate': 1.1773308957952467e-06, 'epoch': 1.29}

 43%|████▎     | 243/564 [03:09<04:00,  1.34it/s]
 43%|████▎     | 244/564 [03:10<03:55,  1.36it/s]
                                                 
{'loss': 1.6925, 'grad_norm': 6.162460318237296, 'learning_rate': 1.1736745886654477e-06, 'epoch': 1.3}

 43%|████▎     | 244/564 [03:10<03:55,  1.36it/s]
 43%|████▎     | 245/564 [03:10<03:59,  1.33it/s]
                                                 
{'loss': 1.607, 'grad_norm': 5.949346647603266, 'learning_rate': 1.170018281535649e-06, 'epoch': 1.3}

 43%|████▎     | 245/564 [03:10<03:59,  1.33it/s]
 44%|████▎     | 246/564 [03:11<03:59,  1.33it/s]
                                                 
{'loss': 1.618, 'grad_norm': 6.618338282409801, 'learning_rate': 1.16636197440585e-06, 'epoch': 1.31}

 44%|████▎     | 246/564 [03:11<03:59,  1.33it/s]
 44%|████▍     | 247/564 [03:12<04:00,  1.32it/s]
                                                 
{'loss': 1.6495, 'grad_norm': 7.8558803669081385, 'learning_rate': 1.1627056672760512e-06, 'epoch': 1.31}

 44%|████▍     | 247/564 [03:12<04:00,  1.32it/s]
 44%|████▍     | 248/564 [03:13<03:56,  1.34it/s]
                                                 
{'loss': 1.837, 'grad_norm': 6.696088246312051, 'learning_rate': 1.1590493601462523e-06, 'epoch': 1.32}

 44%|████▍     | 248/564 [03:13<03:56,  1.34it/s]
 44%|████▍     | 249/564 [03:14<03:59,  1.31it/s]
                                                 
{'loss': 1.7156, 'grad_norm': 6.1288676208637884, 'learning_rate': 1.1553930530164533e-06, 'epoch': 1.32}

 44%|████▍     | 249/564 [03:14<03:59,  1.31it/s]
 44%|████▍     | 250/564 [03:14<03:54,  1.34it/s]
                                                 
{'loss': 1.7419, 'grad_norm': 6.676360075089093, 'learning_rate': 1.1517367458866545e-06, 'epoch': 1.33}

 44%|████▍     | 250/564 [03:14<03:54,  1.34it/s]
 45%|████▍     | 251/564 [03:15<03:55,  1.33it/s]
                                                 
{'loss': 1.764, 'grad_norm': 6.67481372251543, 'learning_rate': 1.1480804387568555e-06, 'epoch': 1.34}

 45%|████▍     | 251/564 [03:15<03:55,  1.33it/s]
 45%|████▍     | 252/564 [03:16<03:55,  1.33it/s]
                                                 
{'loss': 1.7662, 'grad_norm': 7.9546341592176875, 'learning_rate': 1.1444241316270566e-06, 'epoch': 1.34}

 45%|████▍     | 252/564 [03:16<03:55,  1.33it/s]
 45%|████▍     | 253/564 [03:16<03:50,  1.35it/s]
                                                 
{'loss': 1.804, 'grad_norm': 6.823137281244877, 'learning_rate': 1.1407678244972576e-06, 'epoch': 1.35}

 45%|████▍     | 253/564 [03:16<03:50,  1.35it/s]
 45%|████▌     | 254/564 [03:17<03:46,  1.37it/s]
                                                 
{'loss': 1.9354, 'grad_norm': 7.591522388932892, 'learning_rate': 1.1371115173674588e-06, 'epoch': 1.35}

 45%|████▌     | 254/564 [03:17<03:46,  1.37it/s]
 45%|████▌     | 255/564 [03:18<03:50,  1.34it/s]
                                                 
{'loss': 1.6848, 'grad_norm': 7.453245008410401, 'learning_rate': 1.13345521023766e-06, 'epoch': 1.36}

 45%|████▌     | 255/564 [03:18<03:50,  1.34it/s]
 45%|████▌     | 256/564 [03:19<03:54,  1.31it/s]
                                                 
{'loss': 1.7363, 'grad_norm': 6.766264799526632, 'learning_rate': 1.1297989031078609e-06, 'epoch': 1.36}

 45%|████▌     | 256/564 [03:19<03:54,  1.31it/s]
 46%|████▌     | 257/564 [03:19<03:49,  1.34it/s]
                                                 
{'loss': 1.6603, 'grad_norm': 6.412743683133514, 'learning_rate': 1.126142595978062e-06, 'epoch': 1.37}

 46%|████▌     | 257/564 [03:19<03:49,  1.34it/s]
 46%|████▌     | 258/564 [03:20<03:45,  1.36it/s]
                                                 
{'loss': 1.8302, 'grad_norm': 6.477817166891688, 'learning_rate': 1.1224862888482631e-06, 'epoch': 1.37}

 46%|████▌     | 258/564 [03:20<03:45,  1.36it/s]
 46%|████▌     | 259/564 [03:21<03:42,  1.37it/s]
                                                 
{'loss': 1.5024, 'grad_norm': 5.73836542040701, 'learning_rate': 1.1188299817184644e-06, 'epoch': 1.38}

 46%|████▌     | 259/564 [03:21<03:42,  1.37it/s]
 46%|████▌     | 260/564 [03:22<03:47,  1.34it/s]
                                                 
{'loss': 1.6621, 'grad_norm': 6.256822523989544, 'learning_rate': 1.1151736745886654e-06, 'epoch': 1.38}

 46%|████▌     | 260/564 [03:22<03:47,  1.34it/s]
 46%|████▋     | 261/564 [03:22<03:47,  1.33it/s]
                                                 
{'loss': 1.6322, 'grad_norm': 6.869778861046742, 'learning_rate': 1.1115173674588664e-06, 'epoch': 1.39}

 46%|████▋     | 261/564 [03:22<03:47,  1.33it/s]
 46%|████▋     | 262/564 [03:23<03:43,  1.35it/s]
                                                 
{'loss': 1.6022, 'grad_norm': 7.329935314196057, 'learning_rate': 1.1078610603290676e-06, 'epoch': 1.39}

 46%|████▋     | 262/564 [03:23<03:43,  1.35it/s]
 47%|████▋     | 263/564 [03:24<03:46,  1.33it/s]
                                                 
{'loss': 1.6802, 'grad_norm': 6.405854410681538, 'learning_rate': 1.1042047531992687e-06, 'epoch': 1.4}

 47%|████▋     | 263/564 [03:24<03:46,  1.33it/s]
 47%|████▋     | 264/564 [03:25<03:45,  1.33it/s]
                                                 
{'loss': 1.7777, 'grad_norm': 7.608278650467643, 'learning_rate': 1.10054844606947e-06, 'epoch': 1.4}

 47%|████▋     | 264/564 [03:25<03:45,  1.33it/s]
 47%|████▋     | 265/564 [03:25<03:48,  1.31it/s]
                                                 
{'loss': 1.6474, 'grad_norm': 6.39336604427052, 'learning_rate': 1.0968921389396707e-06, 'epoch': 1.41}

 47%|████▋     | 265/564 [03:25<03:48,  1.31it/s]
 47%|████▋     | 266/564 [03:26<03:42,  1.34it/s]
                                                 
{'loss': 1.6231, 'grad_norm': 6.7977366987121135, 'learning_rate': 1.093235831809872e-06, 'epoch': 1.41}

 47%|████▋     | 266/564 [03:26<03:42,  1.34it/s]
 47%|████▋     | 267/564 [03:27<03:39,  1.36it/s]
                                                 
{'loss': 1.5887, 'grad_norm': 5.709319258357424, 'learning_rate': 1.089579524680073e-06, 'epoch': 1.42}

 47%|████▋     | 267/564 [03:27<03:39,  1.36it/s]
 48%|████▊     | 268/564 [03:28<03:42,  1.33it/s]
                                                 
{'loss': 1.8118, 'grad_norm': 7.526763512071448, 'learning_rate': 1.0859232175502742e-06, 'epoch': 1.43}

 48%|████▊     | 268/564 [03:28<03:42,  1.33it/s]
 48%|████▊     | 269/564 [03:28<03:39,  1.34it/s]
                                                 
{'loss': 1.5473, 'grad_norm': 5.744934661735262, 'learning_rate': 1.0822669104204754e-06, 'epoch': 1.43}

 48%|████▊     | 269/564 [03:28<03:39,  1.34it/s]
 48%|████▊     | 270/564 [03:29<03:42,  1.32it/s]
                                                 
{'loss': 1.6391, 'grad_norm': 5.5044996198887235, 'learning_rate': 1.0786106032906763e-06, 'epoch': 1.44}

 48%|████▊     | 270/564 [03:29<03:42,  1.32it/s]
 48%|████▊     | 271/564 [03:30<03:37,  1.35it/s]
                                                 
{'loss': 1.6409, 'grad_norm': 7.317716026530474, 'learning_rate': 1.0749542961608775e-06, 'epoch': 1.44}

 48%|████▊     | 271/564 [03:30<03:37,  1.35it/s]
 48%|████▊     | 272/564 [03:31<03:34,  1.36it/s]
                                                 
{'loss': 1.391, 'grad_norm': 4.696552583593289, 'learning_rate': 1.0712979890310785e-06, 'epoch': 1.45}

 48%|████▊     | 272/564 [03:31<03:34,  1.36it/s]
 48%|████▊     | 273/564 [03:31<03:31,  1.38it/s]
                                                 
{'loss': 1.6557, 'grad_norm': 5.634133556408441, 'learning_rate': 1.0676416819012797e-06, 'epoch': 1.45}

 48%|████▊     | 273/564 [03:31<03:31,  1.38it/s]
 49%|████▊     | 274/564 [03:32<03:36,  1.34it/s]
                                                 
{'loss': 1.5963, 'grad_norm': 6.021099648760917, 'learning_rate': 1.0639853747714806e-06, 'epoch': 1.46}

 49%|████▊     | 274/564 [03:32<03:36,  1.34it/s]
 49%|████▉     | 275/564 [03:33<03:35,  1.34it/s]
                                                 
{'loss': 1.6978, 'grad_norm': 6.192278539828671, 'learning_rate': 1.0603290676416818e-06, 'epoch': 1.46}

 49%|████▉     | 275/564 [03:33<03:35,  1.34it/s]
 49%|████▉     | 276/564 [03:34<03:38,  1.32it/s]
                                                 
{'loss': 1.526, 'grad_norm': 6.911389512376077, 'learning_rate': 1.056672760511883e-06, 'epoch': 1.47}

 49%|████▉     | 276/564 [03:34<03:38,  1.32it/s]
 49%|████▉     | 277/564 [03:34<03:33,  1.34it/s]
                                                 
{'loss': 1.3967, 'grad_norm': 5.913454797667615, 'learning_rate': 1.053016453382084e-06, 'epoch': 1.47}

 49%|████▉     | 277/564 [03:34<03:33,  1.34it/s]
 49%|████▉     | 278/564 [03:35<03:29,  1.36it/s]
                                                 
{'loss': 1.3741, 'grad_norm': 5.168499814066812, 'learning_rate': 1.0493601462522853e-06, 'epoch': 1.48}

 49%|████▉     | 278/564 [03:35<03:29,  1.36it/s]
 49%|████▉     | 279/564 [03:36<03:27,  1.38it/s]
                                                 
{'loss': 1.5804, 'grad_norm': 6.346813513867612, 'learning_rate': 1.045703839122486e-06, 'epoch': 1.48}

 49%|████▉     | 279/564 [03:36<03:27,  1.38it/s]
 50%|████▉     | 280/564 [03:36<03:25,  1.38it/s]
                                                 
{'loss': 1.4841, 'grad_norm': 5.586218541045116, 'learning_rate': 1.0420475319926873e-06, 'epoch': 1.49}

 50%|████▉     | 280/564 [03:36<03:25,  1.38it/s]
 50%|████▉     | 281/564 [03:37<03:24,  1.39it/s]
                                                 
{'loss': 1.5724, 'grad_norm': 7.310392467137633, 'learning_rate': 1.0383912248628884e-06, 'epoch': 1.49}

 50%|████▉     | 281/564 [03:37<03:24,  1.39it/s]
 50%|█████     | 282/564 [03:38<03:29,  1.35it/s]
                                                 
{'loss': 1.6059, 'grad_norm': 6.07855542543942, 'learning_rate': 1.0347349177330896e-06, 'epoch': 1.5}

 50%|█████     | 282/564 [03:38<03:29,  1.35it/s]
 50%|█████     | 283/564 [03:39<03:32,  1.32it/s]
                                                 
{'loss': 1.416, 'grad_norm': 4.907897113427253, 'learning_rate': 1.0310786106032906e-06, 'epoch': 1.51}

 50%|█████     | 283/564 [03:39<03:32,  1.32it/s]
 50%|█████     | 284/564 [03:40<03:28,  1.34it/s]
                                                 
{'loss': 1.3636, 'grad_norm': 5.283578505167578, 'learning_rate': 1.0274223034734916e-06, 'epoch': 1.51}

 50%|█████     | 284/564 [03:40<03:28,  1.34it/s]
 51%|█████     | 285/564 [03:40<03:30,  1.32it/s]
                                                 
{'loss': 1.4432, 'grad_norm': 4.861661505600423, 'learning_rate': 1.0237659963436929e-06, 'epoch': 1.52}

 51%|█████     | 285/564 [03:40<03:30,  1.32it/s]
 51%|█████     | 286/564 [03:41<03:33,  1.30it/s]
                                                 
{'loss': 1.4921, 'grad_norm': 5.560798345148604, 'learning_rate': 1.020109689213894e-06, 'epoch': 1.52}

 51%|█████     | 286/564 [03:41<03:33,  1.30it/s]
 51%|█████     | 287/564 [03:42<03:35,  1.29it/s]
                                                 
{'loss': 1.4832, 'grad_norm': 4.492930839942661, 'learning_rate': 1.0164533820840951e-06, 'epoch': 1.53}

 51%|█████     | 287/564 [03:42<03:35,  1.29it/s]
 51%|█████     | 288/564 [03:43<03:29,  1.32it/s]
                                                 
{'loss': 1.4252, 'grad_norm': 5.951734560686299, 'learning_rate': 1.012797074954296e-06, 'epoch': 1.53}

 51%|█████     | 288/564 [03:43<03:29,  1.32it/s]
 51%|█████     | 289/564 [03:43<03:27,  1.33it/s]
                                                 
{'loss': 1.5026, 'grad_norm': 4.98893114350589, 'learning_rate': 1.0091407678244972e-06, 'epoch': 1.54}

 51%|█████     | 289/564 [03:43<03:27,  1.33it/s]
 51%|█████▏    | 290/564 [03:44<03:23,  1.35it/s]
                                                 
{'loss': 1.4437, 'grad_norm': 4.699155840884539, 'learning_rate': 1.0054844606946984e-06, 'epoch': 1.54}

 51%|█████▏    | 290/564 [03:44<03:23,  1.35it/s]
 52%|█████▏    | 291/564 [03:45<03:24,  1.33it/s]
                                                 
{'loss': 1.5782, 'grad_norm': 5.0407543894643885, 'learning_rate': 1.0018281535648994e-06, 'epoch': 1.55}

 52%|█████▏    | 291/564 [03:45<03:24,  1.33it/s]
 52%|█████▏    | 292/564 [03:46<03:27,  1.31it/s]
                                                 
{'loss': 1.5053, 'grad_norm': 5.498092835930448, 'learning_rate': 9.981718464351005e-07, 'epoch': 1.55}

 52%|█████▏    | 292/564 [03:46<03:27,  1.31it/s]
 52%|█████▏    | 293/564 [03:46<03:22,  1.34it/s]
                                                 
{'loss': 1.4771, 'grad_norm': 4.896113614666874, 'learning_rate': 9.945155393053017e-07, 'epoch': 1.56}

 52%|█████▏    | 293/564 [03:46<03:22,  1.34it/s]
 52%|█████▏    | 294/564 [03:47<03:24,  1.32it/s]
                                                 
{'loss': 1.5503, 'grad_norm': 4.92423002848989, 'learning_rate': 9.908592321755027e-07, 'epoch': 1.56}

 52%|█████▏    | 294/564 [03:47<03:24,  1.32it/s]
 52%|█████▏    | 295/564 [03:48<03:26,  1.30it/s]
                                                 
{'loss': 1.4266, 'grad_norm': 4.774314452320406, 'learning_rate': 9.872029250457037e-07, 'epoch': 1.57}

 52%|█████▏    | 295/564 [03:48<03:26,  1.30it/s]
 52%|█████▏    | 296/564 [03:49<03:27,  1.29it/s]
                                                 
{'loss': 1.6124, 'grad_norm': 4.323542720992609, 'learning_rate': 9.835466179159048e-07, 'epoch': 1.57}

 52%|█████▏    | 296/564 [03:49<03:27,  1.29it/s]
 53%|█████▎    | 297/564 [03:49<03:22,  1.32it/s]
                                                 
{'loss': 1.3897, 'grad_norm': 4.948909126628803, 'learning_rate': 9.79890310786106e-07, 'epoch': 1.58}

 53%|█████▎    | 297/564 [03:49<03:22,  1.32it/s]
 53%|█████▎    | 298/564 [03:50<03:21,  1.32it/s]
                                                 
{'loss': 1.4043, 'grad_norm': 3.9811548394801055, 'learning_rate': 9.76234003656307e-07, 'epoch': 1.59}

 53%|█████▎    | 298/564 [03:50<03:21,  1.32it/s]
 53%|█████▎    | 299/564 [03:51<03:19,  1.33it/s]
                                                 
{'loss': 1.37, 'grad_norm': 4.812614012249302, 'learning_rate': 9.725776965265083e-07, 'epoch': 1.59}

 53%|█████▎    | 299/564 [03:51<03:19,  1.33it/s]
 53%|█████▎    | 300/564 [03:52<03:15,  1.35it/s]
                                                 
{'loss': 1.5364, 'grad_norm': 5.193070263814162, 'learning_rate': 9.689213893967093e-07, 'epoch': 1.6}

 53%|█████▎    | 300/564 [03:52<03:15,  1.35it/s]
 53%|█████▎    | 301/564 [03:52<03:12,  1.36it/s]
                                                 
{'loss': 1.4095, 'grad_norm': 4.874072816618653, 'learning_rate': 9.652650822669103e-07, 'epoch': 1.6}

 53%|█████▎    | 301/564 [03:52<03:12,  1.36it/s]
 54%|█████▎    | 302/564 [03:53<03:16,  1.33it/s]
                                                 
{'loss': 1.4221, 'grad_norm': 4.7137715245530245, 'learning_rate': 9.616087751371115e-07, 'epoch': 1.61}

 54%|█████▎    | 302/564 [03:53<03:16,  1.33it/s]
 54%|█████▎    | 303/564 [03:54<03:19,  1.31it/s]
                                                 
{'loss': 1.404, 'grad_norm': 4.765498323131964, 'learning_rate': 9.579524680073126e-07, 'epoch': 1.61}

 54%|█████▎    | 303/564 [03:54<03:19,  1.31it/s]
 54%|█████▍    | 304/564 [03:55<03:14,  1.34it/s]
                                                 
{'loss': 1.4208, 'grad_norm': 4.180738629562436, 'learning_rate': 9.542961608775136e-07, 'epoch': 1.62}

 54%|█████▍    | 304/564 [03:55<03:14,  1.34it/s]
 54%|█████▍    | 305/564 [03:55<03:11,  1.36it/s]
                                                 
{'loss': 1.4432, 'grad_norm': 4.558727336525248, 'learning_rate': 9.506398537477147e-07, 'epoch': 1.62}

 54%|█████▍    | 305/564 [03:55<03:11,  1.36it/s]
 54%|█████▍    | 306/564 [03:56<03:15,  1.32it/s]
                                                 
{'loss': 1.2719, 'grad_norm': 3.579170585428942, 'learning_rate': 9.469835466179159e-07, 'epoch': 1.63}

 54%|█████▍    | 306/564 [03:56<03:15,  1.32it/s]
 54%|█████▍    | 307/564 [03:57<03:17,  1.30it/s]
                                                 
{'loss': 1.2072, 'grad_norm': 3.460792947093797, 'learning_rate': 9.43327239488117e-07, 'epoch': 1.63}

 54%|█████▍    | 307/564 [03:57<03:17,  1.30it/s]
 55%|█████▍    | 308/564 [03:58<03:18,  1.29it/s]
                                                 
{'loss': 1.3458, 'grad_norm': 4.20638634790193, 'learning_rate': 9.396709323583181e-07, 'epoch': 1.64}

 55%|█████▍    | 308/564 [03:58<03:18,  1.29it/s]
 55%|█████▍    | 309/564 [03:58<03:13,  1.32it/s]
                                                 
{'loss': 1.3167, 'grad_norm': 3.7109201471090305, 'learning_rate': 9.360146252285191e-07, 'epoch': 1.64}

 55%|█████▍    | 309/564 [03:58<03:13,  1.32it/s]
 55%|█████▍    | 310/564 [03:59<03:09,  1.34it/s]
                                                 
{'loss': 1.4077, 'grad_norm': 3.7440360590930135, 'learning_rate': 9.323583180987203e-07, 'epoch': 1.65}

 55%|█████▍    | 310/564 [03:59<03:09,  1.34it/s]
 55%|█████▌    | 311/564 [04:00<03:11,  1.32it/s]
                                                 
{'loss': 1.3962, 'grad_norm': 3.591229669350976, 'learning_rate': 9.287020109689213e-07, 'epoch': 1.65}

 55%|█████▌    | 311/564 [04:00<03:11,  1.32it/s]
 55%|█████▌    | 312/564 [04:01<03:10,  1.33it/s]
                                                 
{'loss': 1.3617, 'grad_norm': 4.196930191739549, 'learning_rate': 9.250457038391224e-07, 'epoch': 1.66}

 55%|█████▌    | 312/564 [04:01<03:10,  1.33it/s]
 55%|█████▌    | 313/564 [04:01<03:11,  1.31it/s]
                                                 
{'loss': 1.4467, 'grad_norm': 4.687540100494487, 'learning_rate': 9.213893967093235e-07, 'epoch': 1.66}

 55%|█████▌    | 313/564 [04:01<03:11,  1.31it/s]
 56%|█████▌    | 314/564 [04:02<03:06,  1.34it/s]
                                                 
{'loss': 1.4281, 'grad_norm': 4.725498940922037, 'learning_rate': 9.177330895795247e-07, 'epoch': 1.67}

 56%|█████▌    | 314/564 [04:02<03:06,  1.34it/s]
 56%|█████▌    | 315/564 [04:03<03:03,  1.36it/s]
                                                 
{'loss': 1.318, 'grad_norm': 4.555622378976032, 'learning_rate': 9.140767824497257e-07, 'epoch': 1.68}

 56%|█████▌    | 315/564 [04:03<03:03,  1.36it/s]
 56%|█████▌    | 316/564 [04:04<03:01,  1.37it/s]
                                                 
{'loss': 1.4706, 'grad_norm': 4.424174619072532, 'learning_rate': 9.104204753199268e-07, 'epoch': 1.68}

 56%|█████▌    | 316/564 [04:04<03:01,  1.37it/s]
 56%|█████▌    | 317/564 [04:04<02:59,  1.38it/s]
                                                 
{'loss': 1.2631, 'grad_norm': 3.871052240748985, 'learning_rate': 9.06764168190128e-07, 'epoch': 1.69}

 56%|█████▌    | 317/564 [04:04<02:59,  1.38it/s]
 56%|█████▋    | 318/564 [04:05<03:00,  1.36it/s]
                                                 
{'loss': 1.3179, 'grad_norm': 4.121261223167093, 'learning_rate': 9.03107861060329e-07, 'epoch': 1.69}

 56%|█████▋    | 318/564 [04:05<03:00,  1.36it/s]
 57%|█████▋    | 319/564 [04:06<02:59,  1.37it/s]
                                                 
{'loss': 1.4314, 'grad_norm': 3.270290703347564, 'learning_rate': 8.994515539305301e-07, 'epoch': 1.7}

 57%|█████▋    | 319/564 [04:06<02:59,  1.37it/s]
 57%|█████▋    | 320/564 [04:07<02:57,  1.37it/s]
                                                 
{'loss': 1.4806, 'grad_norm': 4.5608427073826965, 'learning_rate': 8.957952468007312e-07, 'epoch': 1.7}

 57%|█████▋    | 320/564 [04:07<02:57,  1.37it/s]
 57%|█████▋    | 321/564 [04:07<03:00,  1.35it/s]
                                                 
{'loss': 1.3653, 'grad_norm': 3.574102089136862, 'learning_rate': 8.921389396709324e-07, 'epoch': 1.71}

 57%|█████▋    | 321/564 [04:07<03:00,  1.35it/s]
 57%|█████▋    | 322/564 [04:08<02:58,  1.36it/s]
                                                 
{'loss': 1.2285, 'grad_norm': 3.3865859765047994, 'learning_rate': 8.884826325411334e-07, 'epoch': 1.71}

 57%|█████▋    | 322/564 [04:08<02:58,  1.36it/s]
 57%|█████▋    | 323/564 [04:09<02:55,  1.37it/s]
                                                 
{'loss': 1.2704, 'grad_norm': 3.4193736788862434, 'learning_rate': 8.848263254113345e-07, 'epoch': 1.72}

 57%|█████▋    | 323/564 [04:09<02:55,  1.37it/s]
 57%|█████▋    | 324/564 [04:09<02:53,  1.38it/s]
                                                 
{'loss': 1.3341, 'grad_norm': 3.892528177120486, 'learning_rate': 8.811700182815355e-07, 'epoch': 1.72}

 57%|█████▋    | 324/564 [04:09<02:53,  1.38it/s]
 58%|█████▊    | 325/564 [04:10<02:58,  1.34it/s]
                                                 
{'loss': 1.223, 'grad_norm': 4.02243614261271, 'learning_rate': 8.775137111517367e-07, 'epoch': 1.73}

 58%|█████▊    | 325/564 [04:10<02:58,  1.34it/s]
 58%|█████▊    | 326/564 [04:11<03:00,  1.32it/s]
                                                 
{'loss': 1.1761, 'grad_norm': 3.678139325388252, 'learning_rate': 8.738574040219377e-07, 'epoch': 1.73}

 58%|█████▊    | 326/564 [04:11<03:00,  1.32it/s]
 58%|█████▊    | 327/564 [04:12<02:56,  1.34it/s]
                                                 
{'loss': 1.3209, 'grad_norm': 3.247749571650745, 'learning_rate': 8.702010968921389e-07, 'epoch': 1.74}

 58%|█████▊    | 327/564 [04:12<02:56,  1.34it/s]
 58%|█████▊    | 328/564 [04:13<03:00,  1.31it/s]
                                                 
{'loss': 1.3015, 'grad_norm': 2.9116554869899245, 'learning_rate': 8.665447897623401e-07, 'epoch': 1.74}

 58%|█████▊    | 328/564 [04:13<03:00,  1.31it/s]
 58%|█████▊    | 329/564 [04:13<02:56,  1.33it/s]
                                                 
{'loss': 1.3171, 'grad_norm': 3.9696594264990943, 'learning_rate': 8.628884826325411e-07, 'epoch': 1.75}

 58%|█████▊    | 329/564 [04:13<02:56,  1.33it/s]
 59%|█████▊    | 330/564 [04:14<02:53,  1.35it/s]
                                                 
{'loss': 1.3414, 'grad_norm': 3.1058608059076342, 'learning_rate': 8.592321755027422e-07, 'epoch': 1.76}

 59%|█████▊    | 330/564 [04:14<02:53,  1.35it/s]
 59%|█████▊    | 331/564 [04:15<02:51,  1.36it/s]
                                                 
{'loss': 1.3125, 'grad_norm': 4.1579781848176784, 'learning_rate': 8.555758683729432e-07, 'epoch': 1.76}

 59%|█████▊    | 331/564 [04:15<02:51,  1.36it/s]
 59%|█████▉    | 332/564 [04:15<02:51,  1.35it/s]
                                                 
{'loss': 1.2725, 'grad_norm': 3.5522761641297804, 'learning_rate': 8.519195612431444e-07, 'epoch': 1.77}

 59%|█████▉    | 332/564 [04:15<02:51,  1.35it/s]
 59%|█████▉    | 333/564 [04:16<02:49,  1.36it/s]
                                                 
{'loss': 1.3518, 'grad_norm': 4.0811352395656755, 'learning_rate': 8.482632541133454e-07, 'epoch': 1.77}

 59%|█████▉    | 333/564 [04:16<02:49,  1.36it/s]
 59%|█████▉    | 334/564 [04:17<02:47,  1.37it/s]
                                                 
{'loss': 1.3009, 'grad_norm': 3.838084054468904, 'learning_rate': 8.446069469835466e-07, 'epoch': 1.78}

 59%|█████▉    | 334/564 [04:17<02:47,  1.37it/s]
 59%|█████▉    | 335/564 [04:18<02:46,  1.37it/s]
                                                 
{'loss': 1.1663, 'grad_norm': 3.8352198541723976, 'learning_rate': 8.409506398537477e-07, 'epoch': 1.78}

 59%|█████▉    | 335/564 [04:18<02:46,  1.37it/s]
 60%|█████▉    | 336/564 [04:18<02:45,  1.38it/s]
                                                 
{'loss': 1.2242, 'grad_norm': 3.1868941660905956, 'learning_rate': 8.372943327239488e-07, 'epoch': 1.79}

 60%|█████▉    | 336/564 [04:18<02:45,  1.38it/s]
 60%|█████▉    | 337/564 [04:19<02:47,  1.36it/s]
                                                 
{'loss': 1.3085, 'grad_norm': 3.4644741497076446, 'learning_rate': 8.336380255941499e-07, 'epoch': 1.79}

 60%|█████▉    | 337/564 [04:19<02:47,  1.36it/s]
 60%|█████▉    | 338/564 [04:20<02:49,  1.33it/s]
                                                 
{'loss': 1.2396, 'grad_norm': 3.52947259340011, 'learning_rate': 8.299817184643509e-07, 'epoch': 1.8}

 60%|█████▉    | 338/564 [04:20<02:49,  1.33it/s]
 60%|██████    | 339/564 [04:21<02:46,  1.35it/s]
                                                 
{'loss': 1.1468, 'grad_norm': 3.708847076782075, 'learning_rate': 8.263254113345521e-07, 'epoch': 1.8}

 60%|██████    | 339/564 [04:21<02:46,  1.35it/s]
 60%|██████    | 340/564 [04:21<02:47,  1.34it/s]
                                                 
{'loss': 1.3144, 'grad_norm': 3.68905938038211, 'learning_rate': 8.226691042047532e-07, 'epoch': 1.81}

 60%|██████    | 340/564 [04:21<02:47,  1.34it/s]
 60%|██████    | 341/564 [04:22<02:44,  1.36it/s]
                                                 
{'loss': 1.2678, 'grad_norm': 3.2549332498165584, 'learning_rate': 8.190127970749543e-07, 'epoch': 1.81}

 60%|██████    | 341/564 [04:22<02:44,  1.36it/s]
 61%|██████    | 342/564 [04:23<02:47,  1.33it/s]
                                                 
{'loss': 1.3344, 'grad_norm': 3.9196773986715003, 'learning_rate': 8.153564899451553e-07, 'epoch': 1.82}

 61%|██████    | 342/564 [04:23<02:47,  1.33it/s]
 61%|██████    | 343/564 [04:24<02:44,  1.35it/s]
                                                 
{'loss': 1.3385, 'grad_norm': 3.285459882798634, 'learning_rate': 8.117001828153565e-07, 'epoch': 1.82}

 61%|██████    | 343/564 [04:24<02:44,  1.35it/s]
 61%|██████    | 344/564 [04:24<02:41,  1.36it/s]
                                                 
{'loss': 1.275, 'grad_norm': 3.8748615697416064, 'learning_rate': 8.080438756855575e-07, 'epoch': 1.83}

 61%|██████    | 344/564 [04:24<02:41,  1.36it/s]
 61%|██████    | 345/564 [04:25<02:45,  1.32it/s]
                                                 
{'loss': 1.1741, 'grad_norm': 2.9092489342743226, 'learning_rate': 8.043875685557586e-07, 'epoch': 1.84}

 61%|██████    | 345/564 [04:25<02:45,  1.32it/s]
 61%|██████▏   | 346/564 [04:26<02:42,  1.34it/s]
                                                 
{'loss': 1.3627, 'grad_norm': 4.018790080808557, 'learning_rate': 8.007312614259597e-07, 'epoch': 1.84}

 61%|██████▏   | 346/564 [04:26<02:42,  1.34it/s]
 62%|██████▏   | 347/564 [04:27<02:45,  1.31it/s]
                                                 
{'loss': 1.276, 'grad_norm': 2.9015246157997754, 'learning_rate': 7.970749542961609e-07, 'epoch': 1.85}

 62%|██████▏   | 347/564 [04:27<02:45,  1.31it/s]
 62%|██████▏   | 348/564 [04:27<02:41,  1.34it/s]
                                                 
{'loss': 1.2308, 'grad_norm': 2.8402100031136053, 'learning_rate': 7.93418647166362e-07, 'epoch': 1.85}

 62%|██████▏   | 348/564 [04:27<02:41,  1.34it/s]
 62%|██████▏   | 349/564 [04:28<02:38,  1.35it/s]
                                                 
{'loss': 1.2211, 'grad_norm': 3.044809965694903, 'learning_rate': 7.89762340036563e-07, 'epoch': 1.86}

 62%|██████▏   | 349/564 [04:28<02:38,  1.35it/s]
 62%|██████▏   | 350/564 [04:29<02:41,  1.32it/s]
                                                 
{'loss': 1.0765, 'grad_norm': 3.316402188324235, 'learning_rate': 7.861060329067642e-07, 'epoch': 1.86}

 62%|██████▏   | 350/564 [04:29<02:41,  1.32it/s]
 62%|██████▏   | 351/564 [04:30<02:43,  1.30it/s]
                                                 
{'loss': 1.1158, 'grad_norm': 3.2540891298151084, 'learning_rate': 7.824497257769652e-07, 'epoch': 1.87}

 62%|██████▏   | 351/564 [04:30<02:43,  1.30it/s]
 62%|██████▏   | 352/564 [04:30<02:39,  1.33it/s]
                                                 
{'loss': 1.0987, 'grad_norm': 3.6575661521347738, 'learning_rate': 7.787934186471663e-07, 'epoch': 1.87}

 62%|██████▏   | 352/564 [04:30<02:39,  1.33it/s]
 63%|██████▎   | 353/564 [04:31<02:35,  1.35it/s]
                                                 
{'loss': 1.2532, 'grad_norm': 3.8134995287583884, 'learning_rate': 7.751371115173673e-07, 'epoch': 1.88}

 63%|██████▎   | 353/564 [04:31<02:35,  1.35it/s]
 63%|██████▎   | 354/564 [04:32<02:35,  1.35it/s]
                                                 
{'loss': 1.2426, 'grad_norm': 3.302435254222088, 'learning_rate': 7.714808043875686e-07, 'epoch': 1.88}

 63%|██████▎   | 354/564 [04:32<02:35,  1.35it/s]
 63%|██████▎   | 355/564 [04:33<02:33,  1.36it/s]
                                                 
{'loss': 1.1202, 'grad_norm': 3.1124175269391934, 'learning_rate': 7.678244972577696e-07, 'epoch': 1.89}

 63%|██████▎   | 355/564 [04:33<02:33,  1.36it/s]
 63%|██████▎   | 356/564 [04:33<02:31,  1.37it/s]
                                                 
{'loss': 1.2434, 'grad_norm': 2.9751579059715496, 'learning_rate': 7.641681901279707e-07, 'epoch': 1.89}

 63%|██████▎   | 356/564 [04:33<02:31,  1.37it/s]
 63%|██████▎   | 357/564 [04:34<02:30,  1.38it/s]
                                                 
{'loss': 1.3132, 'grad_norm': 3.252411620482913, 'learning_rate': 7.605118829981719e-07, 'epoch': 1.9}

 63%|██████▎   | 357/564 [04:34<02:30,  1.38it/s]
 63%|██████▎   | 358/564 [04:35<02:28,  1.38it/s]
                                                 
{'loss': 1.2271, 'grad_norm': 2.5364227035605467, 'learning_rate': 7.568555758683729e-07, 'epoch': 1.9}

 63%|██████▎   | 358/564 [04:35<02:28,  1.38it/s]
 64%|██████▎   | 359/564 [04:36<02:32,  1.34it/s]
                                                 
{'loss': 1.1491, 'grad_norm': 3.8489928306089087, 'learning_rate': 7.53199268738574e-07, 'epoch': 1.91}

 64%|██████▎   | 359/564 [04:36<02:32,  1.34it/s]
 64%|██████▍   | 360/564 [04:36<02:30,  1.36it/s]
                                                 
{'loss': 1.1973, 'grad_norm': 3.3736324426399813, 'learning_rate': 7.49542961608775e-07, 'epoch': 1.91}

 64%|██████▍   | 360/564 [04:36<02:30,  1.36it/s]
 64%|██████▍   | 361/564 [04:37<02:28,  1.37it/s]
                                                 
{'loss': 1.2303, 'grad_norm': 3.0889754192113865, 'learning_rate': 7.458866544789763e-07, 'epoch': 1.92}

 64%|██████▍   | 361/564 [04:37<02:28,  1.37it/s]
 64%|██████▍   | 362/564 [04:38<02:27,  1.37it/s]
                                                 
{'loss': 1.0326, 'grad_norm': 2.3820363440256256, 'learning_rate': 7.422303473491773e-07, 'epoch': 1.93}

 64%|██████▍   | 362/564 [04:38<02:27,  1.37it/s]
 64%|██████▍   | 363/564 [04:38<02:25,  1.38it/s]
                                                 
{'loss': 1.1029, 'grad_norm': 3.2483077365357786, 'learning_rate': 7.385740402193784e-07, 'epoch': 1.93}

 64%|██████▍   | 363/564 [04:38<02:25,  1.38it/s]
 65%|██████▍   | 364/564 [04:39<02:26,  1.37it/s]
                                                 
{'loss': 1.1841, 'grad_norm': 3.019964932892012, 'learning_rate': 7.349177330895795e-07, 'epoch': 1.94}

 65%|██████▍   | 364/564 [04:39<02:26,  1.37it/s]
 65%|██████▍   | 365/564 [04:40<02:29,  1.33it/s]
                                                 
{'loss': 1.1244, 'grad_norm': 2.7463186877620283, 'learning_rate': 7.312614259597806e-07, 'epoch': 1.94}

 65%|██████▍   | 365/564 [04:40<02:29,  1.33it/s]
 65%|██████▍   | 366/564 [04:41<02:27,  1.34it/s]
                                                 
{'loss': 1.3218, 'grad_norm': 2.4881557906501666, 'learning_rate': 7.276051188299816e-07, 'epoch': 1.95}

 65%|██████▍   | 366/564 [04:41<02:27,  1.34it/s]
 65%|██████▌   | 367/564 [04:41<02:30,  1.31it/s]
                                                 
{'loss': 1.0837, 'grad_norm': 3.015395974646368, 'learning_rate': 7.239488117001827e-07, 'epoch': 1.95}

 65%|██████▌   | 367/564 [04:41<02:30,  1.31it/s]
 65%|██████▌   | 368/564 [04:42<02:31,  1.29it/s]
                                                 
{'loss': 1.2162, 'grad_norm': 3.3715893025282697, 'learning_rate': 7.20292504570384e-07, 'epoch': 1.96}

 65%|██████▌   | 368/564 [04:42<02:31,  1.29it/s]
 65%|██████▌   | 369/564 [04:43<02:28,  1.32it/s]
                                                 
{'loss': 1.1393, 'grad_norm': 2.8214156258269556, 'learning_rate': 7.16636197440585e-07, 'epoch': 1.96}

 65%|██████▌   | 369/564 [04:43<02:28,  1.32it/s]
 66%|██████▌   | 370/564 [04:44<02:29,  1.30it/s]
                                                 
{'loss': 1.1828, 'grad_norm': 2.808097421795268, 'learning_rate': 7.129798903107861e-07, 'epoch': 1.97}

 66%|██████▌   | 370/564 [04:44<02:29,  1.30it/s]
 66%|██████▌   | 371/564 [04:45<02:27,  1.31it/s]
                                                 
{'loss': 1.0693, 'grad_norm': 2.241366000105301, 'learning_rate': 7.093235831809871e-07, 'epoch': 1.97}

 66%|██████▌   | 371/564 [04:45<02:27,  1.31it/s]
 66%|██████▌   | 372/564 [04:45<02:28,  1.29it/s]
                                                 
{'loss': 1.1567, 'grad_norm': 3.0900975044655086, 'learning_rate': 7.056672760511883e-07, 'epoch': 1.98}

 66%|██████▌   | 372/564 [04:45<02:28,  1.29it/s]
 66%|██████▌   | 373/564 [04:46<02:26,  1.31it/s]
                                                 
{'loss': 1.2515, 'grad_norm': 2.742950171282613, 'learning_rate': 7.020109689213893e-07, 'epoch': 1.98}

 66%|██████▌   | 373/564 [04:46<02:26,  1.31it/s]
 66%|██████▋   | 374/564 [04:47<02:24,  1.32it/s]
                                                 
{'loss': 1.1699, 'grad_norm': 3.3002347140667943, 'learning_rate': 6.983546617915904e-07, 'epoch': 1.99}

 66%|██████▋   | 374/564 [04:47<02:24,  1.32it/s]
 66%|██████▋   | 375/564 [04:48<02:21,  1.34it/s]
                                                 
{'loss': 1.1254, 'grad_norm': 3.3583280328404763, 'learning_rate': 6.946983546617916e-07, 'epoch': 1.99}

 66%|██████▋   | 375/564 [04:48<02:21,  1.34it/s]
 67%|██████▋   | 376/564 [04:48<02:22,  1.32it/s]
                                                 
{'loss': 1.2667, 'grad_norm': 2.533006306735032, 'learning_rate': 6.910420475319927e-07, 'epoch': 2.0}

 67%|██████▋   | 376/564 [04:48<02:22,  1.32it/s]
 67%|██████▋   | 377/564 [04:49<02:19,  1.34it/s]
                                                 
{'loss': 1.1914, 'grad_norm': 3.094592448952774, 'learning_rate': 6.873857404021938e-07, 'epoch': 2.01}

 67%|██████▋   | 377/564 [04:49<02:19,  1.34it/s]
 67%|██████▋   | 378/564 [04:50<02:17,  1.35it/s]
                                                 
{'loss': 1.198, 'grad_norm': 3.6612633970171578, 'learning_rate': 6.837294332723948e-07, 'epoch': 2.01}

 67%|██████▋   | 378/564 [04:50<02:17,  1.35it/s]
 67%|██████▋   | 379/564 [04:51<02:19,  1.33it/s]
                                                 
{'loss': 1.0391, 'grad_norm': 2.843148594558903, 'learning_rate': 6.80073126142596e-07, 'epoch': 2.02}

 67%|██████▋   | 379/564 [04:51<02:19,  1.33it/s]
 67%|██████▋   | 380/564 [04:51<02:16,  1.35it/s]
                                                 
{'loss': 1.2261, 'grad_norm': 2.819558868923341, 'learning_rate': 6.76416819012797e-07, 'epoch': 2.02}

 67%|██████▋   | 380/564 [04:51<02:16,  1.35it/s]
 68%|██████▊   | 381/564 [04:52<02:18,  1.32it/s]
                                                 
{'loss': 1.0924, 'grad_norm': 3.1325712908491026, 'learning_rate': 6.727605118829981e-07, 'epoch': 2.03}

 68%|██████▊   | 381/564 [04:52<02:18,  1.32it/s]
 68%|██████▊   | 382/564 [04:53<02:19,  1.30it/s]
                                                 
{'loss': 1.1649, 'grad_norm': 2.957922556360849, 'learning_rate': 6.691042047531993e-07, 'epoch': 2.03}

 68%|██████▊   | 382/564 [04:53<02:19,  1.30it/s]
 68%|██████▊   | 383/564 [04:54<02:20,  1.29it/s]
                                                 
{'loss': 1.1586, 'grad_norm': 2.757077532176446, 'learning_rate': 6.654478976234004e-07, 'epoch': 2.04}

 68%|██████▊   | 383/564 [04:54<02:20,  1.29it/s]
 68%|██████▊   | 384/564 [04:54<02:16,  1.32it/s]
                                                 
{'loss': 1.1638, 'grad_norm': 2.788324470054147, 'learning_rate': 6.617915904936014e-07, 'epoch': 2.04}

 68%|██████▊   | 384/564 [04:54<02:16,  1.32it/s]
 68%|██████▊   | 385/564 [04:55<02:13,  1.34it/s]
                                                 
{'loss': 1.0673, 'grad_norm': 2.6102521801830183, 'learning_rate': 6.581352833638025e-07, 'epoch': 2.05}

 68%|██████▊   | 385/564 [04:55<02:13,  1.34it/s]
 68%|██████▊   | 386/564 [04:56<02:14,  1.32it/s]
                                                 
{'loss': 1.0746, 'grad_norm': 2.5671526643253775, 'learning_rate': 6.544789762340036e-07, 'epoch': 2.05}

 68%|██████▊   | 386/564 [04:56<02:14,  1.32it/s]
 69%|██████▊   | 387/564 [04:57<02:11,  1.34it/s]
                                                 
{'loss': 1.1593, 'grad_norm': 3.016448736285564, 'learning_rate': 6.508226691042047e-07, 'epoch': 2.06}

 69%|██████▊   | 387/564 [04:57<02:11,  1.34it/s]
 69%|██████▉   | 388/564 [04:57<02:09,  1.36it/s]
                                                 
{'loss': 1.0479, 'grad_norm': 2.6307622574625262, 'learning_rate': 6.471663619744058e-07, 'epoch': 2.06}

 69%|██████▉   | 388/564 [04:57<02:09,  1.36it/s]
 69%|██████▉   | 389/564 [04:58<02:10,  1.34it/s]
                                                 
{'loss': 1.101, 'grad_norm': 2.673346752731476, 'learning_rate': 6.435100548446069e-07, 'epoch': 2.07}

 69%|██████▉   | 389/564 [04:58<02:10,  1.34it/s]
 69%|██████▉   | 390/564 [04:59<02:08,  1.35it/s]
                                                 
{'loss': 1.0092, 'grad_norm': 2.6943697087633343, 'learning_rate': 6.398537477148081e-07, 'epoch': 2.07}

 69%|██████▉   | 390/564 [04:59<02:08,  1.35it/s]
 69%|██████▉   | 391/564 [05:00<02:06,  1.37it/s]
                                                 
{'loss': 1.2979, 'grad_norm': 2.575878752452428, 'learning_rate': 6.361974405850091e-07, 'epoch': 2.08}

 69%|██████▉   | 391/564 [05:00<02:06,  1.37it/s]
 70%|██████▉   | 392/564 [05:00<02:05,  1.38it/s]
                                                 
{'loss': 1.2437, 'grad_norm': 2.2916885177968194, 'learning_rate': 6.325411334552102e-07, 'epoch': 2.09}

 70%|██████▉   | 392/564 [05:00<02:05,  1.38it/s]
 70%|██████▉   | 393/564 [05:01<02:03,  1.38it/s]
                                                 
{'loss': 1.0496, 'grad_norm': 2.1338873947110515, 'learning_rate': 6.288848263254113e-07, 'epoch': 2.09}

 70%|██████▉   | 393/564 [05:01<02:03,  1.38it/s]
 70%|██████▉   | 394/564 [05:02<02:06,  1.35it/s]
                                                 
{'loss': 1.1299, 'grad_norm': 2.5137586862134156, 'learning_rate': 6.252285191956124e-07, 'epoch': 2.1}

 70%|██████▉   | 394/564 [05:02<02:06,  1.35it/s]
 70%|███████   | 395/564 [05:02<02:03,  1.36it/s]
                                                 
{'loss': 1.1409, 'grad_norm': 2.778635920896721, 'learning_rate': 6.215722120658134e-07, 'epoch': 2.1}

 70%|███████   | 395/564 [05:02<02:03,  1.36it/s]
 70%|███████   | 396/564 [05:03<02:02,  1.37it/s]
                                                 
{'loss': 0.998, 'grad_norm': 2.5673866522923823, 'learning_rate': 6.179159049360146e-07, 'epoch': 2.11}

 70%|███████   | 396/564 [05:03<02:02,  1.37it/s]
 70%|███████   | 397/564 [05:04<02:01,  1.38it/s]
                                                 
{'loss': 1.3889, 'grad_norm': 3.0641558609814843, 'learning_rate': 6.142595978062158e-07, 'epoch': 2.11}

 70%|███████   | 397/564 [05:04<02:01,  1.38it/s]
 71%|███████   | 398/564 [05:05<02:01,  1.37it/s]
                                                 
{'loss': 1.2023, 'grad_norm': 3.3271430047123363, 'learning_rate': 6.106032906764168e-07, 'epoch': 2.12}

 71%|███████   | 398/564 [05:05<02:01,  1.37it/s]
 71%|███████   | 399/564 [05:05<02:03,  1.33it/s]
                                                 
{'loss': 1.0882, 'grad_norm': 2.2581972062417357, 'learning_rate': 6.069469835466179e-07, 'epoch': 2.12}

 71%|███████   | 399/564 [05:05<02:03,  1.33it/s]
 71%|███████   | 400/564 [05:06<02:04,  1.31it/s]
                                                 
{'loss': 1.1101, 'grad_norm': 2.691012352185103, 'learning_rate': 6.032906764168189e-07, 'epoch': 2.13}

 71%|███████   | 400/564 [05:06<02:04,  1.31it/s]
 71%|███████   | 401/564 [05:07<02:06,  1.29it/s]
                                                 
{'loss': 1.1346, 'grad_norm': 2.6069517603295567, 'learning_rate': 5.996343692870201e-07, 'epoch': 2.13}

 71%|███████   | 401/564 [05:07<02:06,  1.29it/s]
 71%|███████▏  | 402/564 [05:08<02:04,  1.30it/s]
                                                 
{'loss': 1.0404, 'grad_norm': 2.5722104668811543, 'learning_rate': 5.959780621572211e-07, 'epoch': 2.14}

 71%|███████▏  | 402/564 [05:08<02:04,  1.30it/s]
 71%|███████▏  | 403/564 [05:08<02:00,  1.33it/s]
                                                 
{'loss': 1.1481, 'grad_norm': 2.750205245203071, 'learning_rate': 5.923217550274223e-07, 'epoch': 2.14}

 71%|███████▏  | 403/564 [05:08<02:00,  1.33it/s]
 72%|███████▏  | 404/564 [05:09<01:59,  1.34it/s]
                                                 
{'loss': 1.1059, 'grad_norm': 2.528376820654886, 'learning_rate': 5.886654478976234e-07, 'epoch': 2.15}

 72%|███████▏  | 404/564 [05:09<01:59,  1.34it/s]
 72%|███████▏  | 405/564 [05:10<02:00,  1.32it/s]
                                                 
{'loss': 1.088, 'grad_norm': 2.3950130352594123, 'learning_rate': 5.850091407678245e-07, 'epoch': 2.15}

 72%|███████▏  | 405/564 [05:10<02:00,  1.32it/s]
 72%|███████▏  | 406/564 [05:11<02:01,  1.30it/s]
                                                 
{'loss': 1.0995, 'grad_norm': 2.484790528342127, 'learning_rate': 5.813528336380256e-07, 'epoch': 2.16}

 72%|███████▏  | 406/564 [05:11<02:01,  1.30it/s]
 72%|███████▏  | 407/564 [05:12<01:57,  1.33it/s]
                                                 
{'loss': 1.1168, 'grad_norm': 2.3155719012521265, 'learning_rate': 5.776965265082266e-07, 'epoch': 2.16}

 72%|███████▏  | 407/564 [05:12<01:57,  1.33it/s]
 72%|███████▏  | 408/564 [05:12<01:58,  1.31it/s]
                                                 
{'loss': 1.1559, 'grad_norm': 2.2472269646054324, 'learning_rate': 5.740402193784278e-07, 'epoch': 2.17}

 72%|███████▏  | 408/564 [05:12<01:58,  1.31it/s]
 73%|███████▎  | 409/564 [05:13<01:57,  1.32it/s]
                                                 
{'loss': 1.0552, 'grad_norm': 2.821764006933077, 'learning_rate': 5.703839122486288e-07, 'epoch': 2.18}

 73%|███████▎  | 409/564 [05:13<01:57,  1.32it/s]
 73%|███████▎  | 410/564 [05:14<01:58,  1.30it/s]
                                                 
{'loss': 1.0251, 'grad_norm': 1.773044021450103, 'learning_rate': 5.6672760511883e-07, 'epoch': 2.18}

 73%|███████▎  | 410/564 [05:14<01:58,  1.30it/s]
 73%|███████▎  | 411/564 [05:15<01:54,  1.33it/s]
                                                 
{'loss': 1.1731, 'grad_norm': 2.1173125932134074, 'learning_rate': 5.63071297989031e-07, 'epoch': 2.19}

 73%|███████▎  | 411/564 [05:15<01:54,  1.33it/s]
 73%|███████▎  | 412/564 [05:15<01:52,  1.35it/s]
                                                 
{'loss': 1.0764, 'grad_norm': 2.1269288479705404, 'learning_rate': 5.594149908592322e-07, 'epoch': 2.19}

 73%|███████▎  | 412/564 [05:15<01:52,  1.35it/s]
 73%|███████▎  | 413/564 [05:16<01:53,  1.33it/s]
                                                 
{'loss': 1.184, 'grad_norm': 2.5662974422370697, 'learning_rate': 5.557586837294332e-07, 'epoch': 2.2}

 73%|███████▎  | 413/564 [05:16<01:53,  1.33it/s]
 73%|███████▎  | 414/564 [05:17<01:51,  1.34it/s]
                                                 
{'loss': 1.0746, 'grad_norm': 2.4370247889586514, 'learning_rate': 5.521023765996343e-07, 'epoch': 2.2}

 73%|███████▎  | 414/564 [05:17<01:51,  1.34it/s]
 74%|███████▎  | 415/564 [05:17<01:49,  1.36it/s]
                                                 
{'loss': 1.1885, 'grad_norm': 2.382418885109969, 'learning_rate': 5.484460694698354e-07, 'epoch': 2.21}

 74%|███████▎  | 415/564 [05:17<01:49,  1.36it/s]
 74%|███████▍  | 416/564 [05:18<01:50,  1.34it/s]
                                                 
{'loss': 1.1884, 'grad_norm': 2.6194024303684604, 'learning_rate': 5.447897623400365e-07, 'epoch': 2.21}

 74%|███████▍  | 416/564 [05:18<01:50,  1.34it/s]
 74%|███████▍  | 417/564 [05:19<01:51,  1.32it/s]
                                                 
{'loss': 1.0651, 'grad_norm': 2.2417346472271684, 'learning_rate': 5.411334552102377e-07, 'epoch': 2.22}

 74%|███████▍  | 417/564 [05:19<01:51,  1.32it/s]
 74%|███████▍  | 418/564 [05:20<01:48,  1.35it/s]
                                                 
{'loss': 1.084, 'grad_norm': 2.6741732933553215, 'learning_rate': 5.374771480804387e-07, 'epoch': 2.22}

 74%|███████▍  | 418/564 [05:20<01:48,  1.35it/s]
 74%|███████▍  | 419/564 [05:20<01:46,  1.37it/s]
                                                 
{'loss': 1.178, 'grad_norm': 2.085717417761367, 'learning_rate': 5.338208409506399e-07, 'epoch': 2.23}

 74%|███████▍  | 419/564 [05:20<01:46,  1.37it/s]
 74%|███████▍  | 420/564 [05:21<01:45,  1.36it/s]
                                                 
{'loss': 1.0318, 'grad_norm': 2.045351210098704, 'learning_rate': 5.301645338208409e-07, 'epoch': 2.23}

 74%|███████▍  | 420/564 [05:21<01:45,  1.36it/s]
 75%|███████▍  | 421/564 [05:22<01:44,  1.37it/s]
                                                 
{'loss': 1.2789, 'grad_norm': 2.5573198015899052, 'learning_rate': 5.26508226691042e-07, 'epoch': 2.24}

 75%|███████▍  | 421/564 [05:22<01:44,  1.37it/s]
 75%|███████▍  | 422/564 [05:23<01:42,  1.38it/s]
                                                 
{'loss': 1.1613, 'grad_norm': 2.408118069368757, 'learning_rate': 5.22851919561243e-07, 'epoch': 2.24}

 75%|███████▍  | 422/564 [05:23<01:42,  1.38it/s]
 75%|███████▌  | 423/564 [05:23<01:43,  1.37it/s]
                                                 
{'loss': 1.0725, 'grad_norm': 2.302655560701034, 'learning_rate': 5.191956124314442e-07, 'epoch': 2.25}

 75%|███████▌  | 423/564 [05:23<01:43,  1.37it/s]
 75%|███████▌  | 424/564 [05:24<01:41,  1.38it/s]
                                                 
{'loss': 1.1172, 'grad_norm': 2.3096569759027115, 'learning_rate': 5.155393053016453e-07, 'epoch': 2.26}

 75%|███████▌  | 424/564 [05:24<01:41,  1.38it/s]
 75%|███████▌  | 425/564 [05:25<01:43,  1.34it/s]
                                                 
{'loss': 1.1746, 'grad_norm': 2.2822135075879397, 'learning_rate': 5.118829981718464e-07, 'epoch': 2.26}

 75%|███████▌  | 425/564 [05:25<01:43,  1.34it/s]
 76%|███████▌  | 426/564 [05:26<01:42,  1.34it/s]
                                                 
{'loss': 1.1245, 'grad_norm': 2.338830576136024, 'learning_rate': 5.082266910420476e-07, 'epoch': 2.27}

 76%|███████▌  | 426/564 [05:26<01:42,  1.34it/s]
 76%|███████▌  | 427/564 [05:26<01:43,  1.32it/s]
                                                 
{'loss': 1.1713, 'grad_norm': 2.6604169474155346, 'learning_rate': 5.045703839122486e-07, 'epoch': 2.27}

 76%|███████▌  | 427/564 [05:26<01:43,  1.32it/s]
 76%|███████▌  | 428/564 [05:27<01:41,  1.34it/s]
                                                 
{'loss': 1.1364, 'grad_norm': 2.0874452132420744, 'learning_rate': 5.009140767824497e-07, 'epoch': 2.28}

 76%|███████▌  | 428/564 [05:27<01:41,  1.34it/s]
 76%|███████▌  | 429/564 [05:28<01:39,  1.36it/s]
                                                 
{'loss': 1.0923, 'grad_norm': 2.647773371394521, 'learning_rate': 4.972577696526509e-07, 'epoch': 2.28}

 76%|███████▌  | 429/564 [05:28<01:39,  1.36it/s]
 76%|███████▌  | 430/564 [05:29<01:37,  1.37it/s]
                                                 
{'loss': 1.0425, 'grad_norm': 2.056481350006846, 'learning_rate': 4.936014625228519e-07, 'epoch': 2.29}

 76%|███████▌  | 430/564 [05:29<01:37,  1.37it/s]
 76%|███████▋  | 431/564 [05:29<01:38,  1.35it/s]
                                                 
{'loss': 1.0448, 'grad_norm': 2.224349266058487, 'learning_rate': 4.89945155393053e-07, 'epoch': 2.29}

 76%|███████▋  | 431/564 [05:29<01:38,  1.35it/s]
 77%|███████▋  | 432/564 [05:30<01:36,  1.37it/s]
                                                 
{'loss': 1.0264, 'grad_norm': 2.2834518276848876, 'learning_rate': 4.862888482632541e-07, 'epoch': 2.3}

 77%|███████▋  | 432/564 [05:30<01:36,  1.37it/s]
 77%|███████▋  | 433/564 [05:31<01:36,  1.36it/s]
                                                 
{'loss': 1.0693, 'grad_norm': 2.671293806974645, 'learning_rate': 4.826325411334552e-07, 'epoch': 2.3}

 77%|███████▋  | 433/564 [05:31<01:36,  1.36it/s]
 77%|███████▋  | 434/564 [05:32<01:37,  1.33it/s]
                                                 
{'loss': 1.0952, 'grad_norm': 2.0068782621528967, 'learning_rate': 4.789762340036563e-07, 'epoch': 2.31}

 77%|███████▋  | 434/564 [05:32<01:37,  1.33it/s]
 77%|███████▋  | 435/564 [05:32<01:35,  1.35it/s]
                                                 
{'loss': 0.9573, 'grad_norm': 2.060839816137513, 'learning_rate': 4.7531992687385736e-07, 'epoch': 2.31}

 77%|███████▋  | 435/564 [05:32<01:35,  1.35it/s]
 77%|███████▋  | 436/564 [05:33<01:35,  1.34it/s]
                                                 
{'loss': 1.096, 'grad_norm': 2.5982393940357134, 'learning_rate': 4.716636197440585e-07, 'epoch': 2.32}

 77%|███████▋  | 436/564 [05:33<01:35,  1.34it/s]
 77%|███████▋  | 437/564 [05:34<01:34,  1.34it/s]
                                                 
{'loss': 1.0691, 'grad_norm': 1.837062462713158, 'learning_rate': 4.6800731261425957e-07, 'epoch': 2.32}

 77%|███████▋  | 437/564 [05:34<01:34,  1.34it/s]
 78%|███████▊  | 438/564 [05:35<01:35,  1.33it/s]
                                                 
{'loss': 1.18, 'grad_norm': 2.60438593226111, 'learning_rate': 4.6435100548446064e-07, 'epoch': 2.33}

 78%|███████▊  | 438/564 [05:35<01:35,  1.33it/s]
 78%|███████▊  | 439/564 [05:35<01:34,  1.33it/s]
                                                 
{'loss': 0.9335, 'grad_norm': 1.7791520155636895, 'learning_rate': 4.606946983546618e-07, 'epoch': 2.34}

 78%|███████▊  | 439/564 [05:35<01:34,  1.33it/s]
 78%|███████▊  | 440/564 [05:36<01:31,  1.35it/s]
                                                 
{'loss': 0.9982, 'grad_norm': 1.9405788935154726, 'learning_rate': 4.5703839122486285e-07, 'epoch': 2.34}

 78%|███████▊  | 440/564 [05:36<01:31,  1.35it/s]
 78%|███████▊  | 441/564 [05:37<01:29,  1.37it/s]
                                                 
{'loss': 1.0491, 'grad_norm': 2.1466183713228286, 'learning_rate': 4.53382084095064e-07, 'epoch': 2.35}

 78%|███████▊  | 441/564 [05:37<01:29,  1.37it/s]
 78%|███████▊  | 442/564 [05:37<01:30,  1.35it/s]
                                                 
{'loss': 1.081, 'grad_norm': 2.3480712199358527, 'learning_rate': 4.4972577696526506e-07, 'epoch': 2.35}

 78%|███████▊  | 442/564 [05:37<01:30,  1.35it/s]
 79%|███████▊  | 443/564 [05:38<01:31,  1.33it/s]
                                                 
{'loss': 1.1415, 'grad_norm': 2.6609313299429043, 'learning_rate': 4.460694698354662e-07, 'epoch': 2.36}

 79%|███████▊  | 443/564 [05:38<01:31,  1.33it/s]
 79%|███████▊  | 444/564 [05:39<01:31,  1.31it/s]
                                                 
{'loss': 0.9744, 'grad_norm': 2.213325409932819, 'learning_rate': 4.4241316270566726e-07, 'epoch': 2.36}

 79%|███████▊  | 444/564 [05:39<01:31,  1.31it/s]
 79%|███████▉  | 445/564 [05:40<01:28,  1.34it/s]
                                                 
{'loss': 1.1346, 'grad_norm': 2.0821003696357634, 'learning_rate': 4.3875685557586834e-07, 'epoch': 2.37}

 79%|███████▉  | 445/564 [05:40<01:28,  1.34it/s]
 79%|███████▉  | 446/564 [05:41<01:29,  1.31it/s]
                                                 
{'loss': 0.9141, 'grad_norm': 2.415360976425837, 'learning_rate': 4.3510054844606947e-07, 'epoch': 2.37}

 79%|███████▉  | 446/564 [05:41<01:29,  1.31it/s]
 79%|███████▉  | 447/564 [05:41<01:28,  1.32it/s]
                                                 
{'loss': 1.1449, 'grad_norm': 2.0898508342659095, 'learning_rate': 4.3144424131627054e-07, 'epoch': 2.38}

 79%|███████▉  | 447/564 [05:41<01:28,  1.32it/s]
 79%|███████▉  | 448/564 [05:42<01:26,  1.35it/s]
                                                 
{'loss': 1.0813, 'grad_norm': 2.521450252920866, 'learning_rate': 4.277879341864716e-07, 'epoch': 2.38}

 79%|███████▉  | 448/564 [05:42<01:26,  1.35it/s]
 80%|███████▉  | 449/564 [05:43<01:24,  1.37it/s]
                                                 
{'loss': 0.9658, 'grad_norm': 2.0650202762529193, 'learning_rate': 4.241316270566727e-07, 'epoch': 2.39}

 80%|███████▉  | 449/564 [05:43<01:24,  1.37it/s]
 80%|███████▉  | 450/564 [05:43<01:22,  1.38it/s]
                                                 
{'loss': 1.0702, 'grad_norm': 2.148259115918155, 'learning_rate': 4.2047531992687383e-07, 'epoch': 2.39}

 80%|███████▉  | 450/564 [05:43<01:22,  1.38it/s]
 80%|███████▉  | 451/564 [05:44<01:21,  1.38it/s]
                                                 
{'loss': 0.9865, 'grad_norm': 1.7616330098076396, 'learning_rate': 4.1681901279707496e-07, 'epoch': 2.4}

 80%|███████▉  | 451/564 [05:44<01:21,  1.38it/s]
 80%|████████  | 452/564 [05:45<01:23,  1.35it/s]
                                                 
{'loss': 1.0122, 'grad_norm': 1.9506580470533224, 'learning_rate': 4.1316270566727603e-07, 'epoch': 2.4}

 80%|████████  | 452/564 [05:45<01:23,  1.35it/s]
 80%|████████  | 453/564 [05:46<01:24,  1.32it/s]
                                                 
{'loss': 1.2129, 'grad_norm': 1.968783850974395, 'learning_rate': 4.0950639853747716e-07, 'epoch': 2.41}

 80%|████████  | 453/564 [05:46<01:24,  1.32it/s]
 80%|████████  | 454/564 [05:47<01:24,  1.30it/s]
                                                 
{'loss': 1.1619, 'grad_norm': 1.8863239034509742, 'learning_rate': 4.0585009140767824e-07, 'epoch': 2.41}

 80%|████████  | 454/564 [05:47<01:24,  1.30it/s]
 81%|████████  | 455/564 [05:47<01:23,  1.30it/s]
                                                 
{'loss': 1.1087, 'grad_norm': 2.1027868602311233, 'learning_rate': 4.021937842778793e-07, 'epoch': 2.42}

 81%|████████  | 455/564 [05:47<01:23,  1.30it/s]
 81%|████████  | 456/564 [05:48<01:22,  1.30it/s]
                                                 
{'loss': 1.0247, 'grad_norm': 2.1732949930666132, 'learning_rate': 3.9853747714808044e-07, 'epoch': 2.43}

 81%|████████  | 456/564 [05:48<01:22,  1.30it/s]
 81%|████████  | 457/564 [05:49<01:22,  1.29it/s]
                                                 
{'loss': 1.0788, 'grad_norm': 1.968698520792538, 'learning_rate': 3.948811700182815e-07, 'epoch': 2.43}

 81%|████████  | 457/564 [05:49<01:22,  1.29it/s]
 81%|████████  | 458/564 [05:50<01:22,  1.28it/s]
                                                 
{'loss': 1.0199, 'grad_norm': 2.5852030840293097, 'learning_rate': 3.912248628884826e-07, 'epoch': 2.44}

 81%|████████  | 458/564 [05:50<01:22,  1.28it/s]
 81%|████████▏ | 459/564 [05:50<01:20,  1.30it/s]
                                                 
{'loss': 1.0073, 'grad_norm': 2.741968401631217, 'learning_rate': 3.875685557586837e-07, 'epoch': 2.44}

 81%|████████▏ | 459/564 [05:50<01:20,  1.30it/s]
 82%|████████▏ | 460/564 [05:51<01:18,  1.32it/s]
                                                 
{'loss': 0.9817, 'grad_norm': 2.5364971207019034, 'learning_rate': 3.839122486288848e-07, 'epoch': 2.45}

 82%|████████▏ | 460/564 [05:51<01:18,  1.32it/s]
 82%|████████▏ | 461/564 [05:52<01:16,  1.34it/s]
                                                 
{'loss': 1.0234, 'grad_norm': 1.9669120585061108, 'learning_rate': 3.8025594149908593e-07, 'epoch': 2.45}

 82%|████████▏ | 461/564 [05:52<01:16,  1.34it/s]
 82%|████████▏ | 462/564 [05:53<01:17,  1.32it/s]
                                                 
{'loss': 1.0621, 'grad_norm': 1.8662875726534944, 'learning_rate': 3.76599634369287e-07, 'epoch': 2.46}

 82%|████████▏ | 462/564 [05:53<01:17,  1.32it/s]
 82%|████████▏ | 463/564 [05:53<01:15,  1.34it/s]
                                                 
{'loss': 0.9629, 'grad_norm': 2.3552920103068202, 'learning_rate': 3.7294332723948814e-07, 'epoch': 2.46}

 82%|████████▏ | 463/564 [05:53<01:15,  1.34it/s]
 82%|████████▏ | 464/564 [05:54<01:13,  1.35it/s]
                                                 
{'loss': 1.0266, 'grad_norm': 2.381398638941888, 'learning_rate': 3.692870201096892e-07, 'epoch': 2.47}

 82%|████████▏ | 464/564 [05:54<01:13,  1.35it/s]
 82%|████████▏ | 465/564 [05:55<01:12,  1.36it/s]
                                                 
{'loss': 0.8679, 'grad_norm': 2.586040133254318, 'learning_rate': 3.656307129798903e-07, 'epoch': 2.47}

 82%|████████▏ | 465/564 [05:55<01:12,  1.36it/s]
 83%|████████▎ | 466/564 [05:56<01:13,  1.33it/s]
                                                 
{'loss': 1.0092, 'grad_norm': 1.883140903632611, 'learning_rate': 3.6197440585009137e-07, 'epoch': 2.48}

 83%|████████▎ | 466/564 [05:56<01:13,  1.33it/s]
 83%|████████▎ | 467/564 [05:56<01:14,  1.31it/s]
                                                 
{'loss': 0.9189, 'grad_norm': 1.8903720894905238, 'learning_rate': 3.583180987202925e-07, 'epoch': 2.48}

 83%|████████▎ | 467/564 [05:56<01:14,  1.31it/s]
 83%|████████▎ | 468/564 [05:57<01:14,  1.29it/s]
                                                 
{'loss': 1.1009, 'grad_norm': 1.7623063159813688, 'learning_rate': 3.5466179159049357e-07, 'epoch': 2.49}

 83%|████████▎ | 468/564 [05:57<01:14,  1.29it/s]
 83%|████████▎ | 469/564 [05:58<01:12,  1.32it/s]
                                                 
{'loss': 0.979, 'grad_norm': 1.7358472389499093, 'learning_rate': 3.5100548446069465e-07, 'epoch': 2.49}

 83%|████████▎ | 469/564 [05:58<01:12,  1.32it/s]
 83%|████████▎ | 470/564 [05:59<01:12,  1.30it/s]
                                                 
{'loss': 1.1076, 'grad_norm': 2.1585313270185225, 'learning_rate': 3.473491773308958e-07, 'epoch': 2.5}

 83%|████████▎ | 470/564 [05:59<01:12,  1.30it/s]
 84%|████████▎ | 471/564 [05:59<01:10,  1.33it/s]
                                                 
{'loss': 1.0922, 'grad_norm': 2.0819392092678783, 'learning_rate': 3.436928702010969e-07, 'epoch': 2.51}

 84%|████████▎ | 471/564 [05:59<01:10,  1.33it/s]
 84%|████████▎ | 472/564 [06:00<01:10,  1.30it/s]
                                                 
{'loss': 1.0974, 'grad_norm': 1.9599604913002273, 'learning_rate': 3.40036563071298e-07, 'epoch': 2.51}

 84%|████████▎ | 472/564 [06:00<01:10,  1.30it/s]
 84%|████████▍ | 473/564 [06:01<01:10,  1.29it/s]
                                                 
{'loss': 1.0156, 'grad_norm': 2.713063715941267, 'learning_rate': 3.3638025594149906e-07, 'epoch': 2.52}

 84%|████████▍ | 473/564 [06:01<01:10,  1.29it/s]
 84%|████████▍ | 474/564 [06:02<01:08,  1.31it/s]
                                                 
{'loss': 1.1024, 'grad_norm': 2.446639997850026, 'learning_rate': 3.327239488117002e-07, 'epoch': 2.52}

 84%|████████▍ | 474/564 [06:02<01:08,  1.31it/s]
 84%|████████▍ | 475/564 [06:02<01:06,  1.33it/s]
                                                 
{'loss': 0.9909, 'grad_norm': 2.0393903201955874, 'learning_rate': 3.2906764168190127e-07, 'epoch': 2.53}

 84%|████████▍ | 475/564 [06:02<01:06,  1.33it/s]
 84%|████████▍ | 476/564 [06:03<01:06,  1.31it/s]
                                                 
{'loss': 1.0478, 'grad_norm': 1.958774304003773, 'learning_rate': 3.2541133455210234e-07, 'epoch': 2.53}

 84%|████████▍ | 476/564 [06:03<01:06,  1.31it/s]
 85%|████████▍ | 477/564 [06:04<01:05,  1.33it/s]
                                                 
{'loss': 1.1005, 'grad_norm': 2.208818777986977, 'learning_rate': 3.2175502742230347e-07, 'epoch': 2.54}

 85%|████████▍ | 477/564 [06:04<01:05,  1.33it/s]
 85%|████████▍ | 478/564 [06:05<01:05,  1.31it/s]
                                                 
{'loss': 1.0566, 'grad_norm': 2.5804895675185158, 'learning_rate': 3.1809872029250455e-07, 'epoch': 2.54}

 85%|████████▍ | 478/564 [06:05<01:05,  1.31it/s]
 85%|████████▍ | 479/564 [06:06<01:05,  1.29it/s]
                                                 
{'loss': 1.0708, 'grad_norm': 1.8482167829430232, 'learning_rate': 3.144424131627056e-07, 'epoch': 2.55}

 85%|████████▍ | 479/564 [06:06<01:05,  1.29it/s]
 85%|████████▌ | 480/564 [06:06<01:03,  1.32it/s]
                                                 
{'loss': 1.098, 'grad_norm': 2.158631230909328, 'learning_rate': 3.107861060329067e-07, 'epoch': 2.55}

 85%|████████▌ | 480/564 [06:06<01:03,  1.32it/s]
 85%|████████▌ | 481/564 [06:07<01:01,  1.34it/s]
                                                 
{'loss': 1.0283, 'grad_norm': 1.7753672949928858, 'learning_rate': 3.071297989031079e-07, 'epoch': 2.56}

 85%|████████▌ | 481/564 [06:07<01:01,  1.34it/s]
 85%|████████▌ | 482/564 [06:08<01:02,  1.31it/s]
                                                 
{'loss': 1.0522, 'grad_norm': 1.8731796896840516, 'learning_rate': 3.0347349177330896e-07, 'epoch': 2.56}

 85%|████████▌ | 482/564 [06:08<01:02,  1.31it/s]
 86%|████████▌ | 483/564 [06:09<01:02,  1.31it/s]
                                                 
{'loss': 1.1845, 'grad_norm': 1.9846336885909863, 'learning_rate': 2.9981718464351004e-07, 'epoch': 2.57}

 86%|████████▌ | 483/564 [06:09<01:02,  1.31it/s]
 86%|████████▌ | 484/564 [06:09<01:00,  1.33it/s]
                                                 
{'loss': 0.9332, 'grad_norm': 1.7817362672340096, 'learning_rate': 2.9616087751371117e-07, 'epoch': 2.57}

 86%|████████▌ | 484/564 [06:09<01:00,  1.33it/s]
 86%|████████▌ | 485/564 [06:10<00:58,  1.35it/s]
                                                 
{'loss': 1.0145, 'grad_norm': 2.5309659441261894, 'learning_rate': 2.9250457038391224e-07, 'epoch': 2.58}

 86%|████████▌ | 485/564 [06:10<00:58,  1.35it/s]
 86%|████████▌ | 486/564 [06:11<00:57,  1.36it/s]
                                                 
{'loss': 0.8359, 'grad_norm': 2.0021062647417036, 'learning_rate': 2.888482632541133e-07, 'epoch': 2.59}

 86%|████████▌ | 486/564 [06:11<00:57,  1.36it/s]
 86%|████████▋ | 487/564 [06:11<00:56,  1.37it/s]
                                                 
{'loss': 1.0217, 'grad_norm': 1.5259480267678385, 'learning_rate': 2.851919561243144e-07, 'epoch': 2.59}

 86%|████████▋ | 487/564 [06:11<00:56,  1.37it/s]
 87%|████████▋ | 488/564 [06:12<00:55,  1.37it/s]
                                                 
{'loss': 0.9914, 'grad_norm': 1.7687913039948127, 'learning_rate': 2.815356489945155e-07, 'epoch': 2.6}

 87%|████████▋ | 488/564 [06:12<00:55,  1.37it/s]
 87%|████████▋ | 489/564 [06:13<00:54,  1.38it/s]
                                                 
{'loss': 1.0114, 'grad_norm': 2.2299986654424573, 'learning_rate': 2.778793418647166e-07, 'epoch': 2.6}

 87%|████████▋ | 489/564 [06:13<00:54,  1.38it/s]
 87%|████████▋ | 490/564 [06:14<00:53,  1.38it/s]
                                                 
{'loss': 0.9183, 'grad_norm': 1.98285756685818, 'learning_rate': 2.742230347349177e-07, 'epoch': 2.61}

 87%|████████▋ | 490/564 [06:14<00:53,  1.38it/s]
 87%|████████▋ | 491/564 [06:14<00:54,  1.34it/s]
                                                 
{'loss': 0.9438, 'grad_norm': 2.128344347439141, 'learning_rate': 2.7056672760511886e-07, 'epoch': 2.61}

 87%|████████▋ | 491/564 [06:14<00:54,  1.34it/s]
 87%|████████▋ | 492/564 [06:15<00:53,  1.36it/s]
                                                 
{'loss': 1.1334, 'grad_norm': 1.773257037227179, 'learning_rate': 2.6691042047531994e-07, 'epoch': 2.62}

 87%|████████▋ | 492/564 [06:15<00:53,  1.36it/s]
 87%|████████▋ | 493/564 [06:16<00:53,  1.32it/s]
                                                 
{'loss': 0.974, 'grad_norm': 1.7984892873930032, 'learning_rate': 2.63254113345521e-07, 'epoch': 2.62}

 87%|████████▋ | 493/564 [06:16<00:53,  1.32it/s]
 88%|████████▊ | 494/564 [06:17<00:53,  1.30it/s]
                                                 
{'loss': 0.9778, 'grad_norm': 1.9604780716958425, 'learning_rate': 2.595978062157221e-07, 'epoch': 2.63}

 88%|████████▊ | 494/564 [06:17<00:53,  1.30it/s]
 88%|████████▊ | 495/564 [06:18<00:53,  1.29it/s]
                                                 
{'loss': 1.1039, 'grad_norm': 1.816619169053287, 'learning_rate': 2.559414990859232e-07, 'epoch': 2.63}

 88%|████████▊ | 495/564 [06:18<00:53,  1.29it/s]
 88%|████████▊ | 496/564 [06:18<00:53,  1.28it/s]
                                                 
{'loss': 0.9792, 'grad_norm': 1.5177022110644087, 'learning_rate': 2.522851919561243e-07, 'epoch': 2.64}

 88%|████████▊ | 496/564 [06:18<00:53,  1.28it/s]
 88%|████████▊ | 497/564 [06:19<00:51,  1.31it/s]
                                                 
{'loss': 1.0694, 'grad_norm': 1.9509936075769814, 'learning_rate': 2.486288848263254e-07, 'epoch': 2.64}

 88%|████████▊ | 497/564 [06:19<00:51,  1.31it/s]
 88%|████████▊ | 498/564 [06:20<00:51,  1.29it/s]
                                                 
{'loss': 0.9398, 'grad_norm': 1.8544651096280094, 'learning_rate': 2.449725776965265e-07, 'epoch': 2.65}

 88%|████████▊ | 498/564 [06:20<00:51,  1.29it/s]
 88%|████████▊ | 499/564 [06:21<00:49,  1.32it/s]
                                                 
{'loss': 1.1047, 'grad_norm': 2.1425788870768816, 'learning_rate': 2.413162705667276e-07, 'epoch': 2.65}

 88%|████████▊ | 499/564 [06:21<00:49,  1.32it/s]
 89%|████████▊ | 500/564 [06:21<00:47,  1.34it/s]
                                                 
{'loss': 1.0251, 'grad_norm': 1.8066899424214111, 'learning_rate': 2.3765996343692868e-07, 'epoch': 2.66}

 89%|████████▊ | 500/564 [06:21<00:47,  1.34it/s]/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(

 89%|████████▉ | 501/564 [06:32<03:53,  3.71s/it]
                                                 
{'loss': 0.9444, 'grad_norm': 2.2125372109922368, 'learning_rate': 2.3400365630712978e-07, 'epoch': 2.66}

 89%|████████▉ | 501/564 [06:32<03:53,  3.71s/it]
 89%|████████▉ | 502/564 [06:33<02:54,  2.81s/it]
                                                 
{'loss': 1.0199, 'grad_norm': 2.3415013280414305, 'learning_rate': 2.303473491773309e-07, 'epoch': 2.67}

 89%|████████▉ | 502/564 [06:33<02:54,  2.81s/it]
 89%|████████▉ | 503/564 [06:33<02:12,  2.18s/it]
                                                 
{'loss': 1.1108, 'grad_norm': 2.5326705932005886, 'learning_rate': 2.26691042047532e-07, 'epoch': 2.68}

 89%|████████▉ | 503/564 [06:33<02:12,  2.18s/it]
 89%|████████▉ | 504/564 [06:34<01:44,  1.74s/it]
                                                 
{'loss': 1.0168, 'grad_norm': 2.173449287237455, 'learning_rate': 2.230347349177331e-07, 'epoch': 2.68}

 89%|████████▉ | 504/564 [06:34<01:44,  1.74s/it]
 90%|████████▉ | 505/564 [06:35<01:25,  1.46s/it]
                                                 
{'loss': 0.8978, 'grad_norm': 1.6413197063331393, 'learning_rate': 2.1937842778793417e-07, 'epoch': 2.69}

 90%|████████▉ | 505/564 [06:35<01:25,  1.46s/it]
 90%|████████▉ | 506/564 [06:36<01:12,  1.24s/it]
                                                 
{'loss': 0.9791, 'grad_norm': 2.3836406398797183, 'learning_rate': 2.1572212065813527e-07, 'epoch': 2.69}

 90%|████████▉ | 506/564 [06:36<01:12,  1.24s/it]
 90%|████████▉ | 507/564 [06:36<01:02,  1.10s/it]
                                                 
{'loss': 1.0216, 'grad_norm': 2.136807899488256, 'learning_rate': 2.1206581352833635e-07, 'epoch': 2.7}

 90%|████████▉ | 507/564 [06:36<01:02,  1.10s/it]
 90%|█████████ | 508/564 [06:37<00:56,  1.00s/it]
                                                 
{'loss': 1.0215, 'grad_norm': 1.8473567541805722, 'learning_rate': 2.0840950639853748e-07, 'epoch': 2.7}

 90%|█████████ | 508/564 [06:37<00:56,  1.00s/it]
 90%|█████████ | 509/564 [06:38<00:52,  1.06it/s]
                                                 
{'loss': 1.0697, 'grad_norm': 2.448939620340706, 'learning_rate': 2.0475319926873858e-07, 'epoch': 2.71}

 90%|█████████ | 509/564 [06:38<00:52,  1.06it/s]
 90%|█████████ | 510/564 [06:39<00:47,  1.13it/s]
                                                 
{'loss': 0.9389, 'grad_norm': 2.1456275574526487, 'learning_rate': 2.0109689213893966e-07, 'epoch': 2.71}

 90%|█████████ | 510/564 [06:39<00:47,  1.13it/s]
 91%|█████████ | 511/564 [06:39<00:45,  1.16it/s]
                                                 
{'loss': 1.0557, 'grad_norm': 1.423518538801332, 'learning_rate': 1.9744058500914076e-07, 'epoch': 2.72}

 91%|█████████ | 511/564 [06:39<00:45,  1.16it/s]
 91%|█████████ | 512/564 [06:40<00:43,  1.19it/s]
                                                 
{'loss': 1.0355, 'grad_norm': 2.082658618582188, 'learning_rate': 1.9378427787934184e-07, 'epoch': 2.72}

 91%|█████████ | 512/564 [06:40<00:43,  1.19it/s]
 91%|█████████ | 513/564 [06:41<00:41,  1.24it/s]
                                                 
{'loss': 1.1377, 'grad_norm': 1.740264952347327, 'learning_rate': 1.9012797074954297e-07, 'epoch': 2.73}

 91%|█████████ | 513/564 [06:41<00:41,  1.24it/s]
 91%|█████████ | 514/564 [06:42<00:38,  1.29it/s]
                                                 
{'loss': 1.0814, 'grad_norm': 1.5738219289866413, 'learning_rate': 1.8647166361974407e-07, 'epoch': 2.73}

 91%|█████████ | 514/564 [06:42<00:38,  1.29it/s]
 91%|█████████▏| 515/564 [06:42<00:37,  1.32it/s]
                                                 
{'loss': 1.0393, 'grad_norm': 2.957947270446189, 'learning_rate': 1.8281535648994515e-07, 'epoch': 2.74}

 91%|█████████▏| 515/564 [06:42<00:37,  1.32it/s]
 91%|█████████▏| 516/564 [06:43<00:36,  1.30it/s]
                                                 
{'loss': 1.0233, 'grad_norm': 1.8216545514115123, 'learning_rate': 1.7915904936014625e-07, 'epoch': 2.74}

 91%|█████████▏| 516/564 [06:43<00:36,  1.30it/s]
 92%|█████████▏| 517/564 [06:44<00:35,  1.33it/s]
                                                 
{'loss': 0.9731, 'grad_norm': 2.8275762521487815, 'learning_rate': 1.7550274223034732e-07, 'epoch': 2.75}

 92%|█████████▏| 517/564 [06:44<00:35,  1.33it/s]
 92%|█████████▏| 518/564 [06:45<00:34,  1.31it/s]
                                                 
{'loss': 0.9801, 'grad_norm': 2.0447815865454553, 'learning_rate': 1.7184643510054845e-07, 'epoch': 2.76}

 92%|█████████▏| 518/564 [06:45<00:34,  1.31it/s]
 92%|█████████▏| 519/564 [06:45<00:33,  1.33it/s]
                                                 
{'loss': 1.0432, 'grad_norm': 1.8012942818023545, 'learning_rate': 1.6819012797074953e-07, 'epoch': 2.76}

 92%|█████████▏| 519/564 [06:45<00:33,  1.33it/s]
 92%|█████████▏| 520/564 [06:46<00:33,  1.31it/s]
                                                 
{'loss': 1.0317, 'grad_norm': 1.7091780098950085, 'learning_rate': 1.6453382084095063e-07, 'epoch': 2.77}

 92%|█████████▏| 520/564 [06:46<00:33,  1.31it/s]
 92%|█████████▏| 521/564 [06:47<00:32,  1.31it/s]
                                                 
{'loss': 1.0484, 'grad_norm': 1.6282547782845367, 'learning_rate': 1.6087751371115174e-07, 'epoch': 2.77}

 92%|█████████▏| 521/564 [06:47<00:32,  1.31it/s]
 93%|█████████▎| 522/564 [06:48<00:31,  1.32it/s]
                                                 
{'loss': 1.1216, 'grad_norm': 1.5682000231830768, 'learning_rate': 1.572212065813528e-07, 'epoch': 2.78}

 93%|█████████▎| 522/564 [06:48<00:31,  1.32it/s]
 93%|█████████▎| 523/564 [06:48<00:30,  1.34it/s]
                                                 
{'loss': 1.065, 'grad_norm': 1.6416737837667863, 'learning_rate': 1.5356489945155394e-07, 'epoch': 2.78}

 93%|█████████▎| 523/564 [06:48<00:30,  1.34it/s]
 93%|█████████▎| 524/564 [06:49<00:29,  1.35it/s]
                                                 
{'loss': 1.0028, 'grad_norm': 1.6806762336573509, 'learning_rate': 1.4990859232175502e-07, 'epoch': 2.79}

 93%|█████████▎| 524/564 [06:49<00:29,  1.35it/s]
 93%|█████████▎| 525/564 [06:50<00:28,  1.37it/s]
                                                 
{'loss': 1.0054, 'grad_norm': 1.642719206197457, 'learning_rate': 1.4625228519195612e-07, 'epoch': 2.79}

 93%|█████████▎| 525/564 [06:50<00:28,  1.37it/s]
 93%|█████████▎| 526/564 [06:51<00:31,  1.22it/s]
                                                 
{'loss': 0.961, 'grad_norm': 1.4973258185169231, 'learning_rate': 1.425959780621572e-07, 'epoch': 2.8}

 93%|█████████▎| 526/564 [06:51<00:31,  1.22it/s]
 93%|█████████▎| 527/564 [06:52<00:29,  1.24it/s]
                                                 
{'loss': 1.0935, 'grad_norm': 1.7675122392088416, 'learning_rate': 1.389396709323583e-07, 'epoch': 2.8}

 93%|█████████▎| 527/564 [06:52<00:29,  1.24it/s]
 94%|█████████▎| 528/564 [06:52<00:28,  1.24it/s]
                                                 
{'loss': 0.9745, 'grad_norm': 1.7830221519818041, 'learning_rate': 1.3528336380255943e-07, 'epoch': 2.81}

 94%|█████████▎| 528/564 [06:52<00:28,  1.24it/s]
 94%|█████████▍| 529/564 [06:53<00:27,  1.27it/s]
                                                 
{'loss': 0.9687, 'grad_norm': 2.454087443809219, 'learning_rate': 1.316270566727605e-07, 'epoch': 2.81}

 94%|█████████▍| 529/564 [06:53<00:27,  1.27it/s]
 94%|█████████▍| 530/564 [06:54<00:25,  1.31it/s]
                                                 
{'loss': 0.9828, 'grad_norm': 2.4965055421500346, 'learning_rate': 1.279707495429616e-07, 'epoch': 2.82}

 94%|█████████▍| 530/564 [06:54<00:25,  1.31it/s]
 94%|█████████▍| 531/564 [06:55<00:24,  1.33it/s]
                                                 
{'loss': 0.95, 'grad_norm': 1.647191069073074, 'learning_rate': 1.243144424131627e-07, 'epoch': 2.82}

 94%|█████████▍| 531/564 [06:55<00:24,  1.33it/s]
 94%|█████████▍| 532/564 [06:55<00:23,  1.34it/s]
                                                 
{'loss': 1.0063, 'grad_norm': 2.5321426239733573, 'learning_rate': 1.206581352833638e-07, 'epoch': 2.83}

 94%|█████████▍| 532/564 [06:55<00:23,  1.34it/s]
 95%|█████████▍| 533/564 [06:56<00:22,  1.35it/s]
                                                 
{'loss': 0.9727, 'grad_norm': 1.8175755601865182, 'learning_rate': 1.1700182815356489e-07, 'epoch': 2.84}

 95%|█████████▍| 533/564 [06:56<00:22,  1.35it/s]
 95%|█████████▍| 534/564 [06:57<00:22,  1.32it/s]
                                                 
{'loss': 0.9467, 'grad_norm': 2.7620191874963194, 'learning_rate': 1.13345521023766e-07, 'epoch': 2.84}

 95%|█████████▍| 534/564 [06:57<00:22,  1.32it/s]
 95%|█████████▍| 535/564 [06:58<00:21,  1.34it/s]
                                                 
{'loss': 0.8767, 'grad_norm': 2.4592816290456296, 'learning_rate': 1.0968921389396708e-07, 'epoch': 2.85}

 95%|█████████▍| 535/564 [06:58<00:21,  1.34it/s]
 95%|█████████▌| 536/564 [06:58<00:20,  1.36it/s]
                                                 
{'loss': 1.0257, 'grad_norm': 2.005469767521012, 'learning_rate': 1.0603290676416817e-07, 'epoch': 2.85}

 95%|█████████▌| 536/564 [06:58<00:20,  1.36it/s]
 95%|█████████▌| 537/564 [06:59<00:19,  1.37it/s]
                                                 
{'loss': 0.9935, 'grad_norm': 1.948826058160829, 'learning_rate': 1.0237659963436929e-07, 'epoch': 2.86}

 95%|█████████▌| 537/564 [06:59<00:19,  1.37it/s]
 95%|█████████▌| 538/564 [07:00<00:18,  1.38it/s]
                                                 
{'loss': 1.0724, 'grad_norm': 1.852053383545378, 'learning_rate': 9.872029250457038e-08, 'epoch': 2.86}

 95%|█████████▌| 538/564 [07:00<00:18,  1.38it/s]
 96%|█████████▌| 539/564 [07:00<00:18,  1.39it/s]
                                                 
{'loss': 1.1959, 'grad_norm': 1.6703878771560843, 'learning_rate': 9.506398537477148e-08, 'epoch': 2.87}

 96%|█████████▌| 539/564 [07:00<00:18,  1.39it/s]
 96%|█████████▌| 540/564 [07:01<00:17,  1.39it/s]
                                                 
{'loss': 0.9618, 'grad_norm': 1.4717631129415547, 'learning_rate': 9.140767824497257e-08, 'epoch': 2.87}

 96%|█████████▌| 540/564 [07:01<00:17,  1.39it/s]
 96%|█████████▌| 541/564 [07:02<00:17,  1.35it/s]
                                                 
{'loss': 0.994, 'grad_norm': 2.175010507091207, 'learning_rate': 8.775137111517366e-08, 'epoch': 2.88}

 96%|█████████▌| 541/564 [07:02<00:17,  1.35it/s]
 96%|█████████▌| 542/564 [07:03<00:16,  1.32it/s]
                                                 
{'loss': 1.0031, 'grad_norm': 2.1594991527911693, 'learning_rate': 8.409506398537477e-08, 'epoch': 2.88}

 96%|█████████▌| 542/564 [07:03<00:16,  1.32it/s]
 96%|█████████▋| 543/564 [07:04<00:15,  1.34it/s]
                                                 
{'loss': 1.0376, 'grad_norm': 2.0437677993257646, 'learning_rate': 8.043875685557587e-08, 'epoch': 2.89}

 96%|█████████▋| 543/564 [07:04<00:15,  1.34it/s]
 96%|█████████▋| 544/564 [07:04<00:14,  1.34it/s]
                                                 
{'loss': 0.924, 'grad_norm': 2.031993092093413, 'learning_rate': 7.678244972577697e-08, 'epoch': 2.89}

 96%|█████████▋| 544/564 [07:04<00:14,  1.34it/s]
 97%|█████████▋| 545/564 [07:05<00:14,  1.32it/s]
                                                 
{'loss': 1.0029, 'grad_norm': 2.613857544485338, 'learning_rate': 7.312614259597806e-08, 'epoch': 2.9}

 97%|█████████▋| 545/564 [07:05<00:14,  1.32it/s]
 97%|█████████▋| 546/564 [07:06<00:13,  1.34it/s]
                                                 
{'loss': 1.0194, 'grad_norm': 1.9390723023800296, 'learning_rate': 6.946983546617915e-08, 'epoch': 2.9}

 97%|█████████▋| 546/564 [07:06<00:13,  1.34it/s]
 97%|█████████▋| 547/564 [07:07<00:12,  1.33it/s]
                                                 
{'loss': 0.9881, 'grad_norm': 1.617856321996547, 'learning_rate': 6.581352833638025e-08, 'epoch': 2.91}

 97%|█████████▋| 547/564 [07:07<00:12,  1.33it/s]
 97%|█████████▋| 548/564 [07:07<00:11,  1.35it/s]
                                                 
{'loss': 0.9973, 'grad_norm': 1.9682711969510018, 'learning_rate': 6.215722120658136e-08, 'epoch': 2.91}

 97%|█████████▋| 548/564 [07:07<00:11,  1.35it/s]
 97%|█████████▋| 549/564 [07:08<00:10,  1.37it/s]
                                                 
{'loss': 0.9322, 'grad_norm': 2.0974592909809644, 'learning_rate': 5.8500914076782446e-08, 'epoch': 2.92}

 97%|█████████▋| 549/564 [07:08<00:10,  1.37it/s]
 98%|█████████▊| 550/564 [07:09<00:10,  1.38it/s]
                                                 
{'loss': 1.0334, 'grad_norm': 1.5935413237026876, 'learning_rate': 5.484460694698354e-08, 'epoch': 2.93}

 98%|█████████▊| 550/564 [07:09<00:10,  1.38it/s]
 98%|█████████▊| 551/564 [07:09<00:09,  1.34it/s]
                                                 
{'loss': 1.0465, 'grad_norm': 1.7383802334243827, 'learning_rate': 5.1188299817184645e-08, 'epoch': 2.93}

 98%|█████████▊| 551/564 [07:09<00:09,  1.34it/s]
 98%|█████████▊| 552/564 [07:10<00:09,  1.32it/s]
                                                 
{'loss': 1.0878, 'grad_norm': 2.1663614396527935, 'learning_rate': 4.753199268738574e-08, 'epoch': 2.94}

 98%|█████████▊| 552/564 [07:10<00:09,  1.32it/s]
 98%|█████████▊| 553/564 [07:11<00:08,  1.34it/s]
                                                 
{'loss': 0.9152, 'grad_norm': 1.7973548534984005, 'learning_rate': 4.387568555758683e-08, 'epoch': 2.94}

 98%|█████████▊| 553/564 [07:11<00:08,  1.34it/s]
 98%|█████████▊| 554/564 [07:12<00:07,  1.36it/s]
                                                 
{'loss': 1.0766, 'grad_norm': 1.6986774676116736, 'learning_rate': 4.0219378427787934e-08, 'epoch': 2.95}

 98%|█████████▊| 554/564 [07:12<00:07,  1.36it/s]
 98%|█████████▊| 555/564 [07:12<00:06,  1.33it/s]
                                                 
{'loss': 1.1395, 'grad_norm': 1.890497236636227, 'learning_rate': 3.656307129798903e-08, 'epoch': 2.95}

 98%|█████████▊| 555/564 [07:12<00:06,  1.33it/s]
 99%|█████████▊| 556/564 [07:13<00:05,  1.34it/s]
                                                 
{'loss': 0.8762, 'grad_norm': 1.4577496383467579, 'learning_rate': 3.290676416819013e-08, 'epoch': 2.96}

 99%|█████████▊| 556/564 [07:13<00:05,  1.34it/s]
 99%|█████████▉| 557/564 [07:14<00:05,  1.32it/s]
                                                 
{'loss': 1.1078, 'grad_norm': 1.9608899107827087, 'learning_rate': 2.9250457038391223e-08, 'epoch': 2.96}

 99%|█████████▉| 557/564 [07:14<00:05,  1.32it/s]
 99%|█████████▉| 558/564 [07:15<00:04,  1.32it/s]
                                                 
{'loss': 1.0295, 'grad_norm': 1.8585610982246137, 'learning_rate': 2.5594149908592323e-08, 'epoch': 2.97}

 99%|█████████▉| 558/564 [07:15<00:04,  1.32it/s]
 99%|█████████▉| 559/564 [07:15<00:03,  1.34it/s]
                                                 
{'loss': 1.1271, 'grad_norm': 2.4840723763527435, 'learning_rate': 2.1937842778793416e-08, 'epoch': 2.97}

 99%|█████████▉| 559/564 [07:15<00:03,  1.34it/s]
 99%|█████████▉| 560/564 [07:16<00:02,  1.36it/s]
                                                 
{'loss': 1.0428, 'grad_norm': 3.0427671098469893, 'learning_rate': 1.8281535648994515e-08, 'epoch': 2.98}

 99%|█████████▉| 560/564 [07:16<00:02,  1.36it/s]
 99%|█████████▉| 561/564 [07:17<00:02,  1.37it/s]
                                                 
{'loss': 0.9923, 'grad_norm': 1.4587061420251537, 'learning_rate': 1.4625228519195612e-08, 'epoch': 2.98}

 99%|█████████▉| 561/564 [07:17<00:02,  1.37it/s]
100%|█████████▉| 562/564 [07:18<00:01,  1.38it/s]
                                                 
{'loss': 1.0517, 'grad_norm': 2.1774810958091484, 'learning_rate': 1.0968921389396708e-08, 'epoch': 2.99}

100%|█████████▉| 562/564 [07:18<00:01,  1.38it/s]
100%|█████████▉| 563/564 [07:18<00:00,  1.39it/s]
                                                 
{'loss': 0.9638, 'grad_norm': 1.9359741499619867, 'learning_rate': 7.312614259597806e-09, 'epoch': 2.99}

100%|█████████▉| 563/564 [07:18<00:00,  1.39it/s]
100%|██████████| 564/564 [07:19<00:00,  1.39it/s]
                                                 
{'loss': 0.9979, 'grad_norm': 1.846372891175323, 'learning_rate': 3.656307129798903e-09, 'epoch': 3.0}

100%|██████████| 564/564 [07:19<00:00,  1.39it/s]
                                                 
{'train_runtime': 448.2352, 'train_samples_per_second': 100.394, 'train_steps_per_second': 1.258, 'train_loss': 1.7933345148538022, 'epoch': 3.0}

100%|██████████| 564/564 [07:28<00:00,  1.39it/s]
100%|██████████| 564/564 [07:28<00:00,  1.26it/s]
[2024-08-13 05:33:35,369] [INFO] [launch.py:351:main] Process 3579006 exits successfully.
[2024-08-13 05:33:36,370] [INFO] [launch.py:351:main] Process 3579004 exits successfully.
[2024-08-13 05:33:36,370] [INFO] [launch.py:351:main] Process 3579002 exits successfully.
[2024-08-13 05:33:36,371] [INFO] [launch.py:351:main] Process 3579003 exits successfully.
[2024-08-13 05:33:36,371] [INFO] [launch.py:351:main] Process 3579005 exits successfully.
[2024-08-13 05:33:37,371] [INFO] [launch.py:351:main] Process 3578999 exits successfully.
[2024-08-13 05:33:37,372] [INFO] [launch.py:351:main] Process 3579000 exits successfully.
[2024-08-13 05:33:37,372] [INFO] [launch.py:351:main] Process 3579001 exits successfully.
