Namespace(mode=['alpaca'], base_model='mistralai/Mistral-7B-v0.3', template_name='barebone', total_bsz=64, epoch=3, lr=2e-05, data_path='', task_name='oasst1', tuned_dir='./cache', use_peft=False, lora_r=16, lora_alpha=32)
num gpus:  8
Running 1/1: deepspeed --num_gpus=8 train.py --deepspeed ../deepspeed_config/zero3.json --bf16 --tf32=True
        --model_name_or_path mistralai/Mistral-7B-v0.3 --data_path ../data/stanford_alpaca/oasst1_data.json
        --output_dir /fsx-project/yunyun/models/mistralai_Mistral-7B-v0.3_oasst1_tuned
        --num_train_epochs 3
        --per_device_train_batch_size 10
        --per_device_eval_batch_size 4
        --gradient_accumulation_steps 1
        --gradient_checkpointing=True
        --evaluation_strategy=no
        --save_strategy=steps
        --save_steps 500
        --save_total_limit 1
        --learning_rate 2e-6
        --weight_decay 0.
        --report_to tensorboard
        --warmup_ratio 0.03
        --lr_scheduler_type=cosine
        --logging_steps 1
        --use_peft False 
        --lora_r 16 --lora_alpha 32
['deepspeed', '--num_gpus=8', 'train.py', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'mistralai/Mistral-7B-v0.3', '--data_path', '../data/stanford_alpaca/oasst1_data.json', '--output_dir', '/fsx-project/yunyun/models/mistralai_Mistral-7B-v0.3_oasst1_tuned', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1', '--use_peft', 'False', '--lora_r', '16', '--lora_alpha', '32']
[2024-08-28 21:02:17,641] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2024-08-28 21:02:25,353] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2024-08-28 21:02:25,353] [INFO] [runner.py:568:main] cmd = /data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNSwgNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None train.py --deepspeed ../deepspeed_config/zero3.json --bf16 --tf32=True --model_name_or_path mistralai/Mistral-7B-v0.3 --data_path ../data/stanford_alpaca/oasst1_data.json --output_dir /fsx-project/yunyun/models/mistralai_Mistral-7B-v0.3_oasst1_tuned --num_train_epochs 3 --per_device_train_batch_size 10 --per_device_eval_batch_size 4 --gradient_accumulation_steps 1 --gradient_checkpointing=True --evaluation_strategy=no --save_strategy=steps --save_steps 500 --save_total_limit 1 --learning_rate 2e-6 --weight_decay 0. --report_to tensorboard --warmup_ratio 0.03 --lr_scheduler_type=cosine --logging_steps 1 --use_peft False --lora_r 16 --lora_alpha 32
[2024-08-28 21:02:27,895] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2024-08-28 21:02:31,286] [INFO] [launch.py:139:main] 0 NCCL_ASYNC_ERROR_HANDLING=1
[2024-08-28 21:02:31,286] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]}
[2024-08-28 21:02:31,286] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=8, node_rank=0
[2024-08-28 21:02:31,286] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]})
[2024-08-28 21:02:31,286] [INFO] [launch.py:164:main] dist_world_size=8
[2024-08-28 21:02:31,286] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
[2024-08-28 21:02:31,287] [INFO] [launch.py:256:main] process 3765063 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=0', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'mistralai/Mistral-7B-v0.3', '--data_path', '../data/stanford_alpaca/oasst1_data.json', '--output_dir', '/fsx-project/yunyun/models/mistralai_Mistral-7B-v0.3_oasst1_tuned', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1', '--use_peft', 'False', '--lora_r', '16', '--lora_alpha', '32']
[2024-08-28 21:02:31,288] [INFO] [launch.py:256:main] process 3765064 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=1', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'mistralai/Mistral-7B-v0.3', '--data_path', '../data/stanford_alpaca/oasst1_data.json', '--output_dir', '/fsx-project/yunyun/models/mistralai_Mistral-7B-v0.3_oasst1_tuned', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1', '--use_peft', 'False', '--lora_r', '16', '--lora_alpha', '32']
[2024-08-28 21:02:31,288] [INFO] [launch.py:256:main] process 3765065 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=2', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'mistralai/Mistral-7B-v0.3', '--data_path', '../data/stanford_alpaca/oasst1_data.json', '--output_dir', '/fsx-project/yunyun/models/mistralai_Mistral-7B-v0.3_oasst1_tuned', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1', '--use_peft', 'False', '--lora_r', '16', '--lora_alpha', '32']
[2024-08-28 21:02:31,289] [INFO] [launch.py:256:main] process 3765066 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=3', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'mistralai/Mistral-7B-v0.3', '--data_path', '../data/stanford_alpaca/oasst1_data.json', '--output_dir', '/fsx-project/yunyun/models/mistralai_Mistral-7B-v0.3_oasst1_tuned', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1', '--use_peft', 'False', '--lora_r', '16', '--lora_alpha', '32']
[2024-08-28 21:02:31,289] [INFO] [launch.py:256:main] process 3765067 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=4', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'mistralai/Mistral-7B-v0.3', '--data_path', '../data/stanford_alpaca/oasst1_data.json', '--output_dir', '/fsx-project/yunyun/models/mistralai_Mistral-7B-v0.3_oasst1_tuned', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1', '--use_peft', 'False', '--lora_r', '16', '--lora_alpha', '32']
[2024-08-28 21:02:31,290] [INFO] [launch.py:256:main] process 3765068 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=5', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'mistralai/Mistral-7B-v0.3', '--data_path', '../data/stanford_alpaca/oasst1_data.json', '--output_dir', '/fsx-project/yunyun/models/mistralai_Mistral-7B-v0.3_oasst1_tuned', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1', '--use_peft', 'False', '--lora_r', '16', '--lora_alpha', '32']
[2024-08-28 21:02:31,290] [INFO] [launch.py:256:main] process 3765069 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=6', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'mistralai/Mistral-7B-v0.3', '--data_path', '../data/stanford_alpaca/oasst1_data.json', '--output_dir', '/fsx-project/yunyun/models/mistralai_Mistral-7B-v0.3_oasst1_tuned', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1', '--use_peft', 'False', '--lora_r', '16', '--lora_alpha', '32']
[2024-08-28 21:02:31,291] [INFO] [launch.py:256:main] process 3765070 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=7', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'mistralai/Mistral-7B-v0.3', '--data_path', '../data/stanford_alpaca/oasst1_data.json', '--output_dir', '/fsx-project/yunyun/models/mistralai_Mistral-7B-v0.3_oasst1_tuned', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1', '--use_peft', 'False', '--lora_r', '16', '--lora_alpha', '32']
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
2024-08-28 21:02:42.851259: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-08-28 21:02:42.851258: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-08-28 21:02:42.851265: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-08-28 21:02:42.851269: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-08-28 21:02:42.851252: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-08-28 21:02:42.851276: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-08-28 21:02:42.851275: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-08-28 21:02:42.851267: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-08-28 21:02:43.044459: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-08-28 21:02:43.044470: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-08-28 21:02:43.044469: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-08-28 21:02:43.044461: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-08-28 21:02:43.044475: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-08-28 21:02:43.044474: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-08-28 21:02:43.044478: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-08-28 21:02:43.044486: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-08-28 21:02:43.100728: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-08-28 21:02:43.100731: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-08-28 21:02:43.100734: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-08-28 21:02:43.100734: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-08-28 21:02:43.100738: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-08-28 21:02:43.100749: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-08-28 21:02:43.100845: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-08-28 21:02:43.100843: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-08-28 21:02:43.506907: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-08-28 21:02:43.506900: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-08-28 21:02:43.506909: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-08-28 21:02:43.506901: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-08-28 21:02:43.506900: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-08-28 21:02:43.506910: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-08-28 21:02:43.506901: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-08-28 21:02:43.506909: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-08-28 21:02:48.459199: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-08-28 21:02:48.459242: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-08-28 21:02:48.459270: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-08-28 21:02:48.459328: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-08-28 21:02:48.459382: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-08-28 21:02:48.459400: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-08-28 21:02:48.459430: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-08-28 21:02:48.459455: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
[2024-08-28 21:03:05,859] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
[2024-08-28 21:03:06,057] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[2024-08-28 21:03:06,077] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-28 21:03:06,092] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2024-08-28 21:03:06,109] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-28 21:03:06,122] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-28 21:03:06,127] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-28 21:03:06,151] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2024-08-28 21:03:06,624] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-08-28 21:03:06,814] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-08-28 21:03:06,835] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-08-28 21:03:06,836] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-08-28 21:03:06,836] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[2024-08-28 21:03:06,847] [INFO] [comm.py:637:init_distributed] cdb=None
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[2024-08-28 21:03:06,859] [INFO] [comm.py:637:init_distributed] cdb=None
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[2024-08-28 21:03:06,879] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-08-28 21:03:06,879] [INFO] [comm.py:637:init_distributed] cdb=None
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 389.07it/s]Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 390.30it/s]Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 394.96it/s]
Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 383.92it/s]
Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 391.94it/s]
Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 394.13it/s]Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 379.29it/s]



Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 373.40it/s]
[2024-08-28 21:03:17,876] [INFO] [partition_parameters.py:345:__exit__] finished initializing model - num_params = 291, num_elems = 7.25B
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–      | 1/3 [00:00<00:01,  1.99it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–      | 1/3 [00:00<00:01,  1.82it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–      | 1/3 [00:00<00:01,  1.81it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–      | 1/3 [00:00<00:01,  1.81it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–      | 1/3 [00:00<00:01,  1.80it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–      | 1/3 [00:00<00:01,  1.78it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–      | 1/3 [00:00<00:01,  1.73it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–      | 1/3 [00:09<00:19,  9.74s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:10<00:06,  6.00s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:10<00:06,  6.02s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:10<00:06,  6.04s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:10<00:06,  6.03s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:10<00:06,  6.03s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:10<00:06,  6.04s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:10<00:06,  6.05s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:18<00:00,  6.94s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:18<00:00,  6.94s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:18<00:00,  6.15s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:18<00:00,  6.14s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:18<00:00,  6.94s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:18<00:00,  6.15s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:18<00:00,  6.94s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:18<00:00,  6.15s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:18<00:00,  6.94s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:18<00:00,  6.15s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:18<00:00,  6.94s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:18<00:00,  6.15s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:18<00:00,  6.95s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:18<00:00,  6.15s/it]
Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 1961.79it/s]
Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 1406.07it/s]
Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 1534.31it/s]
Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 1767.01it/s]
Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 1558.26it/s]
Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 1574.04it/s]
Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 1458.38it/s]
Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:19<00:09,  9.78s/it]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:28<00:00,  9.37s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:28<00:00,  9.48s/it]
Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 1561.16it/s]
[2024-08-28 21:03:46,817] [INFO] [partition_parameters.py:345:__exit__] finished initializing model - num_params = 582, num_elems = 14.50B
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–      | 1/3 [00:02<00:05,  2.77s/it]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–      | 1/3 [00:02<00:05,  2.77s/it]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–      | 1/3 [00:02<00:05,  2.80s/it]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–      | 1/3 [00:02<00:05,  2.80s/it]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–      | 1/3 [00:02<00:05,  2.81s/it]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–      | 1/3 [00:02<00:05,  2.82s/it]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–      | 1/3 [00:02<00:05,  2.82s/it]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–      | 1/3 [00:02<00:05,  2.97s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:05<00:02,  2.90s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:05<00:02,  2.91s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:05<00:02,  2.91s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:05<00:02,  2.91s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:05<00:02,  2.91s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:05<00:02,  2.91s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:05<00:02,  2.93s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:07<00:00,  2.52s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:07<00:00,  2.61s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:07<00:00,  2.52s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:07<00:00,  2.62s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:07<00:00,  2.53s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:07<00:00,  2.62s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:07<00:00,  2.53s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:07<00:00,  2.62s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:07<00:00,  2.54s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:07<00:00,  2.63s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:07<00:00,  2.54s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:07<00:00,  2.63s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:07<00:00,  2.55s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:07<00:00,  2.63s/it]
Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:06<00:03,  3.04s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:08<00:00,  2.96s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:08<00:00,  2.97s/it]
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
[rank0]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[rank2]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[rank3]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[rank1]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[rank7]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...

[rank4]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[rank6]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
[rank5]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121/fused_adam/build.ninja...
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/cpp_extension.py:1967: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Loading extension module fused_adam...
Loading extension module fused_adam...
Loading extension module fused_adam...
Loading extension module fused_adam...Loading extension module fused_adam...

Loading extension module fused_adam...
Loading extension module fused_adam...
Time to load fused_adam op: 3.011066436767578 secondsTime to load fused_adam op: 3.009413957595825 secondsTime to load fused_adam op: 3.0150094032287598 secondsTime to load fused_adam op: 3.0142228603363037 seconds
Time to load fused_adam op: 3.0122103691101074 secondsTime to load fused_adam op: 3.019124984741211 secondsTime to load fused_adam op: 3.0189523696899414 secondsTime to load fused_adam op: 3.014711380004883 seconds






Parameter Offload: Total persistent parameters: 268701696 in 129 params
  0%|          | 0/189 [00:00<?, ?it/s]/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
  1%|          | 1/189 [00:07<24:35,  7.85s/it]                                               {'loss': 1.1112, 'grad_norm': 14.504425136552069, 'learning_rate': 0.0, 'epoch': 0.02}
  1%|          | 1/189 [00:07<24:35,  7.85s/it]  1%|          | 2/189 [00:08<12:09,  3.90s/it]                                               {'loss': 1.0079, 'grad_norm': 13.452383457293204, 'learning_rate': 7.73705614469083e-07, 'epoch': 0.03}
  1%|          | 2/189 [00:08<12:09,  3.90s/it]  2%|â–         | 3/189 [00:09<07:51,  2.53s/it]                                               {'loss': 1.012, 'grad_norm': 13.567587250622614, 'learning_rate': 1.2262943855309167e-06, 'epoch': 0.05}
  2%|â–         | 3/189 [00:09<07:51,  2.53s/it]  2%|â–         | 4/189 [00:10<05:49,  1.89s/it]                                               {'loss': 0.949, 'grad_norm': 8.332824474763923, 'learning_rate': 1.547411228938166e-06, 'epoch': 0.06}
  2%|â–         | 4/189 [00:10<05:49,  1.89s/it]  3%|â–         | 5/189 [00:11<04:42,  1.53s/it]                                               {'loss': 0.9467, 'grad_norm': 9.254517623854877, 'learning_rate': 1.796488803407854e-06, 'epoch': 0.08}
  3%|â–         | 5/189 [00:11<04:42,  1.53s/it]  3%|â–         | 6/189 [00:12<04:00,  1.32s/it]                                               {'loss': 1.0073, 'grad_norm': 8.825450152001062, 'learning_rate': 1.9999999999999995e-06, 'epoch': 0.1}
  3%|â–         | 6/189 [00:12<04:00,  1.32s/it]  4%|â–         | 7/189 [00:13<03:34,  1.18s/it]                                               {'loss': 0.9236, 'grad_norm': 9.039229427807282, 'learning_rate': 2e-06, 'epoch': 0.11}
  4%|â–         | 7/189 [00:13<03:34,  1.18s/it]  4%|â–         | 8/189 [00:14<03:17,  1.09s/it]                                               {'loss': 0.9612, 'grad_norm': 8.389193871595422, 'learning_rate': 1.989071038251366e-06, 'epoch': 0.13}
  4%|â–         | 8/189 [00:14<03:17,  1.09s/it]  5%|â–         | 9/189 [00:15<03:05,  1.03s/it]                                               {'loss': 0.9493, 'grad_norm': 9.150678407598331, 'learning_rate': 1.978142076502732e-06, 'epoch': 0.14}
  5%|â–         | 9/189 [00:15<03:05,  1.03s/it]  5%|â–Œ         | 10/189 [00:16<02:56,  1.02it/s]                                                {'loss': 0.9178, 'grad_norm': 8.391263462989302, 'learning_rate': 1.967213114754098e-06, 'epoch': 0.16}
  5%|â–Œ         | 10/189 [00:16<02:56,  1.02it/s]  6%|â–Œ         | 11/189 [00:17<02:50,  1.05it/s]                                                {'loss': 0.9255, 'grad_norm': 8.394396054952553, 'learning_rate': 1.9562841530054644e-06, 'epoch': 0.17}
  6%|â–Œ         | 11/189 [00:17<02:50,  1.05it/s]  6%|â–‹         | 12/189 [00:17<02:45,  1.07it/s]                                                {'loss': 0.9754, 'grad_norm': 7.664684592492169, 'learning_rate': 1.9453551912568304e-06, 'epoch': 0.19}
  6%|â–‹         | 12/189 [00:17<02:45,  1.07it/s]  7%|â–‹         | 13/189 [00:18<02:42,  1.09it/s]                                                {'loss': 0.9312, 'grad_norm': 8.344623190388532, 'learning_rate': 1.9344262295081967e-06, 'epoch': 0.21}
  7%|â–‹         | 13/189 [00:18<02:42,  1.09it/s]  7%|â–‹         | 14/189 [00:19<02:39,  1.10it/s]                                                {'loss': 0.8811, 'grad_norm': 8.108579732521097, 'learning_rate': 1.9234972677595626e-06, 'epoch': 0.22}
  7%|â–‹         | 14/189 [00:19<02:39,  1.10it/s]  8%|â–Š         | 15/189 [00:20<02:36,  1.11it/s]                                                {'loss': 0.9109, 'grad_norm': 7.655669130761758, 'learning_rate': 1.912568306010929e-06, 'epoch': 0.24}
  8%|â–Š         | 15/189 [00:20<02:36,  1.11it/s]  8%|â–Š         | 16/189 [00:21<02:35,  1.11it/s]                                                {'loss': 0.8934, 'grad_norm': 8.694664598209018, 'learning_rate': 1.901639344262295e-06, 'epoch': 0.25}
  8%|â–Š         | 16/189 [00:21<02:35,  1.11it/s]  9%|â–‰         | 17/189 [00:22<02:34,  1.12it/s]                                                {'loss': 0.943, 'grad_norm': 7.425789898791041, 'learning_rate': 1.8907103825136612e-06, 'epoch': 0.27}
  9%|â–‰         | 17/189 [00:22<02:34,  1.12it/s] 10%|â–‰         | 18/189 [00:23<02:33,  1.12it/s]                                                {'loss': 1.0146, 'grad_norm': 7.816952848643976, 'learning_rate': 1.8797814207650274e-06, 'epoch': 0.29}
 10%|â–‰         | 18/189 [00:23<02:33,  1.12it/s] 10%|â–ˆ         | 19/189 [00:24<02:32,  1.12it/s]                                                {'loss': 0.9439, 'grad_norm': 8.094405057035623, 'learning_rate': 1.8688524590163935e-06, 'epoch': 0.3}
 10%|â–ˆ         | 19/189 [00:24<02:32,  1.12it/s] 11%|â–ˆ         | 20/189 [00:25<02:31,  1.12it/s]                                                {'loss': 0.9194, 'grad_norm': 9.6805655893265, 'learning_rate': 1.8579234972677596e-06, 'epoch': 0.32}
 11%|â–ˆ         | 20/189 [00:25<02:31,  1.12it/s] 11%|â–ˆ         | 21/189 [00:25<02:30,  1.12it/s]                                                {'loss': 0.8064, 'grad_norm': 7.113737494549356, 'learning_rate': 1.8469945355191256e-06, 'epoch': 0.33}
 11%|â–ˆ         | 21/189 [00:25<02:30,  1.12it/s] 12%|â–ˆâ–        | 22/189 [00:26<02:30,  1.11it/s]                                                {'loss': 0.9616, 'grad_norm': 9.376836541170524, 'learning_rate': 1.8360655737704917e-06, 'epoch': 0.35}
 12%|â–ˆâ–        | 22/189 [00:26<02:30,  1.11it/s] 12%|â–ˆâ–        | 23/189 [00:27<02:29,  1.11it/s]                                                {'loss': 0.8142, 'grad_norm': 9.079465363546483, 'learning_rate': 1.8251366120218578e-06, 'epoch': 0.37}
 12%|â–ˆâ–        | 23/189 [00:27<02:29,  1.11it/s] 13%|â–ˆâ–        | 24/189 [00:28<02:27,  1.12it/s]                                                {'loss': 0.7909, 'grad_norm': 8.281276779174371, 'learning_rate': 1.814207650273224e-06, 'epoch': 0.38}
 13%|â–ˆâ–        | 24/189 [00:28<02:27,  1.12it/s] 13%|â–ˆâ–        | 25/189 [00:29<02:27,  1.11it/s]                                                {'loss': 0.8927, 'grad_norm': 7.480119507545526, 'learning_rate': 1.80327868852459e-06, 'epoch': 0.4}
 13%|â–ˆâ–        | 25/189 [00:29<02:27,  1.11it/s] 14%|â–ˆâ–        | 26/189 [00:30<02:25,  1.12it/s]                                                {'loss': 0.8558, 'grad_norm': 8.849784932679665, 'learning_rate': 1.7923497267759562e-06, 'epoch': 0.41}
 14%|â–ˆâ–        | 26/189 [00:30<02:25,  1.12it/s] 14%|â–ˆâ–        | 27/189 [00:31<02:25,  1.11it/s]                                                {'loss': 0.7563, 'grad_norm': 7.602167352451258, 'learning_rate': 1.7814207650273224e-06, 'epoch': 0.43}
 14%|â–ˆâ–        | 27/189 [00:31<02:25,  1.11it/s] 15%|â–ˆâ–        | 28/189 [00:32<02:24,  1.11it/s]                                                {'loss': 0.8019, 'grad_norm': 7.061582400675004, 'learning_rate': 1.7704918032786885e-06, 'epoch': 0.44}
 15%|â–ˆâ–        | 28/189 [00:32<02:24,  1.11it/s] 15%|â–ˆâ–Œ        | 29/189 [00:33<02:23,  1.12it/s]                                                {'loss': 0.7142, 'grad_norm': 7.996830357742688, 'learning_rate': 1.7595628415300544e-06, 'epoch': 0.46}
 15%|â–ˆâ–Œ        | 29/189 [00:33<02:23,  1.12it/s] 16%|â–ˆâ–Œ        | 30/189 [00:34<02:23,  1.11it/s]                                                {'loss': 0.9478, 'grad_norm': 9.094899688214122, 'learning_rate': 1.7486338797814206e-06, 'epoch': 0.48}
 16%|â–ˆâ–Œ        | 30/189 [00:34<02:23,  1.11it/s] 16%|â–ˆâ–‹        | 31/189 [00:34<02:22,  1.11it/s]                                                {'loss': 0.8036, 'grad_norm': 9.48088068312298, 'learning_rate': 1.7377049180327867e-06, 'epoch': 0.49}
 16%|â–ˆâ–‹        | 31/189 [00:34<02:22,  1.11it/s] 17%|â–ˆâ–‹        | 32/189 [00:35<02:20,  1.11it/s]                                                {'loss': 0.6776, 'grad_norm': 7.1696127406814085, 'learning_rate': 1.7267759562841528e-06, 'epoch': 0.51}
 17%|â–ˆâ–‹        | 32/189 [00:35<02:20,  1.11it/s] 17%|â–ˆâ–‹        | 33/189 [00:36<02:20,  1.11it/s]                                                {'loss': 0.7496, 'grad_norm': 8.351434396575014, 'learning_rate': 1.715846994535519e-06, 'epoch': 0.52}
 17%|â–ˆâ–‹        | 33/189 [00:36<02:20,  1.11it/s] 18%|â–ˆâ–Š        | 34/189 [00:37<02:19,  1.11it/s]                                                {'loss': 0.6003, 'grad_norm': 6.920034888664039, 'learning_rate': 1.704918032786885e-06, 'epoch': 0.54}
 18%|â–ˆâ–Š        | 34/189 [00:37<02:19,  1.11it/s] 19%|â–ˆâ–Š        | 35/189 [00:38<02:19,  1.10it/s]                                                {'loss': 0.7129, 'grad_norm': 8.31175548915154, 'learning_rate': 1.6939890710382514e-06, 'epoch': 0.56}
 19%|â–ˆâ–Š        | 35/189 [00:38<02:19,  1.10it/s] 19%|â–ˆâ–‰        | 36/189 [00:39<02:18,  1.11it/s]                                                {'loss': 0.6728, 'grad_norm': 7.612059168024281, 'learning_rate': 1.6830601092896176e-06, 'epoch': 0.57}
 19%|â–ˆâ–‰        | 36/189 [00:39<02:18,  1.11it/s] 20%|â–ˆâ–‰        | 37/189 [00:40<02:17,  1.11it/s]                                                {'loss': 0.7676, 'grad_norm': 8.22596845055222, 'learning_rate': 1.6721311475409837e-06, 'epoch': 0.59}
 20%|â–ˆâ–‰        | 37/189 [00:40<02:17,  1.11it/s] 20%|â–ˆâ–ˆ        | 38/189 [00:41<02:15,  1.11it/s]                                                {'loss': 0.7629, 'grad_norm': 7.8401917445614995, 'learning_rate': 1.6612021857923496e-06, 'epoch': 0.6}
 20%|â–ˆâ–ˆ        | 38/189 [00:41<02:15,  1.11it/s] 21%|â–ˆâ–ˆ        | 39/189 [00:42<02:14,  1.11it/s]                                                {'loss': 0.7411, 'grad_norm': 8.26673033292574, 'learning_rate': 1.6502732240437158e-06, 'epoch': 0.62}
 21%|â–ˆâ–ˆ        | 39/189 [00:42<02:14,  1.11it/s] 21%|â–ˆâ–ˆ        | 40/189 [00:43<02:13,  1.11it/s]                                                {'loss': 0.7353, 'grad_norm': 7.8624577193705845, 'learning_rate': 1.6393442622950819e-06, 'epoch': 0.63}
 21%|â–ˆâ–ˆ        | 40/189 [00:43<02:13,  1.11it/s] 22%|â–ˆâ–ˆâ–       | 41/189 [00:43<02:12,  1.11it/s]                                                {'loss': 0.7094, 'grad_norm': 8.292399506428428, 'learning_rate': 1.628415300546448e-06, 'epoch': 0.65}
 22%|â–ˆâ–ˆâ–       | 41/189 [00:43<02:12,  1.11it/s] 22%|â–ˆâ–ˆâ–       | 42/189 [00:44<02:12,  1.11it/s]                                                {'loss': 0.6233, 'grad_norm': 8.195563364974522, 'learning_rate': 1.6174863387978142e-06, 'epoch': 0.67}
 22%|â–ˆâ–ˆâ–       | 42/189 [00:44<02:12,  1.11it/s] 23%|â–ˆâ–ˆâ–       | 43/189 [00:45<02:11,  1.11it/s]                                                {'loss': 0.6492, 'grad_norm': 7.358844450022619, 'learning_rate': 1.6065573770491803e-06, 'epoch': 0.68}
 23%|â–ˆâ–ˆâ–       | 43/189 [00:45<02:11,  1.11it/s] 23%|â–ˆâ–ˆâ–       | 44/189 [00:46<02:10,  1.11it/s]                                                {'loss': 0.7491, 'grad_norm': 9.230283828251626, 'learning_rate': 1.5956284153005464e-06, 'epoch': 0.7}
 23%|â–ˆâ–ˆâ–       | 44/189 [00:46<02:10,  1.11it/s] 24%|â–ˆâ–ˆâ–       | 45/189 [00:47<02:09,  1.11it/s]                                                {'loss': 0.6992, 'grad_norm': 8.99322783466418, 'learning_rate': 1.5846994535519126e-06, 'epoch': 0.71}
 24%|â–ˆâ–ˆâ–       | 45/189 [00:47<02:09,  1.11it/s] 24%|â–ˆâ–ˆâ–       | 46/189 [00:48<02:09,  1.11it/s]                                                {'loss': 0.8531, 'grad_norm': 9.005524014061143, 'learning_rate': 1.5737704918032787e-06, 'epoch': 0.73}
 24%|â–ˆâ–ˆâ–       | 46/189 [00:48<02:09,  1.11it/s] 25%|â–ˆâ–ˆâ–       | 47/189 [00:49<02:08,  1.10it/s]                                                {'loss': 0.651, 'grad_norm': 7.856751156594149, 'learning_rate': 1.5628415300546446e-06, 'epoch': 0.75}
 25%|â–ˆâ–ˆâ–       | 47/189 [00:49<02:08,  1.10it/s] 25%|â–ˆâ–ˆâ–Œ       | 48/189 [00:50<02:07,  1.11it/s]                                                {'loss': 0.639, 'grad_norm': 8.072922658621938, 'learning_rate': 1.5519125683060107e-06, 'epoch': 0.76}
 25%|â–ˆâ–ˆâ–Œ       | 48/189 [00:50<02:07,  1.11it/s] 26%|â–ˆâ–ˆâ–Œ       | 49/189 [00:51<02:06,  1.11it/s]                                                {'loss': 0.7622, 'grad_norm': 7.859002067070117, 'learning_rate': 1.5409836065573769e-06, 'epoch': 0.78}
 26%|â–ˆâ–ˆâ–Œ       | 49/189 [00:51<02:06,  1.11it/s] 26%|â–ˆâ–ˆâ–‹       | 50/189 [00:52<02:05,  1.11it/s]                                                {'loss': 0.6387, 'grad_norm': 7.7814274022981795, 'learning_rate': 1.530054644808743e-06, 'epoch': 0.79}
 26%|â–ˆâ–ˆâ–‹       | 50/189 [00:52<02:05,  1.11it/s] 27%|â–ˆâ–ˆâ–‹       | 51/189 [00:52<02:04,  1.11it/s]                                                {'loss': 0.6744, 'grad_norm': 8.096248746953535, 'learning_rate': 1.5191256830601091e-06, 'epoch': 0.81}
 27%|â–ˆâ–ˆâ–‹       | 51/189 [00:52<02:04,  1.11it/s] 28%|â–ˆâ–ˆâ–Š       | 52/189 [00:53<02:03,  1.11it/s]                                                {'loss': 0.6858, 'grad_norm': 8.890525897960158, 'learning_rate': 1.5081967213114753e-06, 'epoch': 0.83}
 28%|â–ˆâ–ˆâ–Š       | 52/189 [00:53<02:03,  1.11it/s] 28%|â–ˆâ–ˆâ–Š       | 53/189 [00:54<02:03,  1.11it/s]                                                {'loss': 0.6746, 'grad_norm': 7.971078084449339, 'learning_rate': 1.4972677595628416e-06, 'epoch': 0.84}
 28%|â–ˆâ–ˆâ–Š       | 53/189 [00:54<02:03,  1.11it/s] 29%|â–ˆâ–ˆâ–Š       | 54/189 [00:55<02:01,  1.11it/s]                                                {'loss': 0.68, 'grad_norm': 7.347696272918509, 'learning_rate': 1.4863387978142078e-06, 'epoch': 0.86}
 29%|â–ˆâ–ˆâ–Š       | 54/189 [00:55<02:01,  1.11it/s] 29%|â–ˆâ–ˆâ–‰       | 55/189 [00:56<02:00,  1.11it/s]                                                {'loss': 0.5907, 'grad_norm': 8.10654839226375, 'learning_rate': 1.4754098360655739e-06, 'epoch': 0.87}
 29%|â–ˆâ–ˆâ–‰       | 55/189 [00:56<02:00,  1.11it/s] 30%|â–ˆâ–ˆâ–‰       | 56/189 [00:57<01:59,  1.11it/s]                                                {'loss': 0.6105, 'grad_norm': 8.683826913365719, 'learning_rate': 1.4644808743169398e-06, 'epoch': 0.89}
 30%|â–ˆâ–ˆâ–‰       | 56/189 [00:57<01:59,  1.11it/s] 30%|â–ˆâ–ˆâ–ˆ       | 57/189 [00:58<01:58,  1.11it/s]                                                {'loss': 0.58, 'grad_norm': 8.39568009642311, 'learning_rate': 1.453551912568306e-06, 'epoch': 0.9}
 30%|â–ˆâ–ˆâ–ˆ       | 57/189 [00:58<01:58,  1.11it/s] 31%|â–ˆâ–ˆâ–ˆ       | 58/189 [00:59<01:58,  1.11it/s]                                                {'loss': 0.6187, 'grad_norm': 8.426839360243646, 'learning_rate': 1.442622950819672e-06, 'epoch': 0.92}
 31%|â–ˆâ–ˆâ–ˆ       | 58/189 [00:59<01:58,  1.11it/s] 31%|â–ˆâ–ˆâ–ˆ       | 59/189 [01:00<01:57,  1.11it/s]                                                {'loss': 0.7197, 'grad_norm': 8.678328279269772, 'learning_rate': 1.4316939890710382e-06, 'epoch': 0.94}
 31%|â–ˆâ–ˆâ–ˆ       | 59/189 [01:00<01:57,  1.11it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 60/189 [01:01<01:56,  1.11it/s]                                                {'loss': 0.5391, 'grad_norm': 8.382796097636295, 'learning_rate': 1.4207650273224043e-06, 'epoch': 0.95}
 32%|â–ˆâ–ˆâ–ˆâ–      | 60/189 [01:01<01:56,  1.11it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 61/189 [01:02<01:55,  1.11it/s]                                                {'loss': 0.6639, 'grad_norm': 8.543518167388303, 'learning_rate': 1.4098360655737705e-06, 'epoch': 0.97}
 32%|â–ˆâ–ˆâ–ˆâ–      | 61/189 [01:02<01:55,  1.11it/s] 33%|â–ˆâ–ˆâ–ˆâ–      | 62/189 [01:02<01:54,  1.11it/s]                                                {'loss': 0.5435, 'grad_norm': 7.633505685018854, 'learning_rate': 1.3989071038251366e-06, 'epoch': 0.98}
 33%|â–ˆâ–ˆâ–ˆâ–      | 62/189 [01:02<01:54,  1.11it/s] 33%|â–ˆâ–ˆâ–ˆâ–      | 63/189 [01:03<01:53,  1.11it/s]                                                {'loss': 0.4773, 'grad_norm': 9.988717629925999, 'learning_rate': 1.3879781420765027e-06, 'epoch': 1.0}
 33%|â–ˆâ–ˆâ–ˆâ–      | 63/189 [01:03<01:53,  1.11it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 64/189 [01:04<01:52,  1.11it/s]                                                {'loss': 0.3341, 'grad_norm': 6.853731194239042, 'learning_rate': 1.3770491803278687e-06, 'epoch': 1.02}
 34%|â–ˆâ–ˆâ–ˆâ–      | 64/189 [01:04<01:52,  1.11it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 65/189 [01:05<01:55,  1.08it/s]                                                {'loss': 0.3309, 'grad_norm': 7.622634241869209, 'learning_rate': 1.3661202185792348e-06, 'epoch': 1.03}
 34%|â–ˆâ–ˆâ–ˆâ–      | 65/189 [01:05<01:55,  1.08it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 66/189 [01:06<01:53,  1.09it/s]                                                {'loss': 0.3493, 'grad_norm': 7.7668494476519, 'learning_rate': 1.355191256830601e-06, 'epoch': 1.05}
 35%|â–ˆâ–ˆâ–ˆâ–      | 66/189 [01:06<01:53,  1.09it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 67/189 [01:07<01:51,  1.09it/s]                                                {'loss': 0.3068, 'grad_norm': 8.756164675182657, 'learning_rate': 1.344262295081967e-06, 'epoch': 1.06}
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 67/189 [01:07<01:51,  1.09it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 68/189 [01:08<01:50,  1.10it/s]                                                {'loss': 0.375, 'grad_norm': 8.037646482003833, 'learning_rate': 1.3333333333333332e-06, 'epoch': 1.08}
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 68/189 [01:08<01:50,  1.10it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 69/189 [01:09<01:48,  1.10it/s]                                                {'loss': 0.3565, 'grad_norm': 7.90651166447845, 'learning_rate': 1.3224043715846993e-06, 'epoch': 1.1}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 69/189 [01:09<01:48,  1.10it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 70/189 [01:10<01:47,  1.10it/s]                                                {'loss': 0.3583, 'grad_norm': 8.794562704003507, 'learning_rate': 1.3114754098360655e-06, 'epoch': 1.11}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 70/189 [01:10<01:47,  1.10it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 71/189 [01:11<01:46,  1.11it/s]                                                {'loss': 0.3215, 'grad_norm': 8.552491283778673, 'learning_rate': 1.3005464480874316e-06, 'epoch': 1.13}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 71/189 [01:11<01:46,  1.11it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 72/189 [01:12<01:45,  1.11it/s]                                                {'loss': 0.4395, 'grad_norm': 7.926809576178946, 'learning_rate': 1.289617486338798e-06, 'epoch': 1.14}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 72/189 [01:12<01:45,  1.11it/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 73/189 [01:12<01:44,  1.11it/s]                                                {'loss': 0.3337, 'grad_norm': 8.079507316102411, 'learning_rate': 1.2786885245901639e-06, 'epoch': 1.16}
 39%|â–ˆâ–ˆâ–ˆâ–Š      | 73/189 [01:12<01:44,  1.11it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 74/189 [01:13<01:43,  1.11it/s]                                                {'loss': 0.3395, 'grad_norm': 7.587623892703289, 'learning_rate': 1.26775956284153e-06, 'epoch': 1.17}
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 74/189 [01:13<01:43,  1.11it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 75/189 [01:14<01:42,  1.11it/s]                                                {'loss': 0.3732, 'grad_norm': 10.42563151898881, 'learning_rate': 1.2568306010928961e-06, 'epoch': 1.19}
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 75/189 [01:14<01:42,  1.11it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 76/189 [01:15<01:42,  1.11it/s]                                                {'loss': 0.2898, 'grad_norm': 7.10520834779176, 'learning_rate': 1.2459016393442623e-06, 'epoch': 1.21}
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 76/189 [01:15<01:42,  1.11it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 77/189 [01:16<01:41,  1.11it/s]                                                {'loss': 0.2755, 'grad_norm': 7.74305930786967, 'learning_rate': 1.2349726775956284e-06, 'epoch': 1.22}
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 77/189 [01:16<01:41,  1.11it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 78/189 [01:17<01:39,  1.11it/s]                                                {'loss': 0.2692, 'grad_norm': 6.9252165035578646, 'learning_rate': 1.2240437158469945e-06, 'epoch': 1.24}
 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 78/189 [01:17<01:39,  1.11it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 79/189 [01:18<01:39,  1.11it/s]                                                {'loss': 0.3238, 'grad_norm': 7.760629162946424, 'learning_rate': 1.2131147540983607e-06, 'epoch': 1.25}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 79/189 [01:18<01:39,  1.11it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 80/189 [01:19<01:38,  1.11it/s]                                                {'loss': 0.3022, 'grad_norm': 9.010166511494319, 'learning_rate': 1.2021857923497268e-06, 'epoch': 1.27}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 80/189 [01:19<01:38,  1.11it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 81/189 [01:20<01:37,  1.11it/s]                                                {'loss': 0.3266, 'grad_norm': 8.749371815862785, 'learning_rate': 1.191256830601093e-06, 'epoch': 1.29}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 81/189 [01:20<01:37,  1.11it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 82/189 [01:21<01:36,  1.11it/s]                                                {'loss': 0.3355, 'grad_norm': 8.118216040797291, 'learning_rate': 1.1803278688524589e-06, 'epoch': 1.3}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 82/189 [01:21<01:36,  1.11it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 83/189 [01:21<01:35,  1.11it/s]                                                {'loss': 0.3141, 'grad_norm': 6.701487755477223, 'learning_rate': 1.169398907103825e-06, 'epoch': 1.32}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 83/189 [01:21<01:35,  1.11it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 84/189 [01:22<01:34,  1.11it/s]                                                {'loss': 0.2914, 'grad_norm': 9.405860749279746, 'learning_rate': 1.1584699453551911e-06, 'epoch': 1.33}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 84/189 [01:22<01:34,  1.11it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 85/189 [01:23<01:33,  1.11it/s]                                                {'loss': 0.3981, 'grad_norm': 7.707827146402779, 'learning_rate': 1.1475409836065573e-06, 'epoch': 1.35}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 85/189 [01:23<01:33,  1.11it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 86/189 [01:24<01:33,  1.11it/s]                                                {'loss': 0.2433, 'grad_norm': 6.861955607748236, 'learning_rate': 1.1366120218579234e-06, 'epoch': 1.37}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 86/189 [01:24<01:33,  1.11it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 87/189 [01:25<01:32,  1.11it/s]                                                {'loss': 0.2936, 'grad_norm': 7.2186641446067465, 'learning_rate': 1.1256830601092895e-06, 'epoch': 1.38}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 87/189 [01:25<01:32,  1.11it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 88/189 [01:26<01:31,  1.11it/s]                                                {'loss': 0.194, 'grad_norm': 6.946252128239447, 'learning_rate': 1.1147540983606557e-06, 'epoch': 1.4}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 88/189 [01:26<01:31,  1.11it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 89/189 [01:27<01:29,  1.11it/s]                                                {'loss': 0.3622, 'grad_norm': 7.849833232108047, 'learning_rate': 1.1038251366120218e-06, 'epoch': 1.41}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 89/189 [01:27<01:29,  1.11it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 90/189 [01:28<01:28,  1.11it/s]                                                {'loss': 0.2878, 'grad_norm': 6.856550460162274, 'learning_rate': 1.092896174863388e-06, 'epoch': 1.43}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 90/189 [01:28<01:28,  1.11it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 91/189 [01:29<01:27,  1.11it/s]                                                {'loss': 0.3045, 'grad_norm': 8.010345897107797, 'learning_rate': 1.081967213114754e-06, 'epoch': 1.44}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 91/189 [01:29<01:27,  1.11it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 92/189 [01:30<01:26,  1.12it/s]                                                {'loss': 0.2684, 'grad_norm': 7.71670159936545, 'learning_rate': 1.0710382513661202e-06, 'epoch': 1.46}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 92/189 [01:30<01:26,  1.12it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 93/189 [01:30<01:25,  1.12it/s]                                                {'loss': 0.4233, 'grad_norm': 9.170663241010246, 'learning_rate': 1.0601092896174863e-06, 'epoch': 1.48}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 93/189 [01:30<01:25,  1.12it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 94/189 [01:31<01:24,  1.12it/s]                                                {'loss': 0.2298, 'grad_norm': 7.393146726461782, 'learning_rate': 1.0491803278688525e-06, 'epoch': 1.49}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 94/189 [01:31<01:24,  1.12it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 95/189 [01:32<01:23,  1.12it/s]                                                {'loss': 0.1879, 'grad_norm': 6.8749767706704, 'learning_rate': 1.0382513661202186e-06, 'epoch': 1.51}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 95/189 [01:32<01:23,  1.12it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 96/189 [01:33<01:23,  1.12it/s]                                                {'loss': 0.3059, 'grad_norm': 7.21063639156565, 'learning_rate': 1.0273224043715847e-06, 'epoch': 1.52}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 96/189 [01:33<01:23,  1.12it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 97/189 [01:34<01:21,  1.12it/s]                                                {'loss': 0.2486, 'grad_norm': 8.90951249049427, 'learning_rate': 1.0163934426229509e-06, 'epoch': 1.54}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 97/189 [01:34<01:21,  1.12it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 98/189 [01:35<01:21,  1.11it/s]                                                {'loss': 0.1764, 'grad_norm': 6.962123541084211, 'learning_rate': 1.005464480874317e-06, 'epoch': 1.56}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 98/189 [01:35<01:21,  1.11it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 99/189 [01:36<01:20,  1.11it/s]                                                {'loss': 0.3278, 'grad_norm': 7.8554040560238665, 'learning_rate': 9.94535519125683e-07, 'epoch': 1.57}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 99/189 [01:36<01:20,  1.11it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 100/189 [01:37<01:19,  1.12it/s]                                                 {'loss': 0.2941, 'grad_norm': 8.73599894516902, 'learning_rate': 9.83606557377049e-07, 'epoch': 1.59}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 100/189 [01:37<01:19,  1.12it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 101/189 [01:38<01:19,  1.11it/s]                                                 {'loss': 0.2241, 'grad_norm': 6.084917096859545, 'learning_rate': 9.726775956284152e-07, 'epoch': 1.6}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 101/189 [01:38<01:19,  1.11it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 102/189 [01:38<01:17,  1.12it/s]                                                 {'loss': 0.2416, 'grad_norm': 6.661807003534388, 'learning_rate': 9.617486338797813e-07, 'epoch': 1.62}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 102/189 [01:38<01:17,  1.12it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 103/189 [01:39<01:17,  1.11it/s]                                                 {'loss': 0.2213, 'grad_norm': 7.90786906583195, 'learning_rate': 9.508196721311474e-07, 'epoch': 1.63}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 103/189 [01:39<01:17,  1.11it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 104/189 [01:40<01:16,  1.11it/s]                                                 {'loss': 0.2387, 'grad_norm': 9.865104064536254, 'learning_rate': 9.398907103825137e-07, 'epoch': 1.65}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 104/189 [01:40<01:16,  1.11it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 105/189 [01:41<01:15,  1.11it/s]                                                 {'loss': 0.2412, 'grad_norm': 7.322012902640134, 'learning_rate': 9.289617486338798e-07, 'epoch': 1.67}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 105/189 [01:41<01:15,  1.11it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 106/189 [01:42<01:14,  1.11it/s]                                                 {'loss': 0.2847, 'grad_norm': 9.980350897327826, 'learning_rate': 9.180327868852458e-07, 'epoch': 1.68}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 106/189 [01:42<01:14,  1.11it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 107/189 [01:43<01:13,  1.11it/s]                                                 {'loss': 0.2504, 'grad_norm': 7.5852500211139375, 'learning_rate': 9.07103825136612e-07, 'epoch': 1.7}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 107/189 [01:43<01:13,  1.11it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 108/189 [01:44<01:12,  1.12it/s]                                                 {'loss': 0.2654, 'grad_norm': 7.728915563105364, 'learning_rate': 8.961748633879781e-07, 'epoch': 1.71}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 108/189 [01:44<01:12,  1.12it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 109/189 [01:45<01:11,  1.12it/s]                                                 {'loss': 0.2521, 'grad_norm': 7.589472205793682, 'learning_rate': 8.852459016393443e-07, 'epoch': 1.73}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 109/189 [01:45<01:11,  1.12it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 110/189 [01:46<01:10,  1.11it/s]                                                 {'loss': 0.2529, 'grad_norm': 7.9102054559822355, 'learning_rate': 8.743169398907103e-07, 'epoch': 1.75}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 110/189 [01:46<01:10,  1.11it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 111/189 [01:47<01:10,  1.11it/s]                                                 {'loss': 0.2566, 'grad_norm': 9.516862982941513, 'learning_rate': 8.633879781420764e-07, 'epoch': 1.76}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 111/189 [01:47<01:10,  1.11it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 112/189 [01:47<01:09,  1.11it/s]                                                 {'loss': 0.2324, 'grad_norm': 7.4634564631392255, 'learning_rate': 8.524590163934425e-07, 'epoch': 1.78}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 112/189 [01:47<01:09,  1.11it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 113/189 [01:48<01:08,  1.12it/s]                                                 {'loss': 0.1903, 'grad_norm': 6.849152360559806, 'learning_rate': 8.415300546448088e-07, 'epoch': 1.79}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 113/189 [01:48<01:08,  1.12it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 114/189 [01:49<01:07,  1.12it/s]                                                 {'loss': 0.3261, 'grad_norm': 8.061831014215409, 'learning_rate': 8.306010928961748e-07, 'epoch': 1.81}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 114/189 [01:49<01:07,  1.12it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 115/189 [01:50<01:06,  1.12it/s]                                                 {'loss': 0.274, 'grad_norm': 6.914270674641748, 'learning_rate': 8.196721311475409e-07, 'epoch': 1.83}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 115/189 [01:50<01:06,  1.12it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 116/189 [01:51<01:05,  1.11it/s]                                                 {'loss': 0.1865, 'grad_norm': 6.598850687136647, 'learning_rate': 8.087431693989071e-07, 'epoch': 1.84}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 116/189 [01:51<01:05,  1.11it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 117/189 [01:52<01:04,  1.12it/s]                                                 {'loss': 0.2924, 'grad_norm': 9.347456815283012, 'learning_rate': 7.978142076502732e-07, 'epoch': 1.86}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 117/189 [01:52<01:04,  1.12it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 118/189 [01:53<01:03,  1.11it/s]                                                 {'loss': 0.2396, 'grad_norm': 8.29817300968125, 'learning_rate': 7.868852459016393e-07, 'epoch': 1.87}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 118/189 [01:53<01:03,  1.11it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 119/189 [01:54<01:02,  1.11it/s]                                                 {'loss': 0.2321, 'grad_norm': 8.606697850032878, 'learning_rate': 7.759562841530054e-07, 'epoch': 1.89}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 119/189 [01:54<01:02,  1.11it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 120/189 [01:55<01:01,  1.11it/s]                                                 {'loss': 0.1932, 'grad_norm': 7.280900244084597, 'learning_rate': 7.650273224043715e-07, 'epoch': 1.9}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 120/189 [01:55<01:01,  1.11it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 121/189 [01:56<01:00,  1.12it/s]                                                 {'loss': 0.2464, 'grad_norm': 7.862793601442654, 'learning_rate': 7.540983606557376e-07, 'epoch': 1.92}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 121/189 [01:56<01:00,  1.12it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 122/189 [01:56<01:00,  1.12it/s]                                                 {'loss': 0.1972, 'grad_norm': 6.070229091414754, 'learning_rate': 7.431693989071039e-07, 'epoch': 1.94}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 122/189 [01:56<01:00,  1.12it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 123/189 [01:57<00:59,  1.11it/s]                                                 {'loss': 0.1846, 'grad_norm': 6.733241439716965, 'learning_rate': 7.322404371584699e-07, 'epoch': 1.95}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 123/189 [01:57<00:59,  1.11it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 124/189 [01:58<00:58,  1.11it/s]                                                 {'loss': 0.1711, 'grad_norm': 6.316885302010198, 'learning_rate': 7.21311475409836e-07, 'epoch': 1.97}
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 124/189 [01:58<00:58,  1.11it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 125/189 [01:59<00:57,  1.11it/s]                                                 {'loss': 0.2391, 'grad_norm': 8.18397153283545, 'learning_rate': 7.103825136612022e-07, 'epoch': 1.98}
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 125/189 [01:59<00:57,  1.11it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 126/189 [02:00<00:56,  1.11it/s]                                                 {'loss': 0.2032, 'grad_norm': 7.454570477358954, 'learning_rate': 6.994535519125683e-07, 'epoch': 2.0}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 126/189 [02:00<00:56,  1.11it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 127/189 [02:01<00:55,  1.11it/s]                                                 {'loss': 0.1192, 'grad_norm': 6.368246401236904, 'learning_rate': 6.885245901639343e-07, 'epoch': 2.02}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 127/189 [02:01<00:55,  1.11it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 128/189 [02:02<00:54,  1.12it/s]                                                 {'loss': 0.1585, 'grad_norm': 8.623833770785737, 'learning_rate': 6.775956284153005e-07, 'epoch': 2.03}
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 128/189 [02:02<00:54,  1.12it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 129/189 [02:03<00:53,  1.12it/s]                                                 {'loss': 0.1304, 'grad_norm': 7.197356004365732, 'learning_rate': 6.666666666666666e-07, 'epoch': 2.05}
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 129/189 [02:03<00:53,  1.12it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 130/189 [02:04<00:52,  1.12it/s]                                                 {'loss': 0.1162, 'grad_norm': 6.976333259109358, 'learning_rate': 6.557377049180327e-07, 'epoch': 2.06}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 130/189 [02:04<00:52,  1.12it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 131/189 [02:04<00:52,  1.11it/s]                                                 {'loss': 0.093, 'grad_norm': 6.749433649763889, 'learning_rate': 6.44808743169399e-07, 'epoch': 2.08}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 131/189 [02:04<00:52,  1.11it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 132/189 [02:05<00:51,  1.11it/s]                                                 {'loss': 0.1422, 'grad_norm': 10.513315377023844, 'learning_rate': 6.33879781420765e-07, 'epoch': 2.1}
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 132/189 [02:05<00:51,  1.11it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 133/189 [02:06<00:50,  1.11it/s]                                                 {'loss': 0.1295, 'grad_norm': 9.19514010876763, 'learning_rate': 6.229508196721311e-07, 'epoch': 2.11}
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 133/189 [02:06<00:50,  1.11it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 134/189 [02:07<00:49,  1.11it/s]                                                 {'loss': 0.1285, 'grad_norm': 8.068417686038446, 'learning_rate': 6.120218579234973e-07, 'epoch': 2.13}
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 134/189 [02:07<00:49,  1.11it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 135/189 [02:08<00:48,  1.11it/s]                                                 {'loss': 0.0792, 'grad_norm': 5.856523567625177, 'learning_rate': 6.010928961748634e-07, 'epoch': 2.14}
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 135/189 [02:08<00:48,  1.11it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 136/189 [02:09<00:47,  1.11it/s]                                                 {'loss': 0.0878, 'grad_norm': 6.18306726098196, 'learning_rate': 5.901639344262294e-07, 'epoch': 2.16}
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 136/189 [02:09<00:47,  1.11it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 137/189 [02:10<00:46,  1.11it/s]                                                 {'loss': 0.0792, 'grad_norm': 5.493804585543901, 'learning_rate': 5.792349726775956e-07, 'epoch': 2.17}
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 137/189 [02:10<00:46,  1.11it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 138/189 [02:11<00:45,  1.11it/s]                                                 {'loss': 0.1041, 'grad_norm': 7.025650440284253, 'learning_rate': 5.683060109289617e-07, 'epoch': 2.19}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 138/189 [02:11<00:45,  1.11it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 139/189 [02:12<00:44,  1.11it/s]                                                 {'loss': 0.101, 'grad_norm': 7.288169464429801, 'learning_rate': 5.573770491803278e-07, 'epoch': 2.21}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 139/189 [02:12<00:44,  1.11it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 140/189 [02:13<00:44,  1.11it/s]                                                 {'loss': 0.1017, 'grad_norm': 7.6212101370904985, 'learning_rate': 5.46448087431694e-07, 'epoch': 2.22}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 140/189 [02:13<00:44,  1.11it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 141/189 [02:13<00:43,  1.11it/s]                                                 {'loss': 0.1245, 'grad_norm': 5.299050415821979, 'learning_rate': 5.355191256830601e-07, 'epoch': 2.24}
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 141/189 [02:13<00:43,  1.11it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 142/189 [02:14<00:42,  1.11it/s]                                                 {'loss': 0.127, 'grad_norm': 8.35479005474619, 'learning_rate': 5.245901639344262e-07, 'epoch': 2.25}
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 142/189 [02:14<00:42,  1.11it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 143/189 [02:15<00:41,  1.11it/s]                                                 {'loss': 0.1104, 'grad_norm': 6.338698429074236, 'learning_rate': 5.136612021857924e-07, 'epoch': 2.27}
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 143/189 [02:15<00:41,  1.11it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 144/189 [02:16<00:40,  1.11it/s]                                                 {'loss': 0.1362, 'grad_norm': 6.892574818731632, 'learning_rate': 5.027322404371585e-07, 'epoch': 2.29}
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 144/189 [02:16<00:40,  1.11it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 145/189 [02:17<00:39,  1.11it/s]                                                 {'loss': 0.103, 'grad_norm': 6.248802472412413, 'learning_rate': 4.918032786885245e-07, 'epoch': 2.3}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 145/189 [02:17<00:39,  1.11it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 146/189 [02:18<00:38,  1.10it/s]                                                 {'loss': 0.0914, 'grad_norm': 5.162313308118606, 'learning_rate': 4.808743169398907e-07, 'epoch': 2.32}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 146/189 [02:18<00:38,  1.10it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 147/189 [02:19<00:37,  1.11it/s]                                                 {'loss': 0.0935, 'grad_norm': 6.555375135546455, 'learning_rate': 4.6994535519125684e-07, 'epoch': 2.33}
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 147/189 [02:19<00:37,  1.11it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 148/189 [02:20<00:37,  1.11it/s]                                                 {'loss': 0.1465, 'grad_norm': 11.36726216077363, 'learning_rate': 4.590163934426229e-07, 'epoch': 2.35}
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 148/189 [02:20<00:37,  1.11it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 149/189 [02:21<00:36,  1.10it/s]                                                 {'loss': 0.0578, 'grad_norm': 5.372950375038808, 'learning_rate': 4.4808743169398906e-07, 'epoch': 2.37}
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 149/189 [02:21<00:36,  1.10it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 150/189 [02:22<00:35,  1.10it/s]                                                 {'loss': 0.1288, 'grad_norm': 6.275266870197495, 'learning_rate': 4.3715846994535514e-07, 'epoch': 2.38}
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 150/189 [02:22<00:35,  1.10it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 151/189 [02:23<00:34,  1.11it/s]                                                 {'loss': 0.1079, 'grad_norm': 6.773276340362838, 'learning_rate': 4.2622950819672127e-07, 'epoch': 2.4}
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 151/189 [02:23<00:34,  1.11it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 152/189 [02:23<00:33,  1.11it/s]                                                 {'loss': 0.1003, 'grad_norm': 5.898460351857024, 'learning_rate': 4.153005464480874e-07, 'epoch': 2.41}
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 152/189 [02:23<00:33,  1.11it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 153/189 [02:24<00:32,  1.11it/s]                                                 {'loss': 0.178, 'grad_norm': 8.025899479715834, 'learning_rate': 4.0437158469945354e-07, 'epoch': 2.43}
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 153/189 [02:24<00:32,  1.11it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 154/189 [02:25<00:31,  1.11it/s]                                                 {'loss': 0.1484, 'grad_norm': 6.8579165656356285, 'learning_rate': 3.9344262295081967e-07, 'epoch': 2.44}
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 154/189 [02:25<00:31,  1.11it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 155/189 [02:26<00:30,  1.11it/s]                                                 {'loss': 0.1138, 'grad_norm': 8.554338736511403, 'learning_rate': 3.8251366120218575e-07, 'epoch': 2.46}
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 155/189 [02:26<00:30,  1.11it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 156/189 [02:27<00:29,  1.11it/s]                                                 {'loss': 0.1665, 'grad_norm': 6.9491831464573615, 'learning_rate': 3.7158469945355194e-07, 'epoch': 2.48}
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 156/189 [02:27<00:29,  1.11it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 157/189 [02:28<00:28,  1.11it/s]                                                 {'loss': 0.1253, 'grad_norm': 6.913469672795883, 'learning_rate': 3.60655737704918e-07, 'epoch': 2.49}
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 157/189 [02:28<00:28,  1.11it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 158/189 [02:29<00:27,  1.11it/s]                                                 {'loss': 0.0784, 'grad_norm': 4.436659366082934, 'learning_rate': 3.4972677595628415e-07, 'epoch': 2.51}
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 158/189 [02:29<00:27,  1.11it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 159/189 [02:30<00:27,  1.11it/s]                                                 {'loss': 0.1179, 'grad_norm': 7.741068604789258, 'learning_rate': 3.3879781420765023e-07, 'epoch': 2.52}
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 159/189 [02:30<00:27,  1.11it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 160/189 [02:31<00:26,  1.11it/s]                                                 {'loss': 0.075, 'grad_norm': 6.804466817416468, 'learning_rate': 3.2786885245901637e-07, 'epoch': 2.54}
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 160/189 [02:31<00:26,  1.11it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 161/189 [02:32<00:25,  1.11it/s]                                                 {'loss': 0.0935, 'grad_norm': 5.352390886413666, 'learning_rate': 3.169398907103825e-07, 'epoch': 2.56}
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 161/189 [02:32<00:25,  1.11it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 162/189 [02:32<00:24,  1.11it/s]                                                 {'loss': 0.1215, 'grad_norm': 7.977100950048517, 'learning_rate': 3.0601092896174863e-07, 'epoch': 2.57}
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 162/189 [02:32<00:24,  1.11it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 163/189 [02:33<00:23,  1.11it/s]                                                 {'loss': 0.0738, 'grad_norm': 5.263971580210431, 'learning_rate': 2.950819672131147e-07, 'epoch': 2.59}
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 163/189 [02:33<00:23,  1.11it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 164/189 [02:34<00:22,  1.11it/s]                                                 {'loss': 0.063, 'grad_norm': 6.282188965056375, 'learning_rate': 2.8415300546448085e-07, 'epoch': 2.6}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 164/189 [02:34<00:22,  1.11it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 165/189 [02:35<00:21,  1.11it/s]                                                 {'loss': 0.0603, 'grad_norm': 4.795368042378086, 'learning_rate': 2.73224043715847e-07, 'epoch': 2.62}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 165/189 [02:35<00:21,  1.11it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 166/189 [02:36<00:20,  1.11it/s]                                                 {'loss': 0.1433, 'grad_norm': 8.651738914686833, 'learning_rate': 2.622950819672131e-07, 'epoch': 2.63}
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 166/189 [02:36<00:20,  1.11it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 167/189 [02:37<00:19,  1.11it/s]                                                 {'loss': 0.0834, 'grad_norm': 6.403068867691498, 'learning_rate': 2.5136612021857925e-07, 'epoch': 2.65}
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 167/189 [02:37<00:19,  1.11it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 168/189 [02:38<00:18,  1.11it/s]                                                 {'loss': 0.1258, 'grad_norm': 7.615404574491873, 'learning_rate': 2.4043715846994533e-07, 'epoch': 2.67}
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 168/189 [02:38<00:18,  1.11it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 169/189 [02:39<00:18,  1.11it/s]                                                 {'loss': 0.0665, 'grad_norm': 6.362589734675869, 'learning_rate': 2.2950819672131146e-07, 'epoch': 2.68}
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 169/189 [02:39<00:18,  1.11it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 170/189 [02:40<00:17,  1.11it/s]                                                 {'loss': 0.0754, 'grad_norm': 6.062939471327578, 'learning_rate': 2.1857923497267757e-07, 'epoch': 2.7}
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 170/189 [02:40<00:17,  1.11it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 171/189 [02:41<00:16,  1.11it/s]                                                 {'loss': 0.0602, 'grad_norm': 4.099760449703392, 'learning_rate': 2.076502732240437e-07, 'epoch': 2.71}
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 171/189 [02:41<00:16,  1.11it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 172/189 [02:41<00:15,  1.11it/s]                                                 {'loss': 0.095, 'grad_norm': 5.738196469107002, 'learning_rate': 1.9672131147540984e-07, 'epoch': 2.73}
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 172/189 [02:41<00:15,  1.11it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 173/189 [02:42<00:14,  1.11it/s]                                                 {'loss': 0.1756, 'grad_norm': 8.311957225562411, 'learning_rate': 1.8579234972677597e-07, 'epoch': 2.75}
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 173/189 [02:42<00:14,  1.11it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 174/189 [02:43<00:13,  1.11it/s]                                                 {'loss': 0.0799, 'grad_norm': 6.101606457747265, 'learning_rate': 1.7486338797814208e-07, 'epoch': 2.76}
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 174/189 [02:43<00:13,  1.11it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 175/189 [02:44<00:12,  1.10it/s]                                                 {'loss': 0.1016, 'grad_norm': 5.946405615614879, 'learning_rate': 1.6393442622950818e-07, 'epoch': 2.78}
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 175/189 [02:44<00:12,  1.10it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 176/189 [02:45<00:11,  1.10it/s]                                                 {'loss': 0.0659, 'grad_norm': 6.936120077643548, 'learning_rate': 1.5300546448087432e-07, 'epoch': 2.79}
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 176/189 [02:45<00:11,  1.10it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 177/189 [02:46<00:10,  1.11it/s]                                                 {'loss': 0.0854, 'grad_norm': 6.181353530403999, 'learning_rate': 1.4207650273224042e-07, 'epoch': 2.81}
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 177/189 [02:46<00:10,  1.11it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 178/189 [02:47<00:09,  1.10it/s]                                                 {'loss': 0.0943, 'grad_norm': 5.62236583718149, 'learning_rate': 1.3114754098360656e-07, 'epoch': 2.83}
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 178/189 [02:47<00:09,  1.10it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 179/189 [02:48<00:09,  1.10it/s]                                                 {'loss': 0.1211, 'grad_norm': 6.173716605506718, 'learning_rate': 1.2021857923497266e-07, 'epoch': 2.84}
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 179/189 [02:48<00:09,  1.10it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 180/189 [02:49<00:08,  1.10it/s]                                                 {'loss': 0.0925, 'grad_norm': 8.735461139148585, 'learning_rate': 1.0928961748633878e-07, 'epoch': 2.86}
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 180/189 [02:49<00:08,  1.10it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 181/189 [02:50<00:07,  1.11it/s]                                                 {'loss': 0.0684, 'grad_norm': 4.841515421904873, 'learning_rate': 9.836065573770492e-08, 'epoch': 2.87}
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 181/189 [02:50<00:07,  1.11it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 182/189 [02:51<00:06,  1.11it/s]                                                 {'loss': 0.0758, 'grad_norm': 5.970158694471343, 'learning_rate': 8.743169398907104e-08, 'epoch': 2.89}
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 182/189 [02:51<00:06,  1.11it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 183/189 [02:51<00:05,  1.11it/s]                                                 {'loss': 0.0601, 'grad_norm': 6.857358908967085, 'learning_rate': 7.650273224043716e-08, 'epoch': 2.9}
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 183/189 [02:51<00:05,  1.11it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 184/189 [02:52<00:04,  1.11it/s]                                                 {'loss': 0.1202, 'grad_norm': 8.38378541476307, 'learning_rate': 6.557377049180328e-08, 'epoch': 2.92}
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 184/189 [02:52<00:04,  1.11it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 185/189 [02:53<00:03,  1.11it/s]                                                 {'loss': 0.0613, 'grad_norm': 4.489937082520457, 'learning_rate': 5.464480874316939e-08, 'epoch': 2.94}
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 185/189 [02:53<00:03,  1.11it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 186/189 [02:54<00:02,  1.11it/s]                                                 {'loss': 0.031, 'grad_norm': 4.161578390572969, 'learning_rate': 4.371584699453552e-08, 'epoch': 2.95}
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 186/189 [02:54<00:02,  1.11it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 187/189 [02:55<00:01,  1.11it/s]                                                 {'loss': 0.116, 'grad_norm': 6.185576536238697, 'learning_rate': 3.278688524590164e-08, 'epoch': 2.97}
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 187/189 [02:55<00:01,  1.11it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 188/189 [02:56<00:00,  1.11it/s]                                                 {'loss': 0.0776, 'grad_norm': 5.029343177566107, 'learning_rate': 2.185792349726776e-08, 'epoch': 2.98}
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 188/189 [02:56<00:00,  1.11it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 189/189 [02:57<00:00,  1.11it/s]                                                 {'loss': 0.0875, 'grad_norm': 5.496568741296699, 'learning_rate': 1.092896174863388e-08, 'epoch': 3.0}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 189/189 [02:57<00:00,  1.11it/s]                                                 {'train_runtime': 242.7886, 'train_samples_per_second': 61.622, 'train_steps_per_second': 0.778, 'train_loss': 0.39062483325876574, 'epoch': 3.0}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 189/189 [04:02<00:00,  1.11it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 189/189 [04:02<00:00,  1.28s/it]
[2024-08-28 21:08:27,332] [INFO] [launch.py:351:main] Process 3765070 exits successfully.
[2024-08-28 21:08:27,333] [INFO] [launch.py:351:main] Process 3765066 exits successfully.
[2024-08-28 21:08:27,333] [INFO] [launch.py:351:main] Process 3765067 exits successfully.
[2024-08-28 21:08:28,333] [INFO] [launch.py:351:main] Process 3765065 exits successfully.
[2024-08-28 21:08:28,334] [INFO] [launch.py:351:main] Process 3765069 exits successfully.
[2024-08-28 21:08:28,334] [INFO] [launch.py:351:main] Process 3765064 exits successfully.
[2024-08-28 21:08:28,334] [INFO] [launch.py:351:main] Process 3765068 exits successfully.
[2024-08-28 21:08:53,337] [INFO] [launch.py:351:main] Process 3765063 exits successfully.
