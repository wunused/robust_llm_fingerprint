Namespace(mode=['alpaca'], base_model='meta-llama/Llama-2-7b-hf', task_name='roleplay', tuned_dir='./cache')
num gpus:  8
Running 1/1: deepspeed --num_gpus=8 train.py --deepspeed ../deepspeed_config/zero3.json --bf16 --tf32=True
    --model_name_or_path meta-llama/Llama-2-7b-hf --data_path ../data/stanford_alpaca/roleplay_data.json
    --output_dir /fsx-project/yunyun/models/meta-llama_meta-llama_roleplay_tuned
    --num_train_epochs 3
    --per_device_train_batch_size 10
    --per_device_eval_batch_size 4
    --gradient_accumulation_steps 1
    --gradient_checkpointing=True
    --evaluation_strategy=no
    --save_strategy=steps
    --save_steps 500
    --save_total_limit 1
    --learning_rate 2e-6
    --weight_decay 0.
    --report_to tensorboard
    --warmup_ratio 0.03
    --lr_scheduler_type=cosine
    --logging_steps 1
['deepspeed', '--num_gpus=8', 'train.py', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Llama-2-7b-hf', '--data_path', '../data/stanford_alpaca/roleplay_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_meta-llama_roleplay_tuned', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1']
[2024-07-24 18:27:16,100] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2024-07-24 18:27:19,633] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2024-07-24 18:27:19,633] [INFO] [runner.py:568:main] cmd = /data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNSwgNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None train.py --deepspeed ../deepspeed_config/zero3.json --bf16 --tf32=True --model_name_or_path meta-llama/Llama-2-7b-hf --data_path ../data/stanford_alpaca/roleplay_data.json --output_dir /fsx-project/yunyun/models/meta-llama_meta-llama_roleplay_tuned --num_train_epochs 3 --per_device_train_batch_size 10 --per_device_eval_batch_size 4 --gradient_accumulation_steps 1 --gradient_checkpointing=True --evaluation_strategy=no --save_strategy=steps --save_steps 500 --save_total_limit 1 --learning_rate 2e-6 --weight_decay 0. --report_to tensorboard --warmup_ratio 0.03 --lr_scheduler_type=cosine --logging_steps 1
[2024-07-24 18:27:22,289] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2024-07-24 18:27:25,710] [INFO] [launch.py:139:main] 0 NCCL_ASYNC_ERROR_HANDLING=1
[2024-07-24 18:27:25,710] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]}
[2024-07-24 18:27:25,710] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=8, node_rank=0
[2024-07-24 18:27:25,710] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]})
[2024-07-24 18:27:25,710] [INFO] [launch.py:164:main] dist_world_size=8
[2024-07-24 18:27:25,710] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
[2024-07-24 18:27:25,711] [INFO] [launch.py:256:main] process 278521 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=0', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Llama-2-7b-hf', '--data_path', '../data/stanford_alpaca/roleplay_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_meta-llama_roleplay_tuned', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1']
[2024-07-24 18:27:25,712] [INFO] [launch.py:256:main] process 278522 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=1', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Llama-2-7b-hf', '--data_path', '../data/stanford_alpaca/roleplay_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_meta-llama_roleplay_tuned', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1']
[2024-07-24 18:27:25,712] [INFO] [launch.py:256:main] process 278523 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=2', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Llama-2-7b-hf', '--data_path', '../data/stanford_alpaca/roleplay_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_meta-llama_roleplay_tuned', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1']
[2024-07-24 18:27:25,713] [INFO] [launch.py:256:main] process 278524 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=3', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Llama-2-7b-hf', '--data_path', '../data/stanford_alpaca/roleplay_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_meta-llama_roleplay_tuned', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1']
[2024-07-24 18:27:25,714] [INFO] [launch.py:256:main] process 278525 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=4', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Llama-2-7b-hf', '--data_path', '../data/stanford_alpaca/roleplay_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_meta-llama_roleplay_tuned', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1']
[2024-07-24 18:27:25,714] [INFO] [launch.py:256:main] process 278526 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=5', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Llama-2-7b-hf', '--data_path', '../data/stanford_alpaca/roleplay_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_meta-llama_roleplay_tuned', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1']
[2024-07-24 18:27:25,715] [INFO] [launch.py:256:main] process 278527 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=6', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Llama-2-7b-hf', '--data_path', '../data/stanford_alpaca/roleplay_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_meta-llama_roleplay_tuned', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1']
[2024-07-24 18:27:25,715] [INFO] [launch.py:256:main] process 278528 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=7', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Llama-2-7b-hf', '--data_path', '../data/stanford_alpaca/roleplay_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_meta-llama_roleplay_tuned', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1']
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
[2024-07-24 18:27:39,440] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2024-07-24 18:27:39,736] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-24 18:27:39,794] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-24 18:27:39,824] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-24 18:27:39,833] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-24 18:27:39,835] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-24 18:27:39,842] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-24 18:27:39,868] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt[93m [WARNING] [0m async_io: please install the libaio-dev package with apt

[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.

[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2024-07-24 18:27:40,288] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-24 18:27:40,563] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-24 18:27:40,640] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-24 18:27:40,642] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-24 18:27:40,650] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-24 18:27:40,661] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-24 18:27:40,665] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-24 18:27:40,680] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-24 18:27:40,680] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 2/2 [00:00<00:00, 1347.57it/s]
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 2/2 [00:00<00:00, 1372.48it/s]
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 2/2 [00:00<00:00, 1187.68it/s]
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 2/2 [00:00<00:00, 1243.13it/s]
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 2/2 [00:00<00:00, 1355.84it/s]
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 2/2 [00:00<00:00, 1097.55it/s]
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 2/2 [00:00<00:00, 1239.82it/s]
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 2/2 [00:00<00:00, 1155.14it/s]
[2024-07-24 18:27:52,154] [INFO] [partition_parameters.py:345:__exit__] finished initializing model - num_params = 291, num_elems = 6.74B
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.82s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.83s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.82s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.84s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.84s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.84s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.85s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.11s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.22s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.10s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.21s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.11s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.22s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.11s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.22s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.11s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.22s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.12s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.22s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.13s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.23s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.62s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.48s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.80s/it]
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
[rank0]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
[rank7]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
[rank6]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
Detected CUDA files, patching ldflags
Emitting ninja build file /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121/fused_adam/build.ninja...
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/cpp_extension.py:1967: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
[rank3]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[rank4]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
[rank2]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[rank1]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
[rank5]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.24312496185302734 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.10280394554138184 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.1029353141784668 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.10269474983215332 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.20326757431030273 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.20293688774108887 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.20292162895202637 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.20320415496826172 seconds
Parameter Offload: Total persistent parameters: 266240 in 65 params
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  0%|          | 0/75 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
  1%|▏         | 1/75 [00:07<09:27,  7.68s/it]                                              {'loss': 1.5829, 'grad_norm': 6.362265070910641, 'learning_rate': 0.0, 'epoch': 0.04}
  1%|▏         | 1/75 [00:07<09:27,  7.68s/it]  3%|▎         | 2/75 [00:09<04:55,  4.04s/it]                                              {'loss': 1.5056, 'grad_norm': 6.564538626961796, 'learning_rate': 1.2618595071429145e-06, 'epoch': 0.08}
  3%|▎         | 2/75 [00:09<04:55,  4.04s/it]  4%|▍         | 3/75 [00:10<03:09,  2.63s/it]                                              {'loss': 1.4742, 'grad_norm': 6.029105797558148, 'learning_rate': 2e-06, 'epoch': 0.12}
  4%|▍         | 3/75 [00:10<03:09,  2.63s/it]  5%|▌         | 4/75 [00:11<02:17,  1.94s/it]                                              {'loss': 1.5293, 'grad_norm': 5.37844171942772, 'learning_rate': 2e-06, 'epoch': 0.16}
  5%|▌         | 4/75 [00:11<02:17,  1.94s/it]  7%|▋         | 5/75 [00:11<01:46,  1.53s/it]                                              {'loss': 1.4635, 'grad_norm': 5.350401533564471, 'learning_rate': 1.9722222222222224e-06, 'epoch': 0.2}
  7%|▋         | 5/75 [00:11<01:46,  1.53s/it]  8%|▊         | 6/75 [00:12<01:28,  1.29s/it]                                              {'loss': 1.458, 'grad_norm': 4.482735449436817, 'learning_rate': 1.9444444444444444e-06, 'epoch': 0.24}
  8%|▊         | 6/75 [00:12<01:28,  1.29s/it]  9%|▉         | 7/75 [00:13<01:25,  1.25s/it]                                              {'loss': 1.4338, 'grad_norm': 3.8242494229087654, 'learning_rate': 1.916666666666667e-06, 'epoch': 0.28}
  9%|▉         | 7/75 [00:13<01:25,  1.25s/it] 11%|█         | 8/75 [00:14<01:15,  1.13s/it]                                              {'loss': 1.3878, 'grad_norm': 3.873488550597787, 'learning_rate': 1.8888888888888888e-06, 'epoch': 0.32}
 11%|█         | 8/75 [00:14<01:15,  1.13s/it] 12%|█▏        | 9/75 [00:16<01:20,  1.22s/it]                                              {'loss': 1.4138, 'grad_norm': 3.9585433995276884, 'learning_rate': 1.861111111111111e-06, 'epoch': 0.36}
 12%|█▏        | 9/75 [00:16<01:20,  1.22s/it] 13%|█▎        | 10/75 [00:16<01:11,  1.10s/it]                                               {'loss': 1.4193, 'grad_norm': 3.6029164496060564, 'learning_rate': 1.833333333333333e-06, 'epoch': 0.4}
 13%|█▎        | 10/75 [00:16<01:11,  1.10s/it] 15%|█▍        | 11/75 [00:18<01:16,  1.20s/it]                                               {'loss': 1.3476, 'grad_norm': 2.4560771877373586, 'learning_rate': 1.8055555555555555e-06, 'epoch': 0.44}
 15%|█▍        | 11/75 [00:18<01:16,  1.20s/it] 16%|█▌        | 12/75 [00:19<01:07,  1.07s/it]                                               {'loss': 1.4103, 'grad_norm': 2.5045294779322327, 'learning_rate': 1.7777777777777775e-06, 'epoch': 0.48}
 16%|█▌        | 12/75 [00:19<01:07,  1.07s/it] 17%|█▋        | 13/75 [00:19<01:02,  1.01s/it]                                               {'loss': 1.3245, 'grad_norm': 2.2454547795245046, 'learning_rate': 1.75e-06, 'epoch': 0.52}
 17%|█▋        | 13/75 [00:19<01:02,  1.01s/it] 19%|█▊        | 14/75 [00:20<00:59,  1.02it/s]                                               {'loss': 1.2833, 'grad_norm': 2.147442886564773, 'learning_rate': 1.7222222222222222e-06, 'epoch': 0.56}
 19%|█▊        | 14/75 [00:20<00:59,  1.02it/s] 20%|██        | 15/75 [00:22<01:04,  1.07s/it]                                               {'loss': 1.3206, 'grad_norm': 2.235589714587406, 'learning_rate': 1.6944444444444444e-06, 'epoch': 0.6}
 20%|██        | 15/75 [00:22<01:04,  1.07s/it] 21%|██▏       | 16/75 [00:23<00:59,  1.01s/it]                                               {'loss': 1.3048, 'grad_norm': 2.055358987764133, 'learning_rate': 1.6666666666666667e-06, 'epoch': 0.64}
 21%|██▏       | 16/75 [00:23<00:59,  1.01s/it] 23%|██▎       | 17/75 [00:23<00:56,  1.02it/s]                                               {'loss': 1.3055, 'grad_norm': 1.9159071117408002, 'learning_rate': 1.6388888888888887e-06, 'epoch': 0.68}
 23%|██▎       | 17/75 [00:23<00:56,  1.02it/s] 24%|██▍       | 18/75 [00:24<00:52,  1.09it/s]                                               {'loss': 1.3159, 'grad_norm': 1.8821283534536668, 'learning_rate': 1.6111111111111111e-06, 'epoch': 0.72}
 24%|██▍       | 18/75 [00:24<00:52,  1.09it/s] 25%|██▌       | 19/75 [00:25<00:53,  1.05it/s]                                               {'loss': 1.347, 'grad_norm': 1.9358739569046124, 'learning_rate': 1.5833333333333331e-06, 'epoch': 0.76}
 25%|██▌       | 19/75 [00:25<00:53,  1.05it/s] 27%|██▋       | 20/75 [00:27<00:58,  1.06s/it]                                               {'loss': 1.3129, 'grad_norm': 1.8664963689551317, 'learning_rate': 1.5555555555555556e-06, 'epoch': 0.8}
 27%|██▋       | 20/75 [00:27<00:58,  1.06s/it] 28%|██▊       | 21/75 [00:27<00:52,  1.03it/s]                                               {'loss': 1.3447, 'grad_norm': 1.9230673465576187, 'learning_rate': 1.5277777777777776e-06, 'epoch': 0.84}
 28%|██▊       | 21/75 [00:27<00:52,  1.03it/s] 29%|██▉       | 22/75 [00:28<00:49,  1.07it/s]                                               {'loss': 1.3687, 'grad_norm': 1.9643499620306522, 'learning_rate': 1.5e-06, 'epoch': 0.88}
 29%|██▉       | 22/75 [00:28<00:49,  1.07it/s] 31%|███       | 23/75 [00:29<00:46,  1.12it/s]                                               {'loss': 1.3425, 'grad_norm': 1.9892587520341698, 'learning_rate': 1.4722222222222223e-06, 'epoch': 0.92}
 31%|███       | 23/75 [00:29<00:46,  1.12it/s] 32%|███▏      | 24/75 [00:30<00:51,  1.01s/it]                                               {'loss': 1.3069, 'grad_norm': 1.9574519714919567, 'learning_rate': 1.4444444444444443e-06, 'epoch': 0.96}
 32%|███▏      | 24/75 [00:30<00:51,  1.01s/it] 33%|███▎      | 25/75 [00:31<00:50,  1.00s/it]                                               {'loss': 1.3064, 'grad_norm': 1.9097746981923005, 'learning_rate': 1.4166666666666667e-06, 'epoch': 1.0}
 33%|███▎      | 25/75 [00:31<00:50,  1.00s/it] 35%|███▍      | 26/75 [00:32<00:47,  1.04it/s]                                               {'loss': 1.2952, 'grad_norm': 1.9214153588280962, 'learning_rate': 1.3888888888888887e-06, 'epoch': 1.04}
 35%|███▍      | 26/75 [00:32<00:47,  1.04it/s] 36%|███▌      | 27/75 [00:33<00:43,  1.10it/s]                                               {'loss': 1.2471, 'grad_norm': 2.078358971687556, 'learning_rate': 1.3611111111111112e-06, 'epoch': 1.08}
 36%|███▌      | 27/75 [00:33<00:43,  1.10it/s] 37%|███▋      | 28/75 [00:34<00:45,  1.03it/s]                                               {'loss': 1.2368, 'grad_norm': 1.8118834837473667, 'learning_rate': 1.3333333333333332e-06, 'epoch': 1.12}
 37%|███▋      | 28/75 [00:34<00:45,  1.03it/s] 39%|███▊      | 29/75 [00:35<00:50,  1.10s/it]                                               {'loss': 1.2071, 'grad_norm': 1.7973945795515383, 'learning_rate': 1.3055555555555554e-06, 'epoch': 1.16}
 39%|███▊      | 29/75 [00:35<00:50,  1.10s/it] 40%|████      | 30/75 [00:36<00:46,  1.04s/it]                                               {'loss': 1.2443, 'grad_norm': 1.9086239848786775, 'learning_rate': 1.2777777777777777e-06, 'epoch': 1.2}
 40%|████      | 30/75 [00:36<00:46,  1.04s/it] 41%|████▏     | 31/75 [00:37<00:42,  1.03it/s]                                               {'loss': 1.3054, 'grad_norm': 1.788871635192824, 'learning_rate': 1.2499999999999999e-06, 'epoch': 1.24}
 41%|████▏     | 31/75 [00:37<00:42,  1.03it/s] 43%|████▎     | 32/75 [00:38<00:39,  1.09it/s]                                               {'loss': 1.2283, 'grad_norm': 1.8469993328486392, 'learning_rate': 1.2222222222222223e-06, 'epoch': 1.28}
 43%|████▎     | 32/75 [00:38<00:39,  1.09it/s] 44%|████▍     | 33/75 [00:39<00:43,  1.05s/it]                                               {'loss': 1.2289, 'grad_norm': 1.8139352386034642, 'learning_rate': 1.1944444444444443e-06, 'epoch': 1.32}
 44%|████▍     | 33/75 [00:39<00:43,  1.05s/it] 45%|████▌     | 34/75 [00:40<00:40,  1.02it/s]                                               {'loss': 1.2636, 'grad_norm': 1.8000939317501106, 'learning_rate': 1.1666666666666668e-06, 'epoch': 1.36}
 45%|████▌     | 34/75 [00:40<00:40,  1.02it/s] 47%|████▋     | 35/75 [00:41<00:37,  1.06it/s]                                               {'loss': 1.2361, 'grad_norm': 1.8596467222849884, 'learning_rate': 1.1388888888888888e-06, 'epoch': 1.4}
 47%|████▋     | 35/75 [00:41<00:37,  1.06it/s] 48%|████▊     | 36/75 [00:42<00:35,  1.09it/s]                                               {'loss': 1.249, 'grad_norm': 1.8864366506896137, 'learning_rate': 1.111111111111111e-06, 'epoch': 1.44}
 48%|████▊     | 36/75 [00:42<00:35,  1.09it/s] 49%|████▉     | 37/75 [00:43<00:37,  1.02it/s]                                               {'loss': 1.2337, 'grad_norm': 1.7682666315116313, 'learning_rate': 1.0833333333333333e-06, 'epoch': 1.48}
 49%|████▉     | 37/75 [00:43<00:37,  1.02it/s] 51%|█████     | 38/75 [00:44<00:36,  1.03it/s]                                               {'loss': 1.1725, 'grad_norm': 1.79143358778557, 'learning_rate': 1.0555555555555555e-06, 'epoch': 1.52}
 51%|█████     | 38/75 [00:44<00:36,  1.03it/s] 52%|█████▏    | 39/75 [00:45<00:33,  1.08it/s]                                               {'loss': 1.2445, 'grad_norm': 1.7560665066295582, 'learning_rate': 1.0277777777777777e-06, 'epoch': 1.56}
 52%|█████▏    | 39/75 [00:45<00:33,  1.08it/s] 53%|█████▎    | 40/75 [00:46<00:31,  1.12it/s]                                               {'loss': 1.1826, 'grad_norm': 1.7674374491948757, 'learning_rate': 1e-06, 'epoch': 1.6}
 53%|█████▎    | 40/75 [00:46<00:31,  1.12it/s] 55%|█████▍    | 41/75 [00:46<00:28,  1.18it/s]                                               {'loss': 1.2465, 'grad_norm': 1.8498868724373363, 'learning_rate': 9.722222222222222e-07, 'epoch': 1.64}
 55%|█████▍    | 41/75 [00:46<00:28,  1.18it/s] 56%|█████▌    | 42/75 [00:48<00:34,  1.04s/it]                                               {'loss': 1.2392, 'grad_norm': 1.7399981324561469, 'learning_rate': 9.444444444444444e-07, 'epoch': 1.68}
 56%|█████▌    | 42/75 [00:48<00:34,  1.04s/it] 57%|█████▋    | 43/75 [00:48<00:30,  1.05it/s]                                               {'loss': 1.2534, 'grad_norm': 1.822439821232867, 'learning_rate': 9.166666666666665e-07, 'epoch': 1.72}
 57%|█████▋    | 43/75 [00:48<00:30,  1.05it/s] 59%|█████▊    | 44/75 [00:49<00:28,  1.08it/s]                                               {'loss': 1.23, 'grad_norm': 1.8455683183897647, 'learning_rate': 8.888888888888888e-07, 'epoch': 1.76}
 59%|█████▊    | 44/75 [00:49<00:28,  1.08it/s] 60%|██████    | 45/75 [00:50<00:26,  1.12it/s]                                               {'loss': 1.2092, 'grad_norm': 1.7349501778361611, 'learning_rate': 8.611111111111111e-07, 'epoch': 1.8}
 60%|██████    | 45/75 [00:50<00:26,  1.12it/s] 61%|██████▏   | 46/75 [00:51<00:28,  1.02it/s]                                               {'loss': 1.2519, 'grad_norm': 1.7445861914984695, 'learning_rate': 8.333333333333333e-07, 'epoch': 1.84}
 61%|██████▏   | 46/75 [00:51<00:28,  1.02it/s] 63%|██████▎   | 47/75 [00:52<00:28,  1.00s/it]                                               {'loss': 1.2385, 'grad_norm': 1.830244061879762, 'learning_rate': 8.055555555555556e-07, 'epoch': 1.88}
 63%|██████▎   | 47/75 [00:52<00:28,  1.00s/it] 64%|██████▍   | 48/75 [00:53<00:25,  1.05it/s]                                               {'loss': 1.1941, 'grad_norm': 1.7112721462362002, 'learning_rate': 7.777777777777778e-07, 'epoch': 1.92}
 64%|██████▍   | 48/75 [00:53<00:25,  1.05it/s] 65%|██████▌   | 49/75 [00:54<00:23,  1.10it/s]                                               {'loss': 1.2083, 'grad_norm': 1.7599267492907962, 'learning_rate': 7.5e-07, 'epoch': 1.96}
 65%|██████▌   | 49/75 [00:54<00:23,  1.10it/s] 67%|██████▋   | 50/75 [00:55<00:22,  1.13it/s]                                               {'loss': 1.1969, 'grad_norm': 1.7948454190021832, 'learning_rate': 7.222222222222221e-07, 'epoch': 2.0}
 67%|██████▋   | 50/75 [00:55<00:22,  1.13it/s] 68%|██████▊   | 51/75 [00:56<00:24,  1.03s/it]                                               {'loss': 1.2188, 'grad_norm': 1.8201289022725315, 'learning_rate': 6.944444444444444e-07, 'epoch': 2.04}
 68%|██████▊   | 51/75 [00:56<00:24,  1.03s/it] 69%|██████▉   | 52/75 [00:57<00:22,  1.04it/s]                                               {'loss': 1.2227, 'grad_norm': 1.8229481306943593, 'learning_rate': 6.666666666666666e-07, 'epoch': 2.08}
 69%|██████▉   | 52/75 [00:57<00:22,  1.04it/s] 71%|███████   | 53/75 [00:58<00:20,  1.06it/s]                                               {'loss': 1.154, 'grad_norm': 1.7654009500652819, 'learning_rate': 6.388888888888888e-07, 'epoch': 2.12}
 71%|███████   | 53/75 [00:58<00:20,  1.06it/s] 72%|███████▏  | 54/75 [00:59<00:19,  1.08it/s]                                               {'loss': 1.1215, 'grad_norm': 1.707165355969042, 'learning_rate': 6.111111111111112e-07, 'epoch': 2.16}
 72%|███████▏  | 54/75 [00:59<00:19,  1.08it/s] 73%|███████▎  | 55/75 [01:00<00:19,  1.02it/s]                                               {'loss': 1.1681, 'grad_norm': 1.7533817240379308, 'learning_rate': 5.833333333333334e-07, 'epoch': 2.2}
 73%|███████▎  | 55/75 [01:00<00:19,  1.02it/s] 75%|███████▍  | 56/75 [01:01<00:18,  1.01it/s]                                               {'loss': 1.2059, 'grad_norm': 1.818755365522138, 'learning_rate': 5.555555555555555e-07, 'epoch': 2.24}
 75%|███████▍  | 56/75 [01:01<00:18,  1.01it/s] 76%|███████▌  | 57/75 [01:02<00:17,  1.05it/s]                                               {'loss': 1.1166, 'grad_norm': 1.9036071101014869, 'learning_rate': 5.277777777777777e-07, 'epoch': 2.28}
 76%|███████▌  | 57/75 [01:02<00:17,  1.05it/s] 77%|███████▋  | 58/75 [01:03<00:15,  1.09it/s]                                               {'loss': 1.1956, 'grad_norm': 1.7792785472730503, 'learning_rate': 5e-07, 'epoch': 2.32}
 77%|███████▋  | 58/75 [01:03<00:15,  1.09it/s] 79%|███████▊  | 59/75 [01:03<00:13,  1.15it/s]                                               {'loss': 1.1614, 'grad_norm': 1.8186157963072827, 'learning_rate': 4.722222222222222e-07, 'epoch': 2.36}
 79%|███████▊  | 59/75 [01:03<00:13,  1.15it/s] 80%|████████  | 60/75 [01:05<00:14,  1.03it/s]                                               {'loss': 1.1523, 'grad_norm': 1.7561179865049736, 'learning_rate': 4.444444444444444e-07, 'epoch': 2.4}
 80%|████████  | 60/75 [01:05<00:14,  1.03it/s] 81%|████████▏ | 61/75 [01:06<00:13,  1.04it/s]                                               {'loss': 1.1459, 'grad_norm': 1.771908636061981, 'learning_rate': 4.1666666666666667e-07, 'epoch': 2.44}
 81%|████████▏ | 61/75 [01:06<00:13,  1.04it/s] 83%|████████▎ | 62/75 [01:06<00:12,  1.07it/s]                                               {'loss': 1.2171, 'grad_norm': 1.962587742200436, 'learning_rate': 3.888888888888889e-07, 'epoch': 2.48}
 83%|████████▎ | 62/75 [01:06<00:12,  1.07it/s] 84%|████████▍ | 63/75 [01:07<00:10,  1.13it/s]                                               {'loss': 1.1858, 'grad_norm': 1.7986960045190268, 'learning_rate': 3.6111111111111107e-07, 'epoch': 2.52}
 84%|████████▍ | 63/75 [01:07<00:10,  1.13it/s] 85%|████████▌ | 64/75 [01:08<00:10,  1.03it/s]                                               {'loss': 1.1853, 'grad_norm': 1.7531439571982292, 'learning_rate': 3.333333333333333e-07, 'epoch': 2.56}
 85%|████████▌ | 64/75 [01:08<00:10,  1.03it/s] 87%|████████▋ | 65/75 [01:09<00:10,  1.02s/it]                                               {'loss': 1.2141, 'grad_norm': 1.7810330327312505, 'learning_rate': 3.055555555555556e-07, 'epoch': 2.6}
 87%|████████▋ | 65/75 [01:09<00:10,  1.02s/it] 88%|████████▊ | 66/75 [01:10<00:08,  1.02it/s]                                               {'loss': 1.182, 'grad_norm': 1.7820678745589968, 'learning_rate': 2.7777777777777776e-07, 'epoch': 2.64}
 88%|████████▊ | 66/75 [01:10<00:08,  1.02it/s] 89%|████████▉ | 67/75 [01:11<00:07,  1.07it/s]                                               {'loss': 1.2134, 'grad_norm': 1.8503713276981988, 'learning_rate': 2.5e-07, 'epoch': 2.68}
 89%|████████▉ | 67/75 [01:11<00:07,  1.07it/s] 91%|█████████ | 68/75 [01:12<00:06,  1.08it/s]                                               {'loss': 1.1476, 'grad_norm': 1.7922983590541446, 'learning_rate': 2.222222222222222e-07, 'epoch': 2.72}
 91%|█████████ | 68/75 [01:12<00:06,  1.08it/s] 92%|█████████▏| 69/75 [01:14<00:06,  1.08s/it]                                               {'loss': 1.15, 'grad_norm': 1.777319622136344, 'learning_rate': 1.9444444444444445e-07, 'epoch': 2.76}
 92%|█████████▏| 69/75 [01:14<00:06,  1.08s/it] 93%|█████████▎| 70/75 [01:14<00:04,  1.02it/s]                                               {'loss': 1.1507, 'grad_norm': 1.7481347039747945, 'learning_rate': 1.6666666666666665e-07, 'epoch': 2.8}
 93%|█████████▎| 70/75 [01:14<00:04,  1.02it/s] 95%|█████████▍| 71/75 [01:15<00:03,  1.06it/s]                                               {'loss': 1.1478, 'grad_norm': 1.7409147598398729, 'learning_rate': 1.3888888888888888e-07, 'epoch': 2.84}
 95%|█████████▍| 71/75 [01:15<00:03,  1.06it/s] 96%|█████████▌| 72/75 [01:16<00:02,  1.10it/s]                                               {'loss': 1.1962, 'grad_norm': 1.8158745159920728, 'learning_rate': 1.111111111111111e-07, 'epoch': 2.88}
 96%|█████████▌| 72/75 [01:16<00:02,  1.10it/s] 97%|█████████▋| 73/75 [01:17<00:02,  1.01s/it]                                               {'loss': 1.1505, 'grad_norm': 1.775461523525782, 'learning_rate': 8.333333333333333e-08, 'epoch': 2.92}
 97%|█████████▋| 73/75 [01:17<00:02,  1.01s/it] 99%|█████████▊| 74/75 [01:18<00:00,  1.02it/s]                                               {'loss': 1.2173, 'grad_norm': 1.8686689812321426, 'learning_rate': 5.555555555555555e-08, 'epoch': 2.96}
 99%|█████████▊| 74/75 [01:18<00:00,  1.02it/s]100%|██████████| 75/75 [01:19<00:00,  1.05it/s]                                               {'loss': 1.1763, 'grad_norm': 1.8048731448986215, 'learning_rate': 2.7777777777777774e-08, 'epoch': 3.0}
100%|██████████| 75/75 [01:19<00:00,  1.05it/s]                                               {'train_runtime': 79.5474, 'train_samples_per_second': 72.523, 'train_steps_per_second': 0.943, 'train_loss': 1.26465926806132, 'epoch': 3.0}
100%|██████████| 75/75 [01:19<00:00,  1.05it/s]100%|██████████| 75/75 [01:19<00:00,  1.06s/it]
[2024-07-24 18:29:35,736] [INFO] [launch.py:351:main] Process 278528 exits successfully.
[2024-07-24 18:29:35,737] [INFO] [launch.py:351:main] Process 278522 exits successfully.
[2024-07-24 18:29:35,737] [INFO] [launch.py:351:main] Process 278527 exits successfully.
[2024-07-24 18:29:36,738] [INFO] [launch.py:351:main] Process 278523 exits successfully.
[2024-07-24 18:29:36,738] [INFO] [launch.py:351:main] Process 278525 exits successfully.
[2024-07-24 18:29:36,738] [INFO] [launch.py:351:main] Process 278526 exits successfully.
[2024-07-24 18:29:36,738] [INFO] [launch.py:351:main] Process 278524 exits successfully.
[2024-07-24 18:30:00,741] [INFO] [launch.py:351:main] Process 278521 exits successfully.
