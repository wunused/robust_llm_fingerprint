Namespace(mode=['alpaca'], base_model='meta-llama/Llama-2-7b-hf', template_name='barebone', total_bsz=64, epoch=3, lr=2e-05, data_path='', task_name='roleplay', tuned_dir='./cache', use_peft=True, lora_r=16, lora_alpha=32)
num gpus:  8
Running 1/1: deepspeed --num_gpus=8 train.py --deepspeed ../deepspeed_config/zero3.json --bf16 --tf32=True
        --model_name_or_path meta-llama/Llama-2-7b-hf --data_path ../data/stanford_alpaca/roleplay_data.json
        --output_dir /fsx-project/yunyun/models/meta-llama_Llama2-7b_roleplay_Lora_tuned/
        --num_train_epochs 3
        --per_device_train_batch_size 10
        --per_device_eval_batch_size 4
        --gradient_accumulation_steps 1
        --gradient_checkpointing=True
        --evaluation_strategy=no
        --save_strategy=steps
        --save_steps 500
        --save_total_limit 1
        --learning_rate 2e-6
        --weight_decay 0.
        --report_to tensorboard
        --warmup_ratio 0.03
        --lr_scheduler_type=cosine
        --logging_steps 1
        --use_peft True 
        --lora_r 16 --lora_alpha 32
['deepspeed', '--num_gpus=8', 'train.py', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Llama-2-7b-hf', '--data_path', '../data/stanford_alpaca/roleplay_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_Llama2-7b_roleplay_Lora_tuned/', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1', '--use_peft', 'True', '--lora_r', '16', '--lora_alpha', '32']
[2024-08-13 05:54:56,116] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2024-08-13 05:55:03,835] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2024-08-13 05:55:03,835] [INFO] [runner.py:568:main] cmd = /data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNSwgNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None train.py --deepspeed ../deepspeed_config/zero3.json --bf16 --tf32=True --model_name_or_path meta-llama/Llama-2-7b-hf --data_path ../data/stanford_alpaca/roleplay_data.json --output_dir /fsx-project/yunyun/models/meta-llama_Llama2-7b_roleplay_Lora_tuned/ --num_train_epochs 3 --per_device_train_batch_size 10 --per_device_eval_batch_size 4 --gradient_accumulation_steps 1 --gradient_checkpointing=True --evaluation_strategy=no --save_strategy=steps --save_steps 500 --save_total_limit 1 --learning_rate 2e-6 --weight_decay 0. --report_to tensorboard --warmup_ratio 0.03 --lr_scheduler_type=cosine --logging_steps 1 --use_peft True --lora_r 16 --lora_alpha 32
[2024-08-13 05:55:06,442] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2024-08-13 05:55:09,922] [INFO] [launch.py:139:main] 0 NCCL_ASYNC_ERROR_HANDLING=1
[2024-08-13 05:55:09,922] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]}
[2024-08-13 05:55:09,922] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=8, node_rank=0
[2024-08-13 05:55:09,922] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]})
[2024-08-13 05:55:09,922] [INFO] [launch.py:164:main] dist_world_size=8
[2024-08-13 05:55:09,923] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
[2024-08-13 05:55:09,923] [INFO] [launch.py:256:main] process 3620551 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=0', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Llama-2-7b-hf', '--data_path', '../data/stanford_alpaca/roleplay_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_Llama2-7b_roleplay_Lora_tuned/', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1', '--use_peft', 'True', '--lora_r', '16', '--lora_alpha', '32']
[2024-08-13 05:55:09,924] [INFO] [launch.py:256:main] process 3620552 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=1', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Llama-2-7b-hf', '--data_path', '../data/stanford_alpaca/roleplay_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_Llama2-7b_roleplay_Lora_tuned/', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1', '--use_peft', 'True', '--lora_r', '16', '--lora_alpha', '32']
[2024-08-13 05:55:09,924] [INFO] [launch.py:256:main] process 3620553 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=2', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Llama-2-7b-hf', '--data_path', '../data/stanford_alpaca/roleplay_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_Llama2-7b_roleplay_Lora_tuned/', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1', '--use_peft', 'True', '--lora_r', '16', '--lora_alpha', '32']
[2024-08-13 05:55:09,925] [INFO] [launch.py:256:main] process 3620554 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=3', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Llama-2-7b-hf', '--data_path', '../data/stanford_alpaca/roleplay_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_Llama2-7b_roleplay_Lora_tuned/', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1', '--use_peft', 'True', '--lora_r', '16', '--lora_alpha', '32']
[2024-08-13 05:55:09,926] [INFO] [launch.py:256:main] process 3620555 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=4', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Llama-2-7b-hf', '--data_path', '../data/stanford_alpaca/roleplay_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_Llama2-7b_roleplay_Lora_tuned/', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1', '--use_peft', 'True', '--lora_r', '16', '--lora_alpha', '32']
[2024-08-13 05:55:09,926] [INFO] [launch.py:256:main] process 3620556 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=5', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Llama-2-7b-hf', '--data_path', '../data/stanford_alpaca/roleplay_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_Llama2-7b_roleplay_Lora_tuned/', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1', '--use_peft', 'True', '--lora_r', '16', '--lora_alpha', '32']
[2024-08-13 05:55:09,927] [INFO] [launch.py:256:main] process 3620557 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=6', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Llama-2-7b-hf', '--data_path', '../data/stanford_alpaca/roleplay_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_Llama2-7b_roleplay_Lora_tuned/', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1', '--use_peft', 'True', '--lora_r', '16', '--lora_alpha', '32']
[2024-08-13 05:55:09,927] [INFO] [launch.py:256:main] process 3620558 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=7', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Llama-2-7b-hf', '--data_path', '../data/stanford_alpaca/roleplay_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_Llama2-7b_roleplay_Lora_tuned/', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1', '--use_peft', 'True', '--lora_r', '16', '--lora_alpha', '32']
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
[2024-08-13 05:55:25,761] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-13 05:55:25,773] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
[2024-08-13 05:55:25,836] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
[2024-08-13 05:55:25,978] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-13 05:55:25,979] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-13 05:55:25,982] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-13 05:55:25,983] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[2024-08-13 05:55:26,011] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible

[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2024-08-13 05:55:26,554] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-08-13 05:55:26,557] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-08-13 05:55:26,590] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-08-13 05:55:26,702] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-08-13 05:55:26,702] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-08-13 05:55:26,712] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-08-13 05:55:26,716] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-08-13 05:55:26,716] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-08-13 05:55:26,768] [INFO] [comm.py:637:init_distributed] cdb=None
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 2/2 [00:00<00:00, 530.52it/s]
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 2/2 [00:00<00:00, 1421.80it/s]
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 2/2 [00:00<00:00, 1389.99it/s]
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 2/2 [00:00<00:00, 1445.07it/s]
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 2/2 [00:00<00:00, 1496.36it/s]
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 2/2 [00:00<00:00, 1228.74it/s]
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 2/2 [00:00<00:00, 1254.84it/s]
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 2/2 [00:00<00:00, 1243.68it/s]
[2024-08-13 05:55:37,792] [INFO] [partition_parameters.py:345:__exit__] finished initializing model - num_params = 291, num_elems = 6.74B
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:27<00:27, 27.31s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:27<00:27, 27.32s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:27<00:27, 27.32s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:27<00:27, 27.32s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:27<00:27, 27.33s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:27<00:27, 27.32s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:27<00:27, 27.32s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:44<00:00, 21.29s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:44<00:00, 21.29s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:44<00:00, 21.30s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:44<00:00, 21.30s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:44<00:00, 22.20s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:44<00:00, 22.20s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:44<00:00, 22.20s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:44<00:00, 22.20s/it]



Loading checkpoint shards: 100%|██████████| 2/2 [00:44<00:00, 21.30s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:44<00:00, 22.20s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:44<00:00, 21.30s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:44<00:00, 22.20s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:44<00:00, 21.32s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:44<00:00, 22.22s/it]
enable_input_require_grads!
enable_input_require_grads!
enable_input_require_grads!enable_input_require_grads!

enable_input_require_grads!
enable_input_require_grads!
enable_input_require_grads!
Loading checkpoint shards:  50%|█████     | 1/2 [00:50<00:50, 50.85s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:09<00:00, 31.74s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:09<00:00, 34.62s/it]
enable_input_require_grads!
trainable params: 8,388,608 || all params: 6,746,804,224 || trainable%: 0.1243
Finetune with LORA setting:  None
trainable params: 8,388,608 || all params: 6,746,804,224 || trainable%: 0.1243
Finetune with LORA setting:  None
trainable params: 8,388,608 || all params: 6,746,804,224 || trainable%: 0.1243
Finetune with LORA setting:  None
trainable params: 8,388,608 || all params: 6,746,804,224 || trainable%: 0.1243
Finetune with LORA setting:  None
trainable params: 8,388,608 || all params: 6,746,804,224 || trainable%: 0.1243
Finetune with LORA setting:  None
trainable params: 8,388,608 || all params: 6,746,804,224 || trainable%: 0.1243
Finetune with LORA setting:  None
trainable params: 8,388,608 || all params: 6,746,804,224 || trainable%: 0.1243
Finetune with LORA setting:  None
trainable params: 8,388,608 || all params: 6,746,804,224 || trainable%: 0.1243
Finetune with LORA setting:  None
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
[rank1]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[rank7]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[rank5]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...


[rank6]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
[rank3]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
[rank2]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
[rank4]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
[rank0]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121/fused_adam/build.ninja...
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/cpp_extension.py:1967: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Loading extension module fused_adam...
Loading extension module fused_adam...
Loading extension module fused_adam...
Loading extension module fused_adam...
Loading extension module fused_adam...
Loading extension module fused_adam...
Loading extension module fused_adam...
Time to load fused_adam op: 2.8578667640686035 secondsTime to load fused_adam op: 2.9004876613616943 secondsTime to load fused_adam op: 2.7805707454681396 seconds
Time to load fused_adam op: 2.8817362785339355 secondsTime to load fused_adam op: 2.802703857421875 seconds
Time to load fused_adam op: 2.8229663372039795 seconds
Time to load fused_adam op: 2.9005422592163086 secondsTime to load fused_adam op: 2.9005818367004395 seconds




Parameter Offload: Total persistent parameters: 8654848 in 193 params
  0%|          | 0/270 [00:00<?, ?it/s]/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
  0%|          | 1/270 [00:08<37:23,  8.34s/it]                                               {'loss': 1.8355, 'grad_norm': 0.7092646553125611, 'learning_rate': 0.0, 'epoch': 0.01}
  0%|          | 1/270 [00:08<37:23,  8.34s/it]  1%|          | 2/270 [00:10<20:17,  4.54s/it]                                               {'loss': 1.8292, 'grad_norm': 0.7962885792578163, 'learning_rate': 6.309297535714572e-07, 'epoch': 0.02}
  1%|          | 2/270 [00:10<20:17,  4.54s/it]  1%|          | 3/270 [00:10<12:32,  2.82s/it]                                               {'loss': 1.8467, 'grad_norm': 0.7282015165538579, 'learning_rate': 1e-06, 'epoch': 0.03}
  1%|          | 3/270 [00:10<12:32,  2.82s/it]  1%|▏         | 4/270 [00:11<08:54,  2.01s/it]                                               {'loss': 1.795, 'grad_norm': 0.6866622097621278, 'learning_rate': 1.2618595071429145e-06, 'epoch': 0.04}
  1%|▏         | 4/270 [00:11<08:54,  2.01s/it]  2%|▏         | 5/270 [00:12<06:55,  1.57s/it]                                               {'loss': 1.8529, 'grad_norm': 0.8563036248629932, 'learning_rate': 1.4649735207179267e-06, 'epoch': 0.06}
  2%|▏         | 5/270 [00:12<06:55,  1.57s/it]  2%|▏         | 6/270 [00:13<05:38,  1.28s/it]                                               {'loss': 1.9116, 'grad_norm': 0.9250094476055877, 'learning_rate': 1.6309297535714573e-06, 'epoch': 0.07}
  2%|▏         | 6/270 [00:13<05:38,  1.28s/it]  3%|▎         | 7/270 [00:14<04:57,  1.13s/it]                                               {'loss': 1.7652, 'grad_norm': 0.7106113282081027, 'learning_rate': 1.771243749161422e-06, 'epoch': 0.08}
  3%|▎         | 7/270 [00:14<04:57,  1.13s/it]  3%|▎         | 8/270 [00:14<04:29,  1.03s/it]                                               {'loss': 1.877, 'grad_norm': 0.8050101784980035, 'learning_rate': 1.8927892607143718e-06, 'epoch': 0.09}
  3%|▎         | 8/270 [00:14<04:29,  1.03s/it]  3%|▎         | 9/270 [00:15<04:11,  1.04it/s]                                               {'loss': 1.8342, 'grad_norm': 0.8027008314042141, 'learning_rate': 2e-06, 'epoch': 0.1}
  3%|▎         | 9/270 [00:15<04:11,  1.04it/s]  4%|▎         | 10/270 [00:16<03:53,  1.11it/s]                                                {'loss': 1.847, 'grad_norm': 0.7469372491738622, 'learning_rate': 2e-06, 'epoch': 0.11}
  4%|▎         | 10/270 [00:16<03:53,  1.11it/s]  4%|▍         | 11/270 [00:17<03:45,  1.15it/s]                                                {'loss': 1.8387, 'grad_norm': 0.7915916104041942, 'learning_rate': 1.9923371647509575e-06, 'epoch': 0.12}
  4%|▍         | 11/270 [00:17<03:45,  1.15it/s]  4%|▍         | 12/270 [00:18<03:36,  1.19it/s]                                                {'loss': 1.8271, 'grad_norm': 0.7444796591623473, 'learning_rate': 1.984674329501916e-06, 'epoch': 0.13}
  4%|▍         | 12/270 [00:18<03:36,  1.19it/s]  5%|▍         | 13/270 [00:18<03:28,  1.23it/s]                                                {'loss': 1.8636, 'grad_norm': 0.850990742897546, 'learning_rate': 1.9770114942528735e-06, 'epoch': 0.14}
  5%|▍         | 13/270 [00:18<03:28,  1.23it/s]  5%|▌         | 14/270 [00:19<03:26,  1.24it/s]                                                {'loss': 1.8303, 'grad_norm': 0.8460039478058238, 'learning_rate': 1.9693486590038315e-06, 'epoch': 0.16}
  5%|▌         | 14/270 [00:19<03:26,  1.24it/s]  6%|▌         | 15/270 [00:20<03:22,  1.26it/s]                                                {'loss': 1.8773, 'grad_norm': 0.7589480295345479, 'learning_rate': 1.961685823754789e-06, 'epoch': 0.17}
  6%|▌         | 15/270 [00:20<03:22,  1.26it/s]  6%|▌         | 16/270 [00:21<03:20,  1.27it/s]                                                {'loss': 1.7551, 'grad_norm': 0.7025876125580652, 'learning_rate': 1.954022988505747e-06, 'epoch': 0.18}
  6%|▌         | 16/270 [00:21<03:20,  1.27it/s]  6%|▋         | 17/270 [00:21<03:17,  1.28it/s]                                                {'loss': 1.7528, 'grad_norm': 0.7204399905400669, 'learning_rate': 1.946360153256705e-06, 'epoch': 0.19}
  6%|▋         | 17/270 [00:21<03:17,  1.28it/s]  7%|▋         | 18/270 [00:22<03:18,  1.27it/s]                                                {'loss': 1.8291, 'grad_norm': 0.7810727083316433, 'learning_rate': 1.9386973180076627e-06, 'epoch': 0.2}
  7%|▋         | 18/270 [00:22<03:18,  1.27it/s]  7%|▋         | 19/270 [00:23<03:12,  1.30it/s]                                                {'loss': 1.8475, 'grad_norm': 0.8509907287388913, 'learning_rate': 1.9310344827586207e-06, 'epoch': 0.21}
  7%|▋         | 19/270 [00:23<03:12,  1.30it/s]  7%|▋         | 20/270 [00:24<03:09,  1.32it/s]                                                {'loss': 1.8819, 'grad_norm': 0.7955807505778252, 'learning_rate': 1.9233716475095787e-06, 'epoch': 0.22}
  7%|▋         | 20/270 [00:24<03:09,  1.32it/s]  8%|▊         | 21/270 [00:24<03:09,  1.32it/s]                                                {'loss': 1.862, 'grad_norm': 0.760688813684456, 'learning_rate': 1.9157088122605362e-06, 'epoch': 0.23}
  8%|▊         | 21/270 [00:24<03:09,  1.32it/s]  8%|▊         | 22/270 [00:25<03:09,  1.31it/s]                                                {'loss': 1.7782, 'grad_norm': 0.6464143507397838, 'learning_rate': 1.9080459770114942e-06, 'epoch': 0.24}
  8%|▊         | 22/270 [00:25<03:09,  1.31it/s]  9%|▊         | 23/270 [00:26<03:06,  1.32it/s]                                                {'loss': 1.8631, 'grad_norm': 0.8814479363636057, 'learning_rate': 1.900383141762452e-06, 'epoch': 0.26}
  9%|▊         | 23/270 [00:26<03:06,  1.32it/s]  9%|▉         | 24/270 [00:27<03:09,  1.30it/s]                                                {'loss': 1.8121, 'grad_norm': 0.6874085912192066, 'learning_rate': 1.8927203065134098e-06, 'epoch': 0.27}
  9%|▉         | 24/270 [00:27<03:09,  1.30it/s]  9%|▉         | 25/270 [00:28<03:11,  1.28it/s]                                                {'loss': 1.8649, 'grad_norm': 0.7012248804281647, 'learning_rate': 1.8850574712643676e-06, 'epoch': 0.28}
  9%|▉         | 25/270 [00:28<03:11,  1.28it/s] 10%|▉         | 26/270 [00:28<03:13,  1.26it/s]                                                {'loss': 1.707, 'grad_norm': 0.7155782025964512, 'learning_rate': 1.8773946360153256e-06, 'epoch': 0.29}
 10%|▉         | 26/270 [00:28<03:13,  1.26it/s] 10%|█         | 27/270 [00:29<03:09,  1.28it/s]                                                {'loss': 1.8572, 'grad_norm': 0.7665183980424102, 'learning_rate': 1.8697318007662834e-06, 'epoch': 0.3}
 10%|█         | 27/270 [00:29<03:09,  1.28it/s] 10%|█         | 28/270 [00:30<03:05,  1.31it/s]                                                {'loss': 1.8729, 'grad_norm': 0.7732184870773587, 'learning_rate': 1.8620689655172412e-06, 'epoch': 0.31}
 10%|█         | 28/270 [00:30<03:05,  1.31it/s] 11%|█         | 29/270 [00:31<03:06,  1.29it/s]                                                {'loss': 1.8996, 'grad_norm': 0.7180497528405754, 'learning_rate': 1.8544061302681992e-06, 'epoch': 0.32}
 11%|█         | 29/270 [00:31<03:06,  1.29it/s] 11%|█         | 30/270 [00:31<03:05,  1.30it/s]                                                {'loss': 1.7136, 'grad_norm': 0.7151875956879264, 'learning_rate': 1.846743295019157e-06, 'epoch': 0.33}
 11%|█         | 30/270 [00:31<03:05,  1.30it/s] 11%|█▏        | 31/270 [00:32<03:04,  1.29it/s]                                                {'loss': 1.7843, 'grad_norm': 0.7500627257148975, 'learning_rate': 1.8390804597701148e-06, 'epoch': 0.34}
 11%|█▏        | 31/270 [00:32<03:04,  1.29it/s] 12%|█▏        | 32/270 [00:33<03:01,  1.31it/s]                                                {'loss': 1.8357, 'grad_norm': 0.7901613817530659, 'learning_rate': 1.8314176245210726e-06, 'epoch': 0.36}
 12%|█▏        | 32/270 [00:33<03:01,  1.31it/s] 12%|█▏        | 33/270 [00:34<03:02,  1.30it/s]                                                {'loss': 1.8573, 'grad_norm': 0.7247516708787713, 'learning_rate': 1.8237547892720306e-06, 'epoch': 0.37}
 12%|█▏        | 33/270 [00:34<03:02,  1.30it/s] 13%|█▎        | 34/270 [00:34<03:00,  1.31it/s]                                                {'loss': 1.8789, 'grad_norm': 0.8869846309366538, 'learning_rate': 1.8160919540229884e-06, 'epoch': 0.38}
 13%|█▎        | 34/270 [00:34<03:00,  1.31it/s] 13%|█▎        | 35/270 [00:35<03:00,  1.30it/s]                                                {'loss': 1.8094, 'grad_norm': 0.7240370459005381, 'learning_rate': 1.8084291187739462e-06, 'epoch': 0.39}
 13%|█▎        | 35/270 [00:35<03:00,  1.30it/s] 13%|█▎        | 36/270 [00:36<03:01,  1.29it/s]                                                {'loss': 1.8127, 'grad_norm': 0.758154373636755, 'learning_rate': 1.8007662835249042e-06, 'epoch': 0.4}
 13%|█▎        | 36/270 [00:36<03:01,  1.29it/s] 14%|█▎        | 37/270 [00:37<03:03,  1.27it/s]                                                {'loss': 1.7745, 'grad_norm': 0.6745393088849277, 'learning_rate': 1.793103448275862e-06, 'epoch': 0.41}
 14%|█▎        | 37/270 [00:37<03:03,  1.27it/s] 14%|█▍        | 38/270 [00:38<03:01,  1.28it/s]                                                {'loss': 1.762, 'grad_norm': 0.6956907916886815, 'learning_rate': 1.7854406130268197e-06, 'epoch': 0.42}
 14%|█▍        | 38/270 [00:38<03:01,  1.28it/s] 14%|█▍        | 39/270 [00:38<02:57,  1.30it/s]                                                {'loss': 1.7724, 'grad_norm': 0.7011074237614985, 'learning_rate': 1.7777777777777775e-06, 'epoch': 0.43}
 14%|█▍        | 39/270 [00:38<02:57,  1.30it/s] 15%|█▍        | 40/270 [00:39<02:55,  1.31it/s]                                                {'loss': 1.8313, 'grad_norm': 0.7369808572040866, 'learning_rate': 1.7701149425287355e-06, 'epoch': 0.44}
 15%|█▍        | 40/270 [00:39<02:55,  1.31it/s] 15%|█▌        | 41/270 [00:40<02:54,  1.31it/s]                                                {'loss': 1.7649, 'grad_norm': 0.7367449633331704, 'learning_rate': 1.7624521072796933e-06, 'epoch': 0.46}
 15%|█▌        | 41/270 [00:40<02:54,  1.31it/s] 16%|█▌        | 42/270 [00:41<02:55,  1.30it/s]                                                {'loss': 1.8476, 'grad_norm': 0.7485977312101899, 'learning_rate': 1.7547892720306511e-06, 'epoch': 0.47}
 16%|█▌        | 42/270 [00:41<02:55,  1.30it/s] 16%|█▌        | 43/270 [00:41<02:56,  1.28it/s]                                                {'loss': 1.8319, 'grad_norm': 0.6512434489164561, 'learning_rate': 1.7471264367816091e-06, 'epoch': 0.48}
 16%|█▌        | 43/270 [00:41<02:56,  1.28it/s] 16%|█▋        | 44/270 [00:42<02:54,  1.29it/s]                                                {'loss': 1.8438, 'grad_norm': 0.7413143286871257, 'learning_rate': 1.739463601532567e-06, 'epoch': 0.49}
 16%|█▋        | 44/270 [00:42<02:54,  1.29it/s] 17%|█▋        | 45/270 [00:43<02:54,  1.29it/s]                                                {'loss': 1.7758, 'grad_norm': 0.6687810977520289, 'learning_rate': 1.7318007662835247e-06, 'epoch': 0.5}
 17%|█▋        | 45/270 [00:43<02:54,  1.29it/s] 17%|█▋        | 46/270 [00:44<02:55,  1.28it/s]                                                {'loss': 1.8804, 'grad_norm': 0.721187490557127, 'learning_rate': 1.7241379310344825e-06, 'epoch': 0.51}
 17%|█▋        | 46/270 [00:44<02:55,  1.28it/s] 17%|█▋        | 47/270 [00:45<02:51,  1.30it/s]                                                {'loss': 1.8261, 'grad_norm': 0.7784307963172772, 'learning_rate': 1.7164750957854405e-06, 'epoch': 0.52}
 17%|█▋        | 47/270 [00:45<02:51,  1.30it/s] 18%|█▊        | 48/270 [00:45<02:48,  1.32it/s]                                                {'loss': 1.8842, 'grad_norm': 0.7320190562150208, 'learning_rate': 1.7088122605363983e-06, 'epoch': 0.53}
 18%|█▊        | 48/270 [00:45<02:48,  1.32it/s] 18%|█▊        | 49/270 [00:46<02:49,  1.31it/s]                                                {'loss': 1.8053, 'grad_norm': 0.7097354946729661, 'learning_rate': 1.701149425287356e-06, 'epoch': 0.54}
 18%|█▊        | 49/270 [00:46<02:49,  1.31it/s] 19%|█▊        | 50/270 [00:47<02:45,  1.33it/s]                                                {'loss': 1.9063, 'grad_norm': 0.8752827568269642, 'learning_rate': 1.693486590038314e-06, 'epoch': 0.56}
 19%|█▊        | 50/270 [00:47<02:45,  1.33it/s] 19%|█▉        | 51/270 [00:48<02:47,  1.31it/s]                                                {'loss': 1.8202, 'grad_norm': 0.8018161593524288, 'learning_rate': 1.6858237547892719e-06, 'epoch': 0.57}
 19%|█▉        | 51/270 [00:48<02:47,  1.31it/s] 19%|█▉        | 52/270 [00:48<02:45,  1.32it/s]                                                {'loss': 1.8082, 'grad_norm': 0.759996253890709, 'learning_rate': 1.6781609195402297e-06, 'epoch': 0.58}
 19%|█▉        | 52/270 [00:48<02:45,  1.32it/s] 20%|█▉        | 53/270 [00:49<02:42,  1.33it/s]                                                {'loss': 1.8881, 'grad_norm': 0.7638603066380969, 'learning_rate': 1.6704980842911879e-06, 'epoch': 0.59}
 20%|█▉        | 53/270 [00:49<02:42,  1.33it/s] 20%|██        | 54/270 [00:50<02:41,  1.33it/s]                                                {'loss': 1.8179, 'grad_norm': 0.7748278802436225, 'learning_rate': 1.6628352490421457e-06, 'epoch': 0.6}
 20%|██        | 54/270 [00:50<02:41,  1.33it/s] 20%|██        | 55/270 [00:51<02:42,  1.32it/s]                                                {'loss': 1.8397, 'grad_norm': 0.7769270737457006, 'learning_rate': 1.6551724137931035e-06, 'epoch': 0.61}
 20%|██        | 55/270 [00:51<02:42,  1.32it/s] 21%|██        | 56/270 [00:51<02:41,  1.33it/s]                                                {'loss': 1.8502, 'grad_norm': 0.7865270203841072, 'learning_rate': 1.6475095785440612e-06, 'epoch': 0.62}
 21%|██        | 56/270 [00:51<02:41,  1.33it/s] 21%|██        | 57/270 [00:52<02:40,  1.33it/s]                                                {'loss': 1.8133, 'grad_norm': 0.7398084606620016, 'learning_rate': 1.6398467432950192e-06, 'epoch': 0.63}
 21%|██        | 57/270 [00:52<02:40,  1.33it/s] 21%|██▏       | 58/270 [00:53<02:42,  1.31it/s]                                                {'loss': 1.7945, 'grad_norm': 0.79715423657214, 'learning_rate': 1.632183908045977e-06, 'epoch': 0.64}
 21%|██▏       | 58/270 [00:53<02:42,  1.31it/s] 22%|██▏       | 59/270 [00:54<02:41,  1.31it/s]                                                {'loss': 1.8269, 'grad_norm': 0.7202520630638565, 'learning_rate': 1.6245210727969348e-06, 'epoch': 0.66}
 22%|██▏       | 59/270 [00:54<02:41,  1.31it/s] 22%|██▏       | 60/270 [00:54<02:42,  1.29it/s]                                                {'loss': 1.7325, 'grad_norm': 0.7914275059624524, 'learning_rate': 1.6168582375478928e-06, 'epoch': 0.67}
 22%|██▏       | 60/270 [00:54<02:42,  1.29it/s] 23%|██▎       | 61/270 [00:55<02:41,  1.29it/s]                                                {'loss': 1.7799, 'grad_norm': 0.7018107657928095, 'learning_rate': 1.6091954022988506e-06, 'epoch': 0.68}
 23%|██▎       | 61/270 [00:55<02:41,  1.29it/s] 23%|██▎       | 62/270 [00:56<02:42,  1.28it/s]                                                {'loss': 1.7049, 'grad_norm': 0.6985280119222774, 'learning_rate': 1.6015325670498084e-06, 'epoch': 0.69}
 23%|██▎       | 62/270 [00:56<02:42,  1.28it/s] 23%|██▎       | 63/270 [00:57<02:38,  1.30it/s]                                                {'loss': 1.8676, 'grad_norm': 0.8483259881657782, 'learning_rate': 1.5938697318007662e-06, 'epoch': 0.7}
 23%|██▎       | 63/270 [00:57<02:38,  1.30it/s] 24%|██▎       | 64/270 [00:57<02:37,  1.31it/s]                                                {'loss': 1.8478, 'grad_norm': 0.7655060561454253, 'learning_rate': 1.5862068965517242e-06, 'epoch': 0.71}
 24%|██▎       | 64/270 [00:57<02:37,  1.31it/s] 24%|██▍       | 65/270 [00:58<02:37,  1.30it/s]                                                {'loss': 1.7821, 'grad_norm': 0.7769245064515089, 'learning_rate': 1.578544061302682e-06, 'epoch': 0.72}
 24%|██▍       | 65/270 [00:58<02:37,  1.30it/s] 24%|██▍       | 66/270 [00:59<02:36,  1.31it/s]                                                {'loss': 1.8009, 'grad_norm': 0.7868235123017133, 'learning_rate': 1.5708812260536398e-06, 'epoch': 0.73}
 24%|██▍       | 66/270 [00:59<02:36,  1.31it/s] 25%|██▍       | 67/270 [01:00<02:36,  1.30it/s]                                                {'loss': 1.7934, 'grad_norm': 0.7610722447430757, 'learning_rate': 1.5632183908045978e-06, 'epoch': 0.74}
 25%|██▍       | 67/270 [01:00<02:36,  1.30it/s] 25%|██▌       | 68/270 [01:01<02:36,  1.29it/s]                                                {'loss': 1.7666, 'grad_norm': 0.6822110286031027, 'learning_rate': 1.5555555555555556e-06, 'epoch': 0.76}
 25%|██▌       | 68/270 [01:01<02:36,  1.29it/s] 26%|██▌       | 69/270 [01:01<02:32,  1.31it/s]                                                {'loss': 1.9271, 'grad_norm': 0.8510384600015257, 'learning_rate': 1.5478927203065134e-06, 'epoch': 0.77}
 26%|██▌       | 69/270 [01:01<02:32,  1.31it/s] 26%|██▌       | 70/270 [01:02<02:30,  1.33it/s]                                                {'loss': 1.7557, 'grad_norm': 0.7390135436180345, 'learning_rate': 1.5402298850574712e-06, 'epoch': 0.78}
 26%|██▌       | 70/270 [01:02<02:30,  1.33it/s] 26%|██▋       | 71/270 [01:03<02:29,  1.33it/s]                                                {'loss': 1.8369, 'grad_norm': 0.7943020169760178, 'learning_rate': 1.5325670498084292e-06, 'epoch': 0.79}
 26%|██▋       | 71/270 [01:03<02:29,  1.33it/s] 27%|██▋       | 72/270 [01:03<02:27,  1.34it/s]                                                {'loss': 1.8229, 'grad_norm': 0.6774147672751284, 'learning_rate': 1.524904214559387e-06, 'epoch': 0.8}
 27%|██▋       | 72/270 [01:04<02:27,  1.34it/s] 27%|██▋       | 73/270 [01:04<02:27,  1.33it/s]                                                {'loss': 1.7779, 'grad_norm': 0.6818034494976652, 'learning_rate': 1.5172413793103447e-06, 'epoch': 0.81}
 27%|██▋       | 73/270 [01:04<02:27,  1.33it/s] 27%|██▋       | 74/270 [01:05<02:25,  1.35it/s]                                                {'loss': 1.7713, 'grad_norm': 0.7949846618291437, 'learning_rate': 1.5095785440613027e-06, 'epoch': 0.82}
 27%|██▋       | 74/270 [01:05<02:25,  1.35it/s] 28%|██▊       | 75/270 [01:06<02:26,  1.33it/s]                                                {'loss': 1.8026, 'grad_norm': 0.6647081006424058, 'learning_rate': 1.5019157088122605e-06, 'epoch': 0.83}
 28%|██▊       | 75/270 [01:06<02:26,  1.33it/s] 28%|██▊       | 76/270 [01:07<02:28,  1.31it/s]                                                {'loss': 1.8113, 'grad_norm': 0.7547044693351879, 'learning_rate': 1.4942528735632183e-06, 'epoch': 0.84}
 28%|██▊       | 76/270 [01:07<02:28,  1.31it/s] 29%|██▊       | 77/270 [01:07<02:27,  1.31it/s]                                                {'loss': 1.7593, 'grad_norm': 0.7179994117101581, 'learning_rate': 1.4865900383141763e-06, 'epoch': 0.86}
 29%|██▊       | 77/270 [01:07<02:27,  1.31it/s] 29%|██▉       | 78/270 [01:08<02:25,  1.32it/s]                                                {'loss': 1.8103, 'grad_norm': 0.7602279219233588, 'learning_rate': 1.4789272030651341e-06, 'epoch': 0.87}
 29%|██▉       | 78/270 [01:08<02:25,  1.32it/s] 29%|██▉       | 79/270 [01:09<02:23,  1.33it/s]                                                {'loss': 1.8374, 'grad_norm': 0.7792290193123927, 'learning_rate': 1.471264367816092e-06, 'epoch': 0.88}
 29%|██▉       | 79/270 [01:09<02:23,  1.33it/s] 30%|██▉       | 80/270 [01:10<02:24,  1.32it/s]                                                {'loss': 1.8241, 'grad_norm': 0.789016373553744, 'learning_rate': 1.4636015325670497e-06, 'epoch': 0.89}
 30%|██▉       | 80/270 [01:10<02:24,  1.32it/s] 30%|███       | 81/270 [01:10<02:23,  1.32it/s]                                                {'loss': 1.9377, 'grad_norm': 0.7319096447874054, 'learning_rate': 1.4559386973180077e-06, 'epoch': 0.9}
 30%|███       | 81/270 [01:10<02:23,  1.32it/s] 30%|███       | 82/270 [01:11<02:22,  1.32it/s]                                                {'loss': 1.7786, 'grad_norm': 0.68012636919804, 'learning_rate': 1.4482758620689655e-06, 'epoch': 0.91}
 30%|███       | 82/270 [01:11<02:22,  1.32it/s] 31%|███       | 83/270 [01:12<02:22,  1.32it/s]                                                {'loss': 1.8001, 'grad_norm': 0.7465962384038645, 'learning_rate': 1.4406130268199233e-06, 'epoch': 0.92}
 31%|███       | 83/270 [01:12<02:22,  1.32it/s] 31%|███       | 84/270 [01:13<02:21,  1.31it/s]                                                {'loss': 1.8054, 'grad_norm': 0.7712971649261459, 'learning_rate': 1.4329501915708813e-06, 'epoch': 0.93}
 31%|███       | 84/270 [01:13<02:21,  1.31it/s] 31%|███▏      | 85/270 [01:13<02:23,  1.29it/s]                                                {'loss': 1.7126, 'grad_norm': 0.6918261094553404, 'learning_rate': 1.425287356321839e-06, 'epoch': 0.94}
 31%|███▏      | 85/270 [01:13<02:23,  1.29it/s] 32%|███▏      | 86/270 [01:14<02:23,  1.28it/s]                                                {'loss': 1.8915, 'grad_norm': 0.7105681948413007, 'learning_rate': 1.4176245210727969e-06, 'epoch': 0.96}
 32%|███▏      | 86/270 [01:14<02:23,  1.28it/s] 32%|███▏      | 87/270 [01:15<02:22,  1.29it/s]                                                {'loss': 1.8529, 'grad_norm': 0.7898251986071794, 'learning_rate': 1.4099616858237547e-06, 'epoch': 0.97}
 32%|███▏      | 87/270 [01:15<02:22,  1.29it/s] 33%|███▎      | 88/270 [01:16<02:20,  1.30it/s]                                                {'loss': 1.8111, 'grad_norm': 0.6935004555719904, 'learning_rate': 1.4022988505747127e-06, 'epoch': 0.98}
 33%|███▎      | 88/270 [01:16<02:20,  1.30it/s] 33%|███▎      | 89/270 [01:17<02:21,  1.28it/s]                                                {'loss': 1.7674, 'grad_norm': 0.6522756125137941, 'learning_rate': 1.3946360153256705e-06, 'epoch': 0.99}
 33%|███▎      | 89/270 [01:17<02:21,  1.28it/s] 33%|███▎      | 90/270 [01:17<02:22,  1.27it/s]                                                {'loss': 1.7786, 'grad_norm': 0.7160721740594291, 'learning_rate': 1.3869731800766282e-06, 'epoch': 1.0}
 33%|███▎      | 90/270 [01:17<02:22,  1.27it/s] 34%|███▎      | 91/270 [01:18<02:19,  1.29it/s]                                                {'loss': 1.8784, 'grad_norm': 0.7773147633249241, 'learning_rate': 1.3793103448275862e-06, 'epoch': 1.01}
 34%|███▎      | 91/270 [01:18<02:19,  1.29it/s] 34%|███▍      | 92/270 [01:19<02:17,  1.30it/s]                                                {'loss': 1.7628, 'grad_norm': 0.7319884404060378, 'learning_rate': 1.371647509578544e-06, 'epoch': 1.02}
 34%|███▍      | 92/270 [01:19<02:17,  1.30it/s] 34%|███▍      | 93/270 [01:20<02:17,  1.29it/s]                                                {'loss': 1.7208, 'grad_norm': 0.6738266267507316, 'learning_rate': 1.3639846743295018e-06, 'epoch': 1.03}
 34%|███▍      | 93/270 [01:20<02:17,  1.29it/s] 35%|███▍      | 94/270 [01:20<02:13,  1.31it/s]                                                {'loss': 1.8768, 'grad_norm': 0.8650304914092375, 'learning_rate': 1.3563218390804596e-06, 'epoch': 1.04}
 35%|███▍      | 94/270 [01:20<02:13,  1.31it/s] 35%|███▌      | 95/270 [01:21<02:15,  1.29it/s]                                                {'loss': 1.831, 'grad_norm': 0.7428680531131359, 'learning_rate': 1.3486590038314176e-06, 'epoch': 1.06}
 35%|███▌      | 95/270 [01:21<02:15,  1.29it/s] 36%|███▌      | 96/270 [01:22<02:16,  1.28it/s]                                                {'loss': 1.77, 'grad_norm': 0.7024150964929161, 'learning_rate': 1.3409961685823754e-06, 'epoch': 1.07}
 36%|███▌      | 96/270 [01:22<02:16,  1.28it/s] 36%|███▌      | 97/270 [01:23<02:14,  1.28it/s]                                                {'loss': 1.7897, 'grad_norm': 0.6902501799693774, 'learning_rate': 1.3333333333333332e-06, 'epoch': 1.08}
 36%|███▌      | 97/270 [01:23<02:14,  1.28it/s] 36%|███▋      | 98/270 [01:23<02:11,  1.31it/s]                                                {'loss': 1.7969, 'grad_norm': 0.6738264928896296, 'learning_rate': 1.3256704980842912e-06, 'epoch': 1.09}
 36%|███▋      | 98/270 [01:23<02:11,  1.31it/s] 37%|███▋      | 99/270 [01:24<02:09,  1.32it/s]                                                {'loss': 1.6881, 'grad_norm': 0.7742419046377095, 'learning_rate': 1.318007662835249e-06, 'epoch': 1.1}
 37%|███▋      | 99/270 [01:24<02:09,  1.32it/s] 37%|███▋      | 100/270 [01:25<02:10,  1.31it/s]                                                 {'loss': 1.7497, 'grad_norm': 0.7174491108179848, 'learning_rate': 1.3103448275862068e-06, 'epoch': 1.11}
 37%|███▋      | 100/270 [01:25<02:10,  1.31it/s] 37%|███▋      | 101/270 [01:26<02:10,  1.29it/s]                                                 {'loss': 1.7967, 'grad_norm': 0.730753544967447, 'learning_rate': 1.3026819923371646e-06, 'epoch': 1.12}
 37%|███▋      | 101/270 [01:26<02:10,  1.29it/s] 38%|███▊      | 102/270 [01:27<02:09,  1.30it/s]                                                 {'loss': 1.7845, 'grad_norm': 0.6881142642926249, 'learning_rate': 1.2950191570881226e-06, 'epoch': 1.13}
 38%|███▊      | 102/270 [01:27<02:09,  1.30it/s] 38%|███▊      | 103/270 [01:27<02:09,  1.29it/s]                                                 {'loss': 1.8107, 'grad_norm': 0.7176971716402408, 'learning_rate': 1.2873563218390804e-06, 'epoch': 1.14}
 38%|███▊      | 103/270 [01:27<02:09,  1.29it/s] 39%|███▊      | 104/270 [01:28<02:09,  1.28it/s]                                                 {'loss': 1.8338, 'grad_norm': 0.6693432747878721, 'learning_rate': 1.2796934865900382e-06, 'epoch': 1.16}
 39%|███▊      | 104/270 [01:28<02:09,  1.28it/s] 39%|███▉      | 105/270 [01:29<02:09,  1.27it/s]                                                 {'loss': 1.7191, 'grad_norm': 0.7024441080448639, 'learning_rate': 1.2720306513409962e-06, 'epoch': 1.17}
 39%|███▉      | 105/270 [01:29<02:09,  1.27it/s] 39%|███▉      | 106/270 [01:30<02:09,  1.26it/s]                                                 {'loss': 1.7779, 'grad_norm': 0.6919304404739057, 'learning_rate': 1.264367816091954e-06, 'epoch': 1.18}
 39%|███▉      | 106/270 [01:30<02:09,  1.26it/s] 40%|███▉      | 107/270 [01:31<02:09,  1.26it/s]                                                 {'loss': 1.811, 'grad_norm': 0.7459054405657105, 'learning_rate': 1.2567049808429117e-06, 'epoch': 1.19}
 40%|███▉      | 107/270 [01:31<02:09,  1.26it/s] 40%|████      | 108/270 [01:31<02:07,  1.27it/s]                                                 {'loss': 1.7952, 'grad_norm': 0.735067587482872, 'learning_rate': 1.2490421455938697e-06, 'epoch': 1.2}
 40%|████      | 108/270 [01:31<02:07,  1.27it/s] 40%|████      | 109/270 [01:32<02:05,  1.29it/s]                                                 {'loss': 1.6869, 'grad_norm': 0.7820972277802376, 'learning_rate': 1.2413793103448275e-06, 'epoch': 1.21}
 40%|████      | 109/270 [01:32<02:05,  1.29it/s] 41%|████      | 110/270 [01:33<02:04,  1.28it/s]                                                 {'loss': 1.8659, 'grad_norm': 0.7264397205894342, 'learning_rate': 1.2337164750957853e-06, 'epoch': 1.22}
 41%|████      | 110/270 [01:33<02:04,  1.28it/s] 41%|████      | 111/270 [01:34<02:01,  1.31it/s]                                                 {'loss': 1.8098, 'grad_norm': 0.7053831424622263, 'learning_rate': 1.2260536398467431e-06, 'epoch': 1.23}
 41%|████      | 111/270 [01:34<02:01,  1.31it/s] 41%|████▏     | 112/270 [01:34<02:00,  1.31it/s]                                                 {'loss': 1.7297, 'grad_norm': 0.6916220537981603, 'learning_rate': 1.2183908045977011e-06, 'epoch': 1.24}
 41%|████▏     | 112/270 [01:34<02:00,  1.31it/s] 42%|████▏     | 113/270 [01:35<01:59,  1.31it/s]                                                 {'loss': 1.8076, 'grad_norm': 0.7142684815299353, 'learning_rate': 1.210727969348659e-06, 'epoch': 1.26}
 42%|████▏     | 113/270 [01:35<01:59,  1.31it/s] 42%|████▏     | 114/270 [01:36<01:58,  1.31it/s]                                                 {'loss': 1.7564, 'grad_norm': 0.7457214695101322, 'learning_rate': 1.2030651340996167e-06, 'epoch': 1.27}
 42%|████▏     | 114/270 [01:36<01:58,  1.31it/s] 43%|████▎     | 115/270 [01:37<01:56,  1.33it/s]                                                 {'loss': 1.8779, 'grad_norm': 0.8019957018147001, 'learning_rate': 1.1954022988505747e-06, 'epoch': 1.28}
 43%|████▎     | 115/270 [01:37<01:56,  1.33it/s] 43%|████▎     | 116/270 [01:37<01:56,  1.32it/s]                                                 {'loss': 1.7103, 'grad_norm': 0.7274622237815211, 'learning_rate': 1.1877394636015325e-06, 'epoch': 1.29}
 43%|████▎     | 116/270 [01:37<01:56,  1.32it/s] 43%|████▎     | 117/270 [01:38<01:56,  1.32it/s]                                                 {'loss': 1.7541, 'grad_norm': 0.699430584682012, 'learning_rate': 1.1800766283524903e-06, 'epoch': 1.3}
 43%|████▎     | 117/270 [01:38<01:56,  1.32it/s] 44%|████▎     | 118/270 [01:39<01:54,  1.33it/s]                                                 {'loss': 1.7461, 'grad_norm': 0.7699653347794044, 'learning_rate': 1.172413793103448e-06, 'epoch': 1.31}
 44%|████▎     | 118/270 [01:39<01:54,  1.33it/s] 44%|████▍     | 119/270 [01:40<01:52,  1.34it/s]                                                 {'loss': 1.8054, 'grad_norm': 0.827303145174945, 'learning_rate': 1.164750957854406e-06, 'epoch': 1.32}
 44%|████▍     | 119/270 [01:40<01:52,  1.34it/s] 44%|████▍     | 120/270 [01:40<01:51,  1.34it/s]                                                 {'loss': 1.8064, 'grad_norm': 0.7135853758658662, 'learning_rate': 1.1570881226053639e-06, 'epoch': 1.33}
 44%|████▍     | 120/270 [01:40<01:51,  1.34it/s] 45%|████▍     | 121/270 [01:41<01:50,  1.34it/s]                                                 {'loss': 1.7509, 'grad_norm': 0.7319084567694233, 'learning_rate': 1.1494252873563217e-06, 'epoch': 1.34}
 45%|████▍     | 121/270 [01:41<01:50,  1.34it/s] 45%|████▌     | 122/270 [01:42<01:48,  1.36it/s]                                                 {'loss': 1.7979, 'grad_norm': 0.7463933561520867, 'learning_rate': 1.1417624521072797e-06, 'epoch': 1.36}
 45%|████▌     | 122/270 [01:42<01:48,  1.36it/s] 46%|████▌     | 123/270 [01:43<01:50,  1.33it/s]                                                 {'loss': 1.7351, 'grad_norm': 0.7736137966410659, 'learning_rate': 1.1340996168582375e-06, 'epoch': 1.37}
 46%|████▌     | 123/270 [01:43<01:50,  1.33it/s] 46%|████▌     | 124/270 [01:43<01:48,  1.34it/s]                                                 {'loss': 1.8419, 'grad_norm': 0.737381698679925, 'learning_rate': 1.1264367816091952e-06, 'epoch': 1.38}
 46%|████▌     | 124/270 [01:43<01:48,  1.34it/s] 46%|████▋     | 125/270 [01:44<01:50,  1.32it/s]                                                 {'loss': 1.7658, 'grad_norm': 0.7375982628083162, 'learning_rate': 1.118773946360153e-06, 'epoch': 1.39}
 46%|████▋     | 125/270 [01:44<01:50,  1.32it/s] 47%|████▋     | 126/270 [01:45<01:50,  1.30it/s]                                                 {'loss': 1.7568, 'grad_norm': 0.698682163987468, 'learning_rate': 1.111111111111111e-06, 'epoch': 1.4}
 47%|████▋     | 126/270 [01:45<01:50,  1.30it/s] 47%|████▋     | 127/270 [01:46<01:49,  1.31it/s]                                                 {'loss': 1.8053, 'grad_norm': 0.7581952125259771, 'learning_rate': 1.1034482758620688e-06, 'epoch': 1.41}
 47%|████▋     | 127/270 [01:46<01:49,  1.31it/s] 47%|████▋     | 128/270 [01:46<01:46,  1.33it/s]                                                 {'loss': 1.7694, 'grad_norm': 0.7259021867959498, 'learning_rate': 1.0957854406130266e-06, 'epoch': 1.42}
 47%|████▋     | 128/270 [01:46<01:46,  1.33it/s] 48%|████▊     | 129/270 [01:47<01:46,  1.32it/s]                                                 {'loss': 1.7506, 'grad_norm': 0.6857576088618044, 'learning_rate': 1.0881226053639846e-06, 'epoch': 1.43}
 48%|████▊     | 129/270 [01:47<01:46,  1.32it/s] 48%|████▊     | 130/270 [01:48<01:47,  1.31it/s]                                                 {'loss': 1.6834, 'grad_norm': 0.6909917677980774, 'learning_rate': 1.0804597701149424e-06, 'epoch': 1.44}
 48%|████▊     | 130/270 [01:48<01:47,  1.31it/s] 49%|████▊     | 131/270 [01:49<01:46,  1.30it/s]                                                 {'loss': 1.7192, 'grad_norm': 0.6368139946681212, 'learning_rate': 1.0727969348659002e-06, 'epoch': 1.46}
 49%|████▊     | 131/270 [01:49<01:46,  1.30it/s] 49%|████▉     | 132/270 [01:49<01:44,  1.32it/s]                                                 {'loss': 1.8036, 'grad_norm': 0.7913640521801737, 'learning_rate': 1.0651340996168582e-06, 'epoch': 1.47}
 49%|████▉     | 132/270 [01:49<01:44,  1.32it/s] 49%|████▉     | 133/270 [01:50<01:45,  1.30it/s]                                                 {'loss': 1.7845, 'grad_norm': 0.6532693751462846, 'learning_rate': 1.057471264367816e-06, 'epoch': 1.48}
 49%|████▉     | 133/270 [01:50<01:45,  1.30it/s] 50%|████▉     | 134/270 [01:51<01:44,  1.30it/s]                                                 {'loss': 1.779, 'grad_norm': 0.6491390323347959, 'learning_rate': 1.0498084291187738e-06, 'epoch': 1.49}
 50%|████▉     | 134/270 [01:51<01:44,  1.30it/s] 50%|█████     | 135/270 [01:52<01:44,  1.29it/s]                                                 {'loss': 1.7931, 'grad_norm': 0.6880115084080981, 'learning_rate': 1.0421455938697316e-06, 'epoch': 1.5}
 50%|█████     | 135/270 [01:52<01:44,  1.29it/s] 50%|█████     | 136/270 [01:53<01:43,  1.30it/s]                                                 {'loss': 1.7301, 'grad_norm': 0.7420496096321682, 'learning_rate': 1.0344827586206896e-06, 'epoch': 1.51}
 50%|█████     | 136/270 [01:53<01:43,  1.30it/s] 51%|█████     | 137/270 [01:53<01:41,  1.31it/s]                                                 {'loss': 1.7691, 'grad_norm': 0.710806812417044, 'learning_rate': 1.0268199233716474e-06, 'epoch': 1.52}
 51%|█████     | 137/270 [01:53<01:41,  1.31it/s] 51%|█████     | 138/270 [01:54<01:41,  1.31it/s]                                                 {'loss': 1.7623, 'grad_norm': 0.7316594456455252, 'learning_rate': 1.0191570881226052e-06, 'epoch': 1.53}
 51%|█████     | 138/270 [01:54<01:41,  1.31it/s] 51%|█████▏    | 139/270 [01:55<01:40,  1.30it/s]                                                 {'loss': 1.8552, 'grad_norm': 0.7680526795698982, 'learning_rate': 1.0114942528735634e-06, 'epoch': 1.54}
 51%|█████▏    | 139/270 [01:55<01:40,  1.30it/s] 52%|█████▏    | 140/270 [01:56<01:40,  1.29it/s]                                                 {'loss': 1.7117, 'grad_norm': 0.6388658083297094, 'learning_rate': 1.0038314176245212e-06, 'epoch': 1.56}
 52%|█████▏    | 140/270 [01:56<01:40,  1.29it/s] 52%|█████▏    | 141/270 [01:56<01:40,  1.28it/s]                                                 {'loss': 1.8198, 'grad_norm': 0.7378900727016433, 'learning_rate': 9.961685823754787e-07, 'epoch': 1.57}
 52%|█████▏    | 141/270 [01:56<01:40,  1.28it/s] 53%|█████▎    | 142/270 [01:57<01:39,  1.29it/s]                                                 {'loss': 1.7484, 'grad_norm': 0.7068982538074987, 'learning_rate': 9.885057471264367e-07, 'epoch': 1.58}
 53%|█████▎    | 142/270 [01:57<01:39,  1.29it/s] 53%|█████▎    | 143/270 [01:58<01:38,  1.29it/s]                                                 {'loss': 1.7375, 'grad_norm': 0.5969228571023667, 'learning_rate': 9.808429118773945e-07, 'epoch': 1.59}
 53%|█████▎    | 143/270 [01:58<01:38,  1.29it/s] 53%|█████▎    | 144/270 [01:59<01:35,  1.32it/s]                                                 {'loss': 1.7705, 'grad_norm': 0.7577693169735624, 'learning_rate': 9.731800766283525e-07, 'epoch': 1.6}
 53%|█████▎    | 144/270 [01:59<01:35,  1.32it/s] 54%|█████▎    | 145/270 [01:59<01:34,  1.33it/s]                                                 {'loss': 1.7605, 'grad_norm': 0.6594239632296107, 'learning_rate': 9.655172413793103e-07, 'epoch': 1.61}
 54%|█████▎    | 145/270 [01:59<01:34,  1.33it/s] 54%|█████▍    | 146/270 [02:00<01:33,  1.32it/s]                                                 {'loss': 1.8251, 'grad_norm': 0.7010324922716691, 'learning_rate': 9.578544061302681e-07, 'epoch': 1.62}
 54%|█████▍    | 146/270 [02:00<01:33,  1.32it/s] 54%|█████▍    | 147/270 [02:01<01:33,  1.31it/s]                                                 {'loss': 1.7411, 'grad_norm': 0.7328852001682966, 'learning_rate': 9.50191570881226e-07, 'epoch': 1.63}
 54%|█████▍    | 147/270 [02:01<01:33,  1.31it/s] 55%|█████▍    | 148/270 [02:02<01:31,  1.34it/s]                                                 {'loss': 1.7003, 'grad_norm': 0.7367102093131657, 'learning_rate': 9.425287356321838e-07, 'epoch': 1.64}
 55%|█████▍    | 148/270 [02:02<01:31,  1.34it/s] 55%|█████▌    | 149/270 [02:02<01:31,  1.32it/s]                                                 {'loss': 1.7912, 'grad_norm': 0.785065139563032, 'learning_rate': 9.348659003831417e-07, 'epoch': 1.66}
 55%|█████▌    | 149/270 [02:02<01:31,  1.32it/s] 56%|█████▌    | 150/270 [02:03<01:30,  1.32it/s]                                                 {'loss': 1.7474, 'grad_norm': 0.6572143881771593, 'learning_rate': 9.272030651340996e-07, 'epoch': 1.67}
 56%|█████▌    | 150/270 [02:03<01:30,  1.32it/s] 56%|█████▌    | 151/270 [02:04<01:28,  1.34it/s]                                                 {'loss': 1.7601, 'grad_norm': 0.7312035036521299, 'learning_rate': 9.195402298850574e-07, 'epoch': 1.68}
 56%|█████▌    | 151/270 [02:04<01:28,  1.34it/s] 56%|█████▋    | 152/270 [02:05<01:29,  1.32it/s]                                                 {'loss': 1.7295, 'grad_norm': 0.7077963277200091, 'learning_rate': 9.118773946360153e-07, 'epoch': 1.69}
 56%|█████▋    | 152/270 [02:05<01:29,  1.32it/s] 57%|█████▋    | 153/270 [02:06<01:29,  1.30it/s]                                                 {'loss': 1.7654, 'grad_norm': 0.7312413837695863, 'learning_rate': 9.042145593869731e-07, 'epoch': 1.7}
 57%|█████▋    | 153/270 [02:06<01:29,  1.30it/s] 57%|█████▋    | 154/270 [02:06<01:28,  1.31it/s]                                                 {'loss': 1.757, 'grad_norm': 0.6836925281128684, 'learning_rate': 8.96551724137931e-07, 'epoch': 1.71}
 57%|█████▋    | 154/270 [02:06<01:28,  1.31it/s] 57%|█████▋    | 155/270 [02:07<01:27,  1.31it/s]                                                 {'loss': 1.7137, 'grad_norm': 0.7417262756722282, 'learning_rate': 8.888888888888888e-07, 'epoch': 1.72}
 57%|█████▋    | 155/270 [02:07<01:27,  1.31it/s] 58%|█████▊    | 156/270 [02:08<01:27,  1.30it/s]                                                 {'loss': 1.6576, 'grad_norm': 0.6944458024954995, 'learning_rate': 8.812260536398467e-07, 'epoch': 1.73}
 58%|█████▊    | 156/270 [02:08<01:27,  1.30it/s] 58%|█████▊    | 157/270 [02:09<01:26,  1.30it/s]                                                 {'loss': 1.7172, 'grad_norm': 0.7456227812787659, 'learning_rate': 8.735632183908046e-07, 'epoch': 1.74}
 58%|█████▊    | 157/270 [02:09<01:26,  1.30it/s] 59%|█████▊    | 158/270 [02:09<01:25,  1.31it/s]                                                 {'loss': 1.8352, 'grad_norm': 0.7235588836389992, 'learning_rate': 8.659003831417624e-07, 'epoch': 1.76}
 59%|█████▊    | 158/270 [02:09<01:25,  1.31it/s] 59%|█████▉    | 159/270 [02:10<01:25,  1.30it/s]                                                 {'loss': 1.7162, 'grad_norm': 0.7099159197039897, 'learning_rate': 8.582375478927202e-07, 'epoch': 1.77}
 59%|█████▉    | 159/270 [02:10<01:25,  1.30it/s] 59%|█████▉    | 160/270 [02:11<01:25,  1.28it/s]                                                 {'loss': 1.7882, 'grad_norm': 0.63469430039918, 'learning_rate': 8.50574712643678e-07, 'epoch': 1.78}
 59%|█████▉    | 160/270 [02:11<01:25,  1.28it/s] 60%|█████▉    | 161/270 [02:12<01:24,  1.28it/s]                                                 {'loss': 1.6887, 'grad_norm': 0.7200164790999818, 'learning_rate': 8.429118773946359e-07, 'epoch': 1.79}
 60%|█████▉    | 161/270 [02:12<01:24,  1.28it/s] 60%|██████    | 162/270 [02:13<01:24,  1.28it/s]                                                 {'loss': 1.7519, 'grad_norm': 0.6976090225239454, 'learning_rate': 8.352490421455939e-07, 'epoch': 1.8}
 60%|██████    | 162/270 [02:13<01:24,  1.28it/s] 60%|██████    | 163/270 [02:13<01:21,  1.31it/s]                                                 {'loss': 1.8018, 'grad_norm': 0.7271802713731871, 'learning_rate': 8.275862068965517e-07, 'epoch': 1.81}
 60%|██████    | 163/270 [02:13<01:21,  1.31it/s] 61%|██████    | 164/270 [02:14<01:20,  1.31it/s]                                                 {'loss': 1.7422, 'grad_norm': 0.7359976519882094, 'learning_rate': 8.199233716475096e-07, 'epoch': 1.82}
 61%|██████    | 164/270 [02:14<01:20,  1.31it/s] 61%|██████    | 165/270 [02:15<01:20,  1.31it/s]                                                 {'loss': 1.7162, 'grad_norm': 0.6825405175089205, 'learning_rate': 8.122605363984674e-07, 'epoch': 1.83}
 61%|██████    | 165/270 [02:15<01:20,  1.31it/s] 61%|██████▏   | 166/270 [02:15<01:18,  1.32it/s]                                                 {'loss': 1.7283, 'grad_norm': 0.7124512869510984, 'learning_rate': 8.045977011494253e-07, 'epoch': 1.84}
 61%|██████▏   | 166/270 [02:15<01:18,  1.32it/s] 62%|██████▏   | 167/270 [02:16<01:19,  1.29it/s]                                                 {'loss': 1.6543, 'grad_norm': 0.6747151069156171, 'learning_rate': 7.969348659003831e-07, 'epoch': 1.86}
 62%|██████▏   | 167/270 [02:16<01:19,  1.29it/s] 62%|██████▏   | 168/270 [02:17<01:18,  1.30it/s]                                                 {'loss': 1.7614, 'grad_norm': 0.7015674874712277, 'learning_rate': 7.89272030651341e-07, 'epoch': 1.87}
 62%|██████▏   | 168/270 [02:17<01:18,  1.30it/s] 63%|██████▎   | 169/270 [02:18<01:16,  1.31it/s]                                                 {'loss': 1.83, 'grad_norm': 0.750788772231233, 'learning_rate': 7.816091954022989e-07, 'epoch': 1.88}
 63%|██████▎   | 169/270 [02:18<01:16,  1.31it/s] 63%|██████▎   | 170/270 [02:19<01:16,  1.31it/s]                                                 {'loss': 1.8127, 'grad_norm': 0.744544580555882, 'learning_rate': 7.739463601532567e-07, 'epoch': 1.89}
 63%|██████▎   | 170/270 [02:19<01:16,  1.31it/s] 63%|██████▎   | 171/270 [02:19<01:17,  1.28it/s]                                                 {'loss': 1.7388, 'grad_norm': 0.8272191615137853, 'learning_rate': 7.662835249042146e-07, 'epoch': 1.9}
 63%|██████▎   | 171/270 [02:19<01:17,  1.28it/s] 64%|██████▎   | 172/270 [02:20<01:15,  1.29it/s]                                                 {'loss': 1.7418, 'grad_norm': 0.7780198955272878, 'learning_rate': 7.586206896551724e-07, 'epoch': 1.91}
 64%|██████▎   | 172/270 [02:20<01:15,  1.29it/s] 64%|██████▍   | 173/270 [02:21<01:16,  1.27it/s]                                                 {'loss': 1.8285, 'grad_norm': 0.837899517786274, 'learning_rate': 7.509578544061303e-07, 'epoch': 1.92}
 64%|██████▍   | 173/270 [02:21<01:16,  1.27it/s] 64%|██████▍   | 174/270 [02:22<01:15,  1.27it/s]                                                 {'loss': 1.7265, 'grad_norm': 0.7296924342340856, 'learning_rate': 7.432950191570882e-07, 'epoch': 1.93}
 64%|██████▍   | 174/270 [02:22<01:15,  1.27it/s] 65%|██████▍   | 175/270 [02:23<01:14,  1.28it/s]                                                 {'loss': 1.7646, 'grad_norm': 0.7975605636748322, 'learning_rate': 7.35632183908046e-07, 'epoch': 1.94}
 65%|██████▍   | 175/270 [02:23<01:14,  1.28it/s] 65%|██████▌   | 176/270 [02:23<01:14,  1.26it/s]                                                 {'loss': 1.7496, 'grad_norm': 0.7370601617549207, 'learning_rate': 7.279693486590039e-07, 'epoch': 1.96}
 65%|██████▌   | 176/270 [02:23<01:14,  1.26it/s] 66%|██████▌   | 177/270 [02:24<01:11,  1.29it/s]                                                 {'loss': 1.782, 'grad_norm': 0.7547708460287075, 'learning_rate': 7.203065134099616e-07, 'epoch': 1.97}
 66%|██████▌   | 177/270 [02:24<01:11,  1.29it/s] 66%|██████▌   | 178/270 [02:25<01:11,  1.28it/s]                                                 {'loss': 1.7021, 'grad_norm': 0.7503596206313067, 'learning_rate': 7.126436781609195e-07, 'epoch': 1.98}
 66%|██████▌   | 178/270 [02:25<01:11,  1.28it/s] 66%|██████▋   | 179/270 [02:26<01:10,  1.30it/s]                                                 {'loss': 1.8239, 'grad_norm': 0.771986825407798, 'learning_rate': 7.049808429118773e-07, 'epoch': 1.99}
 66%|██████▋   | 179/270 [02:26<01:10,  1.30it/s] 67%|██████▋   | 180/270 [02:26<01:10,  1.27it/s]                                                 {'loss': 1.7836, 'grad_norm': 0.7086214708856452, 'learning_rate': 6.973180076628352e-07, 'epoch': 2.0}
 67%|██████▋   | 180/270 [02:26<01:10,  1.27it/s] 67%|██████▋   | 181/270 [02:27<01:10,  1.27it/s]                                                 {'loss': 1.7783, 'grad_norm': 0.7845230799452411, 'learning_rate': 6.896551724137931e-07, 'epoch': 2.01}
 67%|██████▋   | 181/270 [02:27<01:10,  1.27it/s] 67%|██████▋   | 182/270 [02:28<01:09,  1.27it/s]                                                 {'loss': 1.7581, 'grad_norm': 0.7644126654931854, 'learning_rate': 6.819923371647509e-07, 'epoch': 2.02}
 67%|██████▋   | 182/270 [02:28<01:09,  1.27it/s] 68%|██████▊   | 183/270 [02:29<01:07,  1.28it/s]                                                 {'loss': 1.6698, 'grad_norm': 0.7669785903356912, 'learning_rate': 6.743295019157088e-07, 'epoch': 2.03}
 68%|██████▊   | 183/270 [02:29<01:07,  1.28it/s] 68%|██████▊   | 184/270 [02:30<01:06,  1.30it/s]                                                 {'loss': 1.8112, 'grad_norm': 0.8099059251838459, 'learning_rate': 6.666666666666666e-07, 'epoch': 2.04}
 68%|██████▊   | 184/270 [02:30<01:06,  1.30it/s] 69%|██████▊   | 185/270 [02:30<01:04,  1.32it/s]                                                 {'loss': 1.7036, 'grad_norm': 0.7864493227861358, 'learning_rate': 6.590038314176245e-07, 'epoch': 2.06}
 69%|██████▊   | 185/270 [02:30<01:04,  1.32it/s] 69%|██████▉   | 186/270 [02:31<01:03,  1.32it/s]                                                 {'loss': 1.8186, 'grad_norm': 0.6946780131490906, 'learning_rate': 6.513409961685823e-07, 'epoch': 2.07}
 69%|██████▉   | 186/270 [02:31<01:03,  1.32it/s] 69%|██████▉   | 187/270 [02:32<01:04,  1.29it/s]                                                 {'loss': 1.7342, 'grad_norm': 0.8018446861730396, 'learning_rate': 6.436781609195402e-07, 'epoch': 2.08}
 69%|██████▉   | 187/270 [02:32<01:04,  1.29it/s] 70%|██████▉   | 188/270 [02:33<01:03,  1.29it/s]                                                 {'loss': 1.7165, 'grad_norm': 0.7238405239195242, 'learning_rate': 6.360153256704981e-07, 'epoch': 2.09}
 70%|██████▉   | 188/270 [02:33<01:03,  1.29it/s] 70%|███████   | 189/270 [02:33<01:02,  1.30it/s]                                                 {'loss': 1.8009, 'grad_norm': 0.8097631463497785, 'learning_rate': 6.283524904214559e-07, 'epoch': 2.1}
 70%|███████   | 189/270 [02:33<01:02,  1.30it/s] 70%|███████   | 190/270 [02:34<01:00,  1.33it/s]                                                 {'loss': 1.8166, 'grad_norm': 0.7669621459851741, 'learning_rate': 6.206896551724138e-07, 'epoch': 2.11}
 70%|███████   | 190/270 [02:34<01:00,  1.33it/s] 71%|███████   | 191/270 [02:35<01:00,  1.30it/s]                                                 {'loss': 1.7771, 'grad_norm': 0.7231744482180984, 'learning_rate': 6.130268199233716e-07, 'epoch': 2.12}
 71%|███████   | 191/270 [02:35<01:00,  1.30it/s] 71%|███████   | 192/270 [02:36<01:00,  1.29it/s]                                                 {'loss': 1.8027, 'grad_norm': 0.7644601062673373, 'learning_rate': 6.053639846743295e-07, 'epoch': 2.13}
 71%|███████   | 192/270 [02:36<01:00,  1.29it/s] 71%|███████▏  | 193/270 [02:36<01:00,  1.28it/s]                                                 {'loss': 1.7603, 'grad_norm': 0.8608852982895109, 'learning_rate': 5.977011494252874e-07, 'epoch': 2.14}
 71%|███████▏  | 193/270 [02:36<01:00,  1.28it/s] 72%|███████▏  | 194/270 [02:37<00:58,  1.31it/s]                                                 {'loss': 1.6881, 'grad_norm': 0.7448481317093647, 'learning_rate': 5.900383141762451e-07, 'epoch': 2.16}
 72%|███████▏  | 194/270 [02:37<00:58,  1.31it/s] 72%|███████▏  | 195/270 [02:38<00:57,  1.30it/s]                                                 {'loss': 1.7389, 'grad_norm': 0.7975057748777958, 'learning_rate': 5.82375478927203e-07, 'epoch': 2.17}
 72%|███████▏  | 195/270 [02:38<00:57,  1.30it/s] 73%|███████▎  | 196/270 [02:39<00:57,  1.30it/s]                                                 {'loss': 1.7847, 'grad_norm': 0.8788565120852074, 'learning_rate': 5.747126436781608e-07, 'epoch': 2.18}
 73%|███████▎  | 196/270 [02:39<00:57,  1.30it/s] 73%|███████▎  | 197/270 [02:40<00:56,  1.29it/s]                                                 {'loss': 1.7739, 'grad_norm': 0.7507825070210278, 'learning_rate': 5.670498084291187e-07, 'epoch': 2.19}
 73%|███████▎  | 197/270 [02:40<00:56,  1.29it/s] 73%|███████▎  | 198/270 [02:40<00:55,  1.29it/s]                                                 {'loss': 1.7811, 'grad_norm': 0.8235857449529799, 'learning_rate': 5.593869731800765e-07, 'epoch': 2.2}
 73%|███████▎  | 198/270 [02:40<00:55,  1.29it/s] 74%|███████▎  | 199/270 [02:41<00:54,  1.31it/s]                                                 {'loss': 1.6736, 'grad_norm': 0.7292336891554425, 'learning_rate': 5.517241379310344e-07, 'epoch': 2.21}
 74%|███████▎  | 199/270 [02:41<00:54,  1.31it/s] 74%|███████▍  | 200/270 [02:42<00:53,  1.30it/s]                                                 {'loss': 1.7413, 'grad_norm': 0.7559327700981574, 'learning_rate': 5.440613026819923e-07, 'epoch': 2.22}
 74%|███████▍  | 200/270 [02:42<00:53,  1.30it/s] 74%|███████▍  | 201/270 [02:43<00:52,  1.31it/s]                                                 {'loss': 1.7318, 'grad_norm': 0.7960325551866922, 'learning_rate': 5.363984674329501e-07, 'epoch': 2.23}
 74%|███████▍  | 201/270 [02:43<00:52,  1.31it/s] 75%|███████▍  | 202/270 [02:43<00:50,  1.34it/s]                                                 {'loss': 1.724, 'grad_norm': 0.7413399505193391, 'learning_rate': 5.28735632183908e-07, 'epoch': 2.24}
 75%|███████▍  | 202/270 [02:43<00:50,  1.34it/s] 75%|███████▌  | 203/270 [02:44<00:50,  1.34it/s]                                                 {'loss': 1.8286, 'grad_norm': 0.8067019061680109, 'learning_rate': 5.210727969348658e-07, 'epoch': 2.26}
 75%|███████▌  | 203/270 [02:44<00:50,  1.34it/s] 76%|███████▌  | 204/270 [02:45<00:49,  1.34it/s]                                                 {'loss': 1.7666, 'grad_norm': 0.8284446671332232, 'learning_rate': 5.134099616858237e-07, 'epoch': 2.27}
 76%|███████▌  | 204/270 [02:45<00:49,  1.34it/s] 76%|███████▌  | 205/270 [02:46<00:49,  1.31it/s]                                                 {'loss': 1.7263, 'grad_norm': 0.7224771739540126, 'learning_rate': 5.057471264367817e-07, 'epoch': 2.28}
 76%|███████▌  | 205/270 [02:46<00:49,  1.31it/s] 76%|███████▋  | 206/270 [02:46<00:48,  1.31it/s]                                                 {'loss': 1.757, 'grad_norm': 0.7421324235989925, 'learning_rate': 4.980842911877394e-07, 'epoch': 2.29}
 76%|███████▋  | 206/270 [02:46<00:48,  1.31it/s] 77%|███████▋  | 207/270 [02:47<00:49,  1.28it/s]                                                 {'loss': 1.6555, 'grad_norm': 0.7794299930143884, 'learning_rate': 4.904214559386973e-07, 'epoch': 2.3}
 77%|███████▋  | 207/270 [02:47<00:49,  1.28it/s] 77%|███████▋  | 208/270 [02:48<00:47,  1.30it/s]                                                 {'loss': 1.7736, 'grad_norm': 0.7167211246022802, 'learning_rate': 4.827586206896552e-07, 'epoch': 2.31}
 77%|███████▋  | 208/270 [02:48<00:47,  1.30it/s] 77%|███████▋  | 209/270 [02:49<00:47,  1.28it/s]                                                 {'loss': 1.7695, 'grad_norm': 0.6616426063133808, 'learning_rate': 4.75095785440613e-07, 'epoch': 2.32}
 77%|███████▋  | 209/270 [02:49<00:47,  1.28it/s] 78%|███████▊  | 210/270 [02:49<00:46,  1.30it/s]                                                 {'loss': 1.7998, 'grad_norm': 0.8917944755769318, 'learning_rate': 4.6743295019157085e-07, 'epoch': 2.33}
 78%|███████▊  | 210/270 [02:49<00:46,  1.30it/s] 78%|███████▊  | 211/270 [02:50<00:45,  1.29it/s]                                                 {'loss': 1.7856, 'grad_norm': 0.8172594182451727, 'learning_rate': 4.597701149425287e-07, 'epoch': 2.34}
 78%|███████▊  | 211/270 [02:50<00:45,  1.29it/s] 79%|███████▊  | 212/270 [02:51<00:44,  1.30it/s]                                                 {'loss': 1.6559, 'grad_norm': 0.7921360187591904, 'learning_rate': 4.5210727969348654e-07, 'epoch': 2.36}
 79%|███████▊  | 212/270 [02:51<00:44,  1.30it/s] 79%|███████▉  | 213/270 [02:52<00:43,  1.32it/s]                                                 {'loss': 1.7356, 'grad_norm': 0.8319088368083086, 'learning_rate': 4.444444444444444e-07, 'epoch': 2.37}
 79%|███████▉  | 213/270 [02:52<00:43,  1.32it/s] 79%|███████▉  | 214/270 [02:52<00:42,  1.33it/s]                                                 {'loss': 1.6854, 'grad_norm': 0.747103118150827, 'learning_rate': 4.367816091954023e-07, 'epoch': 2.38}
 79%|███████▉  | 214/270 [02:52<00:42,  1.33it/s] 80%|███████▉  | 215/270 [02:53<00:41,  1.31it/s]                                                 {'loss': 1.7569, 'grad_norm': 0.7764542233716726, 'learning_rate': 4.291187739463601e-07, 'epoch': 2.39}
 80%|███████▉  | 215/270 [02:53<00:41,  1.31it/s] 80%|████████  | 216/270 [02:54<00:41,  1.31it/s]                                                 {'loss': 1.7475, 'grad_norm': 0.7025678443717606, 'learning_rate': 4.2145593869731797e-07, 'epoch': 2.4}
 80%|████████  | 216/270 [02:54<00:41,  1.31it/s] 80%|████████  | 217/270 [02:55<00:40,  1.29it/s]                                                 {'loss': 1.7173, 'grad_norm': 0.7002373842091448, 'learning_rate': 4.1379310344827586e-07, 'epoch': 2.41}
 80%|████████  | 217/270 [02:55<00:40,  1.29it/s] 81%|████████  | 218/270 [02:56<00:39,  1.30it/s]                                                 {'loss': 1.7042, 'grad_norm': 0.787512918381074, 'learning_rate': 4.061302681992337e-07, 'epoch': 2.42}
 81%|████████  | 218/270 [02:56<00:39,  1.30it/s] 81%|████████  | 219/270 [02:56<00:39,  1.28it/s]                                                 {'loss': 1.7883, 'grad_norm': 0.8451021847825684, 'learning_rate': 3.9846743295019155e-07, 'epoch': 2.43}
 81%|████████  | 219/270 [02:56<00:39,  1.28it/s] 81%|████████▏ | 220/270 [02:57<00:38,  1.29it/s]                                                 {'loss': 1.7246, 'grad_norm': 0.8384443178407611, 'learning_rate': 3.9080459770114945e-07, 'epoch': 2.44}
 81%|████████▏ | 220/270 [02:57<00:38,  1.29it/s] 82%|████████▏ | 221/270 [02:58<00:38,  1.29it/s]                                                 {'loss': 1.6452, 'grad_norm': 0.7985910991145897, 'learning_rate': 3.831417624521073e-07, 'epoch': 2.46}
 82%|████████▏ | 221/270 [02:58<00:38,  1.29it/s] 82%|████████▏ | 222/270 [02:59<00:36,  1.31it/s]                                                 {'loss': 1.7386, 'grad_norm': 0.8378589432898309, 'learning_rate': 3.7547892720306513e-07, 'epoch': 2.47}
 82%|████████▏ | 222/270 [02:59<00:36,  1.31it/s] 83%|████████▎ | 223/270 [02:59<00:36,  1.29it/s]                                                 {'loss': 1.7744, 'grad_norm': 0.7822949744416965, 'learning_rate': 3.67816091954023e-07, 'epoch': 2.48}
 83%|████████▎ | 223/270 [02:59<00:36,  1.29it/s] 83%|████████▎ | 224/270 [03:00<00:35,  1.29it/s]                                                 {'loss': 1.7068, 'grad_norm': 0.7094340983438191, 'learning_rate': 3.601532567049808e-07, 'epoch': 2.49}
 83%|████████▎ | 224/270 [03:00<00:35,  1.29it/s] 83%|████████▎ | 225/270 [03:01<00:34,  1.30it/s]                                                 {'loss': 1.779, 'grad_norm': 0.8223719714085451, 'learning_rate': 3.5249042145593867e-07, 'epoch': 2.5}
 83%|████████▎ | 225/270 [03:01<00:34,  1.30it/s] 84%|████████▎ | 226/270 [03:02<00:34,  1.29it/s]                                                 {'loss': 1.7887, 'grad_norm': 0.9297264721702331, 'learning_rate': 3.4482758620689656e-07, 'epoch': 2.51}
 84%|████████▎ | 226/270 [03:02<00:34,  1.29it/s] 84%|████████▍ | 227/270 [03:02<00:32,  1.32it/s]                                                 {'loss': 1.6972, 'grad_norm': 0.8737073936535666, 'learning_rate': 3.371647509578544e-07, 'epoch': 2.52}
 84%|████████▍ | 227/270 [03:02<00:32,  1.32it/s] 84%|████████▍ | 228/270 [03:03<00:31,  1.32it/s]                                                 {'loss': 1.6489, 'grad_norm': 0.8523703397728292, 'learning_rate': 3.2950191570881225e-07, 'epoch': 2.53}
 84%|████████▍ | 228/270 [03:03<00:31,  1.32it/s] 85%|████████▍ | 229/270 [03:04<00:30,  1.32it/s]                                                 {'loss': 1.7087, 'grad_norm': 0.7711470311579538, 'learning_rate': 3.218390804597701e-07, 'epoch': 2.54}
 85%|████████▍ | 229/270 [03:04<00:30,  1.32it/s] 85%|████████▌ | 230/270 [03:05<00:30,  1.32it/s]                                                 {'loss': 1.7767, 'grad_norm': 0.7766982067178066, 'learning_rate': 3.1417624521072794e-07, 'epoch': 2.56}
 85%|████████▌ | 230/270 [03:05<00:30,  1.32it/s] 86%|████████▌ | 231/270 [03:06<00:29,  1.31it/s]                                                 {'loss': 1.7416, 'grad_norm': 0.8413829741823882, 'learning_rate': 3.065134099616858e-07, 'epoch': 2.57}
 86%|████████▌ | 231/270 [03:06<00:29,  1.31it/s] 86%|████████▌ | 232/270 [03:06<00:28,  1.31it/s]                                                 {'loss': 1.7255, 'grad_norm': 0.843181434357376, 'learning_rate': 2.988505747126437e-07, 'epoch': 2.58}
 86%|████████▌ | 232/270 [03:06<00:28,  1.31it/s] 86%|████████▋ | 233/270 [03:07<00:28,  1.28it/s]                                                 {'loss': 1.6676, 'grad_norm': 0.7509073036441809, 'learning_rate': 2.911877394636015e-07, 'epoch': 2.59}
 86%|████████▋ | 233/270 [03:07<00:28,  1.28it/s] 87%|████████▋ | 234/270 [03:08<00:28,  1.27it/s]                                                 {'loss': 1.7699, 'grad_norm': 0.7905917541197557, 'learning_rate': 2.8352490421455936e-07, 'epoch': 2.6}
 87%|████████▋ | 234/270 [03:08<00:28,  1.27it/s] 87%|████████▋ | 235/270 [03:09<00:27,  1.26it/s]                                                 {'loss': 1.6652, 'grad_norm': 0.8616839919612891, 'learning_rate': 2.758620689655172e-07, 'epoch': 2.61}
 87%|████████▋ | 235/270 [03:09<00:27,  1.26it/s] 87%|████████▋ | 236/270 [03:09<00:26,  1.28it/s]                                                 {'loss': 1.7399, 'grad_norm': 0.7154083975640231, 'learning_rate': 2.6819923371647505e-07, 'epoch': 2.62}
 87%|████████▋ | 236/270 [03:09<00:26,  1.28it/s] 88%|████████▊ | 237/270 [03:10<00:25,  1.29it/s]                                                 {'loss': 1.6881, 'grad_norm': 0.8327858766898261, 'learning_rate': 2.605363984674329e-07, 'epoch': 2.63}
 88%|████████▊ | 237/270 [03:10<00:25,  1.29it/s] 88%|████████▊ | 238/270 [03:11<00:24,  1.30it/s]                                                 {'loss': 1.7267, 'grad_norm': 0.8002728594641505, 'learning_rate': 2.5287356321839084e-07, 'epoch': 2.64}
 88%|████████▊ | 238/270 [03:11<00:24,  1.30it/s] 89%|████████▊ | 239/270 [03:12<00:24,  1.28it/s]                                                 {'loss': 1.7209, 'grad_norm': 0.7816678475917687, 'learning_rate': 2.4521072796934863e-07, 'epoch': 2.66}
 89%|████████▊ | 239/270 [03:12<00:24,  1.28it/s] 89%|████████▉ | 240/270 [03:13<00:23,  1.27it/s]                                                 {'loss': 1.689, 'grad_norm': 0.7178085522520306, 'learning_rate': 2.375478927203065e-07, 'epoch': 2.67}
 89%|████████▉ | 240/270 [03:13<00:23,  1.27it/s] 89%|████████▉ | 241/270 [03:13<00:22,  1.27it/s]                                                 {'loss': 1.7013, 'grad_norm': 0.791456637522743, 'learning_rate': 2.2988505747126435e-07, 'epoch': 2.68}
 89%|████████▉ | 241/270 [03:13<00:22,  1.27it/s] 90%|████████▉ | 242/270 [03:14<00:22,  1.25it/s]                                                 {'loss': 1.7478, 'grad_norm': 0.748484243830562, 'learning_rate': 2.222222222222222e-07, 'epoch': 2.69}
 90%|████████▉ | 242/270 [03:14<00:22,  1.25it/s] 90%|█████████ | 243/270 [03:15<00:21,  1.25it/s]                                                 {'loss': 1.6785, 'grad_norm': 0.7928181464476989, 'learning_rate': 2.1455938697318006e-07, 'epoch': 2.7}
 90%|█████████ | 243/270 [03:15<00:21,  1.25it/s] 90%|█████████ | 244/270 [03:16<00:20,  1.24it/s]                                                 {'loss': 1.6392, 'grad_norm': 0.7508745506767875, 'learning_rate': 2.0689655172413793e-07, 'epoch': 2.71}
 90%|█████████ | 244/270 [03:16<00:20,  1.24it/s] 91%|█████████ | 245/270 [03:17<00:20,  1.25it/s]                                                 {'loss': 1.7078, 'grad_norm': 0.888997916762879, 'learning_rate': 1.9923371647509578e-07, 'epoch': 2.72}
 91%|█████████ | 245/270 [03:17<00:20,  1.25it/s] 91%|█████████ | 246/270 [03:17<00:19,  1.25it/s]                                                 {'loss': 1.752, 'grad_norm': 0.8504609013482827, 'learning_rate': 1.9157088122605365e-07, 'epoch': 2.73}
 91%|█████████ | 246/270 [03:17<00:19,  1.25it/s] 91%|█████████▏| 247/270 [03:18<00:17,  1.28it/s]                                                 {'loss': 1.7006, 'grad_norm': 0.7414658226433519, 'learning_rate': 1.839080459770115e-07, 'epoch': 2.74}
 91%|█████████▏| 247/270 [03:18<00:17,  1.28it/s] 92%|█████████▏| 248/270 [03:19<00:17,  1.27it/s]                                                 {'loss': 1.6861, 'grad_norm': 0.7942784276333288, 'learning_rate': 1.7624521072796933e-07, 'epoch': 2.76}
 92%|█████████▏| 248/270 [03:19<00:17,  1.27it/s] 92%|█████████▏| 249/270 [03:20<00:16,  1.28it/s]                                                 {'loss': 1.7577, 'grad_norm': 0.8222827947533248, 'learning_rate': 1.685823754789272e-07, 'epoch': 2.77}
 92%|█████████▏| 249/270 [03:20<00:16,  1.28it/s] 93%|█████████▎| 250/270 [03:21<00:15,  1.27it/s]                                                 {'loss': 1.7049, 'grad_norm': 0.771713127214703, 'learning_rate': 1.6091954022988505e-07, 'epoch': 2.78}
 93%|█████████▎| 250/270 [03:21<00:15,  1.27it/s] 93%|█████████▎| 251/270 [03:21<00:14,  1.29it/s]                                                 {'loss': 1.7597, 'grad_norm': 0.917190471090281, 'learning_rate': 1.532567049808429e-07, 'epoch': 2.79}
 93%|█████████▎| 251/270 [03:21<00:14,  1.29it/s] 93%|█████████▎| 252/270 [03:22<00:14,  1.28it/s]                                                 {'loss': 1.7667, 'grad_norm': 0.8329370815894145, 'learning_rate': 1.4559386973180076e-07, 'epoch': 2.8}
 93%|█████████▎| 252/270 [03:22<00:14,  1.28it/s] 94%|█████████▎| 253/270 [03:23<00:13,  1.27it/s]                                                 {'loss': 1.7151, 'grad_norm': 0.8201816073341167, 'learning_rate': 1.379310344827586e-07, 'epoch': 2.81}
 94%|█████████▎| 253/270 [03:23<00:13,  1.27it/s] 94%|█████████▍| 254/270 [03:24<00:12,  1.29it/s]                                                 {'loss': 1.692, 'grad_norm': 0.8327380006577935, 'learning_rate': 1.3026819923371645e-07, 'epoch': 2.82}
 94%|█████████▍| 254/270 [03:24<00:12,  1.29it/s] 94%|█████████▍| 255/270 [03:24<00:11,  1.29it/s]                                                 {'loss': 1.7487, 'grad_norm': 0.782952121847328, 'learning_rate': 1.2260536398467432e-07, 'epoch': 2.83}
 94%|█████████▍| 255/270 [03:24<00:11,  1.29it/s] 95%|█████████▍| 256/270 [03:25<00:10,  1.30it/s]                                                 {'loss': 1.7463, 'grad_norm': 0.7912315192216488, 'learning_rate': 1.1494252873563217e-07, 'epoch': 2.84}
 95%|█████████▍| 256/270 [03:25<00:10,  1.30it/s] 95%|█████████▌| 257/270 [03:26<00:10,  1.28it/s]                                                 {'loss': 1.6631, 'grad_norm': 0.7016312376216705, 'learning_rate': 1.0727969348659003e-07, 'epoch': 2.86}
 95%|█████████▌| 257/270 [03:26<00:10,  1.28it/s] 96%|█████████▌| 258/270 [03:27<00:09,  1.29it/s]                                                 {'loss': 1.7326, 'grad_norm': 0.7395114250039546, 'learning_rate': 9.961685823754789e-08, 'epoch': 2.87}
 96%|█████████▌| 258/270 [03:27<00:09,  1.29it/s] 96%|█████████▌| 259/270 [03:28<00:08,  1.28it/s]                                                 {'loss': 1.7448, 'grad_norm': 0.8046697254522335, 'learning_rate': 9.195402298850574e-08, 'epoch': 2.88}
 96%|█████████▌| 259/270 [03:28<00:08,  1.28it/s] 96%|█████████▋| 260/270 [03:28<00:07,  1.29it/s]                                                 {'loss': 1.6793, 'grad_norm': 0.7863541530722405, 'learning_rate': 8.42911877394636e-08, 'epoch': 2.89}
 96%|█████████▋| 260/270 [03:28<00:07,  1.29it/s] 97%|█████████▋| 261/270 [03:29<00:06,  1.31it/s]                                                 {'loss': 1.719, 'grad_norm': 0.8477938426012559, 'learning_rate': 7.662835249042144e-08, 'epoch': 2.9}
 97%|█████████▋| 261/270 [03:29<00:06,  1.31it/s] 97%|█████████▋| 262/270 [03:30<00:06,  1.32it/s]                                                 {'loss': 1.7212, 'grad_norm': 0.7816998360600921, 'learning_rate': 6.89655172413793e-08, 'epoch': 2.91}
 97%|█████████▋| 262/270 [03:30<00:06,  1.32it/s] 97%|█████████▋| 263/270 [03:31<00:05,  1.32it/s]                                                 {'loss': 1.749, 'grad_norm': 0.8429492280362954, 'learning_rate': 6.130268199233716e-08, 'epoch': 2.92}
 97%|█████████▋| 263/270 [03:31<00:05,  1.32it/s] 98%|█████████▊| 264/270 [03:31<00:04,  1.32it/s]                                                 {'loss': 1.7937, 'grad_norm': 0.8627213080694622, 'learning_rate': 5.3639846743295015e-08, 'epoch': 2.93}
 98%|█████████▊| 264/270 [03:31<00:04,  1.32it/s] 98%|█████████▊| 265/270 [03:32<00:03,  1.30it/s]                                                 {'loss': 1.7876, 'grad_norm': 0.8637768351327183, 'learning_rate': 4.597701149425287e-08, 'epoch': 2.94}
 98%|█████████▊| 265/270 [03:32<00:03,  1.30it/s] 99%|█████████▊| 266/270 [03:33<00:03,  1.30it/s]                                                 {'loss': 1.7377, 'grad_norm': 0.7457628986450595, 'learning_rate': 3.831417624521072e-08, 'epoch': 2.96}
 99%|█████████▊| 266/270 [03:33<00:03,  1.30it/s] 99%|█████████▉| 267/270 [03:34<00:02,  1.31it/s]                                                 {'loss': 1.8064, 'grad_norm': 0.7934863834672033, 'learning_rate': 3.065134099616858e-08, 'epoch': 2.97}
 99%|█████████▉| 267/270 [03:34<00:02,  1.31it/s] 99%|█████████▉| 268/270 [03:34<00:01,  1.31it/s]                                                 {'loss': 1.7168, 'grad_norm': 0.7063863509855071, 'learning_rate': 2.2988505747126436e-08, 'epoch': 2.98}
 99%|█████████▉| 268/270 [03:34<00:01,  1.31it/s]100%|█████████▉| 269/270 [03:35<00:00,  1.31it/s]                                                 {'loss': 1.7508, 'grad_norm': 0.853121348140137, 'learning_rate': 1.532567049808429e-08, 'epoch': 2.99}
100%|█████████▉| 269/270 [03:35<00:00,  1.31it/s]100%|██████████| 270/270 [03:36<00:00,  1.30it/s]                                                 {'loss': 1.7086, 'grad_norm': 0.7614763363216696, 'learning_rate': 7.662835249042145e-09, 'epoch': 3.0}
100%|██████████| 270/270 [03:36<00:00,  1.30it/s]/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
                                                 {'train_runtime': 226.0969, 'train_samples_per_second': 95.375, 'train_steps_per_second': 1.194, 'train_loss': 1.7745300164929143, 'epoch': 3.0}
100%|██████████| 270/270 [03:46<00:00,  1.30it/s]100%|██████████| 270/270 [03:46<00:00,  1.19it/s]
[2024-08-13 06:01:12,005] [INFO] [launch.py:351:main] Process 3620556 exits successfully.
[2024-08-13 06:01:13,006] [INFO] [launch.py:351:main] Process 3620555 exits successfully.
[2024-08-13 06:01:13,006] [INFO] [launch.py:351:main] Process 3620554 exits successfully.
[2024-08-13 06:01:13,006] [INFO] [launch.py:351:main] Process 3620558 exits successfully.
[2024-08-13 06:01:13,007] [INFO] [launch.py:351:main] Process 3620553 exits successfully.
[2024-08-13 06:01:13,007] [INFO] [launch.py:351:main] Process 3620552 exits successfully.
[2024-08-13 06:01:14,007] [INFO] [launch.py:351:main] Process 3620557 exits successfully.
[2024-08-13 06:01:14,008] [INFO] [launch.py:351:main] Process 3620551 exits successfully.
