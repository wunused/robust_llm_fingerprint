Namespace(mode=['alpaca'], base_model='meta-llama/Llama-2-13b-hf', template_name='barebone', total_bsz=64, epoch=3, lr=2e-05, data_path='', task_name='sharegpt', tuned_dir='./cache')
num gpus:  8
Running 1/1: deepspeed --num_gpus=8 train.py --deepspeed ../deepspeed_config/zero3.json --bf16 --tf32=True
        --model_name_or_path meta-llama/Llama-2-13b-hf --data_path ../data/stanford_alpaca/sharegpt_data.json
        --output_dir /fsx-project/yunyun/models/meta-llama_Llama-2-13b-hf_sharegpt_tuned
        --num_train_epochs 3
        --per_device_train_batch_size 10
        --per_device_eval_batch_size 4
        --gradient_accumulation_steps 1
        --gradient_checkpointing=True
        --evaluation_strategy=no
        --save_strategy=steps
        --save_steps 500
        --save_total_limit 1
        --learning_rate 2e-6
        --weight_decay 0.
        --report_to tensorboard
        --warmup_ratio 0.03
        --lr_scheduler_type=cosine
        --logging_steps 1
['deepspeed', '--num_gpus=8', 'train.py', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Llama-2-13b-hf', '--data_path', '../data/stanford_alpaca/sharegpt_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_Llama-2-13b-hf_sharegpt_tuned', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1']
[2024-08-08 06:06:30,870] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2024-08-08 06:06:39,561] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2024-08-08 06:06:39,561] [INFO] [runner.py:568:main] cmd = /data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNSwgNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None train.py --deepspeed ../deepspeed_config/zero3.json --bf16 --tf32=True --model_name_or_path meta-llama/Llama-2-13b-hf --data_path ../data/stanford_alpaca/sharegpt_data.json --output_dir /fsx-project/yunyun/models/meta-llama_Llama-2-13b-hf_sharegpt_tuned --num_train_epochs 3 --per_device_train_batch_size 10 --per_device_eval_batch_size 4 --gradient_accumulation_steps 1 --gradient_checkpointing=True --evaluation_strategy=no --save_strategy=steps --save_steps 500 --save_total_limit 1 --learning_rate 2e-6 --weight_decay 0. --report_to tensorboard --warmup_ratio 0.03 --lr_scheduler_type=cosine --logging_steps 1
[2024-08-08 06:06:42,141] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2024-08-08 06:06:45,975] [INFO] [launch.py:139:main] 0 NCCL_ASYNC_ERROR_HANDLING=1
[2024-08-08 06:06:45,975] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]}
[2024-08-08 06:06:45,975] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=8, node_rank=0
[2024-08-08 06:06:45,975] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]})
[2024-08-08 06:06:45,975] [INFO] [launch.py:164:main] dist_world_size=8
[2024-08-08 06:06:45,975] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
[2024-08-08 06:06:45,976] [INFO] [launch.py:256:main] process 3103784 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=0', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Llama-2-13b-hf', '--data_path', '../data/stanford_alpaca/sharegpt_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_Llama-2-13b-hf_sharegpt_tuned', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1']
[2024-08-08 06:06:45,977] [INFO] [launch.py:256:main] process 3103785 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=1', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Llama-2-13b-hf', '--data_path', '../data/stanford_alpaca/sharegpt_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_Llama-2-13b-hf_sharegpt_tuned', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1']
[2024-08-08 06:06:45,978] [INFO] [launch.py:256:main] process 3103786 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=2', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Llama-2-13b-hf', '--data_path', '../data/stanford_alpaca/sharegpt_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_Llama-2-13b-hf_sharegpt_tuned', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1']
[2024-08-08 06:06:45,978] [INFO] [launch.py:256:main] process 3103787 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=3', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Llama-2-13b-hf', '--data_path', '../data/stanford_alpaca/sharegpt_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_Llama-2-13b-hf_sharegpt_tuned', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1']
[2024-08-08 06:06:45,979] [INFO] [launch.py:256:main] process 3103788 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=4', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Llama-2-13b-hf', '--data_path', '../data/stanford_alpaca/sharegpt_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_Llama-2-13b-hf_sharegpt_tuned', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1']
[2024-08-08 06:06:45,979] [INFO] [launch.py:256:main] process 3103789 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=5', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Llama-2-13b-hf', '--data_path', '../data/stanford_alpaca/sharegpt_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_Llama-2-13b-hf_sharegpt_tuned', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1']
[2024-08-08 06:06:45,980] [INFO] [launch.py:256:main] process 3103790 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=6', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Llama-2-13b-hf', '--data_path', '../data/stanford_alpaca/sharegpt_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_Llama-2-13b-hf_sharegpt_tuned', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1']
[2024-08-08 06:06:45,981] [INFO] [launch.py:256:main] process 3103791 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=7', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Llama-2-13b-hf', '--data_path', '../data/stanford_alpaca/sharegpt_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_Llama-2-13b-hf_sharegpt_tuned', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1']
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
[2024-08-08 06:07:01,763] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2024-08-08 06:07:02,060] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
[2024-08-08 06:07:02,090] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-08 06:07:02,176] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-08 06:07:02,233] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-08 06:07:02,234] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-08 06:07:02,247] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-08 06:07:02,255] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[2024-08-08 06:07:02,509] [INFO] [comm.py:637:init_distributed] cdb=None
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2024-08-08 06:07:02,794] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-08-08 06:07:02,845] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-08-08 06:07:02,895] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-08-08 06:07:02,960] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-08-08 06:07:02,967] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-08-08 06:07:02,975] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-08-08 06:07:02,977] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-08-08 06:07:02,977] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)

Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]
Downloading shards: 100%|██████████| 3/3 [00:00<00:00, 339.88it/s]

Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]
Downloading shards: 100%|██████████| 3/3 [00:00<00:00, 1433.79it/s]

Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]
Downloading shards: 100%|██████████| 3/3 [00:00<00:00, 1447.14it/s]

Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]
Downloading shards: 100%|██████████| 3/3 [00:00<00:00, 1285.94it/s]

Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]
Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]
Downloading shards: 100%|██████████| 3/3 [00:00<00:00, 1345.05it/s]

Downloading shards: 100%|██████████| 3/3 [00:00<00:00, 1204.11it/s]

Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]
Downloading shards: 100%|██████████| 3/3 [00:00<00:00, 1776.24it/s]

Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]
Downloading shards: 100%|██████████| 3/3 [00:00<00:00, 1620.67it/s]
[2024-08-08 06:07:14,204] [INFO] [partition_parameters.py:345:__exit__] finished initializing model - num_params = 363, num_elems = 13.02B

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:14<00:28, 14.16s/it]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:14<00:28, 14.16s/it]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:14<00:28, 14.17s/it]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:14<00:28, 14.16s/it]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:14<00:28, 14.16s/it]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:14<00:28, 14.16s/it]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:14<00:28, 14.23s/it]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:48<01:37, 48.81s/it]
Loading checkpoint shards:  67%|██████▋   | 2/3 [01:01<00:33, 33.52s/it]
Loading checkpoint shards:  67%|██████▋   | 2/3 [01:01<00:33, 33.52s/it]
Loading checkpoint shards:  67%|██████▋   | 2/3 [01:01<00:33, 33.53s/it]
Loading checkpoint shards:  67%|██████▋   | 2/3 [01:01<00:33, 33.52s/it]
Loading checkpoint shards:  67%|██████▋   | 2/3 [01:01<00:33, 33.52s/it]
Loading checkpoint shards:  67%|██████▋   | 2/3 [01:01<00:33, 33.53s/it]
Loading checkpoint shards:  67%|██████▋   | 2/3 [01:01<00:33, 33.53s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [01:31<00:00, 31.94s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [01:31<00:00, 30.44s/it]

Loading checkpoint shards: 100%|██████████| 3/3 [01:31<00:00, 31.94s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [01:31<00:00, 30.43s/it]

Loading checkpoint shards: 100%|██████████| 3/3 [01:31<00:00, 31.94s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [01:31<00:00, 30.43s/it]

Loading checkpoint shards: 100%|██████████| 3/3 [01:31<00:00, 31.94s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [01:31<00:00, 30.43s/it]

Loading checkpoint shards: 100%|██████████| 3/3 [01:31<00:00, 31.94s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [01:31<00:00, 30.43s/it]

Loading checkpoint shards: 100%|██████████| 3/3 [01:31<00:00, 31.94s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [01:31<00:00, 30.43s/it]

Loading checkpoint shards: 100%|██████████| 3/3 [01:31<00:00, 31.95s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [01:31<00:00, 30.44s/it]

Loading checkpoint shards:  67%|██████▋   | 2/3 [01:37<00:48, 48.59s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [02:13<00:00, 42.98s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [02:13<00:00, 44.52s/it]
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
[rank1]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[rank3]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
[rank2]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121/fused_adam/build.ninja...
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/cpp_extension.py:1967: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
[rank0]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
[rank4]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
[rank5]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
[rank6]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
[rank7]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
[1/3] /usr/local/cuda-12.1/bin/nvcc --generate-dependencies-with-compile --dependency-output multi_tensor_adam.cuda.o.d -DTORCH_EXTENSION_NAME=fused_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/deepspeed/ops/csrc/includes -I/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/deepspeed/ops/csrc/adam -isystem /data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/include -isystem /data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/include/TH -isystem /data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/include/THC -isystem /usr/local/cuda-12.1/include -isystem /data/home/yunyun/miniconda3/envs/newtorch2/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_90,code=compute_90 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -lineinfo --use_fast_math -gencode=arch=compute_90,code=sm_90 -gencode=arch=compute_90,code=compute_90 -DBF16_AVAILABLE -U__CUDA_NO_BFLOAT16_OPERATORS__ -U__CUDA_NO_BFLOAT162_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ -std=c++17 -c /data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/deepspeed/ops/csrc/adam/multi_tensor_adam.cu -o multi_tensor_adam.cuda.o 
[2/3] c++ -MMD -MF fused_adam_frontend.o.d -DTORCH_EXTENSION_NAME=fused_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/deepspeed/ops/csrc/includes -I/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/deepspeed/ops/csrc/adam -isystem /data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/include -isystem /data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/include/TH -isystem /data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/include/THC -isystem /usr/local/cuda-12.1/include -isystem /data/home/yunyun/miniconda3/envs/newtorch2/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -std=c++17 -g -Wno-reorder -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DBF16_AVAILABLE -c /data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/deepspeed/ops/csrc/adam/fused_adam_frontend.cpp -o fused_adam_frontend.o 
[3/3] c++ fused_adam_frontend.o multi_tensor_adam.cuda.o -shared -L/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda-12.1/lib64 -lcudart -o fused_adam.so
Loading extension module fused_adam...
Loading extension module fused_adam...
Loading extension module fused_adam...
Loading extension module fused_adam...
Loading extension module fused_adam...
Loading extension module fused_adam...
Loading extension module fused_adam...
Loading extension module fused_adam...
Time to load fused_adam op: 37.62155532836914 secondsTime to load fused_adam op: 37.33340072631836 secondsTime to load fused_adam op: 36.11856389045715 seconds
Time to load fused_adam op: 36.22123312950134 secondsTime to load fused_adam op: 36.98646831512451 secondsTime to load fused_adam op: 36.44482946395874 seconds

Time to load fused_adam op: 37.49839377403259 seconds



Time to load fused_adam op: 37.6238534450531 seconds
Parameter Offload: Total persistent parameters: 414720 in 81 params
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.

  0%|          | 0/630 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

  0%|          | 1/630 [00:09<1:38:13,  9.37s/it]
                                                 
{'loss': 0.8061, 'grad_norm': 4.050934142675164, 'learning_rate': 0.0, 'epoch': 0.0}

  0%|          | 1/630 [00:09<1:38:13,  9.37s/it]
  0%|          | 2/630 [00:10<50:20,  4.81s/it]  
                                               
{'loss': 0.8206, 'grad_norm': 3.5604410085650264, 'learning_rate': 4.7081782673327645e-07, 'epoch': 0.01}

  0%|          | 2/630 [00:10<50:20,  4.81s/it]
  0%|          | 3/630 [00:12<34:50,  3.33s/it]
                                               
{'loss': 0.838, 'grad_norm': 3.771140101012531, 'learning_rate': 7.462286000432739e-07, 'epoch': 0.01}

  0%|          | 3/630 [00:12<34:50,  3.33s/it]
  1%|          | 4/630 [00:14<27:42,  2.66s/it]
                                               
{'loss': 0.7521, 'grad_norm': 3.4071560792119704, 'learning_rate': 9.416356534665529e-07, 'epoch': 0.02}

  1%|          | 4/630 [00:14<27:42,  2.66s/it]
  1%|          | 5/630 [00:15<23:37,  2.27s/it]
                                               
{'loss': 0.8014, 'grad_norm': 3.662974566813687, 'learning_rate': 1.0932051394658049e-06, 'epoch': 0.02}

  1%|          | 5/630 [00:15<23:37,  2.27s/it]
  1%|          | 6/630 [00:17<21:09,  2.03s/it]
                                               
{'loss': 0.7747, 'grad_norm': 3.1880371021790292, 'learning_rate': 1.2170464267765503e-06, 'epoch': 0.03}

  1%|          | 6/630 [00:17<21:09,  2.03s/it]
  1%|          | 7/630 [00:18<19:36,  1.89s/it]
                                               
{'loss': 0.7899, 'grad_norm': 3.7658665618269667, 'learning_rate': 1.3217527432721277e-06, 'epoch': 0.03}

  1%|          | 7/630 [00:18<19:36,  1.89s/it]
  1%|▏         | 8/630 [00:20<18:34,  1.79s/it]
                                               
{'loss': 0.8393, 'grad_norm': 3.818023491908964, 'learning_rate': 1.4124534801998293e-06, 'epoch': 0.04}

  1%|▏         | 8/630 [00:20<18:34,  1.79s/it]
  1%|▏         | 9/630 [00:22<17:53,  1.73s/it]
                                               
{'loss': 0.8077, 'grad_norm': 3.683586229231507, 'learning_rate': 1.4924572000865478e-06, 'epoch': 0.04}

  1%|▏         | 9/630 [00:22<17:53,  1.73s/it]
  2%|▏         | 10/630 [00:23<17:25,  1.69s/it]
                                                
{'loss': 0.705, 'grad_norm': 2.0685978534154006, 'learning_rate': 1.5640229661990816e-06, 'epoch': 0.05}

  2%|▏         | 10/630 [00:23<17:25,  1.69s/it]
  2%|▏         | 11/630 [00:25<17:05,  1.66s/it]
                                                
{'loss': 0.7775, 'grad_norm': 2.486314906100815, 'learning_rate': 1.6287620764191933e-06, 'epoch': 0.05}

  2%|▏         | 11/630 [00:25<17:05,  1.66s/it]
  2%|▏         | 12/630 [00:27<17:14,  1.67s/it]
                                                
{'loss': 0.7055, 'grad_norm': 2.3371241408385357, 'learning_rate': 1.6878642535098268e-06, 'epoch': 0.06}

  2%|▏         | 12/630 [00:27<17:14,  1.67s/it]
  2%|▏         | 13/630 [00:28<16:56,  1.65s/it]
                                                
{'loss': 0.8215, 'grad_norm': 2.62479042127487, 'learning_rate': 1.7422329860526872e-06, 'epoch': 0.06}

  2%|▏         | 13/630 [00:28<16:56,  1.65s/it]
  2%|▏         | 14/630 [00:30<16:43,  1.63s/it]
                                                
{'loss': 0.7502, 'grad_norm': 1.9795954398299402, 'learning_rate': 1.792570570005404e-06, 'epoch': 0.07}

  2%|▏         | 14/630 [00:30<16:43,  1.63s/it]
  2%|▏         | 15/630 [00:31<16:40,  1.63s/it]
                                                
{'loss': 0.6631, 'grad_norm': 1.4878969640901032, 'learning_rate': 1.8394337395090787e-06, 'epoch': 0.07}

  2%|▏         | 15/630 [00:31<16:40,  1.63s/it]
  3%|▎         | 16/630 [00:33<16:32,  1.62s/it]
                                                
{'loss': 0.7021, 'grad_norm': 2.1042253853149857, 'learning_rate': 1.8832713069331058e-06, 'epoch': 0.08}

  3%|▎         | 16/630 [00:33<16:32,  1.62s/it]
  3%|▎         | 17/630 [00:34<16:26,  1.61s/it]
                                                
{'loss': 0.7744, 'grad_norm': 2.4357424865861144, 'learning_rate': 1.924450371770508e-06, 'epoch': 0.08}

  3%|▎         | 17/630 [00:34<16:26,  1.61s/it]
  3%|▎         | 18/630 [00:36<16:25,  1.61s/it]
                                                
{'loss': 0.7084, 'grad_norm': 1.8688175078751568, 'learning_rate': 1.9632750268198243e-06, 'epoch': 0.09}

  3%|▎         | 18/630 [00:36<16:25,  1.61s/it]
  3%|▎         | 19/630 [00:38<16:20,  1.60s/it]
                                                
{'loss': 0.6802, 'grad_norm': 1.7496207306504432, 'learning_rate': 2e-06, 'epoch': 0.09}

  3%|▎         | 19/630 [00:38<16:20,  1.60s/it]
  3%|▎         | 20/630 [00:39<16:19,  1.61s/it]
                                                
{'loss': 0.7295, 'grad_norm': 1.9912332060728182, 'learning_rate': 2e-06, 'epoch': 0.1}

  3%|▎         | 20/630 [00:39<16:19,  1.61s/it]
  3%|▎         | 21/630 [00:41<16:13,  1.60s/it]
                                                
{'loss': 0.6955, 'grad_norm': 2.059326096937497, 'learning_rate': 1.9967266775777412e-06, 'epoch': 0.1}

  3%|▎         | 21/630 [00:41<16:13,  1.60s/it]
  3%|▎         | 22/630 [00:42<16:10,  1.60s/it]
                                                
{'loss': 0.6938, 'grad_norm': 1.568050811816445, 'learning_rate': 1.9934533551554826e-06, 'epoch': 0.1}

  3%|▎         | 22/630 [00:42<16:10,  1.60s/it]
  4%|▎         | 23/630 [00:44<16:13,  1.60s/it]
                                                
{'loss': 0.7868, 'grad_norm': 1.7905299849326985, 'learning_rate': 1.990180032733224e-06, 'epoch': 0.11}

  4%|▎         | 23/630 [00:44<16:13,  1.60s/it]
  4%|▍         | 24/630 [00:46<16:19,  1.62s/it]
                                                
{'loss': 0.7752, 'grad_norm': 2.043645806917682, 'learning_rate': 1.9869067103109657e-06, 'epoch': 0.11}

  4%|▍         | 24/630 [00:46<16:19,  1.62s/it]
  4%|▍         | 25/630 [00:47<16:13,  1.61s/it]
                                                
{'loss': 0.6916, 'grad_norm': 1.8632072460628166, 'learning_rate': 1.983633387888707e-06, 'epoch': 0.12}

  4%|▍         | 25/630 [00:47<16:13,  1.61s/it]
  4%|▍         | 26/630 [00:49<17:48,  1.77s/it]
                                                
{'loss': 0.7269, 'grad_norm': 1.63028574378507, 'learning_rate': 1.9803600654664483e-06, 'epoch': 0.12}

  4%|▍         | 26/630 [00:49<17:48,  1.77s/it]
  4%|▍         | 27/630 [00:51<17:14,  1.72s/it]
                                                
{'loss': 0.6761, 'grad_norm': 1.6269542027624917, 'learning_rate': 1.9770867430441897e-06, 'epoch': 0.13}

  4%|▍         | 27/630 [00:51<17:14,  1.72s/it]
  4%|▍         | 28/630 [00:53<16:48,  1.68s/it]
                                                
{'loss': 0.7336, 'grad_norm': 1.8413244735066852, 'learning_rate': 1.9738134206219314e-06, 'epoch': 0.13}

  4%|▍         | 28/630 [00:53<16:48,  1.68s/it]
  5%|▍         | 29/630 [00:54<16:31,  1.65s/it]
                                                
{'loss': 0.7008, 'grad_norm': 1.683659163915131, 'learning_rate': 1.9705400981996723e-06, 'epoch': 0.14}

  5%|▍         | 29/630 [00:54<16:31,  1.65s/it]
  5%|▍         | 30/630 [00:56<16:20,  1.63s/it]
                                                
{'loss': 0.826, 'grad_norm': 1.9425515016334973, 'learning_rate': 1.967266775777414e-06, 'epoch': 0.14}

  5%|▍         | 30/630 [00:56<16:20,  1.63s/it]
  5%|▍         | 31/630 [00:57<16:11,  1.62s/it]
                                                
{'loss': 0.6569, 'grad_norm': 1.596336980176858, 'learning_rate': 1.9639934533551554e-06, 'epoch': 0.15}

  5%|▍         | 31/630 [00:57<16:11,  1.62s/it]
  5%|▌         | 32/630 [00:59<16:05,  1.61s/it]
                                                
{'loss': 0.649, 'grad_norm': 2.3156488092692427, 'learning_rate': 1.9607201309328968e-06, 'epoch': 0.15}

  5%|▌         | 32/630 [00:59<16:05,  1.61s/it]
  5%|▌         | 33/630 [01:01<16:00,  1.61s/it]
                                                
{'loss': 0.6866, 'grad_norm': 1.5310129890467734, 'learning_rate': 1.957446808510638e-06, 'epoch': 0.16}

  5%|▌         | 33/630 [01:01<16:00,  1.61s/it]
  5%|▌         | 34/630 [01:02<15:56,  1.60s/it]
                                                
{'loss': 0.6231, 'grad_norm': 1.4748587706690088, 'learning_rate': 1.9541734860883794e-06, 'epoch': 0.16}

  5%|▌         | 34/630 [01:02<15:56,  1.60s/it]
  6%|▌         | 35/630 [01:04<15:54,  1.60s/it]
                                                
{'loss': 0.6399, 'grad_norm': 1.4503878090616162, 'learning_rate': 1.950900163666121e-06, 'epoch': 0.17}

  6%|▌         | 35/630 [01:04<15:54,  1.60s/it]
  6%|▌         | 36/630 [01:05<15:57,  1.61s/it]
                                                
{'loss': 0.7024, 'grad_norm': 1.636713881940829, 'learning_rate': 1.9476268412438625e-06, 'epoch': 0.17}

  6%|▌         | 36/630 [01:05<15:57,  1.61s/it]
  6%|▌         | 37/630 [01:07<15:51,  1.60s/it]
                                                
{'loss': 0.6585, 'grad_norm': 2.864593495867582, 'learning_rate': 1.944353518821604e-06, 'epoch': 0.18}

  6%|▌         | 37/630 [01:07<15:51,  1.60s/it]
  6%|▌         | 38/630 [01:09<15:47,  1.60s/it]
                                                
{'loss': 0.6862, 'grad_norm': 1.8915776369304325, 'learning_rate': 1.941080196399345e-06, 'epoch': 0.18}

  6%|▌         | 38/630 [01:09<15:47,  1.60s/it]
  6%|▌         | 39/630 [01:10<15:44,  1.60s/it]
                                                
{'loss': 0.6646, 'grad_norm': 1.4173760887382651, 'learning_rate': 1.9378068739770865e-06, 'epoch': 0.19}

  6%|▌         | 39/630 [01:10<15:44,  1.60s/it]
  6%|▋         | 40/630 [01:12<15:42,  1.60s/it]
                                                
{'loss': 0.6863, 'grad_norm': 1.410886753285256, 'learning_rate': 1.934533551554828e-06, 'epoch': 0.19}

  6%|▋         | 40/630 [01:12<15:42,  1.60s/it]
  7%|▋         | 41/630 [01:13<15:38,  1.59s/it]
                                                
{'loss': 0.6868, 'grad_norm': 1.959672667515751, 'learning_rate': 1.9312602291325696e-06, 'epoch': 0.2}

  7%|▋         | 41/630 [01:13<15:38,  1.59s/it]
  7%|▋         | 42/630 [01:15<15:38,  1.60s/it]
                                                
{'loss': 0.6137, 'grad_norm': 1.2159583712906477, 'learning_rate': 1.927986906710311e-06, 'epoch': 0.2}

  7%|▋         | 42/630 [01:15<15:38,  1.60s/it]
  7%|▋         | 43/630 [01:17<15:38,  1.60s/it]
                                                
{'loss': 0.7529, 'grad_norm': 1.8001203851222902, 'learning_rate': 1.9247135842880523e-06, 'epoch': 0.2}

  7%|▋         | 43/630 [01:17<15:38,  1.60s/it]
  7%|▋         | 44/630 [01:18<15:35,  1.60s/it]
                                                
{'loss': 0.7291, 'grad_norm': 1.7047931030610246, 'learning_rate': 1.9214402618657936e-06, 'epoch': 0.21}

  7%|▋         | 44/630 [01:18<15:35,  1.60s/it]
  7%|▋         | 45/630 [01:20<15:33,  1.60s/it]
                                                
{'loss': 0.7025, 'grad_norm': 1.976416724980408, 'learning_rate': 1.918166939443535e-06, 'epoch': 0.21}

  7%|▋         | 45/630 [01:20<15:33,  1.60s/it]
  7%|▋         | 46/630 [01:21<15:32,  1.60s/it]
                                                
{'loss': 0.6817, 'grad_norm': 1.4771206340754919, 'learning_rate': 1.9148936170212767e-06, 'epoch': 0.22}

  7%|▋         | 46/630 [01:21<15:32,  1.60s/it]
  7%|▋         | 47/630 [01:23<15:30,  1.60s/it]
                                                
{'loss': 0.6796, 'grad_norm': 1.6926125929389246, 'learning_rate': 1.911620294599018e-06, 'epoch': 0.22}

  7%|▋         | 47/630 [01:23<15:30,  1.60s/it]
  8%|▊         | 48/630 [01:25<15:28,  1.60s/it]
                                                
{'loss': 0.6719, 'grad_norm': 1.8194783533955494, 'learning_rate': 1.9083469721767594e-06, 'epoch': 0.23}

  8%|▊         | 48/630 [01:25<15:28,  1.60s/it]
  8%|▊         | 49/630 [01:26<15:26,  1.60s/it]
                                                
{'loss': 0.6688, 'grad_norm': 1.5809700985979347, 'learning_rate': 1.9050736497545007e-06, 'epoch': 0.23}

  8%|▊         | 49/630 [01:26<15:26,  1.60s/it]
  8%|▊         | 50/630 [01:28<15:25,  1.60s/it]
                                                
{'loss': 0.6711, 'grad_norm': 1.5231048313137128, 'learning_rate': 1.901800327332242e-06, 'epoch': 0.24}

  8%|▊         | 50/630 [01:28<15:25,  1.60s/it]
  8%|▊         | 51/630 [01:29<15:22,  1.59s/it]
                                                
{'loss': 0.6199, 'grad_norm': 1.390662939085316, 'learning_rate': 1.8985270049099836e-06, 'epoch': 0.24}

  8%|▊         | 51/630 [01:29<15:22,  1.59s/it]
  8%|▊         | 52/630 [01:31<15:22,  1.60s/it]
                                                
{'loss': 0.6467, 'grad_norm': 1.3421592153339483, 'learning_rate': 1.895253682487725e-06, 'epoch': 0.25}

  8%|▊         | 52/630 [01:31<15:22,  1.60s/it]
  8%|▊         | 53/630 [01:33<15:22,  1.60s/it]
                                                
{'loss': 0.6036, 'grad_norm': 1.5033366642331936, 'learning_rate': 1.8919803600654665e-06, 'epoch': 0.25}

  8%|▊         | 53/630 [01:33<15:22,  1.60s/it]
  9%|▊         | 54/630 [01:34<15:21,  1.60s/it]
                                                
{'loss': 0.7184, 'grad_norm': 1.7346378779454346, 'learning_rate': 1.8887070376432076e-06, 'epoch': 0.26}

  9%|▊         | 54/630 [01:34<15:21,  1.60s/it]
  9%|▊         | 55/630 [01:36<15:19,  1.60s/it]
                                                
{'loss': 0.6828, 'grad_norm': 1.8493659990615536, 'learning_rate': 1.8854337152209492e-06, 'epoch': 0.26}

  9%|▊         | 55/630 [01:36<15:19,  1.60s/it]
  9%|▉         | 56/630 [01:37<15:19,  1.60s/it]
                                                
{'loss': 0.6467, 'grad_norm': 1.3803918553896068, 'learning_rate': 1.8821603927986907e-06, 'epoch': 0.27}

  9%|▉         | 56/630 [01:37<15:19,  1.60s/it]
  9%|▉         | 57/630 [01:39<15:18,  1.60s/it]
                                                
{'loss': 0.6063, 'grad_norm': 1.4744684902537424, 'learning_rate': 1.8788870703764318e-06, 'epoch': 0.27}

  9%|▉         | 57/630 [01:39<15:18,  1.60s/it]
  9%|▉         | 58/630 [01:41<15:17,  1.60s/it]
                                                
{'loss': 0.669, 'grad_norm': 1.3779517257446081, 'learning_rate': 1.8756137479541734e-06, 'epoch': 0.28}

  9%|▉         | 58/630 [01:41<15:17,  1.60s/it]
  9%|▉         | 59/630 [01:42<15:14,  1.60s/it]
                                                
{'loss': 0.6894, 'grad_norm': 1.6039568547929888, 'learning_rate': 1.872340425531915e-06, 'epoch': 0.28}

  9%|▉         | 59/630 [01:42<15:14,  1.60s/it]
 10%|▉         | 60/630 [01:44<15:14,  1.60s/it]
                                                
{'loss': 0.728, 'grad_norm': 1.8914889397616954, 'learning_rate': 1.8690671031096563e-06, 'epoch': 0.29}

 10%|▉         | 60/630 [01:44<15:14,  1.60s/it]
 10%|▉         | 61/630 [01:45<15:13,  1.61s/it]
                                                
{'loss': 0.6521, 'grad_norm': 1.3341155683029757, 'learning_rate': 1.8657937806873976e-06, 'epoch': 0.29}

 10%|▉         | 61/630 [01:45<15:13,  1.61s/it]
 10%|▉         | 62/630 [01:47<15:10,  1.60s/it]
                                                
{'loss': 0.5625, 'grad_norm': 1.4243714189528183, 'learning_rate': 1.862520458265139e-06, 'epoch': 0.3}

 10%|▉         | 62/630 [01:47<15:10,  1.60s/it]
 10%|█         | 63/630 [01:49<15:09,  1.60s/it]
                                                
{'loss': 0.685, 'grad_norm': 1.5486409004699833, 'learning_rate': 1.8592471358428805e-06, 'epoch': 0.3}

 10%|█         | 63/630 [01:49<15:09,  1.60s/it]
 10%|█         | 64/630 [01:50<15:07,  1.60s/it]
                                                
{'loss': 0.6814, 'grad_norm': 1.5071124592748655, 'learning_rate': 1.8559738134206218e-06, 'epoch': 0.3}

 10%|█         | 64/630 [01:50<15:07,  1.60s/it]
 10%|█         | 65/630 [01:52<15:04,  1.60s/it]
                                                
{'loss': 0.6243, 'grad_norm': 1.612469502558154, 'learning_rate': 1.8527004909983632e-06, 'epoch': 0.31}

 10%|█         | 65/630 [01:52<15:04,  1.60s/it]
 10%|█         | 66/630 [01:53<15:02,  1.60s/it]
                                                
{'loss': 0.727, 'grad_norm': 1.4945531400632195, 'learning_rate': 1.8494271685761047e-06, 'epoch': 0.31}

 10%|█         | 66/630 [01:53<15:02,  1.60s/it]
 11%|█         | 67/630 [01:55<15:01,  1.60s/it]
                                                
{'loss': 0.7022, 'grad_norm': 1.8874497957237832, 'learning_rate': 1.8461538461538462e-06, 'epoch': 0.32}

 11%|█         | 67/630 [01:55<15:01,  1.60s/it]
 11%|█         | 68/630 [01:57<14:58,  1.60s/it]
                                                
{'loss': 0.652, 'grad_norm': 1.6550866857301443, 'learning_rate': 1.8428805237315874e-06, 'epoch': 0.32}

 11%|█         | 68/630 [01:57<14:58,  1.60s/it]
 11%|█         | 69/630 [01:58<14:56,  1.60s/it]
                                                
{'loss': 0.6603, 'grad_norm': 1.479651042612746, 'learning_rate': 1.839607201309329e-06, 'epoch': 0.33}

 11%|█         | 69/630 [01:58<14:56,  1.60s/it]
 11%|█         | 70/630 [02:00<14:54,  1.60s/it]
                                                
{'loss': 0.6397, 'grad_norm': 1.4475489748628612, 'learning_rate': 1.8363338788870705e-06, 'epoch': 0.33}

 11%|█         | 70/630 [02:00<14:54,  1.60s/it]
 11%|█▏        | 71/630 [02:01<14:54,  1.60s/it]
                                                
{'loss': 0.6637, 'grad_norm': 1.6532807561246319, 'learning_rate': 1.8330605564648116e-06, 'epoch': 0.34}

 11%|█▏        | 71/630 [02:01<14:54,  1.60s/it]
 11%|█▏        | 72/630 [02:03<14:53,  1.60s/it]
                                                
{'loss': 0.623, 'grad_norm': 1.622925868477613, 'learning_rate': 1.8297872340425531e-06, 'epoch': 0.34}

 11%|█▏        | 72/630 [02:03<14:53,  1.60s/it]
 12%|█▏        | 73/630 [02:05<14:51,  1.60s/it]
                                                
{'loss': 0.6608, 'grad_norm': 2.033382619861869, 'learning_rate': 1.8265139116202945e-06, 'epoch': 0.35}

 12%|█▏        | 73/630 [02:05<14:51,  1.60s/it]
 12%|█▏        | 74/630 [02:06<14:50,  1.60s/it]
                                                
{'loss': 0.71, 'grad_norm': 1.8371783007991227, 'learning_rate': 1.823240589198036e-06, 'epoch': 0.35}

 12%|█▏        | 74/630 [02:06<14:50,  1.60s/it]
 12%|█▏        | 75/630 [02:08<14:46,  1.60s/it]
                                                
{'loss': 0.6763, 'grad_norm': 1.8568706111706998, 'learning_rate': 1.8199672667757774e-06, 'epoch': 0.36}

 12%|█▏        | 75/630 [02:08<14:46,  1.60s/it]
 12%|█▏        | 76/630 [02:09<14:44,  1.60s/it]
                                                
{'loss': 0.6278, 'grad_norm': 1.301547417988927, 'learning_rate': 1.8166939443535187e-06, 'epoch': 0.36}

 12%|█▏        | 76/630 [02:09<14:44,  1.60s/it]
 12%|█▏        | 77/630 [02:11<14:45,  1.60s/it]
                                                
{'loss': 0.6604, 'grad_norm': 1.5986504847029575, 'learning_rate': 1.8134206219312602e-06, 'epoch': 0.37}

 12%|█▏        | 77/630 [02:11<14:45,  1.60s/it]
 12%|█▏        | 78/630 [02:13<14:42,  1.60s/it]
                                                
{'loss': 0.7295, 'grad_norm': 1.6660367671243652, 'learning_rate': 1.8101472995090016e-06, 'epoch': 0.37}

 12%|█▏        | 78/630 [02:13<14:42,  1.60s/it]
 13%|█▎        | 79/630 [02:14<14:40,  1.60s/it]
                                                
{'loss': 0.5834, 'grad_norm': 1.33235505108732, 'learning_rate': 1.806873977086743e-06, 'epoch': 0.38}

 13%|█▎        | 79/630 [02:14<14:40,  1.60s/it]
 13%|█▎        | 80/630 [02:16<14:41,  1.60s/it]
                                                
{'loss': 0.644, 'grad_norm': 1.259092079573149, 'learning_rate': 1.8036006546644845e-06, 'epoch': 0.38}

 13%|█▎        | 80/630 [02:16<14:41,  1.60s/it]
 13%|█▎        | 81/630 [02:17<14:37,  1.60s/it]
                                                
{'loss': 0.7423, 'grad_norm': 1.9069777919417719, 'learning_rate': 1.8003273322422258e-06, 'epoch': 0.39}

 13%|█▎        | 81/630 [02:17<14:37,  1.60s/it]
 13%|█▎        | 82/630 [02:19<14:34,  1.60s/it]
                                                
{'loss': 0.6849, 'grad_norm': 1.6346248007458075, 'learning_rate': 1.7970540098199671e-06, 'epoch': 0.39}

 13%|█▎        | 82/630 [02:19<14:34,  1.60s/it]
 13%|█▎        | 83/630 [02:21<14:32,  1.60s/it]
                                                
{'loss': 0.6221, 'grad_norm': 1.5987850399168868, 'learning_rate': 1.7937806873977087e-06, 'epoch': 0.4}

 13%|█▎        | 83/630 [02:21<14:32,  1.60s/it]
 13%|█▎        | 84/630 [02:22<14:30,  1.60s/it]
                                                
{'loss': 0.6118, 'grad_norm': 1.361272406270783, 'learning_rate': 1.79050736497545e-06, 'epoch': 0.4}

 13%|█▎        | 84/630 [02:22<14:30,  1.60s/it]
 13%|█▎        | 85/630 [02:24<14:30,  1.60s/it]
                                                
{'loss': 0.6442, 'grad_norm': 1.5231784107222435, 'learning_rate': 1.7872340425531913e-06, 'epoch': 0.4}

 13%|█▎        | 85/630 [02:24<14:30,  1.60s/it]
 14%|█▎        | 86/630 [02:25<14:28,  1.60s/it]
                                                
{'loss': 0.6293, 'grad_norm': 1.7465454003851184, 'learning_rate': 1.7839607201309329e-06, 'epoch': 0.41}

 14%|█▎        | 86/630 [02:25<14:28,  1.60s/it]
 14%|█▍        | 87/630 [02:27<14:29,  1.60s/it]
                                                
{'loss': 0.6977, 'grad_norm': 1.5299260161936068, 'learning_rate': 1.7806873977086742e-06, 'epoch': 0.41}

 14%|█▍        | 87/630 [02:27<14:29,  1.60s/it]
 14%|█▍        | 88/630 [02:29<14:27,  1.60s/it]
                                                
{'loss': 0.6021, 'grad_norm': 1.4107845986231997, 'learning_rate': 1.7774140752864158e-06, 'epoch': 0.42}

 14%|█▍        | 88/630 [02:29<14:27,  1.60s/it]
 14%|█▍        | 89/630 [02:30<14:24,  1.60s/it]
                                                
{'loss': 0.6395, 'grad_norm': 2.1901456633108007, 'learning_rate': 1.7741407528641569e-06, 'epoch': 0.42}

 14%|█▍        | 89/630 [02:30<14:24,  1.60s/it]
 14%|█▍        | 90/630 [02:32<14:22,  1.60s/it]
                                                
{'loss': 0.632, 'grad_norm': 1.4720886151400663, 'learning_rate': 1.7708674304418984e-06, 'epoch': 0.43}

 14%|█▍        | 90/630 [02:32<14:22,  1.60s/it]
 14%|█▍        | 91/630 [02:33<14:21,  1.60s/it]
                                                
{'loss': 0.7398, 'grad_norm': 1.529775795046751, 'learning_rate': 1.76759410801964e-06, 'epoch': 0.43}

 14%|█▍        | 91/630 [02:33<14:21,  1.60s/it]
 15%|█▍        | 92/630 [02:35<14:19,  1.60s/it]
                                                
{'loss': 0.6624, 'grad_norm': 1.8724091604316238, 'learning_rate': 1.764320785597381e-06, 'epoch': 0.44}

 15%|█▍        | 92/630 [02:35<14:19,  1.60s/it]
 15%|█▍        | 93/630 [02:37<14:22,  1.61s/it]
                                                
{'loss': 0.7173, 'grad_norm': 1.8499795814407307, 'learning_rate': 1.7610474631751227e-06, 'epoch': 0.44}

 15%|█▍        | 93/630 [02:37<14:22,  1.61s/it]
 15%|█▍        | 94/630 [02:38<14:20,  1.61s/it]
                                                
{'loss': 0.7205, 'grad_norm': 1.4595624117521908, 'learning_rate': 1.7577741407528642e-06, 'epoch': 0.45}

 15%|█▍        | 94/630 [02:38<14:20,  1.61s/it]
 15%|█▌        | 95/630 [02:40<14:17,  1.60s/it]
                                                
{'loss': 0.5724, 'grad_norm': 1.4906017099129152, 'learning_rate': 1.7545008183306055e-06, 'epoch': 0.45}

 15%|█▌        | 95/630 [02:40<14:17,  1.60s/it]
 15%|█▌        | 96/630 [02:41<14:14,  1.60s/it]
                                                
{'loss': 0.719, 'grad_norm': 1.554827985389651, 'learning_rate': 1.7512274959083469e-06, 'epoch': 0.46}

 15%|█▌        | 96/630 [02:41<14:14,  1.60s/it]
 15%|█▌        | 97/630 [02:43<14:13,  1.60s/it]
                                                
{'loss': 0.6792, 'grad_norm': 1.431794940500657, 'learning_rate': 1.7479541734860884e-06, 'epoch': 0.46}

 15%|█▌        | 97/630 [02:43<14:13,  1.60s/it]
 16%|█▌        | 98/630 [02:45<14:22,  1.62s/it]
                                                
{'loss': 0.6132, 'grad_norm': 1.4397754503041786, 'learning_rate': 1.7446808510638297e-06, 'epoch': 0.47}

 16%|█▌        | 98/630 [02:45<14:22,  1.62s/it]
 16%|█▌        | 99/630 [02:46<14:18,  1.62s/it]
                                                
{'loss': 0.644, 'grad_norm': 1.7593468001658576, 'learning_rate': 1.741407528641571e-06, 'epoch': 0.47}

 16%|█▌        | 99/630 [02:46<14:18,  1.62s/it]
 16%|█▌        | 100/630 [02:48<14:14,  1.61s/it]
                                                 
{'loss': 0.6449, 'grad_norm': 1.6192118909114575, 'learning_rate': 1.7381342062193124e-06, 'epoch': 0.48}

 16%|█▌        | 100/630 [02:48<14:14,  1.61s/it]
 16%|█▌        | 101/630 [02:49<14:11,  1.61s/it]
                                                 
{'loss': 0.639, 'grad_norm': 1.3954948052922962, 'learning_rate': 1.734860883797054e-06, 'epoch': 0.48}

 16%|█▌        | 101/630 [02:49<14:11,  1.61s/it]
 16%|█▌        | 102/630 [02:51<14:08,  1.61s/it]
                                                 
{'loss': 0.6394, 'grad_norm': 1.5605487379988263, 'learning_rate': 1.7315875613747955e-06, 'epoch': 0.49}

 16%|█▌        | 102/630 [02:51<14:08,  1.61s/it]
 16%|█▋        | 103/630 [02:53<14:04,  1.60s/it]
                                                 
{'loss': 0.7118, 'grad_norm': 2.000544123504184, 'learning_rate': 1.7283142389525366e-06, 'epoch': 0.49}

 16%|█▋        | 103/630 [02:53<14:04,  1.60s/it]
 17%|█▋        | 104/630 [02:54<14:01,  1.60s/it]
                                                 
{'loss': 0.6795, 'grad_norm': 1.5267143188574246, 'learning_rate': 1.7250409165302782e-06, 'epoch': 0.5}

 17%|█▋        | 104/630 [02:54<14:01,  1.60s/it]
 17%|█▋        | 105/630 [02:56<14:07,  1.61s/it]
                                                 
{'loss': 0.6074, 'grad_norm': 1.7989001728624665, 'learning_rate': 1.7217675941080197e-06, 'epoch': 0.5}

 17%|█▋        | 105/630 [02:56<14:07,  1.61s/it]
 17%|█▋        | 106/630 [02:57<14:04,  1.61s/it]
                                                 
{'loss': 0.605, 'grad_norm': 1.8202211347956845, 'learning_rate': 1.7184942716857609e-06, 'epoch': 0.5}

 17%|█▋        | 106/630 [02:58<14:04,  1.61s/it]
 17%|█▋        | 107/630 [02:59<14:00,  1.61s/it]
                                                 
{'loss': 0.6522, 'grad_norm': 1.5075607698865077, 'learning_rate': 1.7152209492635024e-06, 'epoch': 0.51}

 17%|█▋        | 107/630 [02:59<14:00,  1.61s/it]
 17%|█▋        | 108/630 [03:01<13:57,  1.61s/it]
                                                 
{'loss': 0.6425, 'grad_norm': 1.547705652754808, 'learning_rate': 1.7119476268412437e-06, 'epoch': 0.51}

 17%|█▋        | 108/630 [03:01<13:57,  1.61s/it]
 17%|█▋        | 109/630 [03:02<13:56,  1.61s/it]
                                                 
{'loss': 0.709, 'grad_norm': 1.3951808715619862, 'learning_rate': 1.7086743044189853e-06, 'epoch': 0.52}

 17%|█▋        | 109/630 [03:02<13:56,  1.61s/it]
 17%|█▋        | 110/630 [03:04<13:57,  1.61s/it]
                                                 
{'loss': 0.7908, 'grad_norm': 1.5160157334535684, 'learning_rate': 1.7054009819967266e-06, 'epoch': 0.52}

 17%|█▋        | 110/630 [03:04<13:57,  1.61s/it]
 18%|█▊        | 111/630 [03:06<13:53,  1.61s/it]
                                                 
{'loss': 0.6194, 'grad_norm': 1.709445355341456, 'learning_rate': 1.702127659574468e-06, 'epoch': 0.53}

 18%|█▊        | 111/630 [03:06<13:53,  1.61s/it]
 18%|█▊        | 112/630 [03:07<13:50,  1.60s/it]
                                                 
{'loss': 0.5475, 'grad_norm': 1.4305419576176797, 'learning_rate': 1.6988543371522095e-06, 'epoch': 0.53}

 18%|█▊        | 112/630 [03:07<13:50,  1.60s/it]
 18%|█▊        | 113/630 [03:09<13:47,  1.60s/it]
                                                 
{'loss': 0.6668, 'grad_norm': 1.9313334223557899, 'learning_rate': 1.6955810147299508e-06, 'epoch': 0.54}

 18%|█▊        | 113/630 [03:09<13:47,  1.60s/it]
 18%|█▊        | 114/630 [03:10<13:46,  1.60s/it]
                                                 
{'loss': 0.7088, 'grad_norm': 1.6720803628791734, 'learning_rate': 1.6923076923076922e-06, 'epoch': 0.54}

 18%|█▊        | 114/630 [03:10<13:46,  1.60s/it]
 18%|█▊        | 115/630 [03:12<13:43,  1.60s/it]
                                                 
{'loss': 0.6289, 'grad_norm': 1.5691583901720907, 'learning_rate': 1.6890343698854337e-06, 'epoch': 0.55}

 18%|█▊        | 115/630 [03:12<13:43,  1.60s/it]
 18%|█▊        | 116/630 [03:13<13:40,  1.60s/it]
                                                 
{'loss': 0.6613, 'grad_norm': 1.4040983167763774, 'learning_rate': 1.6857610474631753e-06, 'epoch': 0.55}

 18%|█▊        | 116/630 [03:13<13:40,  1.60s/it]
 19%|█▊        | 117/630 [03:15<13:39,  1.60s/it]
                                                 
{'loss': 0.6271, 'grad_norm': 1.5530095872051612, 'learning_rate': 1.6824877250409164e-06, 'epoch': 0.56}

 19%|█▊        | 117/630 [03:15<13:39,  1.60s/it]
 19%|█▊        | 118/630 [03:17<13:37,  1.60s/it]
                                                 
{'loss': 0.6237, 'grad_norm': 1.4214859482057614, 'learning_rate': 1.679214402618658e-06, 'epoch': 0.56}

 19%|█▊        | 118/630 [03:17<13:37,  1.60s/it]
 19%|█▉        | 119/630 [03:18<13:36,  1.60s/it]
                                                 
{'loss': 0.5536, 'grad_norm': 1.5247280331962114, 'learning_rate': 1.6759410801963993e-06, 'epoch': 0.57}

 19%|█▉        | 119/630 [03:18<13:36,  1.60s/it]
 19%|█▉        | 120/630 [03:20<13:35,  1.60s/it]
                                                 
{'loss': 0.6627, 'grad_norm': 1.8099777424350147, 'learning_rate': 1.6726677577741406e-06, 'epoch': 0.57}

 19%|█▉        | 120/630 [03:20<13:35,  1.60s/it]
 19%|█▉        | 121/630 [03:21<13:31,  1.60s/it]
                                                 
{'loss': 0.5993, 'grad_norm': 1.594914161559894, 'learning_rate': 1.6693944353518821e-06, 'epoch': 0.58}

 19%|█▉        | 121/630 [03:21<13:31,  1.60s/it]
 19%|█▉        | 122/630 [03:23<13:30,  1.60s/it]
                                                 
{'loss': 0.6385, 'grad_norm': 1.8388247336583887, 'learning_rate': 1.6661211129296235e-06, 'epoch': 0.58}

 19%|█▉        | 122/630 [03:23<13:30,  1.60s/it]
 20%|█▉        | 123/630 [03:25<13:29,  1.60s/it]
                                                 
{'loss': 0.6957, 'grad_norm': 1.5337280027679845, 'learning_rate': 1.6628477905073648e-06, 'epoch': 0.59}

 20%|█▉        | 123/630 [03:25<13:29,  1.60s/it]
 20%|█▉        | 124/630 [03:26<13:28,  1.60s/it]
                                                 
{'loss': 0.6397, 'grad_norm': 1.6166963232012685, 'learning_rate': 1.6595744680851064e-06, 'epoch': 0.59}

 20%|█▉        | 124/630 [03:26<13:28,  1.60s/it]
 20%|█▉        | 125/630 [03:28<13:27,  1.60s/it]
                                                 
{'loss': 0.6037, 'grad_norm': 1.6387691592460993, 'learning_rate': 1.6563011456628477e-06, 'epoch': 0.6}

 20%|█▉        | 125/630 [03:28<13:27,  1.60s/it]
 20%|██        | 126/630 [03:29<13:25,  1.60s/it]
                                                 
{'loss': 0.5672, 'grad_norm': 1.5304637782805373, 'learning_rate': 1.6530278232405892e-06, 'epoch': 0.6}

 20%|██        | 126/630 [03:29<13:25,  1.60s/it]
 20%|██        | 127/630 [03:31<13:23,  1.60s/it]
                                                 
{'loss': 0.6515, 'grad_norm': 1.6473277601728966, 'learning_rate': 1.6497545008183304e-06, 'epoch': 0.6}

 20%|██        | 127/630 [03:31<13:23,  1.60s/it]
 20%|██        | 128/630 [03:33<13:24,  1.60s/it]
                                                 
{'loss': 0.6823, 'grad_norm': 1.4440171837112503, 'learning_rate': 1.646481178396072e-06, 'epoch': 0.61}

 20%|██        | 128/630 [03:33<13:24,  1.60s/it]
 20%|██        | 129/630 [03:34<13:23,  1.60s/it]
                                                 
{'loss': 0.6855, 'grad_norm': 1.4907816688169033, 'learning_rate': 1.6432078559738135e-06, 'epoch': 0.61}

 20%|██        | 129/630 [03:34<13:23,  1.60s/it]
 21%|██        | 130/630 [03:36<13:20,  1.60s/it]
                                                 
{'loss': 0.6649, 'grad_norm': 1.5044839358350253, 'learning_rate': 1.6399345335515546e-06, 'epoch': 0.62}

 21%|██        | 130/630 [03:36<13:20,  1.60s/it]
 21%|██        | 131/630 [03:37<13:19,  1.60s/it]
                                                 
{'loss': 0.6493, 'grad_norm': 1.584353132170728, 'learning_rate': 1.6366612111292961e-06, 'epoch': 0.62}

 21%|██        | 131/630 [03:37<13:19,  1.60s/it]
 21%|██        | 132/630 [03:39<13:18,  1.60s/it]
                                                 
{'loss': 0.6683, 'grad_norm': 1.5092446307357374, 'learning_rate': 1.6333878887070377e-06, 'epoch': 0.63}

 21%|██        | 132/630 [03:39<13:18,  1.60s/it]
 21%|██        | 133/630 [03:41<13:20,  1.61s/it]
                                                 
{'loss': 0.5567, 'grad_norm': 1.5674366582422408, 'learning_rate': 1.630114566284779e-06, 'epoch': 0.63}

 21%|██        | 133/630 [03:41<13:20,  1.61s/it]
 21%|██▏       | 134/630 [03:42<13:17,  1.61s/it]
                                                 
{'loss': 0.5798, 'grad_norm': 2.8023462551207228, 'learning_rate': 1.6268412438625203e-06, 'epoch': 0.64}

 21%|██▏       | 134/630 [03:42<13:17,  1.61s/it]
 21%|██▏       | 135/630 [03:44<13:15,  1.61s/it]
                                                 
{'loss': 0.6683, 'grad_norm': 1.7732972719800257, 'learning_rate': 1.6235679214402617e-06, 'epoch': 0.64}

 21%|██▏       | 135/630 [03:44<13:15,  1.61s/it]
 22%|██▏       | 136/630 [03:46<13:07,  1.60s/it]
                                                 
{'loss': 0.6398, 'grad_norm': 1.9115658626446361, 'learning_rate': 1.6202945990180032e-06, 'epoch': 0.65}

 22%|██▏       | 136/630 [03:46<13:07,  1.60s/it]
 22%|██▏       | 137/630 [03:47<13:06,  1.60s/it]
                                                 
{'loss': 0.6781, 'grad_norm': 2.007870491758537, 'learning_rate': 1.6170212765957446e-06, 'epoch': 0.65}

 22%|██▏       | 137/630 [03:47<13:06,  1.60s/it]
 22%|██▏       | 138/630 [03:49<13:04,  1.59s/it]
                                                 
{'loss': 0.6437, 'grad_norm': 1.7527259635331722, 'learning_rate': 1.613747954173486e-06, 'epoch': 0.66}

 22%|██▏       | 138/630 [03:49<13:04,  1.59s/it]
 22%|██▏       | 139/630 [03:50<13:05,  1.60s/it]
                                                 
{'loss': 0.6428, 'grad_norm': 1.8427735152596616, 'learning_rate': 1.6104746317512274e-06, 'epoch': 0.66}

 22%|██▏       | 139/630 [03:50<13:05,  1.60s/it]
 22%|██▏       | 140/630 [03:52<13:05,  1.60s/it]
                                                 
{'loss': 0.622, 'grad_norm': 1.4492648811533595, 'learning_rate': 1.607201309328969e-06, 'epoch': 0.67}

 22%|██▏       | 140/630 [03:52<13:05,  1.60s/it]
 22%|██▏       | 141/630 [03:54<13:06,  1.61s/it]
                                                 
{'loss': 0.5736, 'grad_norm': 1.4416220275111284, 'learning_rate': 1.6039279869067101e-06, 'epoch': 0.67}

 22%|██▏       | 141/630 [03:54<13:06,  1.61s/it]
 23%|██▎       | 142/630 [03:55<13:03,  1.61s/it]
                                                 
{'loss': 0.633, 'grad_norm': 1.6075220503177894, 'learning_rate': 1.6006546644844517e-06, 'epoch': 0.68}

 23%|██▎       | 142/630 [03:55<13:03,  1.61s/it]
 23%|██▎       | 143/630 [03:57<12:59,  1.60s/it]
                                                 
{'loss': 0.6697, 'grad_norm': 2.078006351928103, 'learning_rate': 1.5973813420621932e-06, 'epoch': 0.68}

 23%|██▎       | 143/630 [03:57<12:59,  1.60s/it]
 23%|██▎       | 144/630 [03:58<12:57,  1.60s/it]
                                                 
{'loss': 0.6533, 'grad_norm': 1.3905392835175734, 'learning_rate': 1.5941080196399343e-06, 'epoch': 0.69}

 23%|██▎       | 144/630 [03:58<12:57,  1.60s/it]
 23%|██▎       | 145/630 [04:00<12:55,  1.60s/it]
                                                 
{'loss': 0.621, 'grad_norm': 1.621782276902416, 'learning_rate': 1.5908346972176759e-06, 'epoch': 0.69}

 23%|██▎       | 145/630 [04:00<12:55,  1.60s/it]
 23%|██▎       | 146/630 [04:02<12:55,  1.60s/it]
                                                 
{'loss': 0.693, 'grad_norm': 1.7020681927119135, 'learning_rate': 1.5875613747954172e-06, 'epoch': 0.7}

 23%|██▎       | 146/630 [04:02<12:55,  1.60s/it]
 23%|██▎       | 147/630 [04:03<12:54,  1.60s/it]
                                                 
{'loss': 0.6714, 'grad_norm': 1.4153146967249468, 'learning_rate': 1.5842880523731588e-06, 'epoch': 0.7}

 23%|██▎       | 147/630 [04:03<12:54,  1.60s/it]
 23%|██▎       | 148/630 [04:05<12:50,  1.60s/it]
                                                 
{'loss': 0.6445, 'grad_norm': 1.6576877918665176, 'learning_rate': 1.5810147299509e-06, 'epoch': 0.7}

 23%|██▎       | 148/630 [04:05<12:50,  1.60s/it]
 24%|██▎       | 149/630 [04:06<12:56,  1.61s/it]
                                                 
{'loss': 0.5395, 'grad_norm': 1.5773874914107617, 'learning_rate': 1.5777414075286414e-06, 'epoch': 0.71}

 24%|██▎       | 149/630 [04:06<12:56,  1.61s/it]
 24%|██▍       | 150/630 [04:08<12:52,  1.61s/it]
                                                 
{'loss': 0.6298, 'grad_norm': 1.3105202115852226, 'learning_rate': 1.574468085106383e-06, 'epoch': 0.71}

 24%|██▍       | 150/630 [04:08<12:52,  1.61s/it]
 24%|██▍       | 151/630 [04:10<12:48,  1.61s/it]
                                                 
{'loss': 0.7499, 'grad_norm': 1.8499770506176678, 'learning_rate': 1.5711947626841243e-06, 'epoch': 0.72}

 24%|██▍       | 151/630 [04:10<12:48,  1.61s/it]
 24%|██▍       | 152/630 [04:11<12:45,  1.60s/it]
                                                 
{'loss': 0.6319, 'grad_norm': 1.5879910965022828, 'learning_rate': 1.5679214402618656e-06, 'epoch': 0.72}

 24%|██▍       | 152/630 [04:11<12:45,  1.60s/it]
 24%|██▍       | 153/630 [04:13<12:44,  1.60s/it]
                                                 
{'loss': 0.6522, 'grad_norm': 1.4555474954554948, 'learning_rate': 1.5646481178396072e-06, 'epoch': 0.73}

 24%|██▍       | 153/630 [04:13<12:44,  1.60s/it]
 24%|██▍       | 154/630 [04:14<12:41,  1.60s/it]
                                                 
{'loss': 0.6237, 'grad_norm': 1.5481637622189373, 'learning_rate': 1.5613747954173485e-06, 'epoch': 0.73}

 24%|██▍       | 154/630 [04:14<12:41,  1.60s/it]
 25%|██▍       | 155/630 [04:16<12:39,  1.60s/it]
                                                 
{'loss': 0.6609, 'grad_norm': 1.5093183099806888, 'learning_rate': 1.5581014729950899e-06, 'epoch': 0.74}

 25%|██▍       | 155/630 [04:16<12:39,  1.60s/it]
 25%|██▍       | 156/630 [04:18<12:36,  1.60s/it]
                                                 
{'loss': 0.5789, 'grad_norm': 1.6596037412108307, 'learning_rate': 1.5548281505728314e-06, 'epoch': 0.74}

 25%|██▍       | 156/630 [04:18<12:36,  1.60s/it]
 25%|██▍       | 157/630 [04:19<12:35,  1.60s/it]
                                                 
{'loss': 0.7608, 'grad_norm': 1.819365403726423, 'learning_rate': 1.5515548281505727e-06, 'epoch': 0.75}

 25%|██▍       | 157/630 [04:19<12:35,  1.60s/it]
 25%|██▌       | 158/630 [04:21<12:33,  1.60s/it]
                                                 
{'loss': 0.6039, 'grad_norm': 1.4378163568775775, 'learning_rate': 1.548281505728314e-06, 'epoch': 0.75}

 25%|██▌       | 158/630 [04:21<12:33,  1.60s/it]
 25%|██▌       | 159/630 [04:22<12:32,  1.60s/it]
                                                 
{'loss': 0.5929, 'grad_norm': 1.5476230616375841, 'learning_rate': 1.5450081833060556e-06, 'epoch': 0.76}

 25%|██▌       | 159/630 [04:22<12:32,  1.60s/it]
 25%|██▌       | 160/630 [04:24<12:30,  1.60s/it]
                                                 
{'loss': 0.671, 'grad_norm': 1.7802950037253171, 'learning_rate': 1.541734860883797e-06, 'epoch': 0.76}

 25%|██▌       | 160/630 [04:24<12:30,  1.60s/it]
 26%|██▌       | 161/630 [04:26<12:28,  1.60s/it]
                                                 
{'loss': 0.626, 'grad_norm': 1.6273170753983623, 'learning_rate': 1.5384615384615385e-06, 'epoch': 0.77}

 26%|██▌       | 161/630 [04:26<12:28,  1.60s/it]
 26%|██▌       | 162/630 [04:27<12:26,  1.60s/it]
                                                 
{'loss': 0.7064, 'grad_norm': 1.9035781796659461, 'learning_rate': 1.5351882160392796e-06, 'epoch': 0.77}

 26%|██▌       | 162/630 [04:27<12:26,  1.60s/it]
 26%|██▌       | 163/630 [04:29<12:24,  1.59s/it]
                                                 
{'loss': 0.573, 'grad_norm': 1.582720195411731, 'learning_rate': 1.5319148936170212e-06, 'epoch': 0.78}

 26%|██▌       | 163/630 [04:29<12:24,  1.59s/it]
 26%|██▌       | 164/630 [04:30<12:25,  1.60s/it]
                                                 
{'loss': 0.584, 'grad_norm': 1.5068154137626522, 'learning_rate': 1.5286415711947627e-06, 'epoch': 0.78}

 26%|██▌       | 164/630 [04:30<12:25,  1.60s/it]
 26%|██▌       | 165/630 [04:32<12:24,  1.60s/it]
                                                 
{'loss': 0.6436, 'grad_norm': 1.536855077252709, 'learning_rate': 1.5253682487725038e-06, 'epoch': 0.79}

 26%|██▌       | 165/630 [04:32<12:24,  1.60s/it]
 26%|██▋       | 166/630 [04:34<12:23,  1.60s/it]
                                                 
{'loss': 0.6973, 'grad_norm': 2.358351915859425, 'learning_rate': 1.5220949263502454e-06, 'epoch': 0.79}

 26%|██▋       | 166/630 [04:34<12:23,  1.60s/it]
 27%|██▋       | 167/630 [04:35<12:21,  1.60s/it]
                                                 
{'loss': 0.6703, 'grad_norm': 1.6137629494691377, 'learning_rate': 1.518821603927987e-06, 'epoch': 0.8}

 27%|██▋       | 167/630 [04:35<12:21,  1.60s/it]
 27%|██▋       | 168/630 [04:37<12:19,  1.60s/it]
                                                 
{'loss': 0.6663, 'grad_norm': 1.6222533183732946, 'learning_rate': 1.5155482815057283e-06, 'epoch': 0.8}

 27%|██▋       | 168/630 [04:37<12:19,  1.60s/it]
 27%|██▋       | 169/630 [04:38<12:18,  1.60s/it]
                                                 
{'loss': 0.6, 'grad_norm': 1.4432493073265396, 'learning_rate': 1.5122749590834696e-06, 'epoch': 0.8}

 27%|██▋       | 169/630 [04:38<12:18,  1.60s/it]
 27%|██▋       | 170/630 [04:40<12:17,  1.60s/it]
                                                 
{'loss': 0.6503, 'grad_norm': 1.5197821435323868, 'learning_rate': 1.5090016366612112e-06, 'epoch': 0.81}

 27%|██▋       | 170/630 [04:40<12:17,  1.60s/it]
 27%|██▋       | 171/630 [04:42<12:18,  1.61s/it]
                                                 
{'loss': 0.6103, 'grad_norm': 1.4635843633169263, 'learning_rate': 1.5057283142389525e-06, 'epoch': 0.81}

 27%|██▋       | 171/630 [04:42<12:18,  1.61s/it]
 27%|██▋       | 172/630 [04:43<12:17,  1.61s/it]
                                                 
{'loss': 0.5962, 'grad_norm': 1.5085662369285817, 'learning_rate': 1.5024549918166938e-06, 'epoch': 0.82}

 27%|██▋       | 172/630 [04:43<12:17,  1.61s/it]
 27%|██▋       | 173/630 [04:45<12:16,  1.61s/it]
                                                 
{'loss': 0.6111, 'grad_norm': 1.4926448165750115, 'learning_rate': 1.4991816693944352e-06, 'epoch': 0.82}

 27%|██▋       | 173/630 [04:45<12:16,  1.61s/it]
 28%|██▊       | 174/630 [04:46<12:14,  1.61s/it]
                                                 
{'loss': 0.5502, 'grad_norm': 1.3353393784945502, 'learning_rate': 1.4959083469721767e-06, 'epoch': 0.83}

 28%|██▊       | 174/630 [04:46<12:14,  1.61s/it]
 28%|██▊       | 175/630 [04:48<12:12,  1.61s/it]
                                                 
{'loss': 0.6342, 'grad_norm': 1.5290332203051857, 'learning_rate': 1.4926350245499183e-06, 'epoch': 0.83}

 28%|██▊       | 175/630 [04:48<12:12,  1.61s/it]
 28%|██▊       | 176/630 [04:50<12:09,  1.61s/it]
                                                 
{'loss': 0.5925, 'grad_norm': 1.6656742636548598, 'learning_rate': 1.4893617021276594e-06, 'epoch': 0.84}

 28%|██▊       | 176/630 [04:50<12:09,  1.61s/it]
 28%|██▊       | 177/630 [04:51<12:05,  1.60s/it]
                                                 
{'loss': 0.5828, 'grad_norm': 1.489289900744353, 'learning_rate': 1.486088379705401e-06, 'epoch': 0.84}

 28%|██▊       | 177/630 [04:51<12:05,  1.60s/it]
 28%|██▊       | 178/630 [04:53<12:04,  1.60s/it]
                                                 
{'loss': 0.6362, 'grad_norm': 1.554099470311762, 'learning_rate': 1.4828150572831425e-06, 'epoch': 0.85}

 28%|██▊       | 178/630 [04:53<12:04,  1.60s/it]
 28%|██▊       | 179/630 [04:54<12:02,  1.60s/it]
                                                 
{'loss': 0.6781, 'grad_norm': 1.9128056445174266, 'learning_rate': 1.4795417348608836e-06, 'epoch': 0.85}

 28%|██▊       | 179/630 [04:54<12:02,  1.60s/it]
 29%|██▊       | 180/630 [04:56<12:00,  1.60s/it]
                                                 
{'loss': 0.6243, 'grad_norm': 1.586086719333144, 'learning_rate': 1.4762684124386251e-06, 'epoch': 0.86}

 29%|██▊       | 180/630 [04:56<12:00,  1.60s/it]
 29%|██▊       | 181/630 [04:58<11:58,  1.60s/it]
                                                 
{'loss': 0.7343, 'grad_norm': 1.6768347258399696, 'learning_rate': 1.4729950900163665e-06, 'epoch': 0.86}

 29%|██▊       | 181/630 [04:58<11:58,  1.60s/it]
 29%|██▉       | 182/630 [04:59<11:55,  1.60s/it]
                                                 
{'loss': 0.6164, 'grad_norm': 1.4991553547761993, 'learning_rate': 1.469721767594108e-06, 'epoch': 0.87}

 29%|██▉       | 182/630 [04:59<11:55,  1.60s/it]
 29%|██▉       | 183/630 [05:01<11:55,  1.60s/it]
                                                 
{'loss': 0.6972, 'grad_norm': 1.7922622602872023, 'learning_rate': 1.4664484451718494e-06, 'epoch': 0.87}

 29%|██▉       | 183/630 [05:01<11:55,  1.60s/it]
 29%|██▉       | 184/630 [05:02<11:52,  1.60s/it]
                                                 
{'loss': 0.6086, 'grad_norm': 1.6140546315386801, 'learning_rate': 1.4631751227495907e-06, 'epoch': 0.88}

 29%|██▉       | 184/630 [05:02<11:52,  1.60s/it]
 29%|██▉       | 185/630 [05:04<11:51,  1.60s/it]
                                                 
{'loss': 0.5921, 'grad_norm': 1.352487611743704, 'learning_rate': 1.4599018003273322e-06, 'epoch': 0.88}

 29%|██▉       | 185/630 [05:04<11:51,  1.60s/it]
 30%|██▉       | 186/630 [05:06<11:49,  1.60s/it]
                                                 
{'loss': 0.6158, 'grad_norm': 1.4209173556928156, 'learning_rate': 1.4566284779050736e-06, 'epoch': 0.89}

 30%|██▉       | 186/630 [05:06<11:49,  1.60s/it]
 30%|██▉       | 187/630 [05:07<11:49,  1.60s/it]
                                                 
{'loss': 0.5993, 'grad_norm': 1.5063736509898336, 'learning_rate': 1.453355155482815e-06, 'epoch': 0.89}

 30%|██▉       | 187/630 [05:07<11:49,  1.60s/it]
 30%|██▉       | 188/630 [05:09<11:48,  1.60s/it]
                                                 
{'loss': 0.6423, 'grad_norm': 1.6176153729753462, 'learning_rate': 1.4500818330605565e-06, 'epoch': 0.9}

 30%|██▉       | 188/630 [05:09<11:48,  1.60s/it]
 30%|███       | 189/630 [05:10<11:45,  1.60s/it]
                                                 
{'loss': 0.636, 'grad_norm': 1.3880696163100332, 'learning_rate': 1.446808510638298e-06, 'epoch': 0.9}

 30%|███       | 189/630 [05:10<11:45,  1.60s/it]
 30%|███       | 190/630 [05:12<11:43,  1.60s/it]
                                                 
{'loss': 0.6757, 'grad_norm': 1.5918091215929986, 'learning_rate': 1.4435351882160391e-06, 'epoch': 0.9}

 30%|███       | 190/630 [05:12<11:43,  1.60s/it]
 30%|███       | 191/630 [05:14<11:42,  1.60s/it]
                                                 
{'loss': 0.6056, 'grad_norm': 1.5867033464274856, 'learning_rate': 1.4402618657937807e-06, 'epoch': 0.91}

 30%|███       | 191/630 [05:14<11:42,  1.60s/it]
 30%|███       | 192/630 [05:15<11:40,  1.60s/it]
                                                 
{'loss': 0.566, 'grad_norm': 1.5919654002051196, 'learning_rate': 1.436988543371522e-06, 'epoch': 0.91}

 30%|███       | 192/630 [05:15<11:40,  1.60s/it]
 31%|███       | 193/630 [05:17<11:38,  1.60s/it]
                                                 
{'loss': 0.5988, 'grad_norm': 1.6403114158805305, 'learning_rate': 1.4337152209492633e-06, 'epoch': 0.92}

 31%|███       | 193/630 [05:17<11:38,  1.60s/it]
 31%|███       | 194/630 [05:18<11:36,  1.60s/it]
                                                 
{'loss': 0.5619, 'grad_norm': 1.4649081863103732, 'learning_rate': 1.4304418985270049e-06, 'epoch': 0.92}

 31%|███       | 194/630 [05:18<11:36,  1.60s/it]
 31%|███       | 195/630 [05:20<11:34,  1.60s/it]
                                                 
{'loss': 0.5372, 'grad_norm': 1.3123204579123133, 'learning_rate': 1.4271685761047462e-06, 'epoch': 0.93}

 31%|███       | 195/630 [05:20<11:34,  1.60s/it]
 31%|███       | 196/630 [05:22<11:33,  1.60s/it]
                                                 
{'loss': 0.6417, 'grad_norm': 1.8277479633900535, 'learning_rate': 1.4238952536824878e-06, 'epoch': 0.93}

 31%|███       | 196/630 [05:22<11:33,  1.60s/it]
 31%|███▏      | 197/630 [05:23<11:28,  1.59s/it]
                                                 
{'loss': 0.6067, 'grad_norm': 1.9250976165317155, 'learning_rate': 1.420621931260229e-06, 'epoch': 0.94}

 31%|███▏      | 197/630 [05:23<11:28,  1.59s/it]
 31%|███▏      | 198/630 [05:25<11:26,  1.59s/it]
                                                 
{'loss': 0.6154, 'grad_norm': 1.5330846948431762, 'learning_rate': 1.4173486088379704e-06, 'epoch': 0.94}

 31%|███▏      | 198/630 [05:25<11:26,  1.59s/it]
 32%|███▏      | 199/630 [05:26<11:25,  1.59s/it]
                                                 
{'loss': 0.6208, 'grad_norm': 1.7355144612206468, 'learning_rate': 1.414075286415712e-06, 'epoch': 0.95}

 32%|███▏      | 199/630 [05:26<11:25,  1.59s/it]
 32%|███▏      | 200/630 [05:28<11:24,  1.59s/it]
                                                 
{'loss': 0.5986, 'grad_norm': 1.500030286428438, 'learning_rate': 1.4108019639934531e-06, 'epoch': 0.95}

 32%|███▏      | 200/630 [05:28<11:24,  1.59s/it]
 32%|███▏      | 201/630 [05:30<11:23,  1.59s/it]
                                                 
{'loss': 0.6626, 'grad_norm': 2.279973855121015, 'learning_rate': 1.4075286415711947e-06, 'epoch': 0.96}

 32%|███▏      | 201/630 [05:30<11:23,  1.59s/it]
 32%|███▏      | 202/630 [05:31<11:23,  1.60s/it]
                                                 
{'loss': 0.6029, 'grad_norm': 1.4154661874668186, 'learning_rate': 1.4042553191489362e-06, 'epoch': 0.96}

 32%|███▏      | 202/630 [05:31<11:23,  1.60s/it]
 32%|███▏      | 203/630 [05:33<11:22,  1.60s/it]
                                                 
{'loss': 0.5906, 'grad_norm': 1.6093169276691788, 'learning_rate': 1.4009819967266775e-06, 'epoch': 0.97}

 32%|███▏      | 203/630 [05:33<11:22,  1.60s/it]
 32%|███▏      | 204/630 [05:34<11:22,  1.60s/it]
                                                 
{'loss': 0.7925, 'grad_norm': 2.0799529632867904, 'learning_rate': 1.3977086743044189e-06, 'epoch': 0.97}

 32%|███▏      | 204/630 [05:34<11:22,  1.60s/it]
 33%|███▎      | 205/630 [05:36<11:21,  1.60s/it]
                                                 
{'loss': 0.5296, 'grad_norm': 1.300035569182876, 'learning_rate': 1.3944353518821604e-06, 'epoch': 0.98}

 33%|███▎      | 205/630 [05:36<11:21,  1.60s/it]
 33%|███▎      | 206/630 [05:38<11:18,  1.60s/it]
                                                 
{'loss': 0.7223, 'grad_norm': 1.7753913398897874, 'learning_rate': 1.3911620294599018e-06, 'epoch': 0.98}

 33%|███▎      | 206/630 [05:38<11:18,  1.60s/it]
 33%|███▎      | 207/630 [05:39<11:17,  1.60s/it]
                                                 
{'loss': 0.5968, 'grad_norm': 1.398487874139733, 'learning_rate': 1.387888707037643e-06, 'epoch': 0.99}

 33%|███▎      | 207/630 [05:39<11:17,  1.60s/it]
 33%|███▎      | 208/630 [05:41<11:14,  1.60s/it]
                                                 
{'loss': 0.6735, 'grad_norm': 2.058191836810303, 'learning_rate': 1.3846153846153844e-06, 'epoch': 0.99}

 33%|███▎      | 208/630 [05:41<11:14,  1.60s/it]
 33%|███▎      | 209/630 [05:42<11:13,  1.60s/it]
                                                 
{'loss': 0.6151, 'grad_norm': 1.4539027369551487, 'learning_rate': 1.381342062193126e-06, 'epoch': 1.0}

 33%|███▎      | 209/630 [05:42<11:13,  1.60s/it]
 33%|███▎      | 210/630 [05:44<11:16,  1.61s/it]
                                                 
{'loss': 0.5269, 'grad_norm': 1.4573155756570069, 'learning_rate': 1.3780687397708675e-06, 'epoch': 1.0}

 33%|███▎      | 210/630 [05:44<11:16,  1.61s/it]
 33%|███▎      | 211/630 [05:46<11:30,  1.65s/it]
                                                 
{'loss': 0.4965, 'grad_norm': 1.468302952337727, 'learning_rate': 1.3747954173486086e-06, 'epoch': 1.0}

 33%|███▎      | 211/630 [05:46<11:30,  1.65s/it]
 34%|███▎      | 212/630 [05:47<11:20,  1.63s/it]
                                                 
{'loss': 0.513, 'grad_norm': 1.6413563658819397, 'learning_rate': 1.3715220949263502e-06, 'epoch': 1.01}

 34%|███▎      | 212/630 [05:47<11:20,  1.63s/it]
 34%|███▍      | 213/630 [05:49<11:38,  1.68s/it]
                                                 
{'loss': 0.5564, 'grad_norm': 1.5059824964975506, 'learning_rate': 1.3682487725040917e-06, 'epoch': 1.01}

 34%|███▍      | 213/630 [05:49<11:38,  1.68s/it]
 34%|███▍      | 214/630 [05:51<11:28,  1.66s/it]
                                                 
{'loss': 0.5798, 'grad_norm': 1.732785442743811, 'learning_rate': 1.3649754500818329e-06, 'epoch': 1.02}

 34%|███▍      | 214/630 [05:51<11:28,  1.66s/it]
 34%|███▍      | 215/630 [05:52<11:19,  1.64s/it]
                                                 
{'loss': 0.5204, 'grad_norm': 1.389234190840395, 'learning_rate': 1.3617021276595744e-06, 'epoch': 1.02}

 34%|███▍      | 215/630 [05:52<11:19,  1.64s/it]
 34%|███▍      | 216/630 [05:54<11:13,  1.63s/it]
                                                 
{'loss': 0.459, 'grad_norm': 1.430721523882798, 'learning_rate': 1.358428805237316e-06, 'epoch': 1.03}

 34%|███▍      | 216/630 [05:54<11:13,  1.63s/it]
 34%|███▍      | 217/630 [05:55<11:08,  1.62s/it]
                                                 
{'loss': 0.4572, 'grad_norm': 1.3213485693687737, 'learning_rate': 1.3551554828150573e-06, 'epoch': 1.03}

 34%|███▍      | 217/630 [05:55<11:08,  1.62s/it]
 35%|███▍      | 218/630 [05:57<11:07,  1.62s/it]
                                                 
{'loss': 0.5046, 'grad_norm': 1.3977391064816453, 'learning_rate': 1.3518821603927986e-06, 'epoch': 1.04}

 35%|███▍      | 218/630 [05:57<11:07,  1.62s/it]
 35%|███▍      | 219/630 [05:59<11:02,  1.61s/it]
                                                 
{'loss': 0.5244, 'grad_norm': 1.4645011615368602, 'learning_rate': 1.34860883797054e-06, 'epoch': 1.04}

 35%|███▍      | 219/630 [05:59<11:02,  1.61s/it]
 35%|███▍      | 220/630 [06:00<11:00,  1.61s/it]
                                                 
{'loss': 0.5937, 'grad_norm': 1.4161474692088538, 'learning_rate': 1.3453355155482815e-06, 'epoch': 1.05}

 35%|███▍      | 220/630 [06:00<11:00,  1.61s/it]
 35%|███▌      | 221/630 [06:02<10:58,  1.61s/it]
                                                 
{'loss': 0.5925, 'grad_norm': 1.6833402931060009, 'learning_rate': 1.3420621931260228e-06, 'epoch': 1.05}

 35%|███▌      | 221/630 [06:02<10:58,  1.61s/it]
 35%|███▌      | 222/630 [06:04<10:53,  1.60s/it]
                                                 
{'loss': 0.507, 'grad_norm': 1.4363414339116174, 'learning_rate': 1.3387888707037642e-06, 'epoch': 1.06}

 35%|███▌      | 222/630 [06:04<10:53,  1.60s/it]
 35%|███▌      | 223/630 [06:05<10:53,  1.60s/it]
                                                 
{'loss': 0.5213, 'grad_norm': 1.4355990505661556, 'learning_rate': 1.3355155482815057e-06, 'epoch': 1.06}

 35%|███▌      | 223/630 [06:05<10:53,  1.60s/it]
 36%|███▌      | 224/630 [06:07<10:52,  1.61s/it]
                                                 
{'loss': 0.5351, 'grad_norm': 1.3965624660030738, 'learning_rate': 1.3322422258592473e-06, 'epoch': 1.07}

 36%|███▌      | 224/630 [06:07<10:52,  1.61s/it]
 36%|███▌      | 225/630 [06:08<10:49,  1.60s/it]
                                                 
{'loss': 0.5803, 'grad_norm': 1.6729269393880268, 'learning_rate': 1.3289689034369884e-06, 'epoch': 1.07}

 36%|███▌      | 225/630 [06:08<10:49,  1.60s/it]
 36%|███▌      | 226/630 [06:10<10:47,  1.60s/it]
                                                 
{'loss': 0.532, 'grad_norm': 1.8561658038237479, 'learning_rate': 1.32569558101473e-06, 'epoch': 1.08}

 36%|███▌      | 226/630 [06:10<10:47,  1.60s/it]
 36%|███▌      | 227/630 [06:12<10:46,  1.60s/it]
                                                 
{'loss': 0.5399, 'grad_norm': 1.6156918530580215, 'learning_rate': 1.3224222585924713e-06, 'epoch': 1.08}

 36%|███▌      | 227/630 [06:12<10:46,  1.60s/it]
 36%|███▌      | 228/630 [06:13<10:48,  1.61s/it]
                                                 
{'loss': 0.5734, 'grad_norm': 1.8177297157925865, 'learning_rate': 1.3191489361702126e-06, 'epoch': 1.09}

 36%|███▌      | 228/630 [06:13<10:48,  1.61s/it]
 36%|███▋      | 229/630 [06:15<10:46,  1.61s/it]
                                                 
{'loss': 0.5358, 'grad_norm': 2.452313730666226, 'learning_rate': 1.3158756137479541e-06, 'epoch': 1.09}

 36%|███▋      | 229/630 [06:15<10:46,  1.61s/it]
 37%|███▋      | 230/630 [06:16<10:42,  1.61s/it]
                                                 
{'loss': 0.5435, 'grad_norm': 1.382608215966216, 'learning_rate': 1.3126022913256955e-06, 'epoch': 1.1}

 37%|███▋      | 230/630 [06:16<10:42,  1.61s/it]
 37%|███▋      | 231/630 [06:18<10:40,  1.61s/it]
                                                 
{'loss': 0.4836, 'grad_norm': 1.5972372304262703, 'learning_rate': 1.309328968903437e-06, 'epoch': 1.1}

 37%|███▋      | 231/630 [06:18<10:40,  1.61s/it]
 37%|███▋      | 232/630 [06:20<10:38,  1.61s/it]
                                                 
{'loss': 0.5365, 'grad_norm': 1.6326630961648423, 'learning_rate': 1.3060556464811784e-06, 'epoch': 1.1}

 37%|███▋      | 232/630 [06:20<10:38,  1.61s/it]
 37%|███▋      | 233/630 [06:21<10:35,  1.60s/it]
                                                 
{'loss': 0.4793, 'grad_norm': 1.6212555924216727, 'learning_rate': 1.3027823240589197e-06, 'epoch': 1.11}

 37%|███▋      | 233/630 [06:21<10:35,  1.60s/it]
 37%|███▋      | 234/630 [06:23<10:34,  1.60s/it]
                                                 
{'loss': 0.5565, 'grad_norm': 1.5704238288750103, 'learning_rate': 1.2995090016366612e-06, 'epoch': 1.11}

 37%|███▋      | 234/630 [06:23<10:34,  1.60s/it]
 37%|███▋      | 235/630 [06:24<10:31,  1.60s/it]
                                                 
{'loss': 0.5607, 'grad_norm': 1.8665944229348561, 'learning_rate': 1.2962356792144024e-06, 'epoch': 1.12}

 37%|███▋      | 235/630 [06:24<10:31,  1.60s/it]
 37%|███▋      | 236/630 [06:26<10:29,  1.60s/it]
                                                 
{'loss': 0.5282, 'grad_norm': 1.5484765173205033, 'learning_rate': 1.292962356792144e-06, 'epoch': 1.12}

 37%|███▋      | 236/630 [06:26<10:29,  1.60s/it]
 38%|███▊      | 237/630 [06:28<10:27,  1.60s/it]
                                                 
{'loss': 0.556, 'grad_norm': 1.5989494201918228, 'learning_rate': 1.2896890343698855e-06, 'epoch': 1.13}

 38%|███▊      | 237/630 [06:28<10:27,  1.60s/it]
 38%|███▊      | 238/630 [06:29<10:25,  1.60s/it]
                                                 
{'loss': 0.5396, 'grad_norm': 1.5400593951242667, 'learning_rate': 1.2864157119476268e-06, 'epoch': 1.13}

 38%|███▊      | 238/630 [06:29<10:25,  1.60s/it]
 38%|███▊      | 239/630 [06:31<10:24,  1.60s/it]
                                                 
{'loss': 0.5659, 'grad_norm': 1.550734442869971, 'learning_rate': 1.2831423895253681e-06, 'epoch': 1.14}

 38%|███▊      | 239/630 [06:31<10:24,  1.60s/it]
 38%|███▊      | 240/630 [06:32<10:23,  1.60s/it]
                                                 
{'loss': 0.5541, 'grad_norm': 1.7507635640985164, 'learning_rate': 1.2798690671031097e-06, 'epoch': 1.14}

 38%|███▊      | 240/630 [06:32<10:23,  1.60s/it]
 38%|███▊      | 241/630 [06:34<10:22,  1.60s/it]
                                                 
{'loss': 0.5063, 'grad_norm': 1.315820106944266, 'learning_rate': 1.276595744680851e-06, 'epoch': 1.15}

 38%|███▊      | 241/630 [06:34<10:22,  1.60s/it]
 38%|███▊      | 242/630 [06:36<10:18,  1.59s/it]
                                                 
{'loss': 0.475, 'grad_norm': 1.5689274548947636, 'learning_rate': 1.2733224222585923e-06, 'epoch': 1.15}

 38%|███▊      | 242/630 [06:36<10:18,  1.59s/it]
 39%|███▊      | 243/630 [06:37<10:18,  1.60s/it]
                                                 
{'loss': 0.575, 'grad_norm': 1.4512975922335636, 'learning_rate': 1.270049099836334e-06, 'epoch': 1.16}

 39%|███▊      | 243/630 [06:37<10:18,  1.60s/it]
 39%|███▊      | 244/630 [06:39<10:15,  1.60s/it]
                                                 
{'loss': 0.5158, 'grad_norm': 1.4870922978172525, 'learning_rate': 1.2667757774140752e-06, 'epoch': 1.16}

 39%|███▊      | 244/630 [06:39<10:15,  1.60s/it]
 39%|███▉      | 245/630 [06:40<10:15,  1.60s/it]
                                                 
{'loss': 0.4939, 'grad_norm': 2.484385815916651, 'learning_rate': 1.2635024549918168e-06, 'epoch': 1.17}

 39%|███▉      | 245/630 [06:40<10:15,  1.60s/it]
 39%|███▉      | 246/630 [06:42<10:15,  1.60s/it]
                                                 
{'loss': 0.5267, 'grad_norm': 1.5687309084772045, 'learning_rate': 1.260229132569558e-06, 'epoch': 1.17}

 39%|███▉      | 246/630 [06:42<10:15,  1.60s/it]
 39%|███▉      | 247/630 [06:44<10:12,  1.60s/it]
                                                 
{'loss': 0.5405, 'grad_norm': 1.3960239946774646, 'learning_rate': 1.2569558101472994e-06, 'epoch': 1.18}

 39%|███▉      | 247/630 [06:44<10:12,  1.60s/it]
 39%|███▉      | 248/630 [06:45<10:12,  1.60s/it]
                                                 
{'loss': 0.4515, 'grad_norm': 1.317643723280349, 'learning_rate': 1.253682487725041e-06, 'epoch': 1.18}

 39%|███▉      | 248/630 [06:45<10:12,  1.60s/it]
 40%|███▉      | 249/630 [06:47<10:10,  1.60s/it]
                                                 
{'loss': 0.4908, 'grad_norm': 1.5500460733571313, 'learning_rate': 1.2504091653027821e-06, 'epoch': 1.19}

 40%|███▉      | 249/630 [06:47<10:10,  1.60s/it]
 40%|███▉      | 250/630 [06:48<10:10,  1.61s/it]
                                                 
{'loss': 0.5234, 'grad_norm': 1.5039964220576276, 'learning_rate': 1.2471358428805237e-06, 'epoch': 1.19}

 40%|███▉      | 250/630 [06:48<10:10,  1.61s/it]
 40%|███▉      | 251/630 [06:50<10:07,  1.60s/it]
                                                 
{'loss': 0.5085, 'grad_norm': 1.5559800747483659, 'learning_rate': 1.2438625204582652e-06, 'epoch': 1.2}

 40%|███▉      | 251/630 [06:50<10:07,  1.60s/it]
 40%|████      | 252/630 [06:52<10:05,  1.60s/it]
                                                 
{'loss': 0.5449, 'grad_norm': 1.473529251820383, 'learning_rate': 1.2405891980360065e-06, 'epoch': 1.2}

 40%|████      | 252/630 [06:52<10:05,  1.60s/it]
 40%|████      | 253/630 [06:53<10:04,  1.60s/it]
                                                 
{'loss': 0.5525, 'grad_norm': 1.5799973464091877, 'learning_rate': 1.2373158756137479e-06, 'epoch': 1.2}

 40%|████      | 253/630 [06:53<10:04,  1.60s/it]
 40%|████      | 254/630 [06:55<10:02,  1.60s/it]
                                                 
{'loss': 0.4968, 'grad_norm': 1.6021559363003568, 'learning_rate': 1.2340425531914892e-06, 'epoch': 1.21}

 40%|████      | 254/630 [06:55<10:02,  1.60s/it]
 40%|████      | 255/630 [06:56<10:00,  1.60s/it]
                                                 
{'loss': 0.5281, 'grad_norm': 1.523220674464856, 'learning_rate': 1.2307692307692308e-06, 'epoch': 1.21}

 40%|████      | 255/630 [06:56<10:00,  1.60s/it]
 41%|████      | 256/630 [06:58<09:57,  1.60s/it]
                                                 
{'loss': 0.4779, 'grad_norm': 1.5011675807401812, 'learning_rate': 1.227495908346972e-06, 'epoch': 1.22}

 41%|████      | 256/630 [06:58<09:57,  1.60s/it]
 41%|████      | 257/630 [07:00<09:55,  1.60s/it]
                                                 
{'loss': 0.5549, 'grad_norm': 1.8506943410131964, 'learning_rate': 1.2242225859247134e-06, 'epoch': 1.22}

 41%|████      | 257/630 [07:00<09:55,  1.60s/it]
 41%|████      | 258/630 [07:01<09:54,  1.60s/it]
                                                 
{'loss': 0.478, 'grad_norm': 1.3359190959822091, 'learning_rate': 1.220949263502455e-06, 'epoch': 1.23}

 41%|████      | 258/630 [07:01<09:54,  1.60s/it]
 41%|████      | 259/630 [07:03<09:52,  1.60s/it]
                                                 
{'loss': 0.5679, 'grad_norm': 1.5659253800701751, 'learning_rate': 1.2176759410801965e-06, 'epoch': 1.23}

 41%|████      | 259/630 [07:03<09:52,  1.60s/it]
 41%|████▏     | 260/630 [07:04<09:50,  1.60s/it]
                                                 
{'loss': 0.5118, 'grad_norm': 1.5555048338734083, 'learning_rate': 1.2144026186579376e-06, 'epoch': 1.24}

 41%|████▏     | 260/630 [07:04<09:50,  1.60s/it]
 41%|████▏     | 261/630 [07:06<09:48,  1.60s/it]
                                                 
{'loss': 0.5236, 'grad_norm': 1.695461176123246, 'learning_rate': 1.2111292962356792e-06, 'epoch': 1.24}

 41%|████▏     | 261/630 [07:06<09:48,  1.60s/it]
 42%|████▏     | 262/630 [07:08<09:47,  1.60s/it]
                                                 
{'loss': 0.6201, 'grad_norm': 1.7108914722771276, 'learning_rate': 1.2078559738134207e-06, 'epoch': 1.25}

 42%|████▏     | 262/630 [07:08<09:47,  1.60s/it]
 42%|████▏     | 263/630 [07:09<09:46,  1.60s/it]
                                                 
{'loss': 0.4908, 'grad_norm': 1.7514092551558895, 'learning_rate': 1.2045826513911619e-06, 'epoch': 1.25}

 42%|████▏     | 263/630 [07:09<09:46,  1.60s/it]
 42%|████▏     | 264/630 [07:11<09:44,  1.60s/it]
                                                 
{'loss': 0.508, 'grad_norm': 1.4945910763786203, 'learning_rate': 1.2013093289689034e-06, 'epoch': 1.26}

 42%|████▏     | 264/630 [07:11<09:44,  1.60s/it]
 42%|████▏     | 265/630 [07:12<09:42,  1.60s/it]
                                                 
{'loss': 0.4985, 'grad_norm': 1.4943638689515852, 'learning_rate': 1.1980360065466447e-06, 'epoch': 1.26}

 42%|████▏     | 265/630 [07:12<09:42,  1.60s/it]
 42%|████▏     | 266/630 [07:14<09:40,  1.60s/it]
                                                 
{'loss': 0.5326, 'grad_norm': 1.4501016419154455, 'learning_rate': 1.1947626841243863e-06, 'epoch': 1.27}

 42%|████▏     | 266/630 [07:14<09:40,  1.60s/it]
 42%|████▏     | 267/630 [07:16<09:39,  1.60s/it]
                                                 
{'loss': 0.4899, 'grad_norm': 1.431981254624705, 'learning_rate': 1.1914893617021276e-06, 'epoch': 1.27}

 42%|████▏     | 267/630 [07:16<09:39,  1.60s/it]
 43%|████▎     | 268/630 [07:17<09:37,  1.60s/it]
                                                 
{'loss': 0.5431, 'grad_norm': 1.7720495993866652, 'learning_rate': 1.188216039279869e-06, 'epoch': 1.28}

 43%|████▎     | 268/630 [07:17<09:37,  1.60s/it]
 43%|████▎     | 269/630 [07:19<09:34,  1.59s/it]
                                                 
{'loss': 0.5142, 'grad_norm': 1.6986247435321644, 'learning_rate': 1.1849427168576105e-06, 'epoch': 1.28}

 43%|████▎     | 269/630 [07:19<09:34,  1.59s/it]
 43%|████▎     | 270/630 [07:20<09:32,  1.59s/it]
                                                 
{'loss': 0.465, 'grad_norm': 1.7515599168992735, 'learning_rate': 1.1816693944353518e-06, 'epoch': 1.29}

 43%|████▎     | 270/630 [07:20<09:32,  1.59s/it]
 43%|████▎     | 271/630 [07:22<09:31,  1.59s/it]
                                                 
{'loss': 0.5367, 'grad_norm': 1.8757262558697387, 'learning_rate': 1.1783960720130932e-06, 'epoch': 1.29}

 43%|████▎     | 271/630 [07:22<09:31,  1.59s/it]
 43%|████▎     | 272/630 [07:23<09:29,  1.59s/it]
                                                 
{'loss': 0.5545, 'grad_norm': 1.914734606705717, 'learning_rate': 1.1751227495908347e-06, 'epoch': 1.3}

 43%|████▎     | 272/630 [07:23<09:29,  1.59s/it]
 43%|████▎     | 273/630 [07:25<09:30,  1.60s/it]
                                                 
{'loss': 0.487, 'grad_norm': 1.574450148612718, 'learning_rate': 1.171849427168576e-06, 'epoch': 1.3}

 43%|████▎     | 273/630 [07:25<09:30,  1.60s/it]
 43%|████▎     | 274/630 [07:27<09:29,  1.60s/it]
                                                 
{'loss': 0.4857, 'grad_norm': 1.7017629494117035, 'learning_rate': 1.1685761047463174e-06, 'epoch': 1.3}

 43%|████▎     | 274/630 [07:27<09:29,  1.60s/it]
 44%|████▎     | 275/630 [07:28<09:28,  1.60s/it]
                                                 
{'loss': 0.538, 'grad_norm': 1.7168265895930392, 'learning_rate': 1.165302782324059e-06, 'epoch': 1.31}

 44%|████▎     | 275/630 [07:28<09:28,  1.60s/it]
 44%|████▍     | 276/630 [07:30<09:25,  1.60s/it]
                                                 
{'loss': 0.4681, 'grad_norm': 1.689924444855705, 'learning_rate': 1.1620294599018003e-06, 'epoch': 1.31}

 44%|████▍     | 276/630 [07:30<09:25,  1.60s/it]
 44%|████▍     | 277/630 [07:31<09:25,  1.60s/it]
                                                 
{'loss': 0.4846, 'grad_norm': 1.460457212279621, 'learning_rate': 1.1587561374795416e-06, 'epoch': 1.32}

 44%|████▍     | 277/630 [07:31<09:25,  1.60s/it]
 44%|████▍     | 278/630 [07:33<09:25,  1.61s/it]
                                                 
{'loss': 0.4912, 'grad_norm': 2.483259777004427, 'learning_rate': 1.1554828150572832e-06, 'epoch': 1.32}

 44%|████▍     | 278/630 [07:33<09:25,  1.61s/it]
 44%|████▍     | 279/630 [07:35<09:24,  1.61s/it]
                                                 
{'loss': 0.5024, 'grad_norm': 1.5709434951086685, 'learning_rate': 1.1522094926350245e-06, 'epoch': 1.33}

 44%|████▍     | 279/630 [07:35<09:24,  1.61s/it]
 44%|████▍     | 280/630 [07:36<09:24,  1.61s/it]
                                                 
{'loss': 0.523, 'grad_norm': 1.588433127856156, 'learning_rate': 1.148936170212766e-06, 'epoch': 1.33}

 44%|████▍     | 280/630 [07:36<09:24,  1.61s/it]
 45%|████▍     | 281/630 [07:38<09:22,  1.61s/it]
                                                 
{'loss': 0.5008, 'grad_norm': 10.316189617982111, 'learning_rate': 1.1456628477905072e-06, 'epoch': 1.34}

 45%|████▍     | 281/630 [07:38<09:22,  1.61s/it]
 45%|████▍     | 282/630 [07:40<09:19,  1.61s/it]
                                                 
{'loss': 0.548, 'grad_norm': 1.6713485591112973, 'learning_rate': 1.1423895253682487e-06, 'epoch': 1.34}

 45%|████▍     | 282/630 [07:40<09:19,  1.61s/it]
 45%|████▍     | 283/630 [07:41<09:17,  1.61s/it]
                                                 
{'loss': 0.5117, 'grad_norm': 1.5421027725411287, 'learning_rate': 1.1391162029459903e-06, 'epoch': 1.35}

 45%|████▍     | 283/630 [07:41<09:17,  1.61s/it]
 45%|████▌     | 284/630 [07:43<09:14,  1.60s/it]
                                                 
{'loss': 0.484, 'grad_norm': 1.396779277205541, 'learning_rate': 1.1358428805237314e-06, 'epoch': 1.35}

 45%|████▌     | 284/630 [07:43<09:14,  1.60s/it]
 45%|████▌     | 285/630 [07:44<09:12,  1.60s/it]
                                                 
{'loss': 0.4835, 'grad_norm': 1.6081310616731648, 'learning_rate': 1.132569558101473e-06, 'epoch': 1.36}

 45%|████▌     | 285/630 [07:44<09:12,  1.60s/it]
 45%|████▌     | 286/630 [07:46<09:10,  1.60s/it]
                                                 
{'loss': 0.5049, 'grad_norm': 1.8524465587119527, 'learning_rate': 1.1292962356792145e-06, 'epoch': 1.36}

 45%|████▌     | 286/630 [07:46<09:10,  1.60s/it]
 46%|████▌     | 287/630 [07:48<09:08,  1.60s/it]
                                                 
{'loss': 0.5484, 'grad_norm': 1.5968461699197112, 'learning_rate': 1.1260229132569558e-06, 'epoch': 1.37}

 46%|████▌     | 287/630 [07:48<09:08,  1.60s/it]
 46%|████▌     | 288/630 [07:49<09:07,  1.60s/it]
                                                 
{'loss': 0.4446, 'grad_norm': 1.575238915154477, 'learning_rate': 1.1227495908346971e-06, 'epoch': 1.37}

 46%|████▌     | 288/630 [07:49<09:07,  1.60s/it]
 46%|████▌     | 289/630 [07:51<09:05,  1.60s/it]
                                                 
{'loss': 0.5269, 'grad_norm': 1.4016252851543718, 'learning_rate': 1.1194762684124387e-06, 'epoch': 1.38}

 46%|████▌     | 289/630 [07:51<09:05,  1.60s/it]
 46%|████▌     | 290/630 [07:52<09:03,  1.60s/it]
                                                 
{'loss': 0.4666, 'grad_norm': 1.531987735187326, 'learning_rate': 1.11620294599018e-06, 'epoch': 1.38}

 46%|████▌     | 290/630 [07:52<09:03,  1.60s/it]
 46%|████▌     | 291/630 [07:54<09:02,  1.60s/it]
                                                 
{'loss': 0.4622, 'grad_norm': 1.8314445911844883, 'learning_rate': 1.1129296235679214e-06, 'epoch': 1.39}

 46%|████▌     | 291/630 [07:54<09:02,  1.60s/it]
 46%|████▋     | 292/630 [07:56<09:00,  1.60s/it]
                                                 
{'loss': 0.4843, 'grad_norm': 1.6188931741160146, 'learning_rate': 1.1096563011456627e-06, 'epoch': 1.39}

 46%|████▋     | 292/630 [07:56<09:00,  1.60s/it]
 47%|████▋     | 293/630 [07:57<08:58,  1.60s/it]
                                                 
{'loss': 0.543, 'grad_norm': 1.6592486142411522, 'learning_rate': 1.1063829787234042e-06, 'epoch': 1.4}

 47%|████▋     | 293/630 [07:57<08:58,  1.60s/it]
 47%|████▋     | 294/630 [07:59<08:55,  1.59s/it]
                                                 
{'loss': 0.476, 'grad_norm': 1.5437999364327202, 'learning_rate': 1.1031096563011458e-06, 'epoch': 1.4}

 47%|████▋     | 294/630 [07:59<08:55,  1.59s/it]
 47%|████▋     | 295/630 [08:00<08:54,  1.60s/it]
                                                 
{'loss': 0.4703, 'grad_norm': 1.4226174714230844, 'learning_rate': 1.099836333878887e-06, 'epoch': 1.4}

 47%|████▋     | 295/630 [08:00<08:54,  1.60s/it]
 47%|████▋     | 296/630 [08:02<08:53,  1.60s/it]
                                                 
{'loss': 0.5103, 'grad_norm': 1.5446456566590363, 'learning_rate': 1.0965630114566285e-06, 'epoch': 1.41}

 47%|████▋     | 296/630 [08:02<08:53,  1.60s/it]
 47%|████▋     | 297/630 [08:04<08:51,  1.60s/it]
                                                 
{'loss': 0.4933, 'grad_norm': 1.4975388168570976, 'learning_rate': 1.09328968903437e-06, 'epoch': 1.41}

 47%|████▋     | 297/630 [08:04<08:51,  1.60s/it]
 47%|████▋     | 298/630 [08:05<08:51,  1.60s/it]
                                                 
{'loss': 0.5192, 'grad_norm': 1.6172669134410884, 'learning_rate': 1.0900163666121111e-06, 'epoch': 1.42}

 47%|████▋     | 298/630 [08:05<08:51,  1.60s/it]
 47%|████▋     | 299/630 [08:07<08:50,  1.60s/it]
                                                 
{'loss': 0.4583, 'grad_norm': 1.3526736035988862, 'learning_rate': 1.0867430441898527e-06, 'epoch': 1.42}

 47%|████▋     | 299/630 [08:07<08:50,  1.60s/it]
 48%|████▊     | 300/630 [08:08<08:48,  1.60s/it]
                                                 
{'loss': 0.5343, 'grad_norm': 1.4709575529829577, 'learning_rate': 1.083469721767594e-06, 'epoch': 1.43}

 48%|████▊     | 300/630 [08:08<08:48,  1.60s/it]
 48%|████▊     | 301/630 [08:10<08:45,  1.60s/it]
                                                 
{'loss': 0.4882, 'grad_norm': 1.5936468734895826, 'learning_rate': 1.0801963993453356e-06, 'epoch': 1.43}

 48%|████▊     | 301/630 [08:10<08:45,  1.60s/it]
 48%|████▊     | 302/630 [08:12<08:45,  1.60s/it]
                                                 
{'loss': 0.4967, 'grad_norm': 1.7679652260001628, 'learning_rate': 1.0769230769230769e-06, 'epoch': 1.44}

 48%|████▊     | 302/630 [08:12<08:45,  1.60s/it]
 48%|████▊     | 303/630 [08:13<08:43,  1.60s/it]
                                                 
{'loss': 0.544, 'grad_norm': 1.7607453879458776, 'learning_rate': 1.0736497545008182e-06, 'epoch': 1.44}

 48%|████▊     | 303/630 [08:13<08:43,  1.60s/it]
 48%|████▊     | 304/630 [08:15<08:43,  1.60s/it]
                                                 
{'loss': 0.5296, 'grad_norm': 1.7259784459327743, 'learning_rate': 1.0703764320785598e-06, 'epoch': 1.45}

 48%|████▊     | 304/630 [08:15<08:43,  1.60s/it]
 48%|████▊     | 305/630 [08:16<08:41,  1.60s/it]
                                                 
{'loss': 0.4767, 'grad_norm': 1.6666215725928593, 'learning_rate': 1.0671031096563011e-06, 'epoch': 1.45}

 48%|████▊     | 305/630 [08:16<08:41,  1.60s/it]
 49%|████▊     | 306/630 [08:18<08:39,  1.60s/it]
                                                 
{'loss': 0.5196, 'grad_norm': 2.2196557938441615, 'learning_rate': 1.0638297872340424e-06, 'epoch': 1.46}

 49%|████▊     | 306/630 [08:18<08:39,  1.60s/it]
 49%|████▊     | 307/630 [08:20<08:37,  1.60s/it]
                                                 
{'loss': 0.5201, 'grad_norm': 2.058156897204899, 'learning_rate': 1.060556464811784e-06, 'epoch': 1.46}

 49%|████▊     | 307/630 [08:20<08:37,  1.60s/it]
 49%|████▉     | 308/630 [08:21<08:35,  1.60s/it]
                                                 
{'loss': 0.4799, 'grad_norm': 1.564177653178245, 'learning_rate': 1.0572831423895253e-06, 'epoch': 1.47}

 49%|████▉     | 308/630 [08:21<08:35,  1.60s/it]
 49%|████▉     | 309/630 [08:23<08:33,  1.60s/it]
                                                 
{'loss': 0.4404, 'grad_norm': 1.5097936547214807, 'learning_rate': 1.0540098199672667e-06, 'epoch': 1.47}

 49%|████▉     | 309/630 [08:23<08:33,  1.60s/it]
 49%|████▉     | 310/630 [08:24<08:33,  1.60s/it]
                                                 
{'loss': 0.5417, 'grad_norm': 1.6659424747572482, 'learning_rate': 1.0507364975450082e-06, 'epoch': 1.48}

 49%|████▉     | 310/630 [08:24<08:33,  1.60s/it]
 49%|████▉     | 311/630 [08:26<08:31,  1.60s/it]
                                                 
{'loss': 0.5481, 'grad_norm': 1.875059491713502, 'learning_rate': 1.0474631751227495e-06, 'epoch': 1.48}

 49%|████▉     | 311/630 [08:26<08:31,  1.60s/it]
 50%|████▉     | 312/630 [08:28<08:30,  1.60s/it]
                                                 
{'loss': 0.431, 'grad_norm': 1.4136905361032008, 'learning_rate': 1.0441898527004909e-06, 'epoch': 1.49}

 50%|████▉     | 312/630 [08:28<08:30,  1.60s/it]
 50%|████▉     | 313/630 [08:29<08:29,  1.61s/it]
                                                 
{'loss': 0.4972, 'grad_norm': 1.6606045202317834, 'learning_rate': 1.0409165302782324e-06, 'epoch': 1.49}

 50%|████▉     | 313/630 [08:29<08:29,  1.61s/it]
 50%|████▉     | 314/630 [08:31<08:28,  1.61s/it]
                                                 
{'loss': 0.4902, 'grad_norm': 1.3569320871894268, 'learning_rate': 1.0376432078559738e-06, 'epoch': 1.5}

 50%|████▉     | 314/630 [08:31<08:28,  1.61s/it]
 50%|█████     | 315/630 [08:32<08:26,  1.61s/it]
                                                 
{'loss': 0.5069, 'grad_norm': 1.4765311752822639, 'learning_rate': 1.0343698854337153e-06, 'epoch': 1.5}

 50%|█████     | 315/630 [08:32<08:26,  1.61s/it]
 50%|█████     | 316/630 [08:34<08:26,  1.61s/it]
                                                 
{'loss': 0.4445, 'grad_norm': 1.3159610296406479, 'learning_rate': 1.0310965630114566e-06, 'epoch': 1.5}

 50%|█████     | 316/630 [08:34<08:26,  1.61s/it]
 50%|█████     | 317/630 [08:36<08:56,  1.71s/it]
                                                 
{'loss': 0.4685, 'grad_norm': 1.4010143366668424, 'learning_rate': 1.027823240589198e-06, 'epoch': 1.51}

 50%|█████     | 317/630 [08:36<08:56,  1.71s/it]
 50%|█████     | 318/630 [08:38<08:44,  1.68s/it]
                                                 
{'loss': 0.4729, 'grad_norm': 1.598110331389566, 'learning_rate': 1.0245499181669395e-06, 'epoch': 1.51}

 50%|█████     | 318/630 [08:38<08:44,  1.68s/it]
 51%|█████     | 319/630 [08:39<08:35,  1.66s/it]
                                                 
{'loss': 0.5994, 'grad_norm': 1.5435176780109259, 'learning_rate': 1.0212765957446806e-06, 'epoch': 1.52}

 51%|█████     | 319/630 [08:39<08:35,  1.66s/it]
 51%|█████     | 320/630 [08:41<08:28,  1.64s/it]
                                                 
{'loss': 0.5675, 'grad_norm': 1.9817598956332247, 'learning_rate': 1.0180032733224222e-06, 'epoch': 1.52}

 51%|█████     | 320/630 [08:41<08:28,  1.64s/it]
 51%|█████     | 321/630 [08:42<08:22,  1.63s/it]
                                                 
{'loss': 0.5416, 'grad_norm': 2.176711858336669, 'learning_rate': 1.0147299509001637e-06, 'epoch': 1.53}

 51%|█████     | 321/630 [08:42<08:22,  1.63s/it]
 51%|█████     | 322/630 [08:44<08:18,  1.62s/it]
                                                 
{'loss': 0.4943, 'grad_norm': 1.5169691453168292, 'learning_rate': 1.011456628477905e-06, 'epoch': 1.53}

 51%|█████     | 322/630 [08:44<08:18,  1.62s/it]
 51%|█████▏    | 323/630 [08:46<08:14,  1.61s/it]
                                                 
{'loss': 0.5171, 'grad_norm': 1.8821876954120216, 'learning_rate': 1.0081833060556464e-06, 'epoch': 1.54}

 51%|█████▏    | 323/630 [08:46<08:14,  1.61s/it]
 51%|█████▏    | 324/630 [08:47<08:11,  1.60s/it]
                                                 
{'loss': 0.5376, 'grad_norm': 2.242302047095297, 'learning_rate': 1.004909983633388e-06, 'epoch': 1.54}

 51%|█████▏    | 324/630 [08:47<08:11,  1.60s/it]
 52%|█████▏    | 325/630 [08:49<08:09,  1.60s/it]
                                                 
{'loss': 0.4346, 'grad_norm': 1.3931812464809554, 'learning_rate': 1.0016366612111293e-06, 'epoch': 1.55}

 52%|█████▏    | 325/630 [08:49<08:09,  1.60s/it]
 52%|█████▏    | 326/630 [08:50<08:06,  1.60s/it]
                                                 
{'loss': 0.4499, 'grad_norm': 1.436737236121035, 'learning_rate': 9.983633387888706e-07, 'epoch': 1.55}

 52%|█████▏    | 326/630 [08:50<08:06,  1.60s/it]
 52%|█████▏    | 327/630 [08:52<08:04,  1.60s/it]
                                                 
{'loss': 0.5275, 'grad_norm': 1.794546796500706, 'learning_rate': 9.95090016366612e-07, 'epoch': 1.56}

 52%|█████▏    | 327/630 [08:52<08:04,  1.60s/it]
 52%|█████▏    | 328/630 [08:54<08:03,  1.60s/it]
                                                 
{'loss': 0.4568, 'grad_norm': 1.4604812277316224, 'learning_rate': 9.918166939443535e-07, 'epoch': 1.56}

 52%|█████▏    | 328/630 [08:54<08:03,  1.60s/it]
 52%|█████▏    | 329/630 [08:55<08:02,  1.60s/it]
                                                 
{'loss': 0.5082, 'grad_norm': 1.635481728340859, 'learning_rate': 9.885433715220948e-07, 'epoch': 1.57}

 52%|█████▏    | 329/630 [08:55<08:02,  1.60s/it]
 52%|█████▏    | 330/630 [08:57<07:59,  1.60s/it]
                                                 
{'loss': 0.6151, 'grad_norm': 2.0885991407855826, 'learning_rate': 9.852700490998362e-07, 'epoch': 1.57}

 52%|█████▏    | 330/630 [08:57<07:59,  1.60s/it]
 53%|█████▎    | 331/630 [08:58<07:57,  1.60s/it]
                                                 
{'loss': 0.4808, 'grad_norm': 1.475912863675058, 'learning_rate': 9.819967266775777e-07, 'epoch': 1.58}

 53%|█████▎    | 331/630 [08:58<07:57,  1.60s/it]
 53%|█████▎    | 332/630 [09:00<07:55,  1.60s/it]
                                                 
{'loss': 0.5253, 'grad_norm': 1.575479153762643, 'learning_rate': 9.78723404255319e-07, 'epoch': 1.58}

 53%|█████▎    | 332/630 [09:00<07:55,  1.60s/it]
 53%|█████▎    | 333/630 [09:02<07:53,  1.60s/it]
                                                 
{'loss': 0.5274, 'grad_norm': 1.5138335952800304, 'learning_rate': 9.754500818330606e-07, 'epoch': 1.59}

 53%|█████▎    | 333/630 [09:02<07:53,  1.60s/it]
 53%|█████▎    | 334/630 [09:03<07:52,  1.60s/it]
                                                 
{'loss': 0.5176, 'grad_norm': 1.4879877389072718, 'learning_rate': 9.72176759410802e-07, 'epoch': 1.59}

 53%|█████▎    | 334/630 [09:03<07:52,  1.60s/it]
 53%|█████▎    | 335/630 [09:05<07:51,  1.60s/it]
                                                 
{'loss': 0.4498, 'grad_norm': 1.4101475556218406, 'learning_rate': 9.689034369885433e-07, 'epoch': 1.6}

 53%|█████▎    | 335/630 [09:05<07:51,  1.60s/it]
 53%|█████▎    | 336/630 [09:06<07:56,  1.62s/it]
                                                 
{'loss': 0.4934, 'grad_norm': 2.4683402370831518, 'learning_rate': 9.656301145662848e-07, 'epoch': 1.6}

 53%|█████▎    | 336/630 [09:06<07:56,  1.62s/it]
 53%|█████▎    | 337/630 [09:08<07:52,  1.61s/it]
                                                 
{'loss': 0.5144, 'grad_norm': 1.873222060950154, 'learning_rate': 9.623567921440262e-07, 'epoch': 1.6}

 53%|█████▎    | 337/630 [09:08<07:52,  1.61s/it]
 54%|█████▎    | 338/630 [09:10<07:48,  1.61s/it]
                                                 
{'loss': 0.4969, 'grad_norm': 1.7176815257206064, 'learning_rate': 9.590834697217675e-07, 'epoch': 1.61}

 54%|█████▎    | 338/630 [09:10<07:48,  1.61s/it]
 54%|█████▍    | 339/630 [09:11<07:46,  1.60s/it]
                                                 
{'loss': 0.488, 'grad_norm': 1.380854230403615, 'learning_rate': 9.55810147299509e-07, 'epoch': 1.61}

 54%|█████▍    | 339/630 [09:11<07:46,  1.60s/it]
 54%|█████▍    | 340/630 [09:13<07:44,  1.60s/it]
                                                 
{'loss': 0.52, 'grad_norm': 1.466616267826263, 'learning_rate': 9.525368248772504e-07, 'epoch': 1.62}

 54%|█████▍    | 340/630 [09:13<07:44,  1.60s/it]
 54%|█████▍    | 341/630 [09:14<07:42,  1.60s/it]
                                                 
{'loss': 0.5157, 'grad_norm': 1.7240890101265465, 'learning_rate': 9.492635024549918e-07, 'epoch': 1.62}

 54%|█████▍    | 341/630 [09:14<07:42,  1.60s/it]
 54%|█████▍    | 342/630 [09:16<07:43,  1.61s/it]
                                                 
{'loss': 0.481, 'grad_norm': 1.4991170130671738, 'learning_rate': 9.459901800327333e-07, 'epoch': 1.63}

 54%|█████▍    | 342/630 [09:16<07:43,  1.61s/it]
 54%|█████▍    | 343/630 [09:18<07:41,  1.61s/it]
                                                 
{'loss': 0.4706, 'grad_norm': 1.5337826963374843, 'learning_rate': 9.427168576104746e-07, 'epoch': 1.63}

 54%|█████▍    | 343/630 [09:18<07:41,  1.61s/it]
 55%|█████▍    | 344/630 [09:19<07:38,  1.60s/it]
                                                 
{'loss': 0.4651, 'grad_norm': 1.7130842621847235, 'learning_rate': 9.394435351882159e-07, 'epoch': 1.64}

 55%|█████▍    | 344/630 [09:19<07:38,  1.60s/it]
 55%|█████▍    | 345/630 [09:21<07:36,  1.60s/it]
                                                 
{'loss': 0.5095, 'grad_norm': 1.946200844145647, 'learning_rate': 9.361702127659575e-07, 'epoch': 1.64}

 55%|█████▍    | 345/630 [09:21<07:36,  1.60s/it]
 55%|█████▍    | 346/630 [09:22<07:33,  1.60s/it]
                                                 
{'loss': 0.5246, 'grad_norm': 2.0654911924056067, 'learning_rate': 9.328968903436988e-07, 'epoch': 1.65}

 55%|█████▍    | 346/630 [09:22<07:33,  1.60s/it]
 55%|█████▌    | 347/630 [09:24<07:32,  1.60s/it]
                                                 
{'loss': 0.5038, 'grad_norm': 1.4346328825705796, 'learning_rate': 9.296235679214402e-07, 'epoch': 1.65}

 55%|█████▌    | 347/630 [09:24<07:32,  1.60s/it]
 55%|█████▌    | 348/630 [09:26<07:29,  1.60s/it]
                                                 
{'loss': 0.5452, 'grad_norm': 2.0515655661199745, 'learning_rate': 9.263502454991816e-07, 'epoch': 1.66}

 55%|█████▌    | 348/630 [09:26<07:29,  1.60s/it]
 55%|█████▌    | 349/630 [09:27<07:29,  1.60s/it]
                                                 
{'loss': 0.4946, 'grad_norm': 1.595606140324023, 'learning_rate': 9.230769230769231e-07, 'epoch': 1.66}

 55%|█████▌    | 349/630 [09:27<07:29,  1.60s/it]
 56%|█████▌    | 350/630 [09:29<07:27,  1.60s/it]
                                                 
{'loss': 0.4901, 'grad_norm': 1.8783768743535048, 'learning_rate': 9.198036006546645e-07, 'epoch': 1.67}

 56%|█████▌    | 350/630 [09:29<07:27,  1.60s/it]
 56%|█████▌    | 351/630 [09:30<07:26,  1.60s/it]
                                                 
{'loss': 0.4846, 'grad_norm': 1.566380193126463, 'learning_rate': 9.165302782324058e-07, 'epoch': 1.67}

 56%|█████▌    | 351/630 [09:30<07:26,  1.60s/it]
 56%|█████▌    | 352/630 [09:32<07:24,  1.60s/it]
                                                 
{'loss': 0.4635, 'grad_norm': 1.4931347461330253, 'learning_rate': 9.132569558101472e-07, 'epoch': 1.68}

 56%|█████▌    | 352/630 [09:32<07:24,  1.60s/it]
 56%|█████▌    | 353/630 [09:34<07:26,  1.61s/it]
                                                 
{'loss': 0.4495, 'grad_norm': 1.3385551853674453, 'learning_rate': 9.099836333878887e-07, 'epoch': 1.68}

 56%|█████▌    | 353/630 [09:34<07:26,  1.61s/it]
 56%|█████▌    | 354/630 [09:35<07:24,  1.61s/it]
                                                 
{'loss': 0.4435, 'grad_norm': 1.6596830603568276, 'learning_rate': 9.067103109656301e-07, 'epoch': 1.69}

 56%|█████▌    | 354/630 [09:35<07:24,  1.61s/it]
 56%|█████▋    | 355/630 [09:37<07:21,  1.61s/it]
                                                 
{'loss': 0.4901, 'grad_norm': 1.430450586662703, 'learning_rate': 9.034369885433715e-07, 'epoch': 1.69}

 56%|█████▋    | 355/630 [09:37<07:21,  1.61s/it]
 57%|█████▋    | 356/630 [09:38<07:19,  1.60s/it]
                                                 
{'loss': 0.4301, 'grad_norm': 1.518954933847317, 'learning_rate': 9.001636661211129e-07, 'epoch': 1.7}

 57%|█████▋    | 356/630 [09:38<07:19,  1.60s/it]
 57%|█████▋    | 357/630 [09:40<07:17,  1.60s/it]
                                                 
{'loss': 0.5288, 'grad_norm': 1.4987048775346938, 'learning_rate': 8.968903436988543e-07, 'epoch': 1.7}

 57%|█████▋    | 357/630 [09:40<07:17,  1.60s/it]
 57%|█████▋    | 358/630 [09:42<07:15,  1.60s/it]
                                                 
{'loss': 0.5227, 'grad_norm': 1.5238067944202478, 'learning_rate': 8.936170212765957e-07, 'epoch': 1.7}

 57%|█████▋    | 358/630 [09:42<07:15,  1.60s/it]
 57%|█████▋    | 359/630 [09:43<07:13,  1.60s/it]
                                                 
{'loss': 0.5236, 'grad_norm': 1.4702270945432785, 'learning_rate': 8.903436988543371e-07, 'epoch': 1.71}

 57%|█████▋    | 359/630 [09:43<07:13,  1.60s/it]
 57%|█████▋    | 360/630 [09:45<07:12,  1.60s/it]
                                                 
{'loss': 0.3914, 'grad_norm': 1.3774900260549159, 'learning_rate': 8.870703764320784e-07, 'epoch': 1.71}

 57%|█████▋    | 360/630 [09:45<07:12,  1.60s/it]
 57%|█████▋    | 361/630 [09:46<07:12,  1.61s/it]
                                                 
{'loss': 0.4406, 'grad_norm': 2.130830593149523, 'learning_rate': 8.8379705400982e-07, 'epoch': 1.72}

 57%|█████▋    | 361/630 [09:46<07:12,  1.61s/it]
 57%|█████▋    | 362/630 [09:48<07:09,  1.60s/it]
                                                 
{'loss': 0.5111, 'grad_norm': 1.6997975143403345, 'learning_rate': 8.805237315875613e-07, 'epoch': 1.72}

 57%|█████▋    | 362/630 [09:48<07:09,  1.60s/it]
 58%|█████▊    | 363/630 [09:50<07:08,  1.61s/it]
                                                 
{'loss': 0.4676, 'grad_norm': 1.6811525814542123, 'learning_rate': 8.772504091653028e-07, 'epoch': 1.73}

 58%|█████▊    | 363/630 [09:50<07:08,  1.61s/it]
 58%|█████▊    | 364/630 [09:51<07:06,  1.60s/it]
                                                 
{'loss': 0.5088, 'grad_norm': 1.8770079293576594, 'learning_rate': 8.739770867430442e-07, 'epoch': 1.73}

 58%|█████▊    | 364/630 [09:51<07:06,  1.60s/it]
 58%|█████▊    | 365/630 [09:53<07:04,  1.60s/it]
                                                 
{'loss': 0.5071, 'grad_norm': 1.5175695015752968, 'learning_rate': 8.707037643207855e-07, 'epoch': 1.74}

 58%|█████▊    | 365/630 [09:53<07:04,  1.60s/it]
 58%|█████▊    | 366/630 [09:54<07:03,  1.60s/it]
                                                 
{'loss': 0.4835, 'grad_norm': 1.7038066106724588, 'learning_rate': 8.67430441898527e-07, 'epoch': 1.74}

 58%|█████▊    | 366/630 [09:54<07:03,  1.60s/it]
 58%|█████▊    | 367/630 [09:56<07:01,  1.60s/it]
                                                 
{'loss': 0.404, 'grad_norm': 1.4443126891317875, 'learning_rate': 8.641571194762683e-07, 'epoch': 1.75}

 58%|█████▊    | 367/630 [09:56<07:01,  1.60s/it]
 58%|█████▊    | 368/630 [09:58<06:59,  1.60s/it]
                                                 
{'loss': 0.4715, 'grad_norm': 1.4591058413632867, 'learning_rate': 8.608837970540099e-07, 'epoch': 1.75}

 58%|█████▊    | 368/630 [09:58<06:59,  1.60s/it]
 59%|█████▊    | 369/630 [09:59<06:57,  1.60s/it]
                                                 
{'loss': 0.4643, 'grad_norm': 1.7018225408390983, 'learning_rate': 8.576104746317512e-07, 'epoch': 1.76}

 59%|█████▊    | 369/630 [09:59<06:57,  1.60s/it]
 59%|█████▊    | 370/630 [10:01<06:56,  1.60s/it]
                                                 
{'loss': 0.3896, 'grad_norm': 1.4605981119786997, 'learning_rate': 8.543371522094926e-07, 'epoch': 1.76}

 59%|█████▊    | 370/630 [10:01<06:56,  1.60s/it]
 59%|█████▉    | 371/630 [10:02<06:54,  1.60s/it]
                                                 
{'loss': 0.5174, 'grad_norm': 1.5830635613700619, 'learning_rate': 8.51063829787234e-07, 'epoch': 1.77}

 59%|█████▉    | 371/630 [10:02<06:54,  1.60s/it]
 59%|█████▉    | 372/630 [10:04<06:53,  1.60s/it]
                                                 
{'loss': 0.5042, 'grad_norm': 1.5930842427730876, 'learning_rate': 8.477905073649754e-07, 'epoch': 1.77}

 59%|█████▉    | 372/630 [10:04<06:53,  1.60s/it]
 59%|█████▉    | 373/630 [10:06<06:55,  1.62s/it]
                                                 
{'loss': 0.4765, 'grad_norm': 1.801348690880315, 'learning_rate': 8.445171849427169e-07, 'epoch': 1.78}

 59%|█████▉    | 373/630 [10:06<06:55,  1.62s/it]
 59%|█████▉    | 374/630 [10:07<06:52,  1.61s/it]
                                                 
{'loss': 0.4957, 'grad_norm': 1.5964138790273072, 'learning_rate': 8.412438625204582e-07, 'epoch': 1.78}

 59%|█████▉    | 374/630 [10:07<06:52,  1.61s/it]
 60%|█████▉    | 375/630 [10:09<06:49,  1.61s/it]
                                                 
{'loss': 0.4914, 'grad_norm': 1.8056563746307188, 'learning_rate': 8.379705400981996e-07, 'epoch': 1.79}

 60%|█████▉    | 375/630 [10:09<06:49,  1.61s/it]
 60%|█████▉    | 376/630 [10:10<06:46,  1.60s/it]
                                                 
{'loss': 0.5073, 'grad_norm': 1.6965960778555516, 'learning_rate': 8.346972176759411e-07, 'epoch': 1.79}

 60%|█████▉    | 376/630 [10:10<06:46,  1.60s/it]
 60%|█████▉    | 377/630 [10:12<06:44,  1.60s/it]
                                                 
{'loss': 0.4817, 'grad_norm': 1.3777634614410355, 'learning_rate': 8.314238952536824e-07, 'epoch': 1.8}

 60%|█████▉    | 377/630 [10:12<06:44,  1.60s/it]
 60%|██████    | 378/630 [10:14<06:42,  1.60s/it]
                                                 
{'loss': 0.4119, 'grad_norm': 1.4807424080338758, 'learning_rate': 8.281505728314238e-07, 'epoch': 1.8}

 60%|██████    | 378/630 [10:14<06:42,  1.60s/it]
 60%|██████    | 379/630 [10:15<06:40,  1.60s/it]
                                                 
{'loss': 0.4285, 'grad_norm': 1.5505662889047451, 'learning_rate': 8.248772504091652e-07, 'epoch': 1.8}

 60%|██████    | 379/630 [10:15<06:40,  1.60s/it]
 60%|██████    | 380/630 [10:17<06:38,  1.59s/it]
                                                 
{'loss': 0.5081, 'grad_norm': 1.6148651187975456, 'learning_rate': 8.216039279869067e-07, 'epoch': 1.81}

 60%|██████    | 380/630 [10:17<06:38,  1.59s/it]
 60%|██████    | 381/630 [10:18<06:37,  1.60s/it]
                                                 
{'loss': 0.5448, 'grad_norm': 1.7449404874719738, 'learning_rate': 8.183306055646481e-07, 'epoch': 1.81}

 60%|██████    | 381/630 [10:18<06:37,  1.60s/it]
 61%|██████    | 382/630 [10:20<06:35,  1.59s/it]
                                                 
{'loss': 0.4549, 'grad_norm': 1.494730480522983, 'learning_rate': 8.150572831423895e-07, 'epoch': 1.82}

 61%|██████    | 382/630 [10:20<06:35,  1.59s/it]
 61%|██████    | 383/630 [10:22<06:34,  1.60s/it]
                                                 
{'loss': 0.4345, 'grad_norm': 1.6498660031797994, 'learning_rate': 8.117839607201308e-07, 'epoch': 1.82}

 61%|██████    | 383/630 [10:22<06:34,  1.60s/it]
 61%|██████    | 384/630 [10:23<06:32,  1.60s/it]
                                                 
{'loss': 0.5021, 'grad_norm': 2.216695303335887, 'learning_rate': 8.085106382978723e-07, 'epoch': 1.83}

 61%|██████    | 384/630 [10:23<06:32,  1.60s/it]
 61%|██████    | 385/630 [10:25<06:30,  1.60s/it]
                                                 
{'loss': 0.5018, 'grad_norm': 1.685708578277038, 'learning_rate': 8.052373158756137e-07, 'epoch': 1.83}

 61%|██████    | 385/630 [10:25<06:30,  1.60s/it]
 61%|██████▏   | 386/630 [10:26<06:28,  1.59s/it]
                                                 
{'loss': 0.4942, 'grad_norm': 1.4623855908131047, 'learning_rate': 8.019639934533551e-07, 'epoch': 1.84}

 61%|██████▏   | 386/630 [10:26<06:28,  1.59s/it]
 61%|██████▏   | 387/630 [10:28<06:26,  1.59s/it]
                                                 
{'loss': 0.4815, 'grad_norm': 2.114450920730218, 'learning_rate': 7.986906710310966e-07, 'epoch': 1.84}

 61%|██████▏   | 387/630 [10:28<06:26,  1.59s/it]
 62%|██████▏   | 388/630 [10:30<06:25,  1.59s/it]
                                                 
{'loss': 0.5282, 'grad_norm': 1.7146155873529156, 'learning_rate': 7.954173486088379e-07, 'epoch': 1.85}

 62%|██████▏   | 388/630 [10:30<06:25,  1.59s/it]
 62%|██████▏   | 389/630 [10:31<06:24,  1.60s/it]
                                                 
{'loss': 0.4796, 'grad_norm': 1.4927993381480462, 'learning_rate': 7.921440261865794e-07, 'epoch': 1.85}

 62%|██████▏   | 389/630 [10:31<06:24,  1.60s/it]
 62%|██████▏   | 390/630 [10:33<06:23,  1.60s/it]
                                                 
{'loss': 0.5251, 'grad_norm': 1.5715589586202212, 'learning_rate': 7.888707037643207e-07, 'epoch': 1.86}

 62%|██████▏   | 390/630 [10:33<06:23,  1.60s/it]
 62%|██████▏   | 391/630 [10:34<06:22,  1.60s/it]
                                                 
{'loss': 0.5485, 'grad_norm': 1.530556100589714, 'learning_rate': 7.855973813420622e-07, 'epoch': 1.86}

 62%|██████▏   | 391/630 [10:34<06:22,  1.60s/it]
 62%|██████▏   | 392/630 [10:36<06:22,  1.61s/it]
                                                 
{'loss': 0.4457, 'grad_norm': 1.5713909940835904, 'learning_rate': 7.823240589198036e-07, 'epoch': 1.87}

 62%|██████▏   | 392/630 [10:36<06:22,  1.61s/it]
 62%|██████▏   | 393/630 [10:38<06:20,  1.61s/it]
                                                 
{'loss': 0.5634, 'grad_norm': 1.6398764917459219, 'learning_rate': 7.790507364975449e-07, 'epoch': 1.87}

 62%|██████▏   | 393/630 [10:38<06:20,  1.61s/it]
 63%|██████▎   | 394/630 [10:39<06:18,  1.60s/it]
                                                 
{'loss': 0.4983, 'grad_norm': 1.7535902319832788, 'learning_rate': 7.757774140752864e-07, 'epoch': 1.88}

 63%|██████▎   | 394/630 [10:39<06:18,  1.60s/it]
 63%|██████▎   | 395/630 [10:41<06:16,  1.60s/it]
                                                 
{'loss': 0.4764, 'grad_norm': 1.8701003740603666, 'learning_rate': 7.725040916530278e-07, 'epoch': 1.88}

 63%|██████▎   | 395/630 [10:41<06:16,  1.60s/it]
 63%|██████▎   | 396/630 [10:42<06:14,  1.60s/it]
                                                 
{'loss': 0.4797, 'grad_norm': 1.9125748981578294, 'learning_rate': 7.692307692307693e-07, 'epoch': 1.89}

 63%|██████▎   | 396/630 [10:42<06:14,  1.60s/it]
 63%|██████▎   | 397/630 [10:44<06:17,  1.62s/it]
                                                 
{'loss': 0.4432, 'grad_norm': 1.4488239928090851, 'learning_rate': 7.659574468085106e-07, 'epoch': 1.89}

 63%|██████▎   | 397/630 [10:44<06:17,  1.62s/it]
 63%|██████▎   | 398/630 [10:46<06:18,  1.63s/it]
                                                 
{'loss': 0.4957, 'grad_norm': 1.5418001227318263, 'learning_rate': 7.626841243862519e-07, 'epoch': 1.9}

 63%|██████▎   | 398/630 [10:46<06:18,  1.63s/it]
 63%|██████▎   | 399/630 [10:47<06:14,  1.62s/it]
                                                 
{'loss': 0.5218, 'grad_norm': 1.7352028871074154, 'learning_rate': 7.594108019639935e-07, 'epoch': 1.9}

 63%|██████▎   | 399/630 [10:47<06:14,  1.62s/it]
 63%|██████▎   | 400/630 [10:49<06:30,  1.70s/it]
                                                 
{'loss': 0.437, 'grad_norm': 1.602066715327291, 'learning_rate': 7.561374795417348e-07, 'epoch': 1.9}

 63%|██████▎   | 400/630 [10:49<06:30,  1.70s/it]
 64%|██████▎   | 401/630 [10:51<06:22,  1.67s/it]
                                                 
{'loss': 0.5067, 'grad_norm': 1.5415432336636894, 'learning_rate': 7.528641571194762e-07, 'epoch': 1.91}

 64%|██████▎   | 401/630 [10:51<06:22,  1.67s/it]
 64%|██████▍   | 402/630 [10:52<06:15,  1.65s/it]
                                                 
{'loss': 0.478, 'grad_norm': 1.4434692318729763, 'learning_rate': 7.495908346972176e-07, 'epoch': 1.91}

 64%|██████▍   | 402/630 [10:52<06:15,  1.65s/it]
 64%|██████▍   | 403/630 [10:54<06:10,  1.63s/it]
                                                 
{'loss': 0.4733, 'grad_norm': 1.7620780097250903, 'learning_rate': 7.463175122749591e-07, 'epoch': 1.92}

 64%|██████▍   | 403/630 [10:54<06:10,  1.63s/it]
 64%|██████▍   | 404/630 [10:56<06:07,  1.62s/it]
                                                 
{'loss': 0.4268, 'grad_norm': 1.4862681008039726, 'learning_rate': 7.430441898527005e-07, 'epoch': 1.92}

 64%|██████▍   | 404/630 [10:56<06:07,  1.62s/it]
 64%|██████▍   | 405/630 [10:57<06:03,  1.61s/it]
                                                 
{'loss': 0.4842, 'grad_norm': 1.6963695954614724, 'learning_rate': 7.397708674304418e-07, 'epoch': 1.93}

 64%|██████▍   | 405/630 [10:57<06:03,  1.61s/it]
 64%|██████▍   | 406/630 [10:59<06:00,  1.61s/it]
                                                 
{'loss': 0.4955, 'grad_norm': 1.5729348154489677, 'learning_rate': 7.364975450081832e-07, 'epoch': 1.93}

 64%|██████▍   | 406/630 [10:59<06:00,  1.61s/it]
 65%|██████▍   | 407/630 [11:00<05:58,  1.61s/it]
                                                 
{'loss': 0.4809, 'grad_norm': 1.543048090012802, 'learning_rate': 7.332242225859247e-07, 'epoch': 1.94}

 65%|██████▍   | 407/630 [11:00<05:58,  1.61s/it]
 65%|██████▍   | 408/630 [11:02<05:55,  1.60s/it]
                                                 
{'loss': 0.4827, 'grad_norm': 1.515796844822806, 'learning_rate': 7.299509001636661e-07, 'epoch': 1.94}

 65%|██████▍   | 408/630 [11:02<05:55,  1.60s/it]
 65%|██████▍   | 409/630 [11:04<05:54,  1.60s/it]
                                                 
{'loss': 0.4625, 'grad_norm': 2.4534744263786563, 'learning_rate': 7.266775777414075e-07, 'epoch': 1.95}

 65%|██████▍   | 409/630 [11:04<05:54,  1.60s/it]
 65%|██████▌   | 410/630 [11:05<05:53,  1.61s/it]
                                                 
{'loss': 0.5161, 'grad_norm': 1.7229849908634287, 'learning_rate': 7.23404255319149e-07, 'epoch': 1.95}

 65%|██████▌   | 410/630 [11:05<05:53,  1.61s/it]
 65%|██████▌   | 411/630 [11:07<05:50,  1.60s/it]
                                                 
{'loss': 0.4972, 'grad_norm': 2.035014248491516, 'learning_rate': 7.201309328968903e-07, 'epoch': 1.96}

 65%|██████▌   | 411/630 [11:07<05:50,  1.60s/it]
 65%|██████▌   | 412/630 [11:08<05:47,  1.59s/it]
                                                 
{'loss': 0.4498, 'grad_norm': 1.5173855622858698, 'learning_rate': 7.168576104746317e-07, 'epoch': 1.96}

 65%|██████▌   | 412/630 [11:08<05:47,  1.59s/it]
 66%|██████▌   | 413/630 [11:10<05:45,  1.59s/it]
                                                 
{'loss': 0.5175, 'grad_norm': 1.6031182337293792, 'learning_rate': 7.135842880523731e-07, 'epoch': 1.97}

 66%|██████▌   | 413/630 [11:10<05:45,  1.59s/it]
 66%|██████▌   | 414/630 [11:12<05:44,  1.59s/it]
                                                 
{'loss': 0.475, 'grad_norm': 1.4719462214297918, 'learning_rate': 7.103109656301146e-07, 'epoch': 1.97}

 66%|██████▌   | 414/630 [11:12<05:44,  1.59s/it]
 66%|██████▌   | 415/630 [11:13<05:42,  1.59s/it]
                                                 
{'loss': 0.4604, 'grad_norm': 1.6460200124303168, 'learning_rate': 7.07037643207856e-07, 'epoch': 1.98}

 66%|██████▌   | 415/630 [11:13<05:42,  1.59s/it]
 66%|██████▌   | 416/630 [11:15<05:41,  1.59s/it]
                                                 
{'loss': 0.4651, 'grad_norm': 1.4565093339421153, 'learning_rate': 7.037643207855973e-07, 'epoch': 1.98}

 66%|██████▌   | 416/630 [11:15<05:41,  1.59s/it]
 66%|██████▌   | 417/630 [11:16<05:40,  1.60s/it]
                                                 
{'loss': 0.4928, 'grad_norm': 1.8158749339114995, 'learning_rate': 7.004909983633388e-07, 'epoch': 1.99}

 66%|██████▌   | 417/630 [11:16<05:40,  1.60s/it]
 66%|██████▋   | 418/630 [11:18<05:37,  1.59s/it]
                                                 
{'loss': 0.488, 'grad_norm': 1.5234244889248552, 'learning_rate': 6.972176759410802e-07, 'epoch': 1.99}

 66%|██████▋   | 418/630 [11:18<05:37,  1.59s/it]
 67%|██████▋   | 419/630 [11:20<05:36,  1.59s/it]
                                                 
{'loss': 0.4615, 'grad_norm': 1.971322016470175, 'learning_rate': 6.939443535188215e-07, 'epoch': 2.0}

 67%|██████▋   | 419/630 [11:20<05:36,  1.59s/it]
 67%|██████▋   | 420/630 [11:21<05:33,  1.59s/it]
                                                 
{'loss': 0.3856, 'grad_norm': 1.5421345655581862, 'learning_rate': 6.90671031096563e-07, 'epoch': 2.0}

 67%|██████▋   | 420/630 [11:21<05:33,  1.59s/it]
 67%|██████▋   | 421/630 [11:23<05:32,  1.59s/it]
                                                 
{'loss': 0.405, 'grad_norm': 1.5236899665843728, 'learning_rate': 6.873977086743043e-07, 'epoch': 2.0}

 67%|██████▋   | 421/630 [11:23<05:32,  1.59s/it]
 67%|██████▋   | 422/630 [11:24<05:31,  1.59s/it]
                                                 
{'loss': 0.4066, 'grad_norm': 1.4900296620406357, 'learning_rate': 6.841243862520459e-07, 'epoch': 2.01}

 67%|██████▋   | 422/630 [11:24<05:31,  1.59s/it]
 67%|██████▋   | 423/630 [11:26<05:30,  1.59s/it]
                                                 
{'loss': 0.3436, 'grad_norm': 4.01237268218238, 'learning_rate': 6.808510638297872e-07, 'epoch': 2.01}

 67%|██████▋   | 423/630 [11:26<05:30,  1.59s/it]
 67%|██████▋   | 424/630 [11:28<05:28,  1.59s/it]
                                                 
{'loss': 0.4375, 'grad_norm': 1.7787342880950066, 'learning_rate': 6.775777414075286e-07, 'epoch': 2.02}

 67%|██████▋   | 424/630 [11:28<05:28,  1.59s/it]
 67%|██████▋   | 425/630 [11:29<05:27,  1.60s/it]
                                                 
{'loss': 0.4459, 'grad_norm': 1.8399864569868567, 'learning_rate': 6.7430441898527e-07, 'epoch': 2.02}

 67%|██████▋   | 425/630 [11:29<05:27,  1.60s/it]
 68%|██████▊   | 426/630 [11:31<05:26,  1.60s/it]
                                                 
{'loss': 0.3898, 'grad_norm': 1.3734492407972676, 'learning_rate': 6.710310965630114e-07, 'epoch': 2.03}

 68%|██████▊   | 426/630 [11:31<05:26,  1.60s/it]
 68%|██████▊   | 427/630 [11:32<05:24,  1.60s/it]
                                                 
{'loss': 0.4163, 'grad_norm': 1.7958585619112928, 'learning_rate': 6.677577741407529e-07, 'epoch': 2.03}

 68%|██████▊   | 427/630 [11:32<05:24,  1.60s/it]
 68%|██████▊   | 428/630 [11:34<05:25,  1.61s/it]
                                                 
{'loss': 0.4406, 'grad_norm': 1.4551633327046878, 'learning_rate': 6.644844517184942e-07, 'epoch': 2.04}

 68%|██████▊   | 428/630 [11:34<05:25,  1.61s/it]
 68%|██████▊   | 429/630 [11:36<05:23,  1.61s/it]
                                                 
{'loss': 0.4182, 'grad_norm': 1.565375810962764, 'learning_rate': 6.612111292962356e-07, 'epoch': 2.04}

 68%|██████▊   | 429/630 [11:36<05:23,  1.61s/it]
 68%|██████▊   | 430/630 [11:37<05:21,  1.61s/it]
                                                 
{'loss': 0.4013, 'grad_norm': 1.6547187815152595, 'learning_rate': 6.579378068739771e-07, 'epoch': 2.05}

 68%|██████▊   | 430/630 [11:37<05:21,  1.61s/it]
 68%|██████▊   | 431/630 [11:39<05:19,  1.60s/it]
                                                 
{'loss': 0.4729, 'grad_norm': 1.636040740999944, 'learning_rate': 6.546644844517185e-07, 'epoch': 2.05}

 68%|██████▊   | 431/630 [11:39<05:19,  1.60s/it]
 69%|██████▊   | 432/630 [11:40<05:17,  1.60s/it]
                                                 
{'loss': 0.4158, 'grad_norm': 1.6378654337469067, 'learning_rate': 6.513911620294599e-07, 'epoch': 2.06}

 69%|██████▊   | 432/630 [11:40<05:17,  1.60s/it]
 69%|██████▊   | 433/630 [11:42<05:15,  1.60s/it]
                                                 
{'loss': 0.3574, 'grad_norm': 1.7548738315484154, 'learning_rate': 6.481178396072012e-07, 'epoch': 2.06}

 69%|██████▊   | 433/630 [11:42<05:15,  1.60s/it]
 69%|██████▉   | 434/630 [11:44<05:12,  1.60s/it]
                                                 
{'loss': 0.3904, 'grad_norm': 1.776446385054493, 'learning_rate': 6.448445171849427e-07, 'epoch': 2.07}

 69%|██████▉   | 434/630 [11:44<05:12,  1.60s/it]
 69%|██████▉   | 435/630 [11:45<05:12,  1.60s/it]
                                                 
{'loss': 0.4254, 'grad_norm': 1.9118487649921612, 'learning_rate': 6.415711947626841e-07, 'epoch': 2.07}

 69%|██████▉   | 435/630 [11:45<05:12,  1.60s/it]
 69%|██████▉   | 436/630 [11:47<05:10,  1.60s/it]
                                                 
{'loss': 0.4086, 'grad_norm': 1.6533575725711909, 'learning_rate': 6.382978723404255e-07, 'epoch': 2.08}

 69%|██████▉   | 436/630 [11:47<05:10,  1.60s/it]
 69%|██████▉   | 437/630 [11:48<05:10,  1.61s/it]
                                                 
{'loss': 0.5318, 'grad_norm': 2.5801290421972767, 'learning_rate': 6.35024549918167e-07, 'epoch': 2.08}

 69%|██████▉   | 437/630 [11:48<05:10,  1.61s/it]
 70%|██████▉   | 438/630 [11:50<05:08,  1.61s/it]
                                                 
{'loss': 0.3606, 'grad_norm': 1.487840519259555, 'learning_rate': 6.317512274959084e-07, 'epoch': 2.09}

 70%|██████▉   | 438/630 [11:50<05:08,  1.61s/it]
 70%|██████▉   | 439/630 [11:52<05:06,  1.61s/it]
                                                 
{'loss': 0.3827, 'grad_norm': 2.050228553553068, 'learning_rate': 6.284779050736497e-07, 'epoch': 2.09}

 70%|██████▉   | 439/630 [11:52<05:06,  1.61s/it]
 70%|██████▉   | 440/630 [11:53<05:04,  1.60s/it]
                                                 
{'loss': 0.4253, 'grad_norm': 1.4939596532360517, 'learning_rate': 6.252045826513911e-07, 'epoch': 2.1}

 70%|██████▉   | 440/630 [11:53<05:04,  1.60s/it]
 70%|███████   | 441/630 [11:55<05:02,  1.60s/it]
                                                 
{'loss': 0.4242, 'grad_norm': 1.656054407186078, 'learning_rate': 6.219312602291326e-07, 'epoch': 2.1}

 70%|███████   | 441/630 [11:55<05:02,  1.60s/it]
 70%|███████   | 442/630 [11:56<05:00,  1.60s/it]
                                                 
{'loss': 0.4438, 'grad_norm': 1.744636215926032, 'learning_rate': 6.186579378068739e-07, 'epoch': 2.1}

 70%|███████   | 442/630 [11:56<05:00,  1.60s/it]
 70%|███████   | 443/630 [11:58<04:58,  1.60s/it]
                                                 
{'loss': 0.4785, 'grad_norm': 1.752821959087372, 'learning_rate': 6.153846153846154e-07, 'epoch': 2.11}

 70%|███████   | 443/630 [11:58<04:58,  1.60s/it]
 70%|███████   | 444/630 [12:00<04:57,  1.60s/it]
                                                 
{'loss': 0.4133, 'grad_norm': 1.681083917552431, 'learning_rate': 6.121112929623567e-07, 'epoch': 2.11}

 70%|███████   | 444/630 [12:00<04:57,  1.60s/it]
 71%|███████   | 445/630 [12:01<04:56,  1.60s/it]
                                                 
{'loss': 0.399, 'grad_norm': 1.9686200640495348, 'learning_rate': 6.088379705400983e-07, 'epoch': 2.12}

 71%|███████   | 445/630 [12:01<04:56,  1.60s/it]
 71%|███████   | 446/630 [12:03<04:54,  1.60s/it]
                                                 
{'loss': 0.4743, 'grad_norm': 1.661598272552161, 'learning_rate': 6.055646481178396e-07, 'epoch': 2.12}

 71%|███████   | 446/630 [12:03<04:54,  1.60s/it]
 71%|███████   | 447/630 [12:04<04:52,  1.60s/it]
                                                 
{'loss': 0.4917, 'grad_norm': 1.8668036723940815, 'learning_rate': 6.022913256955809e-07, 'epoch': 2.13}

 71%|███████   | 447/630 [12:04<04:52,  1.60s/it]
 71%|███████   | 448/630 [12:06<04:50,  1.60s/it]
                                                 
{'loss': 0.4131, 'grad_norm': 1.6619417274873631, 'learning_rate': 5.990180032733224e-07, 'epoch': 2.13}

 71%|███████   | 448/630 [12:06<04:50,  1.60s/it]
 71%|███████▏  | 449/630 [12:08<04:48,  1.59s/it]
                                                 
{'loss': 0.3769, 'grad_norm': 1.6690526545017448, 'learning_rate': 5.957446808510638e-07, 'epoch': 2.14}

 71%|███████▏  | 449/630 [12:08<04:48,  1.59s/it]
 71%|███████▏  | 450/630 [12:09<04:47,  1.60s/it]
                                                 
{'loss': 0.4376, 'grad_norm': 1.648141445111369, 'learning_rate': 5.924713584288053e-07, 'epoch': 2.14}

 71%|███████▏  | 450/630 [12:09<04:47,  1.60s/it]
 72%|███████▏  | 451/630 [12:11<04:45,  1.60s/it]
                                                 
{'loss': 0.4228, 'grad_norm': 1.8175834890919493, 'learning_rate': 5.891980360065466e-07, 'epoch': 2.15}

 72%|███████▏  | 451/630 [12:11<04:45,  1.60s/it]
 72%|███████▏  | 452/630 [12:12<04:44,  1.60s/it]
                                                 
{'loss': 0.4173, 'grad_norm': 1.6638694433301857, 'learning_rate': 5.85924713584288e-07, 'epoch': 2.15}

 72%|███████▏  | 452/630 [12:12<04:44,  1.60s/it]
 72%|███████▏  | 453/630 [12:14<04:42,  1.60s/it]
                                                 
{'loss': 0.391, 'grad_norm': 1.5420166004090285, 'learning_rate': 5.826513911620295e-07, 'epoch': 2.16}

 72%|███████▏  | 453/630 [12:14<04:42,  1.60s/it]
 72%|███████▏  | 454/630 [12:16<04:40,  1.59s/it]
                                                 
{'loss': 0.4259, 'grad_norm': 1.6646248961273438, 'learning_rate': 5.793780687397708e-07, 'epoch': 2.16}

 72%|███████▏  | 454/630 [12:16<04:40,  1.59s/it]
 72%|███████▏  | 455/630 [12:17<04:38,  1.59s/it]
                                                 
{'loss': 0.4417, 'grad_norm': 1.6394565252249107, 'learning_rate': 5.761047463175122e-07, 'epoch': 2.17}

 72%|███████▏  | 455/630 [12:17<04:38,  1.59s/it]
 72%|███████▏  | 456/630 [12:19<04:37,  1.59s/it]
                                                 
{'loss': 0.4155, 'grad_norm': 1.5641387049142659, 'learning_rate': 5.728314238952536e-07, 'epoch': 2.17}

 72%|███████▏  | 456/630 [12:19<04:37,  1.59s/it]
 73%|███████▎  | 457/630 [12:20<04:35,  1.59s/it]
                                                 
{'loss': 0.408, 'grad_norm': 1.8144320063677717, 'learning_rate': 5.695581014729951e-07, 'epoch': 2.18}

 73%|███████▎  | 457/630 [12:20<04:35,  1.59s/it]
 73%|███████▎  | 458/630 [12:22<04:34,  1.59s/it]
                                                 
{'loss': 0.3982, 'grad_norm': 1.4830932465088102, 'learning_rate': 5.662847790507365e-07, 'epoch': 2.18}

 73%|███████▎  | 458/630 [12:22<04:34,  1.59s/it]
 73%|███████▎  | 459/630 [12:24<04:33,  1.60s/it]
                                                 
{'loss': 0.3711, 'grad_norm': 1.8418245793721468, 'learning_rate': 5.630114566284779e-07, 'epoch': 2.19}

 73%|███████▎  | 459/630 [12:24<04:33,  1.60s/it]
 73%|███████▎  | 460/630 [12:25<04:32,  1.60s/it]
                                                 
{'loss': 0.4757, 'grad_norm': 1.9570030426963, 'learning_rate': 5.597381342062193e-07, 'epoch': 2.19}

 73%|███████▎  | 460/630 [12:25<04:32,  1.60s/it]
 73%|███████▎  | 461/630 [12:27<04:30,  1.60s/it]
                                                 
{'loss': 0.3565, 'grad_norm': 1.3473528468990494, 'learning_rate': 5.564648117839607e-07, 'epoch': 2.2}

 73%|███████▎  | 461/630 [12:27<04:30,  1.60s/it]
 73%|███████▎  | 462/630 [12:28<04:28,  1.60s/it]
                                                 
{'loss': 0.5097, 'grad_norm': 1.7353321261606953, 'learning_rate': 5.531914893617021e-07, 'epoch': 2.2}

 73%|███████▎  | 462/630 [12:28<04:28,  1.60s/it]
 73%|███████▎  | 463/630 [12:30<04:27,  1.60s/it]
                                                 
{'loss': 0.4101, 'grad_norm': 1.59854502462037, 'learning_rate': 5.499181669394435e-07, 'epoch': 2.2}

 73%|███████▎  | 463/630 [12:30<04:27,  1.60s/it]
 74%|███████▎  | 464/630 [12:32<04:25,  1.60s/it]
                                                 
{'loss': 0.4178, 'grad_norm': 1.5135799896458855, 'learning_rate': 5.46644844517185e-07, 'epoch': 2.21}

 74%|███████▎  | 464/630 [12:32<04:25,  1.60s/it]
 74%|███████▍  | 465/630 [12:33<04:24,  1.60s/it]
                                                 
{'loss': 0.4336, 'grad_norm': 1.7082102943536062, 'learning_rate': 5.433715220949263e-07, 'epoch': 2.21}

 74%|███████▍  | 465/630 [12:33<04:24,  1.60s/it]
 74%|███████▍  | 466/630 [12:35<04:23,  1.61s/it]
                                                 
{'loss': 0.3982, 'grad_norm': 1.5306959025682072, 'learning_rate': 5.400981996726678e-07, 'epoch': 2.22}

 74%|███████▍  | 466/630 [12:35<04:23,  1.61s/it]
 74%|███████▍  | 467/630 [12:36<04:23,  1.62s/it]
                                                 
{'loss': 0.4136, 'grad_norm': 1.656319460861758, 'learning_rate': 5.368248772504091e-07, 'epoch': 2.22}

 74%|███████▍  | 467/630 [12:36<04:23,  1.62s/it]
 74%|███████▍  | 468/630 [12:38<04:21,  1.62s/it]
                                                 
{'loss': 0.4017, 'grad_norm': 1.3956245548638653, 'learning_rate': 5.335515548281506e-07, 'epoch': 2.23}

 74%|███████▍  | 468/630 [12:38<04:21,  1.62s/it]
 74%|███████▍  | 469/630 [12:40<04:19,  1.61s/it]
                                                 
{'loss': 0.3672, 'grad_norm': 1.7284711228518046, 'learning_rate': 5.30278232405892e-07, 'epoch': 2.23}

 74%|███████▍  | 469/630 [12:40<04:19,  1.61s/it]
 75%|███████▍  | 470/630 [12:41<04:17,  1.61s/it]
                                                 
{'loss': 0.3886, 'grad_norm': 1.3823060176054913, 'learning_rate': 5.270049099836333e-07, 'epoch': 2.24}

 75%|███████▍  | 470/630 [12:41<04:17,  1.61s/it]
 75%|███████▍  | 471/630 [12:43<04:15,  1.61s/it]
                                                 
{'loss': 0.4515, 'grad_norm': 2.081452326923269, 'learning_rate': 5.237315875613748e-07, 'epoch': 2.24}

 75%|███████▍  | 471/630 [12:43<04:15,  1.61s/it]
 75%|███████▍  | 472/630 [12:44<04:15,  1.62s/it]
                                                 
{'loss': 0.4017, 'grad_norm': 1.679488685442705, 'learning_rate': 5.204582651391162e-07, 'epoch': 2.25}

 75%|███████▍  | 472/630 [12:44<04:15,  1.62s/it]
 75%|███████▌  | 473/630 [12:46<04:13,  1.61s/it]
                                                 
{'loss': 0.4038, 'grad_norm': 1.6002904574385417, 'learning_rate': 5.171849427168577e-07, 'epoch': 2.25}

 75%|███████▌  | 473/630 [12:46<04:13,  1.61s/it]
 75%|███████▌  | 474/630 [12:48<04:11,  1.61s/it]
                                                 
{'loss': 0.4108, 'grad_norm': 1.8076575337908503, 'learning_rate': 5.13911620294599e-07, 'epoch': 2.26}

 75%|███████▌  | 474/630 [12:48<04:11,  1.61s/it]
 75%|███████▌  | 475/630 [12:49<04:09,  1.61s/it]
                                                 
{'loss': 0.4439, 'grad_norm': 1.7935067210726265, 'learning_rate': 5.106382978723403e-07, 'epoch': 2.26}

 75%|███████▌  | 475/630 [12:49<04:09,  1.61s/it]
 76%|███████▌  | 476/630 [12:51<04:08,  1.61s/it]
                                                 
{'loss': 0.3513, 'grad_norm': 1.3376703499288984, 'learning_rate': 5.073649754500819e-07, 'epoch': 2.27}

 76%|███████▌  | 476/630 [12:51<04:08,  1.61s/it]
 76%|███████▌  | 477/630 [12:53<04:06,  1.61s/it]
                                                 
{'loss': 0.3833, 'grad_norm': 1.482675412555397, 'learning_rate': 5.040916530278232e-07, 'epoch': 2.27}

 76%|███████▌  | 477/630 [12:53<04:06,  1.61s/it]
 76%|███████▌  | 478/630 [12:54<04:03,  1.61s/it]
                                                 
{'loss': 0.3791, 'grad_norm': 1.7360316406148988, 'learning_rate': 5.008183306055646e-07, 'epoch': 2.28}

 76%|███████▌  | 478/630 [12:54<04:03,  1.61s/it]
 76%|███████▌  | 479/630 [12:56<04:02,  1.61s/it]
                                                 
{'loss': 0.4033, 'grad_norm': 1.5343900745643126, 'learning_rate': 4.97545008183306e-07, 'epoch': 2.28}

 76%|███████▌  | 479/630 [12:56<04:02,  1.61s/it]
 76%|███████▌  | 480/630 [12:57<04:00,  1.61s/it]
                                                 
{'loss': 0.4561, 'grad_norm': 1.6132562804593829, 'learning_rate': 4.942716857610474e-07, 'epoch': 2.29}

 76%|███████▌  | 480/630 [12:57<04:00,  1.61s/it]
 76%|███████▋  | 481/630 [12:59<03:58,  1.60s/it]
                                                 
{'loss': 0.4313, 'grad_norm': 1.4483461084158697, 'learning_rate': 4.909983633387889e-07, 'epoch': 2.29}

 76%|███████▋  | 481/630 [12:59<03:58,  1.60s/it]
 77%|███████▋  | 482/630 [13:01<03:57,  1.60s/it]
                                                 
{'loss': 0.4307, 'grad_norm': 1.7948327400268806, 'learning_rate': 4.877250409165303e-07, 'epoch': 2.3}

 77%|███████▋  | 482/630 [13:01<03:57,  1.60s/it]
 77%|███████▋  | 483/630 [13:02<03:55,  1.60s/it]
                                                 
{'loss': 0.3733, 'grad_norm': 2.0940752334766137, 'learning_rate': 4.844517184942716e-07, 'epoch': 2.3}

 77%|███████▋  | 483/630 [13:02<03:55,  1.60s/it]
 77%|███████▋  | 484/630 [13:04<03:54,  1.60s/it]
                                                 
{'loss': 0.3753, 'grad_norm': 1.6456496663786992, 'learning_rate': 4.811783960720131e-07, 'epoch': 2.3}

 77%|███████▋  | 484/630 [13:04<03:54,  1.60s/it]
 77%|███████▋  | 485/630 [13:05<03:53,  1.61s/it]
                                                 
{'loss': 0.3626, 'grad_norm': 1.7772335837656774, 'learning_rate': 4.779050736497545e-07, 'epoch': 2.31}

 77%|███████▋  | 485/630 [13:05<03:53,  1.61s/it]
 77%|███████▋  | 486/630 [13:07<03:50,  1.60s/it]
                                                 
{'loss': 0.415, 'grad_norm': 2.001990846865512, 'learning_rate': 4.746317512274959e-07, 'epoch': 2.31}

 77%|███████▋  | 486/630 [13:07<03:50,  1.60s/it]
 77%|███████▋  | 487/630 [13:09<03:49,  1.60s/it]
                                                 
{'loss': 0.3657, 'grad_norm': 1.7394113299709373, 'learning_rate': 4.713584288052373e-07, 'epoch': 2.32}

 77%|███████▋  | 487/630 [13:09<03:49,  1.60s/it]
 77%|███████▋  | 488/630 [13:10<03:47,  1.60s/it]
                                                 
{'loss': 0.3819, 'grad_norm': 1.556374750453296, 'learning_rate': 4.6808510638297873e-07, 'epoch': 2.32}

 77%|███████▋  | 488/630 [13:10<03:47,  1.60s/it]
 78%|███████▊  | 489/630 [13:12<03:45,  1.60s/it]
                                                 
{'loss': 0.3615, 'grad_norm': 1.876441399052683, 'learning_rate': 4.648117839607201e-07, 'epoch': 2.33}

 78%|███████▊  | 489/630 [13:12<03:45,  1.60s/it]
 78%|███████▊  | 490/630 [13:13<03:45,  1.61s/it]
                                                 
{'loss': 0.4987, 'grad_norm': 1.5727084449520932, 'learning_rate': 4.6153846153846156e-07, 'epoch': 2.33}

 78%|███████▊  | 490/630 [13:13<03:45,  1.61s/it]
 78%|███████▊  | 491/630 [13:15<03:43,  1.61s/it]
                                                 
{'loss': 0.4094, 'grad_norm': 1.7910608856677033, 'learning_rate': 4.582651391162029e-07, 'epoch': 2.34}

 78%|███████▊  | 491/630 [13:15<03:43,  1.61s/it]
 78%|███████▊  | 492/630 [13:17<03:42,  1.61s/it]
                                                 
{'loss': 0.4247, 'grad_norm': 1.7651742667521693, 'learning_rate': 4.5499181669394434e-07, 'epoch': 2.34}

 78%|███████▊  | 492/630 [13:17<03:42,  1.61s/it]
 78%|███████▊  | 493/630 [13:18<03:40,  1.61s/it]
                                                 
{'loss': 0.4083, 'grad_norm': 1.6698178817744802, 'learning_rate': 4.517184942716857e-07, 'epoch': 2.35}

 78%|███████▊  | 493/630 [13:18<03:40,  1.61s/it]
 78%|███████▊  | 494/630 [13:20<03:38,  1.61s/it]
                                                 
{'loss': 0.4808, 'grad_norm': 1.8829854773777082, 'learning_rate': 4.4844517184942717e-07, 'epoch': 2.35}

 78%|███████▊  | 494/630 [13:20<03:38,  1.61s/it]
 79%|███████▊  | 495/630 [13:21<03:37,  1.61s/it]
                                                 
{'loss': 0.3509, 'grad_norm': 1.6614481125336507, 'learning_rate': 4.4517184942716855e-07, 'epoch': 2.36}

 79%|███████▊  | 495/630 [13:21<03:37,  1.61s/it]
 79%|███████▊  | 496/630 [13:23<03:35,  1.61s/it]
                                                 
{'loss': 0.3843, 'grad_norm': 1.5653643055989799, 'learning_rate': 4.4189852700491e-07, 'epoch': 2.36}

 79%|███████▊  | 496/630 [13:23<03:35,  1.61s/it]
 79%|███████▉  | 497/630 [13:25<03:34,  1.61s/it]
                                                 
{'loss': 0.4171, 'grad_norm': 1.4373550968879756, 'learning_rate': 4.386252045826514e-07, 'epoch': 2.37}

 79%|███████▉  | 497/630 [13:25<03:34,  1.61s/it]
 79%|███████▉  | 498/630 [13:26<03:32,  1.61s/it]
                                                 
{'loss': 0.4087, 'grad_norm': 1.4697840379233653, 'learning_rate': 4.3535188216039277e-07, 'epoch': 2.37}

 79%|███████▉  | 498/630 [13:26<03:32,  1.61s/it]
 79%|███████▉  | 499/630 [13:28<03:30,  1.60s/it]
                                                 
{'loss': 0.3788, 'grad_norm': 1.968483359490901, 'learning_rate': 4.3207855973813416e-07, 'epoch': 2.38}

 79%|███████▉  | 499/630 [13:28<03:30,  1.60s/it]
 79%|███████▉  | 500/630 [13:29<03:28,  1.60s/it]
                                                 
{'loss': 0.4436, 'grad_norm': 1.7925554398189707, 'learning_rate': 4.288052373158756e-07, 'epoch': 2.38}

 79%|███████▉  | 500/630 [13:29<03:28,  1.60s/it]/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 80%|███████▉  | 501/630 [15:21<1:14:22, 34.59s/it]
                                                   
{'loss': 0.4089, 'grad_norm': 1.4368201296083587, 'learning_rate': 4.25531914893617e-07, 'epoch': 2.39}

 80%|███████▉  | 501/630 [15:21<1:14:22, 34.59s/it]
 80%|███████▉  | 502/630 [15:23<52:39, 24.69s/it]  
                                                 
{'loss': 0.428, 'grad_norm': 1.7683469549852402, 'learning_rate': 4.2225859247135843e-07, 'epoch': 2.39}

 80%|███████▉  | 502/630 [15:23<52:39, 24.69s/it]
 80%|███████▉  | 503/630 [15:24<37:34, 17.76s/it]
                                                 
{'loss': 0.3358, 'grad_norm': 1.5452401501505362, 'learning_rate': 4.189852700490998e-07, 'epoch': 2.4}

 80%|███████▉  | 503/630 [15:24<37:34, 17.76s/it]
 80%|████████  | 504/630 [15:26<27:06, 12.91s/it]
                                                 
{'loss': 0.4249, 'grad_norm': 1.588618346628157, 'learning_rate': 4.157119476268412e-07, 'epoch': 2.4}

 80%|████████  | 504/630 [15:26<27:06, 12.91s/it]
 80%|████████  | 505/630 [15:27<19:48,  9.51s/it]
                                                 
{'loss': 0.393, 'grad_norm': 1.6674876632026443, 'learning_rate': 4.124386252045826e-07, 'epoch': 2.4}

 80%|████████  | 505/630 [15:27<19:48,  9.51s/it]
 80%|████████  | 506/630 [15:29<14:44,  7.13s/it]
                                                 
{'loss': 0.4098, 'grad_norm': 1.9924117184964347, 'learning_rate': 4.0916530278232403e-07, 'epoch': 2.41}

 80%|████████  | 506/630 [15:29<14:44,  7.13s/it]
 80%|████████  | 507/630 [15:31<11:12,  5.47s/it]
                                                 
{'loss': 0.4235, 'grad_norm': 1.6577025451213279, 'learning_rate': 4.058919803600654e-07, 'epoch': 2.41}

 80%|████████  | 507/630 [15:31<11:12,  5.47s/it]
 81%|████████  | 508/630 [15:32<08:45,  4.31s/it]
                                                 
{'loss': 0.4318, 'grad_norm': 1.6676901522020828, 'learning_rate': 4.0261865793780686e-07, 'epoch': 2.42}

 81%|████████  | 508/630 [15:32<08:45,  4.31s/it]
 81%|████████  | 509/630 [15:34<07:03,  3.50s/it]
                                                 
{'loss': 0.3657, 'grad_norm': 1.4326859292902812, 'learning_rate': 3.993453355155483e-07, 'epoch': 2.42}

 81%|████████  | 509/630 [15:34<07:03,  3.50s/it]
 81%|████████  | 510/630 [15:35<05:51,  2.93s/it]
                                                 
{'loss': 0.3837, 'grad_norm': 1.6219517841124185, 'learning_rate': 3.960720130932897e-07, 'epoch': 2.43}

 81%|████████  | 510/630 [15:35<05:51,  2.93s/it]
 81%|████████  | 511/630 [15:37<05:00,  2.52s/it]
                                                 
{'loss': 0.4496, 'grad_norm': 1.8668445113305943, 'learning_rate': 3.927986906710311e-07, 'epoch': 2.43}

 81%|████████  | 511/630 [15:37<05:00,  2.52s/it]
 81%|████████▏ | 512/630 [15:39<04:24,  2.24s/it]
                                                 
{'loss': 0.3773, 'grad_norm': 1.5016913382932573, 'learning_rate': 3.8952536824877247e-07, 'epoch': 2.44}

 81%|████████▏ | 512/630 [15:39<04:24,  2.24s/it]
 81%|████████▏ | 513/630 [15:40<03:59,  2.05s/it]
                                                 
{'loss': 0.3814, 'grad_norm': 1.527893299984377, 'learning_rate': 3.862520458265139e-07, 'epoch': 2.44}

 81%|████████▏ | 513/630 [15:40<03:59,  2.05s/it]
 82%|████████▏ | 514/630 [15:42<03:41,  1.91s/it]
                                                 
{'loss': 0.3873, 'grad_norm': 1.5438201941691856, 'learning_rate': 3.829787234042553e-07, 'epoch': 2.45}

 82%|████████▏ | 514/630 [15:42<03:41,  1.91s/it]
 82%|████████▏ | 515/630 [15:43<03:28,  1.82s/it]
                                                 
{'loss': 0.3725, 'grad_norm': 1.7440955818444037, 'learning_rate': 3.7970540098199673e-07, 'epoch': 2.45}

 82%|████████▏ | 515/630 [15:43<03:28,  1.82s/it]
 82%|████████▏ | 516/630 [15:45<03:23,  1.79s/it]
                                                 
{'loss': 0.5152, 'grad_norm': 1.8807074993951822, 'learning_rate': 3.764320785597381e-07, 'epoch': 2.46}

 82%|████████▏ | 516/630 [15:45<03:23,  1.79s/it]
 82%|████████▏ | 517/630 [15:47<03:18,  1.76s/it]
                                                 
{'loss': 0.4712, 'grad_norm': 1.6842926176006283, 'learning_rate': 3.7315875613747956e-07, 'epoch': 2.46}

 82%|████████▏ | 517/630 [15:47<03:18,  1.76s/it]
 82%|████████▏ | 518/630 [15:48<03:13,  1.73s/it]
                                                 
{'loss': 0.4201, 'grad_norm': 1.6356561109924528, 'learning_rate': 3.698854337152209e-07, 'epoch': 2.47}

 82%|████████▏ | 518/630 [15:48<03:13,  1.73s/it]
 82%|████████▏ | 519/630 [15:50<03:07,  1.69s/it]
                                                 
{'loss': 0.3406, 'grad_norm': 2.1124874084196943, 'learning_rate': 3.6661211129296234e-07, 'epoch': 2.47}

 82%|████████▏ | 519/630 [15:50<03:07,  1.69s/it]
 83%|████████▎ | 520/630 [15:52<03:01,  1.65s/it]
                                                 
{'loss': 0.3641, 'grad_norm': 1.6122408224798972, 'learning_rate': 3.6333878887070373e-07, 'epoch': 2.48}

 83%|████████▎ | 520/630 [15:52<03:01,  1.65s/it]
 83%|████████▎ | 521/630 [15:53<02:58,  1.63s/it]
                                                 
{'loss': 0.3996, 'grad_norm': 2.151334320313381, 'learning_rate': 3.6006546644844517e-07, 'epoch': 2.48}

 83%|████████▎ | 521/630 [15:53<02:58,  1.63s/it]
 83%|████████▎ | 522/630 [15:55<02:55,  1.62s/it]
                                                 
{'loss': 0.3715, 'grad_norm': 1.5646397825312248, 'learning_rate': 3.5679214402618656e-07, 'epoch': 2.49}

 83%|████████▎ | 522/630 [15:55<02:55,  1.62s/it]
 83%|████████▎ | 523/630 [15:56<02:52,  1.62s/it]
                                                 
{'loss': 0.3973, 'grad_norm': 1.7050836377828504, 'learning_rate': 3.53518821603928e-07, 'epoch': 2.49}

 83%|████████▎ | 523/630 [15:56<02:52,  1.62s/it]
 83%|████████▎ | 524/630 [15:58<02:50,  1.61s/it]
                                                 
{'loss': 0.4072, 'grad_norm': 1.9318183424680075, 'learning_rate': 3.502454991816694e-07, 'epoch': 2.5}

 83%|████████▎ | 524/630 [15:58<02:50,  1.61s/it]
 83%|████████▎ | 525/630 [15:59<02:48,  1.61s/it]
                                                 
{'loss': 0.4019, 'grad_norm': 1.5520820951568048, 'learning_rate': 3.4697217675941077e-07, 'epoch': 2.5}

 83%|████████▎ | 525/630 [15:59<02:48,  1.61s/it]
 83%|████████▎ | 526/630 [16:01<02:46,  1.60s/it]
                                                 
{'loss': 0.4189, 'grad_norm': 2.0112770786841927, 'learning_rate': 3.4369885433715216e-07, 'epoch': 2.5}

 83%|████████▎ | 526/630 [16:01<02:46,  1.60s/it]
 84%|████████▎ | 527/630 [16:03<02:44,  1.60s/it]
                                                 
{'loss': 0.4394, 'grad_norm': 1.863843138206813, 'learning_rate': 3.404255319148936e-07, 'epoch': 2.51}

 84%|████████▎ | 527/630 [16:03<02:44,  1.60s/it]
 84%|████████▍ | 528/630 [16:04<02:43,  1.60s/it]
                                                 
{'loss': 0.3897, 'grad_norm': 1.5010865983502542, 'learning_rate': 3.37152209492635e-07, 'epoch': 2.51}

 84%|████████▍ | 528/630 [16:04<02:43,  1.60s/it]
 84%|████████▍ | 529/630 [16:06<02:41,  1.59s/it]
                                                 
{'loss': 0.4314, 'grad_norm': 1.9776832764132217, 'learning_rate': 3.3387888707037643e-07, 'epoch': 2.52}

 84%|████████▍ | 529/630 [16:06<02:41,  1.59s/it]
 84%|████████▍ | 530/630 [16:07<02:39,  1.60s/it]
                                                 
{'loss': 0.3817, 'grad_norm': 1.6564276668630598, 'learning_rate': 3.306055646481178e-07, 'epoch': 2.52}

 84%|████████▍ | 530/630 [16:07<02:39,  1.60s/it]
 84%|████████▍ | 531/630 [16:09<02:38,  1.60s/it]
                                                 
{'loss': 0.4449, 'grad_norm': 2.4826222989228603, 'learning_rate': 3.2733224222585926e-07, 'epoch': 2.53}

 84%|████████▍ | 531/630 [16:09<02:38,  1.60s/it]
 84%|████████▍ | 532/630 [16:11<02:36,  1.60s/it]
                                                 
{'loss': 0.3195, 'grad_norm': 1.6203235947476842, 'learning_rate': 3.240589198036006e-07, 'epoch': 2.53}

 84%|████████▍ | 532/630 [16:11<02:36,  1.60s/it]
 85%|████████▍ | 533/630 [16:12<02:34,  1.60s/it]
                                                 
{'loss': 0.4349, 'grad_norm': 1.7365390311509836, 'learning_rate': 3.2078559738134203e-07, 'epoch': 2.54}

 85%|████████▍ | 533/630 [16:12<02:34,  1.60s/it]
 85%|████████▍ | 534/630 [16:14<02:33,  1.60s/it]
                                                 
{'loss': 0.3983, 'grad_norm': 1.700761106322992, 'learning_rate': 3.175122749590835e-07, 'epoch': 2.54}

 85%|████████▍ | 534/630 [16:14<02:33,  1.60s/it]
 85%|████████▍ | 535/630 [16:15<02:31,  1.60s/it]
                                                 
{'loss': 0.3558, 'grad_norm': 1.7803914316819667, 'learning_rate': 3.1423895253682486e-07, 'epoch': 2.55}

 85%|████████▍ | 535/630 [16:15<02:31,  1.60s/it]
 85%|████████▌ | 536/630 [16:17<02:30,  1.60s/it]
                                                 
{'loss': 0.458, 'grad_norm': 2.288014403809609, 'learning_rate': 3.109656301145663e-07, 'epoch': 2.55}

 85%|████████▌ | 536/630 [16:17<02:30,  1.60s/it]
 85%|████████▌ | 537/630 [16:19<02:28,  1.60s/it]
                                                 
{'loss': 0.3694, 'grad_norm': 1.7030856878562726, 'learning_rate': 3.076923076923077e-07, 'epoch': 2.56}

 85%|████████▌ | 537/630 [16:19<02:28,  1.60s/it]
 85%|████████▌ | 538/630 [16:20<02:27,  1.60s/it]
                                                 
{'loss': 0.3666, 'grad_norm': 1.7062393584011024, 'learning_rate': 3.0441898527004913e-07, 'epoch': 2.56}

 85%|████████▌ | 538/630 [16:20<02:27,  1.60s/it]
 86%|████████▌ | 539/630 [16:22<02:25,  1.60s/it]
                                                 
{'loss': 0.3777, 'grad_norm': 1.4403841852399342, 'learning_rate': 3.0114566284779047e-07, 'epoch': 2.57}

 86%|████████▌ | 539/630 [16:22<02:25,  1.60s/it]
 86%|████████▌ | 540/630 [16:23<02:23,  1.60s/it]
                                                 
{'loss': 0.4036, 'grad_norm': 1.8941225140750673, 'learning_rate': 2.978723404255319e-07, 'epoch': 2.57}

 86%|████████▌ | 540/630 [16:23<02:23,  1.60s/it]
 86%|████████▌ | 541/630 [16:25<02:23,  1.61s/it]
                                                 
{'loss': 0.4131, 'grad_norm': 1.66850836035733, 'learning_rate': 2.945990180032733e-07, 'epoch': 2.58}

 86%|████████▌ | 541/630 [16:25<02:23,  1.61s/it]
 86%|████████▌ | 542/630 [16:27<02:21,  1.61s/it]
                                                 
{'loss': 0.4267, 'grad_norm': 1.588399073514148, 'learning_rate': 2.9132569558101474e-07, 'epoch': 2.58}

 86%|████████▌ | 542/630 [16:27<02:21,  1.61s/it]
 86%|████████▌ | 543/630 [16:28<02:20,  1.62s/it]
                                                 
{'loss': 0.4524, 'grad_norm': 1.645644578385818, 'learning_rate': 2.880523731587561e-07, 'epoch': 2.59}

 86%|████████▌ | 543/630 [16:28<02:20,  1.62s/it]
 86%|████████▋ | 544/630 [16:30<02:19,  1.62s/it]
                                                 
{'loss': 0.4096, 'grad_norm': 1.8215552916072222, 'learning_rate': 2.8477905073649756e-07, 'epoch': 2.59}

 86%|████████▋ | 544/630 [16:30<02:19,  1.62s/it]
 87%|████████▋ | 545/630 [16:32<02:16,  1.61s/it]
                                                 
{'loss': 0.3158, 'grad_norm': 1.7729240577699394, 'learning_rate': 2.8150572831423895e-07, 'epoch': 2.6}

 87%|████████▋ | 545/630 [16:32<02:16,  1.61s/it]
 87%|████████▋ | 546/630 [16:33<02:15,  1.62s/it]
                                                 
{'loss': 0.3873, 'grad_norm': 1.4407064922248298, 'learning_rate': 2.7823240589198034e-07, 'epoch': 2.6}

 87%|████████▋ | 546/630 [16:33<02:15,  1.62s/it]
 87%|████████▋ | 547/630 [16:35<02:15,  1.63s/it]
                                                 
{'loss': 0.4375, 'grad_norm': 1.5291659150454409, 'learning_rate': 2.7495908346972173e-07, 'epoch': 2.6}

 87%|████████▋ | 547/630 [16:35<02:15,  1.63s/it]
 87%|████████▋ | 548/630 [16:37<02:14,  1.64s/it]
                                                 
{'loss': 0.4445, 'grad_norm': 1.9785382785184558, 'learning_rate': 2.7168576104746317e-07, 'epoch': 2.61}

 87%|████████▋ | 548/630 [16:37<02:14,  1.64s/it]
 87%|████████▋ | 549/630 [16:38<02:11,  1.62s/it]
                                                 
{'loss': 0.4033, 'grad_norm': 1.8428461088637234, 'learning_rate': 2.6841243862520456e-07, 'epoch': 2.61}

 87%|████████▋ | 549/630 [16:38<02:11,  1.62s/it]
 87%|████████▋ | 550/630 [16:40<02:09,  1.61s/it]
                                                 
{'loss': 0.3641, 'grad_norm': 1.6974981326655132, 'learning_rate': 2.65139116202946e-07, 'epoch': 2.62}

 87%|████████▋ | 550/630 [16:40<02:09,  1.61s/it]
 87%|████████▋ | 551/630 [16:41<02:07,  1.61s/it]
                                                 
{'loss': 0.4646, 'grad_norm': 1.6412772203569344, 'learning_rate': 2.618657937806874e-07, 'epoch': 2.62}

 87%|████████▋ | 551/630 [16:41<02:07,  1.61s/it]
 88%|████████▊ | 552/630 [16:43<02:04,  1.60s/it]
                                                 
{'loss': 0.351, 'grad_norm': 1.9465569893282266, 'learning_rate': 2.585924713584288e-07, 'epoch': 2.63}

 88%|████████▊ | 552/630 [16:43<02:04,  1.60s/it]
 88%|████████▊ | 553/630 [16:44<02:03,  1.61s/it]
                                                 
{'loss': 0.4032, 'grad_norm': 1.7218305121045372, 'learning_rate': 2.5531914893617016e-07, 'epoch': 2.63}

 88%|████████▊ | 553/630 [16:45<02:03,  1.61s/it]
 88%|████████▊ | 554/630 [16:46<02:01,  1.60s/it]
                                                 
{'loss': 0.4381, 'grad_norm': 1.833115680042444, 'learning_rate': 2.520458265139116e-07, 'epoch': 2.64}

 88%|████████▊ | 554/630 [16:46<02:01,  1.60s/it]
 88%|████████▊ | 555/630 [16:48<01:59,  1.60s/it]
                                                 
{'loss': 0.5143, 'grad_norm': 2.2822253919906026, 'learning_rate': 2.48772504091653e-07, 'epoch': 2.64}

 88%|████████▊ | 555/630 [16:48<01:59,  1.60s/it]
 88%|████████▊ | 556/630 [16:49<01:57,  1.59s/it]
                                                 
{'loss': 0.4384, 'grad_norm': 1.643702497660397, 'learning_rate': 2.4549918166939443e-07, 'epoch': 2.65}

 88%|████████▊ | 556/630 [16:49<01:57,  1.59s/it]
 88%|████████▊ | 557/630 [16:51<01:56,  1.59s/it]
                                                 
{'loss': 0.4185, 'grad_norm': 1.659589106209818, 'learning_rate': 2.422258592471358e-07, 'epoch': 2.65}

 88%|████████▊ | 557/630 [16:51<01:56,  1.59s/it]
 89%|████████▊ | 558/630 [16:52<01:54,  1.59s/it]
                                                 
{'loss': 0.4386, 'grad_norm': 1.7891855853816052, 'learning_rate': 2.3895253682487726e-07, 'epoch': 2.66}

 89%|████████▊ | 558/630 [16:52<01:54,  1.59s/it]
 89%|████████▊ | 559/630 [16:54<01:53,  1.59s/it]
                                                 
{'loss': 0.4241, 'grad_norm': 1.5553320414959546, 'learning_rate': 2.3567921440261865e-07, 'epoch': 2.66}

 89%|████████▊ | 559/630 [16:54<01:53,  1.59s/it]
 89%|████████▉ | 560/630 [16:56<01:51,  1.59s/it]
                                                 
{'loss': 0.3923, 'grad_norm': 1.8252743258546298, 'learning_rate': 2.3240589198036006e-07, 'epoch': 2.67}

 89%|████████▉ | 560/630 [16:56<01:51,  1.59s/it]
 89%|████████▉ | 561/630 [16:57<01:49,  1.59s/it]
                                                 
{'loss': 0.3684, 'grad_norm': 1.6892336621749524, 'learning_rate': 2.2913256955810145e-07, 'epoch': 2.67}

 89%|████████▉ | 561/630 [16:57<01:49,  1.59s/it]
 89%|████████▉ | 562/630 [16:59<01:48,  1.59s/it]
                                                 
{'loss': 0.3805, 'grad_norm': 1.5127117585966097, 'learning_rate': 2.2585924713584286e-07, 'epoch': 2.68}

 89%|████████▉ | 562/630 [16:59<01:48,  1.59s/it]
 89%|████████▉ | 563/630 [17:00<01:46,  1.60s/it]
                                                 
{'loss': 0.365, 'grad_norm': 1.6881678717445745, 'learning_rate': 2.2258592471358428e-07, 'epoch': 2.68}

 89%|████████▉ | 563/630 [17:00<01:46,  1.60s/it]
 90%|████████▉ | 564/630 [17:02<01:45,  1.59s/it]
                                                 
{'loss': 0.3713, 'grad_norm': 1.4384727852786985, 'learning_rate': 2.193126022913257e-07, 'epoch': 2.69}

 90%|████████▉ | 564/630 [17:02<01:45,  1.59s/it]
 90%|████████▉ | 565/630 [17:04<01:43,  1.60s/it]
                                                 
{'loss': 0.3921, 'grad_norm': 1.96937938113021, 'learning_rate': 2.1603927986906708e-07, 'epoch': 2.69}

 90%|████████▉ | 565/630 [17:04<01:43,  1.60s/it]
 90%|████████▉ | 566/630 [17:05<01:43,  1.61s/it]
                                                 
{'loss': 0.3878, 'grad_norm': 1.6507747952553198, 'learning_rate': 2.127659574468085e-07, 'epoch': 2.7}

 90%|████████▉ | 566/630 [17:05<01:43,  1.61s/it]
 90%|█████████ | 567/630 [17:07<01:40,  1.60s/it]
                                                 
{'loss': 0.4538, 'grad_norm': 1.6415344861923162, 'learning_rate': 2.094926350245499e-07, 'epoch': 2.7}

 90%|█████████ | 567/630 [17:07<01:40,  1.60s/it]
 90%|█████████ | 568/630 [17:08<01:39,  1.60s/it]
                                                 
{'loss': 0.4173, 'grad_norm': 1.5165360973704807, 'learning_rate': 2.062193126022913e-07, 'epoch': 2.7}

 90%|█████████ | 568/630 [17:08<01:39,  1.60s/it]
 90%|█████████ | 569/630 [17:10<01:37,  1.59s/it]
                                                 
{'loss': 0.402, 'grad_norm': 1.5953642419666836, 'learning_rate': 2.029459901800327e-07, 'epoch': 2.71}

 90%|█████████ | 569/630 [17:10<01:37,  1.59s/it]
 90%|█████████ | 570/630 [17:12<01:35,  1.59s/it]
                                                 
{'loss': 0.4338, 'grad_norm': 1.981098808001395, 'learning_rate': 1.9967266775777415e-07, 'epoch': 2.71}

 90%|█████████ | 570/630 [17:12<01:35,  1.59s/it]
 91%|█████████ | 571/630 [17:13<01:34,  1.59s/it]
                                                 
{'loss': 0.4096, 'grad_norm': 1.7680756946641072, 'learning_rate': 1.9639934533551554e-07, 'epoch': 2.72}

 91%|█████████ | 571/630 [17:13<01:34,  1.59s/it]
 91%|█████████ | 572/630 [17:15<01:32,  1.59s/it]
                                                 
{'loss': 0.3712, 'grad_norm': 1.4223695709759838, 'learning_rate': 1.9312602291325695e-07, 'epoch': 2.72}

 91%|█████████ | 572/630 [17:15<01:32,  1.59s/it]
 91%|█████████ | 573/630 [17:16<01:31,  1.60s/it]
                                                 
{'loss': 0.4282, 'grad_norm': 1.8829615404948892, 'learning_rate': 1.8985270049099837e-07, 'epoch': 2.73}

 91%|█████████ | 573/630 [17:16<01:31,  1.60s/it]
 91%|█████████ | 574/630 [17:18<01:29,  1.60s/it]
                                                 
{'loss': 0.4312, 'grad_norm': 1.6236088042472638, 'learning_rate': 1.8657937806873978e-07, 'epoch': 2.73}

 91%|█████████ | 574/630 [17:18<01:29,  1.60s/it]
 91%|█████████▏| 575/630 [17:20<01:27,  1.59s/it]
                                                 
{'loss': 0.3366, 'grad_norm': 1.8352408070725674, 'learning_rate': 1.8330605564648117e-07, 'epoch': 2.74}

 91%|█████████▏| 575/630 [17:20<01:27,  1.59s/it]
 91%|█████████▏| 576/630 [17:21<01:26,  1.60s/it]
                                                 
{'loss': 0.3881, 'grad_norm': 1.871436419420992, 'learning_rate': 1.8003273322422258e-07, 'epoch': 2.74}

 91%|█████████▏| 576/630 [17:21<01:26,  1.60s/it]
 92%|█████████▏| 577/630 [17:23<01:24,  1.59s/it]
                                                 
{'loss': 0.4083, 'grad_norm': 1.7987927637498278, 'learning_rate': 1.76759410801964e-07, 'epoch': 2.75}

 92%|█████████▏| 577/630 [17:23<01:24,  1.59s/it]
 92%|█████████▏| 578/630 [17:24<01:22,  1.59s/it]
                                                 
{'loss': 0.391, 'grad_norm': 1.5831236894990568, 'learning_rate': 1.7348608837970539e-07, 'epoch': 2.75}

 92%|█████████▏| 578/630 [17:24<01:22,  1.59s/it]
 92%|█████████▏| 579/630 [17:26<01:21,  1.60s/it]
                                                 
{'loss': 0.4481, 'grad_norm': 1.749977515274616, 'learning_rate': 1.702127659574468e-07, 'epoch': 2.76}

 92%|█████████▏| 579/630 [17:26<01:21,  1.60s/it]
 92%|█████████▏| 580/630 [17:28<01:20,  1.60s/it]
                                                 
{'loss': 0.4334, 'grad_norm': 1.6098838184080164, 'learning_rate': 1.6693944353518821e-07, 'epoch': 2.76}

 92%|█████████▏| 580/630 [17:28<01:20,  1.60s/it]
 92%|█████████▏| 581/630 [17:29<01:18,  1.60s/it]
                                                 
{'loss': 0.3619, 'grad_norm': 1.5666219695429182, 'learning_rate': 1.6366612111292963e-07, 'epoch': 2.77}

 92%|█████████▏| 581/630 [17:29<01:18,  1.60s/it]
 92%|█████████▏| 582/630 [17:31<01:16,  1.60s/it]
                                                 
{'loss': 0.4168, 'grad_norm': 1.7076228234697135, 'learning_rate': 1.6039279869067102e-07, 'epoch': 2.77}

 92%|█████████▏| 582/630 [17:31<01:16,  1.60s/it]
 93%|█████████▎| 583/630 [17:32<01:15,  1.60s/it]
                                                 
{'loss': 0.4171, 'grad_norm': 2.1318680385424695, 'learning_rate': 1.5711947626841243e-07, 'epoch': 2.78}

 93%|█████████▎| 583/630 [17:32<01:15,  1.60s/it]
 93%|█████████▎| 584/630 [17:34<01:13,  1.61s/it]
                                                 
{'loss': 0.338, 'grad_norm': 1.4924592283369338, 'learning_rate': 1.5384615384615385e-07, 'epoch': 2.78}

 93%|█████████▎| 584/630 [17:34<01:13,  1.61s/it]
 93%|█████████▎| 585/630 [17:36<01:12,  1.60s/it]
                                                 
{'loss': 0.4394, 'grad_norm': 1.733112506646904, 'learning_rate': 1.5057283142389523e-07, 'epoch': 2.79}

 93%|█████████▎| 585/630 [17:36<01:12,  1.60s/it]
 93%|█████████▎| 586/630 [17:37<01:10,  1.60s/it]
                                                 
{'loss': 0.3792, 'grad_norm': 1.5675187865584723, 'learning_rate': 1.4729950900163665e-07, 'epoch': 2.79}

 93%|█████████▎| 586/630 [17:37<01:10,  1.60s/it]
 93%|█████████▎| 587/630 [17:39<01:08,  1.60s/it]
                                                 
{'loss': 0.3558, 'grad_norm': 1.5142728910794696, 'learning_rate': 1.4402618657937806e-07, 'epoch': 2.8}

 93%|█████████▎| 587/630 [17:39<01:08,  1.60s/it]
 93%|█████████▎| 588/630 [17:40<01:07,  1.60s/it]
                                                 
{'loss': 0.3524, 'grad_norm': 1.534256702557214, 'learning_rate': 1.4075286415711948e-07, 'epoch': 2.8}

 93%|█████████▎| 588/630 [17:40<01:07,  1.60s/it]
 93%|█████████▎| 589/630 [17:42<01:05,  1.60s/it]
                                                 
{'loss': 0.3828, 'grad_norm': 1.6019957576340254, 'learning_rate': 1.3747954173486086e-07, 'epoch': 2.8}

 93%|█████████▎| 589/630 [17:42<01:05,  1.60s/it]
 94%|█████████▎| 590/630 [17:44<01:04,  1.61s/it]
                                                 
{'loss': 0.4383, 'grad_norm': 1.52493463108535, 'learning_rate': 1.3420621931260228e-07, 'epoch': 2.81}

 94%|█████████▎| 590/630 [17:44<01:04,  1.61s/it]
 94%|█████████▍| 591/630 [17:45<01:02,  1.61s/it]
                                                 
{'loss': 0.3674, 'grad_norm': 1.6633934853122592, 'learning_rate': 1.309328968903437e-07, 'epoch': 2.81}

 94%|█████████▍| 591/630 [17:45<01:02,  1.61s/it]
 94%|█████████▍| 592/630 [17:47<01:01,  1.61s/it]
                                                 
{'loss': 0.4773, 'grad_norm': 1.7566123398218771, 'learning_rate': 1.2765957446808508e-07, 'epoch': 2.82}

 94%|█████████▍| 592/630 [17:47<01:01,  1.61s/it]
 94%|█████████▍| 593/630 [17:48<00:59,  1.61s/it]
                                                 
{'loss': 0.3753, 'grad_norm': 1.47142437294434, 'learning_rate': 1.243862520458265e-07, 'epoch': 2.82}

 94%|█████████▍| 593/630 [17:48<00:59,  1.61s/it]
 94%|█████████▍| 594/630 [17:50<00:57,  1.61s/it]
                                                 
{'loss': 0.3416, 'grad_norm': 1.6648355904117467, 'learning_rate': 1.211129296235679e-07, 'epoch': 2.83}

 94%|█████████▍| 594/630 [17:50<00:57,  1.61s/it]
 94%|█████████▍| 595/630 [17:52<00:56,  1.60s/it]
                                                 
{'loss': 0.4077, 'grad_norm': 1.6431171205209878, 'learning_rate': 1.1783960720130932e-07, 'epoch': 2.83}

 94%|█████████▍| 595/630 [17:52<00:56,  1.60s/it]
 95%|█████████▍| 596/630 [17:53<00:54,  1.60s/it]
                                                 
{'loss': 0.398, 'grad_norm': 1.6346383879849993, 'learning_rate': 1.1456628477905072e-07, 'epoch': 2.84}

 95%|█████████▍| 596/630 [17:53<00:54,  1.60s/it]
 95%|█████████▍| 597/630 [17:55<00:52,  1.60s/it]
                                                 
{'loss': 0.3893, 'grad_norm': 1.5504823577329474, 'learning_rate': 1.1129296235679214e-07, 'epoch': 2.84}

 95%|█████████▍| 597/630 [17:55<00:52,  1.60s/it]
 95%|█████████▍| 598/630 [17:56<00:51,  1.60s/it]
                                                 
{'loss': 0.4205, 'grad_norm': 1.5909040473414102, 'learning_rate': 1.0801963993453354e-07, 'epoch': 2.85}

 95%|█████████▍| 598/630 [17:56<00:51,  1.60s/it]
 95%|█████████▌| 599/630 [17:58<00:49,  1.60s/it]
                                                 
{'loss': 0.4234, 'grad_norm': 1.6526911666353945, 'learning_rate': 1.0474631751227495e-07, 'epoch': 2.85}

 95%|█████████▌| 599/630 [17:58<00:49,  1.60s/it]
 95%|█████████▌| 600/630 [18:00<00:47,  1.60s/it]
                                                 
{'loss': 0.3893, 'grad_norm': 1.5091340367367378, 'learning_rate': 1.0147299509001636e-07, 'epoch': 2.86}

 95%|█████████▌| 600/630 [18:00<00:47,  1.60s/it]
 95%|█████████▌| 601/630 [18:01<00:46,  1.61s/it]
                                                 
{'loss': 0.3655, 'grad_norm': 1.5482302715985066, 'learning_rate': 9.819967266775777e-08, 'epoch': 2.86}

 95%|█████████▌| 601/630 [18:01<00:46,  1.61s/it]
 96%|█████████▌| 602/630 [18:03<00:44,  1.60s/it]
                                                 
{'loss': 0.4445, 'grad_norm': 1.6695364689148868, 'learning_rate': 9.492635024549918e-08, 'epoch': 2.87}

 96%|█████████▌| 602/630 [18:03<00:44,  1.60s/it]
 96%|█████████▌| 603/630 [18:04<00:43,  1.60s/it]
                                                 
{'loss': 0.3804, 'grad_norm': 1.542444641543543, 'learning_rate': 9.165302782324058e-08, 'epoch': 2.87}

 96%|█████████▌| 603/630 [18:04<00:43,  1.60s/it]
 96%|█████████▌| 604/630 [18:06<00:41,  1.61s/it]
                                                 
{'loss': 0.3833, 'grad_norm': 1.8390230722087637, 'learning_rate': 8.8379705400982e-08, 'epoch': 2.88}

 96%|█████████▌| 604/630 [18:06<00:41,  1.61s/it]
 96%|█████████▌| 605/630 [18:08<00:40,  1.61s/it]
                                                 
{'loss': 0.3904, 'grad_norm': 1.5138110758914272, 'learning_rate': 8.51063829787234e-08, 'epoch': 2.88}

 96%|█████████▌| 605/630 [18:08<00:40,  1.61s/it]
 96%|█████████▌| 606/630 [18:09<00:38,  1.61s/it]
                                                 
{'loss': 0.4579, 'grad_norm': 1.7567527569668528, 'learning_rate': 8.183306055646481e-08, 'epoch': 2.89}

 96%|█████████▌| 606/630 [18:09<00:38,  1.61s/it]
 96%|█████████▋| 607/630 [18:11<00:36,  1.61s/it]
                                                 
{'loss': 0.3668, 'grad_norm': 1.3830596055425788, 'learning_rate': 7.855973813420622e-08, 'epoch': 2.89}

 96%|█████████▋| 607/630 [18:11<00:36,  1.61s/it]
 97%|█████████▋| 608/630 [18:13<00:35,  1.60s/it]
                                                 
{'loss': 0.3799, 'grad_norm': 1.5167149729691671, 'learning_rate': 7.528641571194762e-08, 'epoch': 2.9}

 97%|█████████▋| 608/630 [18:13<00:35,  1.60s/it]
 97%|█████████▋| 609/630 [18:14<00:33,  1.60s/it]
                                                 
{'loss': 0.4182, 'grad_norm': 1.8376900963360876, 'learning_rate': 7.201309328968903e-08, 'epoch': 2.9}

 97%|█████████▋| 609/630 [18:14<00:33,  1.60s/it]
 97%|█████████▋| 610/630 [18:16<00:31,  1.60s/it]
                                                 
{'loss': 0.4263, 'grad_norm': 1.763598870216295, 'learning_rate': 6.873977086743043e-08, 'epoch': 2.9}

 97%|█████████▋| 610/630 [18:16<00:31,  1.60s/it]
 97%|█████████▋| 611/630 [18:17<00:30,  1.60s/it]
                                                 
{'loss': 0.434, 'grad_norm': 2.2250044602545724, 'learning_rate': 6.546644844517185e-08, 'epoch': 2.91}

 97%|█████████▋| 611/630 [18:17<00:30,  1.60s/it]
 97%|█████████▋| 612/630 [18:19<00:28,  1.60s/it]
                                                 
{'loss': 0.4246, 'grad_norm': 1.4415657833516027, 'learning_rate': 6.219312602291325e-08, 'epoch': 2.91}

 97%|█████████▋| 612/630 [18:19<00:28,  1.60s/it]
 97%|█████████▋| 613/630 [18:20<00:27,  1.60s/it]
                                                 
{'loss': 0.4418, 'grad_norm': 1.7894505119676134, 'learning_rate': 5.891980360065466e-08, 'epoch': 2.92}

 97%|█████████▋| 613/630 [18:20<00:27,  1.60s/it]
 97%|█████████▋| 614/630 [18:22<00:25,  1.60s/it]
                                                 
{'loss': 0.381, 'grad_norm': 1.6332564940598688, 'learning_rate': 5.564648117839607e-08, 'epoch': 2.92}

 97%|█████████▋| 614/630 [18:22<00:25,  1.60s/it]
 98%|█████████▊| 615/630 [18:24<00:24,  1.61s/it]
                                                 
{'loss': 0.3307, 'grad_norm': 1.584419457603652, 'learning_rate': 5.237315875613748e-08, 'epoch': 2.93}

 98%|█████████▊| 615/630 [18:24<00:24,  1.61s/it]
 98%|█████████▊| 616/630 [18:25<00:22,  1.60s/it]
                                                 
{'loss': 0.4329, 'grad_norm': 1.9685543856028893, 'learning_rate': 4.9099836333878885e-08, 'epoch': 2.93}

 98%|█████████▊| 616/630 [18:25<00:22,  1.60s/it]
 98%|█████████▊| 617/630 [18:27<00:20,  1.60s/it]
                                                 
{'loss': 0.4583, 'grad_norm': 1.6840063564325576, 'learning_rate': 4.582651391162029e-08, 'epoch': 2.94}

 98%|█████████▊| 617/630 [18:27<00:20,  1.60s/it]
 98%|█████████▊| 618/630 [18:29<00:19,  1.60s/it]
                                                 
{'loss': 0.4329, 'grad_norm': 1.6375795640683668, 'learning_rate': 4.25531914893617e-08, 'epoch': 2.94}

 98%|█████████▊| 618/630 [18:29<00:19,  1.60s/it]
 98%|█████████▊| 619/630 [18:30<00:17,  1.60s/it]
                                                 
{'loss': 0.4028, 'grad_norm': 2.1023138845343654, 'learning_rate': 3.927986906710311e-08, 'epoch': 2.95}

 98%|█████████▊| 619/630 [18:30<00:17,  1.60s/it]
 98%|█████████▊| 620/630 [18:32<00:16,  1.60s/it]
                                                 
{'loss': 0.3886, 'grad_norm': 1.857751068675312, 'learning_rate': 3.6006546644844515e-08, 'epoch': 2.95}

 98%|█████████▊| 620/630 [18:32<00:16,  1.60s/it]
 99%|█████████▊| 621/630 [18:33<00:14,  1.60s/it]
                                                 
{'loss': 0.4658, 'grad_norm': 1.5479015462549717, 'learning_rate': 3.273322422258592e-08, 'epoch': 2.96}

 99%|█████████▊| 621/630 [18:33<00:14,  1.60s/it]
 99%|█████████▊| 622/630 [18:35<00:13,  1.69s/it]
                                                 
{'loss': 0.3509, 'grad_norm': 1.637037582104174, 'learning_rate': 2.945990180032733e-08, 'epoch': 2.96}

 99%|█████████▊| 622/630 [18:35<00:13,  1.69s/it]
 99%|█████████▉| 623/630 [18:37<00:11,  1.67s/it]
                                                 
{'loss': 0.4162, 'grad_norm': 1.6469674853769065, 'learning_rate': 2.618657937806874e-08, 'epoch': 2.97}

 99%|█████████▉| 623/630 [18:37<00:11,  1.67s/it]
 99%|█████████▉| 624/630 [18:38<00:09,  1.65s/it]
                                                 
{'loss': 0.3844, 'grad_norm': 1.755404422600387, 'learning_rate': 2.2913256955810146e-08, 'epoch': 2.97}

 99%|█████████▉| 624/630 [18:38<00:09,  1.65s/it]
 99%|█████████▉| 625/630 [18:40<00:08,  1.64s/it]
                                                 
{'loss': 0.3868, 'grad_norm': 1.6506817504101643, 'learning_rate': 1.9639934533551554e-08, 'epoch': 2.98}

 99%|█████████▉| 625/630 [18:40<00:08,  1.64s/it]
 99%|█████████▉| 626/630 [18:42<00:06,  1.63s/it]
                                                 
{'loss': 0.3363, 'grad_norm': 1.5619388296844172, 'learning_rate': 1.636661211129296e-08, 'epoch': 2.98}

 99%|█████████▉| 626/630 [18:42<00:06,  1.63s/it]
100%|█████████▉| 627/630 [18:43<00:04,  1.63s/it]
                                                 
{'loss': 0.5388, 'grad_norm': 1.9191359138186224, 'learning_rate': 1.309328968903437e-08, 'epoch': 2.99}

100%|█████████▉| 627/630 [18:43<00:04,  1.63s/it]
100%|█████████▉| 628/630 [18:45<00:03,  1.61s/it]
                                                 
{'loss': 0.3768, 'grad_norm': 1.4889443125113029, 'learning_rate': 9.819967266775777e-09, 'epoch': 2.99}

100%|█████████▉| 628/630 [18:45<00:03,  1.61s/it]
100%|█████████▉| 629/630 [18:46<00:01,  1.61s/it]
                                                 
{'loss': 0.4183, 'grad_norm': 1.9066067199092287, 'learning_rate': 6.546644844517185e-09, 'epoch': 3.0}

100%|█████████▉| 629/630 [18:46<00:01,  1.61s/it]
100%|██████████| 630/630 [18:48<00:00,  1.61s/it]
                                                 
{'loss': 0.3447, 'grad_norm': 1.5783989684722415, 'learning_rate': 3.2733224222585923e-09, 'epoch': 3.0}

100%|██████████| 630/630 [18:48<00:00,  1.61s/it]
                                                 
{'train_runtime': 1128.5715, 'train_samples_per_second': 44.459, 'train_steps_per_second': 0.558, 'train_loss': 0.5233189782926014, 'epoch': 3.0}

100%|██████████| 630/630 [18:48<00:00,  1.61s/it]
100%|██████████| 630/630 [18:48<00:00,  1.79s/it]
[2024-08-08 06:31:05,164] [INFO] [launch.py:351:main] Process 3103786 exits successfully.
[2024-08-08 06:31:05,165] [INFO] [launch.py:351:main] Process 3103788 exits successfully.
[2024-08-08 06:31:05,165] [INFO] [launch.py:351:main] Process 3103789 exits successfully.
[2024-08-08 06:31:06,165] [INFO] [launch.py:351:main] Process 3103790 exits successfully.
[2024-08-08 06:31:06,166] [INFO] [launch.py:351:main] Process 3103787 exits successfully.
[2024-08-08 06:31:06,166] [INFO] [launch.py:351:main] Process 3103791 exits successfully.
[2024-08-08 06:31:06,166] [INFO] [launch.py:351:main] Process 3103785 exits successfully.
[2024-08-08 06:31:51,172] [INFO] [launch.py:351:main] Process 3103784 exits successfully.
