Namespace(mode=['alpaca'], base_model='meta-llama/Llama-2-7b-hf', task_name='sharegpt', tuned_dir='./cache')
num gpus:  8
Running 1/1: deepspeed --num_gpus=8 train.py --deepspeed ../deepspeed_config/zero3.json --bf16 --tf32=True
    --model_name_or_path meta-llama/Llama-2-7b-hf --data_path ../data/stanford_alpaca/sharegpt_data.json
    --output_dir /fsx-project/yunyun/models/meta-llama_meta-llama_sharegpt_tuned
    --num_train_epochs 3
    --per_device_train_batch_size 10
    --per_device_eval_batch_size 4
    --gradient_accumulation_steps 1
    --gradient_checkpointing=True
    --evaluation_strategy=no
    --save_strategy=steps
    --save_steps 500
    --save_total_limit 1
    --learning_rate 2e-6
    --weight_decay 0.
    --report_to tensorboard
    --warmup_ratio 0.03
    --lr_scheduler_type=cosine
    --logging_steps 1
['deepspeed', '--num_gpus=8', 'train.py', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Llama-2-7b-hf', '--data_path', '../data/stanford_alpaca/sharegpt_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_meta-llama_sharegpt_tuned', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1']
[2024-07-12 04:55:57,316] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2024-07-12 04:56:01,912] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2024-07-12 04:56:01,912] [INFO] [runner.py:568:main] cmd = /data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNSwgNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None train.py --deepspeed ../deepspeed_config/zero3.json --bf16 --tf32=True --model_name_or_path meta-llama/Llama-2-7b-hf --data_path ../data/stanford_alpaca/sharegpt_data.json --output_dir /fsx-project/yunyun/models/meta-llama_meta-llama_sharegpt_tuned --num_train_epochs 3 --per_device_train_batch_size 10 --per_device_eval_batch_size 4 --gradient_accumulation_steps 1 --gradient_checkpointing=True --evaluation_strategy=no --save_strategy=steps --save_steps 500 --save_total_limit 1 --learning_rate 2e-6 --weight_decay 0. --report_to tensorboard --warmup_ratio 0.03 --lr_scheduler_type=cosine --logging_steps 1
[2024-07-12 04:56:03,996] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2024-07-12 04:56:07,194] [INFO] [launch.py:139:main] 0 NCCL_ASYNC_ERROR_HANDLING=1
[2024-07-12 04:56:07,194] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]}
[2024-07-12 04:56:07,194] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=8, node_rank=0
[2024-07-12 04:56:07,194] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]})
[2024-07-12 04:56:07,194] [INFO] [launch.py:164:main] dist_world_size=8
[2024-07-12 04:56:07,194] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
[2024-07-12 04:56:07,195] [INFO] [launch.py:256:main] process 2148478 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=0', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Llama-2-7b-hf', '--data_path', '../data/stanford_alpaca/sharegpt_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_meta-llama_sharegpt_tuned', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1']
[2024-07-12 04:56:07,195] [INFO] [launch.py:256:main] process 2148479 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=1', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Llama-2-7b-hf', '--data_path', '../data/stanford_alpaca/sharegpt_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_meta-llama_sharegpt_tuned', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1']
[2024-07-12 04:56:07,196] [INFO] [launch.py:256:main] process 2148480 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=2', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Llama-2-7b-hf', '--data_path', '../data/stanford_alpaca/sharegpt_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_meta-llama_sharegpt_tuned', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1']
[2024-07-12 04:56:07,196] [INFO] [launch.py:256:main] process 2148481 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=3', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Llama-2-7b-hf', '--data_path', '../data/stanford_alpaca/sharegpt_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_meta-llama_sharegpt_tuned', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1']
[2024-07-12 04:56:07,197] [INFO] [launch.py:256:main] process 2148482 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=4', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Llama-2-7b-hf', '--data_path', '../data/stanford_alpaca/sharegpt_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_meta-llama_sharegpt_tuned', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1']
[2024-07-12 04:56:07,197] [INFO] [launch.py:256:main] process 2148483 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=5', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Llama-2-7b-hf', '--data_path', '../data/stanford_alpaca/sharegpt_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_meta-llama_sharegpt_tuned', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1']
[2024-07-12 04:56:07,198] [INFO] [launch.py:256:main] process 2148484 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=6', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Llama-2-7b-hf', '--data_path', '../data/stanford_alpaca/sharegpt_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_meta-llama_sharegpt_tuned', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1']
[2024-07-12 04:56:07,198] [INFO] [launch.py:256:main] process 2148485 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=7', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Llama-2-7b-hf', '--data_path', '../data/stanford_alpaca/sharegpt_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_meta-llama_sharegpt_tuned', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1']
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
[2024-07-12 04:56:19,936] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2024-07-12 04:56:20,339] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
[2024-07-12 04:56:20,470] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-12 04:56:20,480] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-12 04:56:20,484] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-12 04:56:20,502] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-12 04:56:20,510] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-12 04:56:20,513] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH

[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[2024-07-12 04:56:20,798] [INFO] [comm.py:637:init_distributed] cdb=None
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2024-07-12 04:56:21,142] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-12 04:56:21,142] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-07-12 04:56:21,238] [INFO] [comm.py:637:init_distributed] cdb=None
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[2024-07-12 04:56:21,242] [INFO] [comm.py:637:init_distributed] cdb=None
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[2024-07-12 04:56:21,259] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-12 04:56:21,262] [INFO] [comm.py:637:init_distributed] cdb=None
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[2024-07-12 04:56:21,269] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-12 04:56:21,275] [INFO] [comm.py:637:init_distributed] cdb=None
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)

Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]
Downloading shards: 100%|██████████| 2/2 [00:00<00:00, 2470.14it/s]

Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]
Downloading shards: 100%|██████████| 2/2 [00:00<00:00, 2314.10it/s]

Downloading shards: 100%|██████████| 2/2 [00:00<00:00, 1725.70it/s]

Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]
Downloading shards: 100%|██████████| 2/2 [00:00<00:00, 1662.43it/s]

Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]
Downloading shards: 100%|██████████| 2/2 [00:00<00:00, 1684.46it/s]

Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]
Downloading shards: 100%|██████████| 2/2 [00:00<00:00, 1686.83it/s]

Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]
Downloading shards: 100%|██████████| 2/2 [00:00<00:00, 1631.07it/s]

Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]
Downloading shards: 100%|██████████| 2/2 [00:00<00:00, 1759.72it/s]
[2024-07-12 04:56:32,308] [INFO] [partition_parameters.py:345:__exit__] finished initializing model - num_params = 291, num_elems = 6.74B

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.50s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.52s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.51s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.52s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.53s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.53s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.53s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.90s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.99s/it]

Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.90s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.99s/it]

Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.90s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.99s/it]

Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.90s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.99s/it]

Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.90s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  2.00s/it]

Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.90s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.99s/it]

Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.90s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  2.00s/it]

Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.82s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.01s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.28s/it]
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
[rank2]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121/fused_adam/build.ninja...
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/cpp_extension.py:1967: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
[rank6]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
[rank0]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
[rank7]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
[rank4]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
[rank3]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
[rank5]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
[rank1]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
[1/3] /usr/local/cuda-12.1/bin/nvcc --generate-dependencies-with-compile --dependency-output multi_tensor_adam.cuda.o.d -DTORCH_EXTENSION_NAME=fused_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/deepspeed/ops/csrc/includes -I/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/deepspeed/ops/csrc/adam -isystem /data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/include -isystem /data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/include/TH -isystem /data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/include/THC -isystem /usr/local/cuda-12.1/include -isystem /data/home/yunyun/miniconda3/envs/newtorch2/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_90,code=compute_90 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -lineinfo --use_fast_math -gencode=arch=compute_90,code=sm_90 -gencode=arch=compute_90,code=compute_90 -DBF16_AVAILABLE -U__CUDA_NO_BFLOAT16_OPERATORS__ -U__CUDA_NO_BFLOAT162_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ -std=c++17 -c /data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/deepspeed/ops/csrc/adam/multi_tensor_adam.cu -o multi_tensor_adam.cuda.o 
[2/3] c++ -MMD -MF fused_adam_frontend.o.d -DTORCH_EXTENSION_NAME=fused_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/deepspeed/ops/csrc/includes -I/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/deepspeed/ops/csrc/adam -isystem /data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/include -isystem /data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/include/TH -isystem /data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/include/THC -isystem /usr/local/cuda-12.1/include -isystem /data/home/yunyun/miniconda3/envs/newtorch2/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -std=c++17 -g -Wno-reorder -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DBF16_AVAILABLE -c /data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/deepspeed/ops/csrc/adam/fused_adam_frontend.cpp -o fused_adam_frontend.o 
[3/3] c++ fused_adam_frontend.o multi_tensor_adam.cuda.o -shared -L/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda-12.1/lib64 -lcudart -o fused_adam.so
Loading extension module fused_adam...
Time to load fused_adam op: 38.01632022857666 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 36.33654713630676 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 37.83568072319031 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 35.03641963005066 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 36.53533172607422 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 35.445773124694824 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 35.636003494262695 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 37.63855314254761 seconds
Parameter Offload: Total persistent parameters: 266240 in 65 params
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.

  0%|          | 0/630 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

  0%|          | 1/630 [00:08<1:26:37,  8.26s/it]
                                                 
{'loss': 0.9013, 'grad_norm': 5.038100829182171, 'learning_rate': 0.0, 'epoch': 0.0}

  0%|          | 1/630 [00:08<1:26:37,  8.26s/it]
  0%|          | 2/630 [00:09<43:10,  4.13s/it]  
                                               
{'loss': 0.8927, 'grad_norm': 4.592293289256242, 'learning_rate': 4.7081782673327645e-07, 'epoch': 0.01}

  0%|          | 2/630 [00:09<43:10,  4.13s/it]
  0%|          | 3/630 [00:10<27:34,  2.64s/it]
                                               
{'loss': 0.9164, 'grad_norm': 4.771796580426167, 'learning_rate': 7.462286000432739e-07, 'epoch': 0.01}

  0%|          | 3/630 [00:10<27:34,  2.64s/it]
  1%|          | 4/630 [00:11<20:20,  1.95s/it]
                                               
{'loss': 0.8249, 'grad_norm': 4.501136482473651, 'learning_rate': 9.416356534665529e-07, 'epoch': 0.02}

  1%|          | 4/630 [00:11<20:20,  1.95s/it]
  1%|          | 5/630 [00:12<16:15,  1.56s/it]
                                               
{'loss': 0.8876, 'grad_norm': 4.597425440183411, 'learning_rate': 1.0932051394658049e-06, 'epoch': 0.02}

  1%|          | 5/630 [00:12<16:15,  1.56s/it]
  1%|          | 6/630 [00:13<13:50,  1.33s/it]
                                               
{'loss': 0.8529, 'grad_norm': 4.382554827048161, 'learning_rate': 1.2170464267765503e-06, 'epoch': 0.03}

  1%|          | 6/630 [00:13<13:50,  1.33s/it]
  1%|          | 7/630 [00:13<12:16,  1.18s/it]
                                               
{'loss': 0.8613, 'grad_norm': 4.133700738873216, 'learning_rate': 1.3217527432721277e-06, 'epoch': 0.03}

  1%|          | 7/630 [00:13<12:16,  1.18s/it]
  1%|▏         | 8/630 [00:14<11:15,  1.09s/it]
                                               
{'loss': 0.9347, 'grad_norm': 4.253769467510221, 'learning_rate': 1.4124534801998293e-06, 'epoch': 0.04}

  1%|▏         | 8/630 [00:14<11:15,  1.09s/it]
  1%|▏         | 9/630 [00:15<10:34,  1.02s/it]
                                               
{'loss': 0.8896, 'grad_norm': 4.334874188543227, 'learning_rate': 1.4924572000865478e-06, 'epoch': 0.04}

  1%|▏         | 9/630 [00:15<10:34,  1.02s/it]
  2%|▏         | 10/630 [00:16<10:06,  1.02it/s]
                                                
{'loss': 0.7808, 'grad_norm': 2.6321259316653984, 'learning_rate': 1.5640229661990816e-06, 'epoch': 0.05}

  2%|▏         | 10/630 [00:16<10:06,  1.02it/s]
  2%|▏         | 11/630 [00:17<09:47,  1.05it/s]
                                                
{'loss': 0.8722, 'grad_norm': 3.3871391484962743, 'learning_rate': 1.6287620764191933e-06, 'epoch': 0.05}

  2%|▏         | 11/630 [00:17<09:47,  1.05it/s]
  2%|▏         | 12/630 [00:18<09:34,  1.08it/s]
                                                
{'loss': 0.7862, 'grad_norm': 2.915718978541835, 'learning_rate': 1.6878642535098268e-06, 'epoch': 0.06}

  2%|▏         | 12/630 [00:18<09:34,  1.08it/s]
  2%|▏         | 13/630 [00:19<09:24,  1.09it/s]
                                                
{'loss': 0.9082, 'grad_norm': 3.439452603229561, 'learning_rate': 1.7422329860526872e-06, 'epoch': 0.06}

  2%|▏         | 13/630 [00:19<09:24,  1.09it/s]
  2%|▏         | 14/630 [00:20<09:17,  1.11it/s]
                                                
{'loss': 0.8241, 'grad_norm': 2.823824186515463, 'learning_rate': 1.792570570005404e-06, 'epoch': 0.07}

  2%|▏         | 14/630 [00:20<09:17,  1.11it/s]
  2%|▏         | 15/630 [00:20<09:11,  1.11it/s]
                                                
{'loss': 0.7215, 'grad_norm': 2.04944735349819, 'learning_rate': 1.8394337395090787e-06, 'epoch': 0.07}

  2%|▏         | 15/630 [00:20<09:11,  1.11it/s]
  3%|▎         | 16/630 [00:21<09:08,  1.12it/s]
                                                
{'loss': 0.7588, 'grad_norm': 2.8319107530646512, 'learning_rate': 1.8832713069331058e-06, 'epoch': 0.08}

  3%|▎         | 16/630 [00:21<09:08,  1.12it/s]
  3%|▎         | 17/630 [00:22<09:06,  1.12it/s]
                                                
{'loss': 0.8407, 'grad_norm': 3.0906375101508456, 'learning_rate': 1.924450371770508e-06, 'epoch': 0.08}

  3%|▎         | 17/630 [00:22<09:06,  1.12it/s]
  3%|▎         | 18/630 [00:23<09:03,  1.13it/s]
                                                
{'loss': 0.7737, 'grad_norm': 2.2412291718993016, 'learning_rate': 1.9632750268198243e-06, 'epoch': 0.09}

  3%|▎         | 18/630 [00:23<09:03,  1.13it/s]
  3%|▎         | 19/630 [00:24<09:04,  1.12it/s]
                                                
{'loss': 0.7404, 'grad_norm': 1.9575379372711625, 'learning_rate': 2e-06, 'epoch': 0.09}

  3%|▎         | 19/630 [00:24<09:04,  1.12it/s]
  3%|▎         | 20/630 [00:25<09:03,  1.12it/s]
                                                
{'loss': 0.8012, 'grad_norm': 2.3690728225733504, 'learning_rate': 2e-06, 'epoch': 0.1}

  3%|▎         | 20/630 [00:25<09:03,  1.12it/s]
  3%|▎         | 21/630 [00:26<09:00,  1.13it/s]
                                                
{'loss': 0.7604, 'grad_norm': 2.587564199059603, 'learning_rate': 1.9967266775777412e-06, 'epoch': 0.1}

  3%|▎         | 21/630 [00:26<09:00,  1.13it/s]
  3%|▎         | 22/630 [00:27<09:00,  1.13it/s]
                                                
{'loss': 0.7541, 'grad_norm': 2.085906561053113, 'learning_rate': 1.9934533551554826e-06, 'epoch': 0.1}

  3%|▎         | 22/630 [00:27<09:00,  1.13it/s]
  4%|▎         | 23/630 [00:28<08:57,  1.13it/s]
                                                
{'loss': 0.8493, 'grad_norm': 2.327278278781046, 'learning_rate': 1.990180032733224e-06, 'epoch': 0.11}

  4%|▎         | 23/630 [00:28<08:57,  1.13it/s]
  4%|▍         | 24/630 [00:28<08:56,  1.13it/s]
                                                
{'loss': 0.8537, 'grad_norm': 2.762395839620767, 'learning_rate': 1.9869067103109657e-06, 'epoch': 0.11}

  4%|▍         | 24/630 [00:28<08:56,  1.13it/s]
  4%|▍         | 25/630 [00:29<08:55,  1.13it/s]
                                                
{'loss': 0.7498, 'grad_norm': 2.3187662956713773, 'learning_rate': 1.983633387888707e-06, 'epoch': 0.12}

  4%|▍         | 25/630 [00:29<08:55,  1.13it/s]
  4%|▍         | 26/630 [00:30<08:54,  1.13it/s]
                                                
{'loss': 0.803, 'grad_norm': 2.1903264543317835, 'learning_rate': 1.9803600654664483e-06, 'epoch': 0.12}

  4%|▍         | 26/630 [00:30<08:54,  1.13it/s]
  4%|▍         | 27/630 [00:31<08:55,  1.13it/s]
                                                
{'loss': 0.7399, 'grad_norm': 2.1550806772192757, 'learning_rate': 1.9770867430441897e-06, 'epoch': 0.13}

  4%|▍         | 27/630 [00:31<08:55,  1.13it/s]
  4%|▍         | 28/630 [00:32<08:54,  1.13it/s]
                                                
{'loss': 0.8136, 'grad_norm': 2.4351405499751637, 'learning_rate': 1.9738134206219314e-06, 'epoch': 0.13}

  4%|▍         | 28/630 [00:32<08:54,  1.13it/s]
  5%|▍         | 29/630 [00:33<08:53,  1.13it/s]
                                                
{'loss': 0.7753, 'grad_norm': 2.3169370459042398, 'learning_rate': 1.9705400981996723e-06, 'epoch': 0.14}

  5%|▍         | 29/630 [00:33<08:53,  1.13it/s]
  5%|▍         | 30/630 [00:34<08:52,  1.13it/s]
                                                
{'loss': 0.9033, 'grad_norm': 2.470639031046341, 'learning_rate': 1.967266775777414e-06, 'epoch': 0.14}

  5%|▍         | 30/630 [00:34<08:52,  1.13it/s]
  5%|▍         | 31/630 [00:35<08:50,  1.13it/s]
                                                
{'loss': 0.7245, 'grad_norm': 1.8536513146144267, 'learning_rate': 1.9639934533551554e-06, 'epoch': 0.15}

  5%|▍         | 31/630 [00:35<08:50,  1.13it/s]
  5%|▌         | 32/630 [00:36<08:49,  1.13it/s]
                                                
{'loss': 0.7061, 'grad_norm': 2.059817353150027, 'learning_rate': 1.9607201309328968e-06, 'epoch': 0.15}

  5%|▌         | 32/630 [00:36<08:49,  1.13it/s]
  5%|▌         | 33/630 [00:36<08:52,  1.12it/s]
                                                
{'loss': 0.7368, 'grad_norm': 1.8630420379401598, 'learning_rate': 1.957446808510638e-06, 'epoch': 0.16}

  5%|▌         | 33/630 [00:36<08:52,  1.12it/s]
  5%|▌         | 34/630 [00:37<08:49,  1.13it/s]
                                                
{'loss': 0.6741, 'grad_norm': 1.9744470804667182, 'learning_rate': 1.9541734860883794e-06, 'epoch': 0.16}

  5%|▌         | 34/630 [00:37<08:49,  1.13it/s]
  6%|▌         | 35/630 [00:38<08:47,  1.13it/s]
                                                
{'loss': 0.7006, 'grad_norm': 1.827201125403566, 'learning_rate': 1.950900163666121e-06, 'epoch': 0.17}

  6%|▌         | 35/630 [00:38<08:47,  1.13it/s]
  6%|▌         | 36/630 [00:39<08:46,  1.13it/s]
                                                
{'loss': 0.762, 'grad_norm': 2.023544023502112, 'learning_rate': 1.9476268412438625e-06, 'epoch': 0.17}

  6%|▌         | 36/630 [00:39<08:46,  1.13it/s]
  6%|▌         | 37/630 [00:40<08:44,  1.13it/s]
                                                
{'loss': 0.7211, 'grad_norm': 2.343244517938174, 'learning_rate': 1.944353518821604e-06, 'epoch': 0.18}

  6%|▌         | 37/630 [00:40<08:44,  1.13it/s]
  6%|▌         | 38/630 [00:41<08:42,  1.13it/s]
                                                
{'loss': 0.7418, 'grad_norm': 2.1101056877981086, 'learning_rate': 1.941080196399345e-06, 'epoch': 0.18}

  6%|▌         | 38/630 [00:41<08:42,  1.13it/s]
  6%|▌         | 39/630 [00:42<08:43,  1.13it/s]
                                                
{'loss': 0.7215, 'grad_norm': 1.8153981237196337, 'learning_rate': 1.9378068739770865e-06, 'epoch': 0.19}

  6%|▌         | 39/630 [00:42<08:43,  1.13it/s]
  6%|▋         | 40/630 [00:43<08:42,  1.13it/s]
                                                
{'loss': 0.7423, 'grad_norm': 1.8234134447487422, 'learning_rate': 1.934533551554828e-06, 'epoch': 0.19}

  6%|▋         | 40/630 [00:43<08:42,  1.13it/s]
  7%|▋         | 41/630 [00:43<08:41,  1.13it/s]
                                                
{'loss': 0.7433, 'grad_norm': 2.5608627596772515, 'learning_rate': 1.9312602291325696e-06, 'epoch': 0.2}

  7%|▋         | 41/630 [00:43<08:41,  1.13it/s]
  7%|▋         | 42/630 [00:44<08:40,  1.13it/s]
                                                
{'loss': 0.6625, 'grad_norm': 1.5404430353210856, 'learning_rate': 1.927986906710311e-06, 'epoch': 0.2}

  7%|▋         | 42/630 [00:44<08:40,  1.13it/s]
  7%|▋         | 43/630 [00:45<08:39,  1.13it/s]
                                                
{'loss': 0.8118, 'grad_norm': 3.5266178134939343, 'learning_rate': 1.9247135842880523e-06, 'epoch': 0.2}

  7%|▋         | 43/630 [00:45<08:39,  1.13it/s]
  7%|▋         | 44/630 [00:46<08:39,  1.13it/s]
                                                
{'loss': 0.8032, 'grad_norm': 3.445497008899811, 'learning_rate': 1.9214402618657936e-06, 'epoch': 0.21}

  7%|▋         | 44/630 [00:46<08:39,  1.13it/s]
  7%|▋         | 45/630 [00:47<08:38,  1.13it/s]
                                                
{'loss': 0.7679, 'grad_norm': 2.6125651596699533, 'learning_rate': 1.918166939443535e-06, 'epoch': 0.21}

  7%|▋         | 45/630 [00:47<08:38,  1.13it/s]
  7%|▋         | 46/630 [00:48<08:37,  1.13it/s]
                                                
{'loss': 0.7413, 'grad_norm': 1.906476068358796, 'learning_rate': 1.9148936170212767e-06, 'epoch': 0.22}

  7%|▋         | 46/630 [00:48<08:37,  1.13it/s]
  7%|▋         | 47/630 [00:49<08:36,  1.13it/s]
                                                
{'loss': 0.7321, 'grad_norm': 2.108449091222174, 'learning_rate': 1.911620294599018e-06, 'epoch': 0.22}

  7%|▋         | 47/630 [00:49<08:36,  1.13it/s]
  8%|▊         | 48/630 [00:50<08:36,  1.13it/s]
                                                
{'loss': 0.7425, 'grad_norm': 2.192950727464338, 'learning_rate': 1.9083469721767594e-06, 'epoch': 0.23}

  8%|▊         | 48/630 [00:50<08:36,  1.13it/s]
  8%|▊         | 49/630 [00:51<08:35,  1.13it/s]
                                                
{'loss': 0.7425, 'grad_norm': 2.0695104095248036, 'learning_rate': 1.9050736497545007e-06, 'epoch': 0.23}

  8%|▊         | 49/630 [00:51<08:35,  1.13it/s]
  8%|▊         | 50/630 [00:51<08:33,  1.13it/s]
                                                
{'loss': 0.7167, 'grad_norm': 1.967358648543863, 'learning_rate': 1.901800327332242e-06, 'epoch': 0.24}

  8%|▊         | 50/630 [00:51<08:33,  1.13it/s]
  8%|▊         | 51/630 [00:52<08:32,  1.13it/s]
                                                
{'loss': 0.6701, 'grad_norm': 1.731511963933302, 'learning_rate': 1.8985270049099836e-06, 'epoch': 0.24}

  8%|▊         | 51/630 [00:52<08:32,  1.13it/s]
  8%|▊         | 52/630 [00:53<08:31,  1.13it/s]
                                                
{'loss': 0.7127, 'grad_norm': 1.7433457529752403, 'learning_rate': 1.895253682487725e-06, 'epoch': 0.25}

  8%|▊         | 52/630 [00:53<08:31,  1.13it/s]
  8%|▊         | 53/630 [00:54<08:30,  1.13it/s]
                                                
{'loss': 0.6684, 'grad_norm': 1.7336735625466384, 'learning_rate': 1.8919803600654665e-06, 'epoch': 0.25}

  8%|▊         | 53/630 [00:54<08:30,  1.13it/s]
  9%|▊         | 54/630 [00:55<08:28,  1.13it/s]
                                                
{'loss': 0.7798, 'grad_norm': 2.1287749163400775, 'learning_rate': 1.8887070376432076e-06, 'epoch': 0.26}

  9%|▊         | 54/630 [00:55<08:28,  1.13it/s]
  9%|▊         | 55/630 [00:56<08:28,  1.13it/s]
                                                
{'loss': 0.7385, 'grad_norm': 2.211243274404645, 'learning_rate': 1.8854337152209492e-06, 'epoch': 0.26}

  9%|▊         | 55/630 [00:56<08:28,  1.13it/s]
  9%|▉         | 56/630 [00:57<08:29,  1.13it/s]
                                                
{'loss': 0.7147, 'grad_norm': 1.7830769984730224, 'learning_rate': 1.8821603927986907e-06, 'epoch': 0.27}

  9%|▉         | 56/630 [00:57<08:29,  1.13it/s]
  9%|▉         | 57/630 [00:58<08:28,  1.13it/s]
                                                
{'loss': 0.6731, 'grad_norm': 1.821159563889558, 'learning_rate': 1.8788870703764318e-06, 'epoch': 0.27}

  9%|▉         | 57/630 [00:58<08:28,  1.13it/s]
  9%|▉         | 58/630 [00:59<08:27,  1.13it/s]
                                                
{'loss': 0.7378, 'grad_norm': 1.7563148721084505, 'learning_rate': 1.8756137479541734e-06, 'epoch': 0.28}

  9%|▉         | 58/630 [00:59<08:27,  1.13it/s]
  9%|▉         | 59/630 [00:59<08:29,  1.12it/s]
                                                
{'loss': 0.7663, 'grad_norm': 2.1100907572761707, 'learning_rate': 1.872340425531915e-06, 'epoch': 0.28}

  9%|▉         | 59/630 [00:59<08:29,  1.12it/s]
 10%|▉         | 60/630 [01:00<08:27,  1.12it/s]
                                                
{'loss': 0.8134, 'grad_norm': 2.419144001354513, 'learning_rate': 1.8690671031096563e-06, 'epoch': 0.29}

 10%|▉         | 60/630 [01:00<08:27,  1.12it/s]
 10%|▉         | 61/630 [01:01<08:25,  1.12it/s]
                                                
{'loss': 0.7219, 'grad_norm': 1.7619644083036279, 'learning_rate': 1.8657937806873976e-06, 'epoch': 0.29}

 10%|▉         | 61/630 [01:01<08:25,  1.12it/s]
 10%|▉         | 62/630 [01:02<08:24,  1.13it/s]
                                                
{'loss': 0.6185, 'grad_norm': 1.7563363231954447, 'learning_rate': 1.862520458265139e-06, 'epoch': 0.3}

 10%|▉         | 62/630 [01:02<08:24,  1.13it/s]
 10%|█         | 63/630 [01:03<08:23,  1.13it/s]
                                                
{'loss': 0.7524, 'grad_norm': 1.9852215326528702, 'learning_rate': 1.8592471358428805e-06, 'epoch': 0.3}

 10%|█         | 63/630 [01:03<08:23,  1.13it/s]
 10%|█         | 64/630 [01:04<08:22,  1.13it/s]
                                                
{'loss': 0.7453, 'grad_norm': 2.071140492416366, 'learning_rate': 1.8559738134206218e-06, 'epoch': 0.3}

 10%|█         | 64/630 [01:04<08:22,  1.13it/s]
 10%|█         | 65/630 [01:05<08:21,  1.13it/s]
                                                
{'loss': 0.6954, 'grad_norm': 2.0661357039636346, 'learning_rate': 1.8527004909983632e-06, 'epoch': 0.31}

 10%|█         | 65/630 [01:05<08:21,  1.13it/s]
 10%|█         | 66/630 [01:06<08:19,  1.13it/s]
                                                
{'loss': 0.7823, 'grad_norm': 1.8444581967438847, 'learning_rate': 1.8494271685761047e-06, 'epoch': 0.31}

 10%|█         | 66/630 [01:06<08:19,  1.13it/s]
 11%|█         | 67/630 [01:07<08:18,  1.13it/s]
                                                
{'loss': 0.7677, 'grad_norm': 2.330762751195722, 'learning_rate': 1.8461538461538462e-06, 'epoch': 0.32}

 11%|█         | 67/630 [01:07<08:18,  1.13it/s]
 11%|█         | 68/630 [01:07<08:17,  1.13it/s]
                                                
{'loss': 0.7254, 'grad_norm': 2.0658752875051536, 'learning_rate': 1.8428805237315874e-06, 'epoch': 0.32}

 11%|█         | 68/630 [01:07<08:17,  1.13it/s]
 11%|█         | 69/630 [01:08<08:16,  1.13it/s]
                                                
{'loss': 0.7113, 'grad_norm': 1.8210864642365774, 'learning_rate': 1.839607201309329e-06, 'epoch': 0.33}

 11%|█         | 69/630 [01:08<08:16,  1.13it/s]
 11%|█         | 70/630 [01:09<08:15,  1.13it/s]
                                                
{'loss': 0.6959, 'grad_norm': 1.8199535911250981, 'learning_rate': 1.8363338788870705e-06, 'epoch': 0.33}

 11%|█         | 70/630 [01:09<08:15,  1.13it/s]
 11%|█▏        | 71/630 [01:10<08:13,  1.13it/s]
                                                
{'loss': 0.7237, 'grad_norm': 2.0427378718330576, 'learning_rate': 1.8330605564648116e-06, 'epoch': 0.34}

 11%|█▏        | 71/630 [01:10<08:13,  1.13it/s]
 11%|█▏        | 72/630 [01:11<08:14,  1.13it/s]
                                                
{'loss': 0.6738, 'grad_norm': 2.084581003651615, 'learning_rate': 1.8297872340425531e-06, 'epoch': 0.34}

 11%|█▏        | 72/630 [01:11<08:14,  1.13it/s]
 12%|█▏        | 73/630 [01:12<08:13,  1.13it/s]
                                                
{'loss': 0.707, 'grad_norm': 2.508453624986831, 'learning_rate': 1.8265139116202945e-06, 'epoch': 0.35}

 12%|█▏        | 73/630 [01:12<08:13,  1.13it/s]
 12%|█▏        | 74/630 [01:13<08:12,  1.13it/s]
                                                
{'loss': 0.7785, 'grad_norm': 2.1393898548317622, 'learning_rate': 1.823240589198036e-06, 'epoch': 0.35}

 12%|█▏        | 74/630 [01:13<08:12,  1.13it/s]
 12%|█▏        | 75/630 [01:14<08:11,  1.13it/s]
                                                
{'loss': 0.7391, 'grad_norm': 2.363141232041714, 'learning_rate': 1.8199672667757774e-06, 'epoch': 0.36}

 12%|█▏        | 75/630 [01:14<08:11,  1.13it/s]
 12%|█▏        | 76/630 [01:14<08:11,  1.13it/s]
                                                
{'loss': 0.6894, 'grad_norm': 1.6539146448368278, 'learning_rate': 1.8166939443535187e-06, 'epoch': 0.36}

 12%|█▏        | 76/630 [01:14<08:11,  1.13it/s]
 12%|█▏        | 77/630 [01:15<08:11,  1.13it/s]
                                                
{'loss': 0.7068, 'grad_norm': 1.966828946267386, 'learning_rate': 1.8134206219312602e-06, 'epoch': 0.37}

 12%|█▏        | 77/630 [01:15<08:11,  1.13it/s]
 12%|█▏        | 78/630 [01:16<08:09,  1.13it/s]
                                                
{'loss': 0.7907, 'grad_norm': 2.107506772447179, 'learning_rate': 1.8101472995090016e-06, 'epoch': 0.37}

 12%|█▏        | 78/630 [01:16<08:09,  1.13it/s]
 13%|█▎        | 79/630 [01:17<08:08,  1.13it/s]
                                                
{'loss': 0.6455, 'grad_norm': 1.728166835248769, 'learning_rate': 1.806873977086743e-06, 'epoch': 0.38}

 13%|█▎        | 79/630 [01:17<08:08,  1.13it/s]
 13%|█▎        | 80/630 [01:18<08:07,  1.13it/s]
                                                
{'loss': 0.7045, 'grad_norm': 1.6378504578121023, 'learning_rate': 1.8036006546644845e-06, 'epoch': 0.38}

 13%|█▎        | 80/630 [01:18<08:07,  1.13it/s]
 13%|█▎        | 81/630 [01:19<08:06,  1.13it/s]
                                                
{'loss': 0.8093, 'grad_norm': 2.3499087609204925, 'learning_rate': 1.8003273322422258e-06, 'epoch': 0.39}

 13%|█▎        | 81/630 [01:19<08:06,  1.13it/s]
 13%|█▎        | 82/630 [01:20<08:07,  1.13it/s]
                                                
{'loss': 0.7405, 'grad_norm': 2.0385960125567912, 'learning_rate': 1.7970540098199671e-06, 'epoch': 0.39}

 13%|█▎        | 82/630 [01:20<08:07,  1.13it/s]
 13%|█▎        | 83/630 [01:21<08:05,  1.13it/s]
                                                
{'loss': 0.679, 'grad_norm': 1.9845087797532266, 'learning_rate': 1.7937806873977087e-06, 'epoch': 0.4}

 13%|█▎        | 83/630 [01:21<08:05,  1.13it/s]
 13%|█▎        | 84/630 [01:22<08:04,  1.13it/s]
                                                
{'loss': 0.6709, 'grad_norm': 1.6380683584388673, 'learning_rate': 1.79050736497545e-06, 'epoch': 0.4}

 13%|█▎        | 84/630 [01:22<08:04,  1.13it/s]
 13%|█▎        | 85/630 [01:22<08:03,  1.13it/s]
                                                
{'loss': 0.7155, 'grad_norm': 1.7421015169582303, 'learning_rate': 1.7872340425531913e-06, 'epoch': 0.4}

 13%|█▎        | 85/630 [01:22<08:03,  1.13it/s]
 14%|█▎        | 86/630 [01:23<08:02,  1.13it/s]
                                                
{'loss': 0.6974, 'grad_norm': 1.9296367165558286, 'learning_rate': 1.7839607201309329e-06, 'epoch': 0.41}

 14%|█▎        | 86/630 [01:23<08:02,  1.13it/s]
 14%|█▍        | 87/630 [01:24<08:03,  1.12it/s]
                                                
{'loss': 0.756, 'grad_norm': 2.0669896615114776, 'learning_rate': 1.7806873977086742e-06, 'epoch': 0.41}

 14%|█▍        | 87/630 [01:24<08:03,  1.12it/s]
 14%|█▍        | 88/630 [01:25<08:01,  1.12it/s]
                                                
{'loss': 0.6763, 'grad_norm': 1.7645118873312178, 'learning_rate': 1.7774140752864158e-06, 'epoch': 0.42}

 14%|█▍        | 88/630 [01:25<08:01,  1.12it/s]
 14%|█▍        | 89/630 [01:26<08:00,  1.13it/s]
                                                
{'loss': 0.7021, 'grad_norm': 2.645828183237106, 'learning_rate': 1.7741407528641569e-06, 'epoch': 0.42}

 14%|█▍        | 89/630 [01:26<08:00,  1.13it/s]
 14%|█▍        | 90/630 [01:27<08:00,  1.12it/s]
                                                
{'loss': 0.6939, 'grad_norm': 1.8570914293631637, 'learning_rate': 1.7708674304418984e-06, 'epoch': 0.43}

 14%|█▍        | 90/630 [01:27<08:00,  1.12it/s]
 14%|█▍        | 91/630 [01:28<07:58,  1.13it/s]
                                                
{'loss': 0.811, 'grad_norm': 1.9987120976168242, 'learning_rate': 1.76759410801964e-06, 'epoch': 0.43}

 14%|█▍        | 91/630 [01:28<07:58,  1.13it/s]
 15%|█▍        | 92/630 [01:29<07:57,  1.13it/s]
                                                
{'loss': 0.7177, 'grad_norm': 2.5738214800058192, 'learning_rate': 1.764320785597381e-06, 'epoch': 0.44}

 15%|█▍        | 92/630 [01:29<07:57,  1.13it/s]
 15%|█▍        | 93/630 [01:30<07:55,  1.13it/s]
                                                
{'loss': 0.7703, 'grad_norm': 2.413963030285759, 'learning_rate': 1.7610474631751227e-06, 'epoch': 0.44}

 15%|█▍        | 93/630 [01:30<07:55,  1.13it/s]
 15%|█▍        | 94/630 [01:30<07:56,  1.13it/s]
                                                
{'loss': 0.7694, 'grad_norm': 1.6999213263358464, 'learning_rate': 1.7577741407528642e-06, 'epoch': 0.45}

 15%|█▍        | 94/630 [01:30<07:56,  1.13it/s]
 15%|█▌        | 95/630 [01:31<07:55,  1.12it/s]
                                                
{'loss': 0.6346, 'grad_norm': 1.8708691194346696, 'learning_rate': 1.7545008183306055e-06, 'epoch': 0.45}

 15%|█▌        | 95/630 [01:31<07:55,  1.12it/s]
 15%|█▌        | 96/630 [01:32<07:53,  1.13it/s]
                                                
{'loss': 0.7817, 'grad_norm': 1.965224804391073, 'learning_rate': 1.7512274959083469e-06, 'epoch': 0.46}

 15%|█▌        | 96/630 [01:32<07:53,  1.13it/s]
 15%|█▌        | 97/630 [01:33<07:52,  1.13it/s]
                                                
{'loss': 0.7473, 'grad_norm': 1.8241634933237174, 'learning_rate': 1.7479541734860884e-06, 'epoch': 0.46}

 15%|█▌        | 97/630 [01:33<07:52,  1.13it/s]
 16%|█▌        | 98/630 [01:34<07:52,  1.12it/s]
                                                
{'loss': 0.6786, 'grad_norm': 1.9053659526695923, 'learning_rate': 1.7446808510638297e-06, 'epoch': 0.47}

 16%|█▌        | 98/630 [01:34<07:52,  1.12it/s]
 16%|█▌        | 99/630 [01:35<07:52,  1.12it/s]
                                                
{'loss': 0.7125, 'grad_norm': 1.661984448461792, 'learning_rate': 1.741407528641571e-06, 'epoch': 0.47}

 16%|█▌        | 99/630 [01:35<07:52,  1.12it/s]
 16%|█▌        | 100/630 [01:36<07:51,  1.13it/s]
                                                 
{'loss': 0.7218, 'grad_norm': 1.9971038627812583, 'learning_rate': 1.7381342062193124e-06, 'epoch': 0.48}

 16%|█▌        | 100/630 [01:36<07:51,  1.13it/s]
 16%|█▌        | 101/630 [01:37<07:50,  1.12it/s]
                                                 
{'loss': 0.6977, 'grad_norm': 1.7715392577892348, 'learning_rate': 1.734860883797054e-06, 'epoch': 0.48}

 16%|█▌        | 101/630 [01:37<07:50,  1.12it/s]
 16%|█▌        | 102/630 [01:38<07:50,  1.12it/s]
                                                 
{'loss': 0.708, 'grad_norm': 2.0618788508822488, 'learning_rate': 1.7315875613747955e-06, 'epoch': 0.49}

 16%|█▌        | 102/630 [01:38<07:50,  1.12it/s]
 16%|█▋        | 103/630 [01:38<07:48,  1.12it/s]
                                                 
{'loss': 0.7829, 'grad_norm': 2.3811340735937017, 'learning_rate': 1.7283142389525366e-06, 'epoch': 0.49}

 16%|█▋        | 103/630 [01:38<07:48,  1.12it/s]
 17%|█▋        | 104/630 [01:39<07:47,  1.13it/s]
                                                 
{'loss': 0.742, 'grad_norm': 1.9002686505267228, 'learning_rate': 1.7250409165302782e-06, 'epoch': 0.5}

 17%|█▋        | 104/630 [01:39<07:47,  1.13it/s]
 17%|█▋        | 105/630 [01:40<07:46,  1.13it/s]
                                                 
{'loss': 0.6709, 'grad_norm': 1.8778667767440766, 'learning_rate': 1.7217675941080197e-06, 'epoch': 0.5}

 17%|█▋        | 105/630 [01:40<07:46,  1.13it/s]
 17%|█▋        | 106/630 [01:41<07:44,  1.13it/s]
                                                 
{'loss': 0.6508, 'grad_norm': 2.3355944864592417, 'learning_rate': 1.7184942716857609e-06, 'epoch': 0.5}

 17%|█▋        | 106/630 [01:41<07:44,  1.13it/s]
 17%|█▋        | 107/630 [01:42<07:44,  1.13it/s]
                                                 
{'loss': 0.715, 'grad_norm': 1.921134083094253, 'learning_rate': 1.7152209492635024e-06, 'epoch': 0.51}

 17%|█▋        | 107/630 [01:42<07:44,  1.13it/s]
 17%|█▋        | 108/630 [01:43<07:45,  1.12it/s]
                                                 
{'loss': 0.705, 'grad_norm': 2.0443802074465585, 'learning_rate': 1.7119476268412437e-06, 'epoch': 0.51}

 17%|█▋        | 108/630 [01:43<07:45,  1.12it/s]
 17%|█▋        | 109/630 [01:44<07:44,  1.12it/s]
                                                 
{'loss': 0.762, 'grad_norm': 1.7142039434630412, 'learning_rate': 1.7086743044189853e-06, 'epoch': 0.52}

 17%|█▋        | 109/630 [01:44<07:44,  1.12it/s]
 17%|█▋        | 110/630 [01:45<07:41,  1.13it/s]
                                                 
{'loss': 0.8545, 'grad_norm': 1.9929413574609938, 'learning_rate': 1.7054009819967266e-06, 'epoch': 0.52}

 17%|█▋        | 110/630 [01:45<07:41,  1.13it/s]
 18%|█▊        | 111/630 [01:46<07:40,  1.13it/s]
                                                 
{'loss': 0.6769, 'grad_norm': 1.931954312830614, 'learning_rate': 1.702127659574468e-06, 'epoch': 0.53}

 18%|█▊        | 111/630 [01:46<07:40,  1.13it/s]
 18%|█▊        | 112/630 [01:46<07:40,  1.13it/s]
                                                 
{'loss': 0.6177, 'grad_norm': 1.831930239705078, 'learning_rate': 1.6988543371522095e-06, 'epoch': 0.53}

 18%|█▊        | 112/630 [01:46<07:40,  1.13it/s]
 18%|█▊        | 113/630 [01:47<07:38,  1.13it/s]
                                                 
{'loss': 0.7311, 'grad_norm': 2.406449617119195, 'learning_rate': 1.6955810147299508e-06, 'epoch': 0.54}

 18%|█▊        | 113/630 [01:47<07:38,  1.13it/s]
 18%|█▊        | 114/630 [01:48<07:39,  1.12it/s]
                                                 
{'loss': 0.7584, 'grad_norm': 1.966817602848419, 'learning_rate': 1.6923076923076922e-06, 'epoch': 0.54}

 18%|█▊        | 114/630 [01:48<07:39,  1.12it/s]
 18%|█▊        | 115/630 [01:49<07:37,  1.13it/s]
                                                 
{'loss': 0.6931, 'grad_norm': 1.8483424297683768, 'learning_rate': 1.6890343698854337e-06, 'epoch': 0.55}

 18%|█▊        | 115/630 [01:49<07:37,  1.13it/s]
 18%|█▊        | 116/630 [01:50<07:36,  1.13it/s]
                                                 
{'loss': 0.7228, 'grad_norm': 1.7715848847806708, 'learning_rate': 1.6857610474631753e-06, 'epoch': 0.55}

 18%|█▊        | 116/630 [01:50<07:36,  1.13it/s]
 19%|█▊        | 117/630 [01:51<07:36,  1.12it/s]
                                                 
{'loss': 0.6919, 'grad_norm': 1.949324045880565, 'learning_rate': 1.6824877250409164e-06, 'epoch': 0.56}

 19%|█▊        | 117/630 [01:51<07:36,  1.12it/s]
 19%|█▊        | 118/630 [01:52<07:35,  1.13it/s]
                                                 
{'loss': 0.6802, 'grad_norm': 1.7675166954177153, 'learning_rate': 1.679214402618658e-06, 'epoch': 0.56}

 19%|█▊        | 118/630 [01:52<07:35,  1.13it/s]
 19%|█▉        | 119/630 [01:53<07:33,  1.13it/s]
                                                 
{'loss': 0.6201, 'grad_norm': 1.6489927593529372, 'learning_rate': 1.6759410801963993e-06, 'epoch': 0.57}

 19%|█▉        | 119/630 [01:53<07:33,  1.13it/s]
 19%|█▉        | 120/630 [01:54<07:32,  1.13it/s]
                                                 
{'loss': 0.7222, 'grad_norm': 2.180825600248101, 'learning_rate': 1.6726677577741406e-06, 'epoch': 0.57}

 19%|█▉        | 120/630 [01:54<07:32,  1.13it/s]
 19%|█▉        | 121/630 [01:54<07:32,  1.12it/s]
                                                 
{'loss': 0.6578, 'grad_norm': 1.960151313762446, 'learning_rate': 1.6693944353518821e-06, 'epoch': 0.58}

 19%|█▉        | 121/630 [01:54<07:32,  1.12it/s]
 19%|█▉        | 122/630 [01:55<07:31,  1.13it/s]
                                                 
{'loss': 0.7039, 'grad_norm': 2.402202565278315, 'learning_rate': 1.6661211129296235e-06, 'epoch': 0.58}

 19%|█▉        | 122/630 [01:55<07:31,  1.13it/s]
 20%|█▉        | 123/630 [01:56<07:30,  1.13it/s]
                                                 
{'loss': 0.7669, 'grad_norm': 1.9671932844002353, 'learning_rate': 1.6628477905073648e-06, 'epoch': 0.59}

 20%|█▉        | 123/630 [01:56<07:30,  1.13it/s]
 20%|█▉        | 124/630 [01:57<07:28,  1.13it/s]
                                                 
{'loss': 0.6943, 'grad_norm': 1.9281925400700053, 'learning_rate': 1.6595744680851064e-06, 'epoch': 0.59}

 20%|█▉        | 124/630 [01:57<07:28,  1.13it/s]
 20%|█▉        | 125/630 [01:58<07:28,  1.13it/s]
                                                 
{'loss': 0.6662, 'grad_norm': 1.9174179558183055, 'learning_rate': 1.6563011456628477e-06, 'epoch': 0.6}

 20%|█▉        | 125/630 [01:58<07:28,  1.13it/s]
 20%|██        | 126/630 [01:59<07:27,  1.13it/s]
                                                 
{'loss': 0.61, 'grad_norm': 1.8075610754938785, 'learning_rate': 1.6530278232405892e-06, 'epoch': 0.6}

 20%|██        | 126/630 [01:59<07:27,  1.13it/s]
 20%|██        | 127/630 [02:00<07:26,  1.13it/s]
                                                 
{'loss': 0.7225, 'grad_norm': 2.083570462610763, 'learning_rate': 1.6497545008183304e-06, 'epoch': 0.6}

 20%|██        | 127/630 [02:00<07:26,  1.13it/s]
 20%|██        | 128/630 [02:01<07:25,  1.13it/s]
                                                 
{'loss': 0.7383, 'grad_norm': 1.8802516050783344, 'learning_rate': 1.646481178396072e-06, 'epoch': 0.61}

 20%|██        | 128/630 [02:01<07:25,  1.13it/s]
 20%|██        | 129/630 [02:02<07:24,  1.13it/s]
                                                 
{'loss': 0.7445, 'grad_norm': 1.9140537120375964, 'learning_rate': 1.6432078559738135e-06, 'epoch': 0.61}

 20%|██        | 129/630 [02:02<07:24,  1.13it/s]
 21%|██        | 130/630 [02:02<07:24,  1.13it/s]
                                                 
{'loss': 0.7144, 'grad_norm': 1.9132937069580584, 'learning_rate': 1.6399345335515546e-06, 'epoch': 0.62}

 21%|██        | 130/630 [02:02<07:24,  1.13it/s]
 21%|██        | 131/630 [02:03<07:23,  1.13it/s]
                                                 
{'loss': 0.7089, 'grad_norm': 2.0102700825252375, 'learning_rate': 1.6366612111292961e-06, 'epoch': 0.62}

 21%|██        | 131/630 [02:03<07:23,  1.13it/s]
 21%|██        | 132/630 [02:04<07:21,  1.13it/s]
                                                 
{'loss': 0.7267, 'grad_norm': 1.9410821453177995, 'learning_rate': 1.6333878887070377e-06, 'epoch': 0.63}

 21%|██        | 132/630 [02:04<07:21,  1.13it/s]
 21%|██        | 133/630 [02:05<07:20,  1.13it/s]
                                                 
{'loss': 0.6102, 'grad_norm': 1.9569892965613873, 'learning_rate': 1.630114566284779e-06, 'epoch': 0.63}

 21%|██        | 133/630 [02:05<07:20,  1.13it/s]
 21%|██▏       | 134/630 [02:06<07:18,  1.13it/s]
                                                 
{'loss': 0.6395, 'grad_norm': 1.8223368366581854, 'learning_rate': 1.6268412438625203e-06, 'epoch': 0.64}

 21%|██▏       | 134/630 [02:06<07:18,  1.13it/s]
 21%|██▏       | 135/630 [02:07<07:17,  1.13it/s]
                                                 
{'loss': 0.7229, 'grad_norm': 1.945412298064907, 'learning_rate': 1.6235679214402617e-06, 'epoch': 0.64}

 21%|██▏       | 135/630 [02:07<07:17,  1.13it/s]
 22%|██▏       | 136/630 [02:08<07:16,  1.13it/s]
                                                 
{'loss': 0.707, 'grad_norm': 2.33823753823234, 'learning_rate': 1.6202945990180032e-06, 'epoch': 0.65}

 22%|██▏       | 136/630 [02:08<07:16,  1.13it/s]
 22%|██▏       | 137/630 [02:09<07:16,  1.13it/s]
                                                 
{'loss': 0.7371, 'grad_norm': 2.2747386682810267, 'learning_rate': 1.6170212765957446e-06, 'epoch': 0.65}

 22%|██▏       | 137/630 [02:09<07:16,  1.13it/s]
 22%|██▏       | 138/630 [02:10<07:16,  1.13it/s]
                                                 
{'loss': 0.7063, 'grad_norm': 2.31988635726732, 'learning_rate': 1.613747954173486e-06, 'epoch': 0.66}

 22%|██▏       | 138/630 [02:10<07:16,  1.13it/s]
 22%|██▏       | 139/630 [02:10<07:15,  1.13it/s]
                                                 
{'loss': 0.6972, 'grad_norm': 2.3231396867159324, 'learning_rate': 1.6104746317512274e-06, 'epoch': 0.66}

 22%|██▏       | 139/630 [02:10<07:15,  1.13it/s]
 22%|██▏       | 140/630 [02:11<07:15,  1.13it/s]
                                                 
{'loss': 0.6945, 'grad_norm': 1.8752708986083395, 'learning_rate': 1.607201309328969e-06, 'epoch': 0.67}

 22%|██▏       | 140/630 [02:11<07:15,  1.13it/s]
 22%|██▏       | 141/630 [02:12<07:14,  1.13it/s]
                                                 
{'loss': 0.6392, 'grad_norm': 1.8411005610366553, 'learning_rate': 1.6039279869067101e-06, 'epoch': 0.67}

 22%|██▏       | 141/630 [02:12<07:14,  1.13it/s]
 23%|██▎       | 142/630 [02:13<07:13,  1.13it/s]
                                                 
{'loss': 0.7014, 'grad_norm': 2.063509018060826, 'learning_rate': 1.6006546644844517e-06, 'epoch': 0.68}

 23%|██▎       | 142/630 [02:13<07:13,  1.13it/s]
 23%|██▎       | 143/630 [02:14<07:11,  1.13it/s]
                                                 
{'loss': 0.7422, 'grad_norm': 2.5834492862045377, 'learning_rate': 1.5973813420621932e-06, 'epoch': 0.68}

 23%|██▎       | 143/630 [02:14<07:11,  1.13it/s]
 23%|██▎       | 144/630 [02:15<07:11,  1.13it/s]
                                                 
{'loss': 0.7245, 'grad_norm': 1.856486274162024, 'learning_rate': 1.5941080196399343e-06, 'epoch': 0.69}

 23%|██▎       | 144/630 [02:15<07:11,  1.13it/s]
 23%|██▎       | 145/630 [02:16<07:10,  1.13it/s]
                                                 
{'loss': 0.6784, 'grad_norm': 2.1140464472348435, 'learning_rate': 1.5908346972176759e-06, 'epoch': 0.69}

 23%|██▎       | 145/630 [02:16<07:10,  1.13it/s]
 23%|██▎       | 146/630 [02:17<07:10,  1.12it/s]
                                                 
{'loss': 0.7457, 'grad_norm': 2.0818177875214805, 'learning_rate': 1.5875613747954172e-06, 'epoch': 0.7}

 23%|██▎       | 146/630 [02:17<07:10,  1.12it/s]
 23%|██▎       | 147/630 [02:18<07:09,  1.12it/s]
                                                 
{'loss': 0.7353, 'grad_norm': 1.770070421175992, 'learning_rate': 1.5842880523731588e-06, 'epoch': 0.7}

 23%|██▎       | 147/630 [02:18<07:09,  1.12it/s]
 23%|██▎       | 148/630 [02:18<07:08,  1.13it/s]
                                                 
{'loss': 0.7062, 'grad_norm': 2.067242595855171, 'learning_rate': 1.5810147299509e-06, 'epoch': 0.7}

 23%|██▎       | 148/630 [02:18<07:08,  1.13it/s]
 24%|██▎       | 149/630 [02:19<07:07,  1.13it/s]
                                                 
{'loss': 0.5949, 'grad_norm': 1.9777408798459466, 'learning_rate': 1.5777414075286414e-06, 'epoch': 0.71}

 24%|██▎       | 149/630 [02:19<07:07,  1.13it/s]
 24%|██▍       | 150/630 [02:20<07:05,  1.13it/s]
                                                 
{'loss': 0.6907, 'grad_norm': 1.6008904546435914, 'learning_rate': 1.574468085106383e-06, 'epoch': 0.71}

 24%|██▍       | 150/630 [02:20<07:05,  1.13it/s]
 24%|██▍       | 151/630 [02:21<07:04,  1.13it/s]
                                                 
{'loss': 0.8104, 'grad_norm': 2.150272895568184, 'learning_rate': 1.5711947626841243e-06, 'epoch': 0.72}

 24%|██▍       | 151/630 [02:21<07:04,  1.13it/s]
 24%|██▍       | 152/630 [02:22<07:03,  1.13it/s]
                                                 
{'loss': 0.6842, 'grad_norm': 2.154091775018266, 'learning_rate': 1.5679214402618656e-06, 'epoch': 0.72}

 24%|██▍       | 152/630 [02:22<07:03,  1.13it/s]
 24%|██▍       | 153/630 [02:23<07:02,  1.13it/s]
                                                 
{'loss': 0.7056, 'grad_norm': 1.8045512461742608, 'learning_rate': 1.5646481178396072e-06, 'epoch': 0.73}

 24%|██▍       | 153/630 [02:23<07:02,  1.13it/s]
 24%|██▍       | 154/630 [02:24<07:02,  1.13it/s]
                                                 
{'loss': 0.674, 'grad_norm': 1.8891420599472257, 'learning_rate': 1.5613747954173485e-06, 'epoch': 0.73}

 24%|██▍       | 154/630 [02:24<07:02,  1.13it/s]
 25%|██▍       | 155/630 [02:25<07:01,  1.13it/s]
                                                 
{'loss': 0.7141, 'grad_norm': 1.881418198177636, 'learning_rate': 1.5581014729950899e-06, 'epoch': 0.74}

 25%|██▍       | 155/630 [02:25<07:01,  1.13it/s]
 25%|██▍       | 156/630 [02:26<07:00,  1.13it/s]
                                                 
{'loss': 0.6333, 'grad_norm': 2.089013638534455, 'learning_rate': 1.5548281505728314e-06, 'epoch': 0.74}

 25%|██▍       | 156/630 [02:26<07:00,  1.13it/s]
 25%|██▍       | 157/630 [02:26<06:58,  1.13it/s]
                                                 
{'loss': 0.806, 'grad_norm': 2.3156394279900643, 'learning_rate': 1.5515548281505727e-06, 'epoch': 0.75}

 25%|██▍       | 157/630 [02:26<06:58,  1.13it/s]
 25%|██▌       | 158/630 [02:27<06:58,  1.13it/s]
                                                 
{'loss': 0.6751, 'grad_norm': 1.9131200665568624, 'learning_rate': 1.548281505728314e-06, 'epoch': 0.75}

 25%|██▌       | 158/630 [02:27<06:58,  1.13it/s]
 25%|██▌       | 159/630 [02:28<06:57,  1.13it/s]
                                                 
{'loss': 0.6551, 'grad_norm': 1.8301730773155995, 'learning_rate': 1.5450081833060556e-06, 'epoch': 0.76}

 25%|██▌       | 159/630 [02:28<06:57,  1.13it/s]
 25%|██▌       | 160/630 [02:29<06:56,  1.13it/s]
                                                 
{'loss': 0.7354, 'grad_norm': 2.180988354646389, 'learning_rate': 1.541734860883797e-06, 'epoch': 0.76}

 25%|██▌       | 160/630 [02:29<06:56,  1.13it/s]
 26%|██▌       | 161/630 [02:30<06:55,  1.13it/s]
                                                 
{'loss': 0.6929, 'grad_norm': 2.025516749537614, 'learning_rate': 1.5384615384615385e-06, 'epoch': 0.77}

 26%|██▌       | 161/630 [02:30<06:55,  1.13it/s]
 26%|██▌       | 162/630 [02:31<06:55,  1.13it/s]
                                                 
{'loss': 0.7894, 'grad_norm': 2.502654702652084, 'learning_rate': 1.5351882160392796e-06, 'epoch': 0.77}

 26%|██▌       | 162/630 [02:31<06:55,  1.13it/s]
 26%|██▌       | 163/630 [02:32<06:54,  1.13it/s]
                                                 
{'loss': 0.6534, 'grad_norm': 2.0141657371848614, 'learning_rate': 1.5319148936170212e-06, 'epoch': 0.78}

 26%|██▌       | 163/630 [02:32<06:54,  1.13it/s]
 26%|██▌       | 164/630 [02:33<06:53,  1.13it/s]
                                                 
{'loss': 0.6455, 'grad_norm': 1.9382549170514463, 'learning_rate': 1.5286415711947627e-06, 'epoch': 0.78}

 26%|██▌       | 164/630 [02:33<06:53,  1.13it/s]
 26%|██▌       | 165/630 [02:34<06:52,  1.13it/s]
                                                 
{'loss': 0.6897, 'grad_norm': 1.864433804519569, 'learning_rate': 1.5253682487725038e-06, 'epoch': 0.79}

 26%|██▌       | 165/630 [02:34<06:52,  1.13it/s]
 26%|██▋       | 166/630 [02:34<06:51,  1.13it/s]
                                                 
{'loss': 0.7731, 'grad_norm': 3.0140821986961, 'learning_rate': 1.5220949263502454e-06, 'epoch': 0.79}

 26%|██▋       | 166/630 [02:34<06:51,  1.13it/s]
 27%|██▋       | 167/630 [02:35<06:50,  1.13it/s]
                                                 
{'loss': 0.7291, 'grad_norm': 1.9795217649615888, 'learning_rate': 1.518821603927987e-06, 'epoch': 0.8}

 27%|██▋       | 167/630 [02:35<06:50,  1.13it/s]
 27%|██▋       | 168/630 [02:36<06:50,  1.13it/s]
                                                 
{'loss': 0.7235, 'grad_norm': 1.9909374431339537, 'learning_rate': 1.5155482815057283e-06, 'epoch': 0.8}

 27%|██▋       | 168/630 [02:36<06:50,  1.13it/s]
 27%|██▋       | 169/630 [02:37<06:48,  1.13it/s]
                                                 
{'loss': 0.6772, 'grad_norm': 1.8860765033659508, 'learning_rate': 1.5122749590834696e-06, 'epoch': 0.8}

 27%|██▋       | 169/630 [02:37<06:48,  1.13it/s]
 27%|██▋       | 170/630 [02:38<06:47,  1.13it/s]
                                                 
{'loss': 0.7096, 'grad_norm': 1.974697614333498, 'learning_rate': 1.5090016366612112e-06, 'epoch': 0.81}

 27%|██▋       | 170/630 [02:38<06:47,  1.13it/s]
 27%|██▋       | 171/630 [02:39<06:47,  1.13it/s]
                                                 
{'loss': 0.6675, 'grad_norm': 1.7819532567892418, 'learning_rate': 1.5057283142389525e-06, 'epoch': 0.81}

 27%|██▋       | 171/630 [02:39<06:47,  1.13it/s]
 27%|██▋       | 172/630 [02:40<06:46,  1.13it/s]
                                                 
{'loss': 0.6586, 'grad_norm': 1.9178413537995382, 'learning_rate': 1.5024549918166938e-06, 'epoch': 0.82}

 27%|██▋       | 172/630 [02:40<06:46,  1.13it/s]
 27%|██▋       | 173/630 [02:41<06:45,  1.13it/s]
                                                 
{'loss': 0.6746, 'grad_norm': 1.8351114527419077, 'learning_rate': 1.4991816693944352e-06, 'epoch': 0.82}

 27%|██▋       | 173/630 [02:41<06:45,  1.13it/s]
 28%|██▊       | 174/630 [02:41<06:45,  1.13it/s]
                                                 
{'loss': 0.6007, 'grad_norm': 1.7394839583922135, 'learning_rate': 1.4959083469721767e-06, 'epoch': 0.83}

 28%|██▊       | 174/630 [02:41<06:45,  1.13it/s]
 28%|██▊       | 175/630 [02:42<06:44,  1.13it/s]
                                                 
{'loss': 0.6914, 'grad_norm': 1.9046514433444541, 'learning_rate': 1.4926350245499183e-06, 'epoch': 0.83}

 28%|██▊       | 175/630 [02:42<06:44,  1.13it/s]
 28%|██▊       | 176/630 [02:43<06:43,  1.13it/s]
                                                 
{'loss': 0.6424, 'grad_norm': 2.036269882697524, 'learning_rate': 1.4893617021276594e-06, 'epoch': 0.84}

 28%|██▊       | 176/630 [02:43<06:43,  1.13it/s]
 28%|██▊       | 177/630 [02:44<06:41,  1.13it/s]
                                                 
{'loss': 0.6464, 'grad_norm': 1.8661101255423973, 'learning_rate': 1.486088379705401e-06, 'epoch': 0.84}

 28%|██▊       | 177/630 [02:44<06:41,  1.13it/s]
 28%|██▊       | 178/630 [02:45<06:40,  1.13it/s]
                                                 
{'loss': 0.6966, 'grad_norm': 2.0045063706669546, 'learning_rate': 1.4828150572831425e-06, 'epoch': 0.85}

 28%|██▊       | 178/630 [02:45<06:40,  1.13it/s]
 28%|██▊       | 179/630 [02:46<06:39,  1.13it/s]
                                                 
{'loss': 0.7412, 'grad_norm': 1.9905898824831931, 'learning_rate': 1.4795417348608836e-06, 'epoch': 0.85}

 28%|██▊       | 179/630 [02:46<06:39,  1.13it/s]
 29%|██▊       | 180/630 [02:47<06:38,  1.13it/s]
                                                 
{'loss': 0.6925, 'grad_norm': 2.0303357358580723, 'learning_rate': 1.4762684124386251e-06, 'epoch': 0.86}

 29%|██▊       | 180/630 [02:47<06:38,  1.13it/s]
 29%|██▊       | 181/630 [02:48<06:38,  1.13it/s]
                                                 
{'loss': 0.787, 'grad_norm': 2.188488984617649, 'learning_rate': 1.4729950900163665e-06, 'epoch': 0.86}

 29%|██▊       | 181/630 [02:48<06:38,  1.13it/s]
 29%|██▉       | 182/630 [02:49<06:37,  1.13it/s]
                                                 
{'loss': 0.6852, 'grad_norm': 2.00598922271954, 'learning_rate': 1.469721767594108e-06, 'epoch': 0.87}

 29%|██▉       | 182/630 [02:49<06:37,  1.13it/s]
 29%|██▉       | 183/630 [02:49<06:37,  1.13it/s]
                                                 
{'loss': 0.7585, 'grad_norm': 2.4940848950907073, 'learning_rate': 1.4664484451718494e-06, 'epoch': 0.87}

 29%|██▉       | 183/630 [02:49<06:37,  1.13it/s]
 29%|██▉       | 184/630 [02:50<06:35,  1.13it/s]
                                                 
{'loss': 0.6707, 'grad_norm': 2.0962838384630147, 'learning_rate': 1.4631751227495907e-06, 'epoch': 0.88}

 29%|██▉       | 184/630 [02:50<06:35,  1.13it/s]
 29%|██▉       | 185/630 [02:51<06:34,  1.13it/s]
                                                 
{'loss': 0.6573, 'grad_norm': 1.7251211717620798, 'learning_rate': 1.4599018003273322e-06, 'epoch': 0.88}

 29%|██▉       | 185/630 [02:51<06:34,  1.13it/s]
 30%|██▉       | 186/630 [02:52<06:33,  1.13it/s]
                                                 
{'loss': 0.6744, 'grad_norm': 1.8747962283876993, 'learning_rate': 1.4566284779050736e-06, 'epoch': 0.89}

 30%|██▉       | 186/630 [02:52<06:33,  1.13it/s]
 30%|██▉       | 187/630 [02:53<06:32,  1.13it/s]
                                                 
{'loss': 0.6639, 'grad_norm': 1.889092233316694, 'learning_rate': 1.453355155482815e-06, 'epoch': 0.89}

 30%|██▉       | 187/630 [02:53<06:32,  1.13it/s]
 30%|██▉       | 188/630 [02:54<06:32,  1.13it/s]
                                                 
{'loss': 0.7074, 'grad_norm': 1.9247264524139394, 'learning_rate': 1.4500818330605565e-06, 'epoch': 0.9}

 30%|██▉       | 188/630 [02:54<06:32,  1.13it/s]
 30%|███       | 189/630 [02:55<06:31,  1.13it/s]
                                                 
{'loss': 0.6985, 'grad_norm': 1.726969530489022, 'learning_rate': 1.446808510638298e-06, 'epoch': 0.9}

 30%|███       | 189/630 [02:55<06:31,  1.13it/s]
 30%|███       | 190/630 [02:56<06:30,  1.13it/s]
                                                 
{'loss': 0.7375, 'grad_norm': 1.9837158813485283, 'learning_rate': 1.4435351882160391e-06, 'epoch': 0.9}

 30%|███       | 190/630 [02:56<06:30,  1.13it/s]
 30%|███       | 191/630 [02:57<06:29,  1.13it/s]
                                                 
{'loss': 0.66, 'grad_norm': 1.8577057460678135, 'learning_rate': 1.4402618657937807e-06, 'epoch': 0.91}

 30%|███       | 191/630 [02:57<06:29,  1.13it/s]
 30%|███       | 192/630 [02:57<06:29,  1.13it/s]
                                                 
{'loss': 0.6315, 'grad_norm': 2.0048264923364707, 'learning_rate': 1.436988543371522e-06, 'epoch': 0.91}

 30%|███       | 192/630 [02:57<06:29,  1.13it/s]
 31%|███       | 193/630 [02:58<06:27,  1.13it/s]
                                                 
{'loss': 0.6662, 'grad_norm': 2.137592573924741, 'learning_rate': 1.4337152209492633e-06, 'epoch': 0.92}

 31%|███       | 193/630 [02:58<06:27,  1.13it/s]
 31%|███       | 194/630 [02:59<06:26,  1.13it/s]
                                                 
{'loss': 0.629, 'grad_norm': 1.8534961311290796, 'learning_rate': 1.4304418985270049e-06, 'epoch': 0.92}

 31%|███       | 194/630 [02:59<06:26,  1.13it/s]
 31%|███       | 195/630 [03:00<06:26,  1.13it/s]
                                                 
{'loss': 0.5961, 'grad_norm': 1.6032601108501923, 'learning_rate': 1.4271685761047462e-06, 'epoch': 0.93}

 31%|███       | 195/630 [03:00<06:26,  1.13it/s]
 31%|███       | 196/630 [03:01<06:24,  1.13it/s]
                                                 
{'loss': 0.7031, 'grad_norm': 2.2395885428150244, 'learning_rate': 1.4238952536824878e-06, 'epoch': 0.93}

 31%|███       | 196/630 [03:01<06:24,  1.13it/s]
 31%|███▏      | 197/630 [03:02<06:23,  1.13it/s]
                                                 
{'loss': 0.668, 'grad_norm': 2.3828558295650506, 'learning_rate': 1.420621931260229e-06, 'epoch': 0.94}

 31%|███▏      | 197/630 [03:02<06:23,  1.13it/s]
 31%|███▏      | 198/630 [03:03<06:22,  1.13it/s]
                                                 
{'loss': 0.6692, 'grad_norm': 1.8871665053461306, 'learning_rate': 1.4173486088379704e-06, 'epoch': 0.94}

 31%|███▏      | 198/630 [03:03<06:22,  1.13it/s]
 32%|███▏      | 199/630 [03:04<06:21,  1.13it/s]
                                                 
{'loss': 0.6934, 'grad_norm': 2.232727911442647, 'learning_rate': 1.414075286415712e-06, 'epoch': 0.95}

 32%|███▏      | 199/630 [03:04<06:21,  1.13it/s]
 32%|███▏      | 200/630 [03:05<06:21,  1.13it/s]
                                                 
{'loss': 0.6732, 'grad_norm': 1.9734695632067039, 'learning_rate': 1.4108019639934531e-06, 'epoch': 0.95}

 32%|███▏      | 200/630 [03:05<06:21,  1.13it/s]
 32%|███▏      | 201/630 [03:05<06:20,  1.13it/s]
                                                 
{'loss': 0.7353, 'grad_norm': 2.7551373643919326, 'learning_rate': 1.4075286415711947e-06, 'epoch': 0.96}

 32%|███▏      | 201/630 [03:05<06:20,  1.13it/s]
 32%|███▏      | 202/630 [03:06<06:20,  1.13it/s]
                                                 
{'loss': 0.6629, 'grad_norm': 1.8373691261253846, 'learning_rate': 1.4042553191489362e-06, 'epoch': 0.96}

 32%|███▏      | 202/630 [03:06<06:20,  1.13it/s]
 32%|███▏      | 203/630 [03:07<06:19,  1.13it/s]
                                                 
{'loss': 0.6508, 'grad_norm': 2.0166418965124917, 'learning_rate': 1.4009819967266775e-06, 'epoch': 0.97}

 32%|███▏      | 203/630 [03:07<06:19,  1.13it/s]
 32%|███▏      | 204/630 [03:08<06:18,  1.13it/s]
                                                 
{'loss': 0.8556, 'grad_norm': 2.592248812390245, 'learning_rate': 1.3977086743044189e-06, 'epoch': 0.97}

 32%|███▏      | 204/630 [03:08<06:18,  1.13it/s]
 33%|███▎      | 205/630 [03:09<06:17,  1.13it/s]
                                                 
{'loss': 0.5891, 'grad_norm': 1.6306350439187347, 'learning_rate': 1.3944353518821604e-06, 'epoch': 0.98}

 33%|███▎      | 205/630 [03:09<06:17,  1.13it/s]
 33%|███▎      | 206/630 [03:10<06:16,  1.13it/s]
                                                 
{'loss': 0.8032, 'grad_norm': 2.2472840307836592, 'learning_rate': 1.3911620294599018e-06, 'epoch': 0.98}

 33%|███▎      | 206/630 [03:10<06:16,  1.13it/s]
 33%|███▎      | 207/630 [03:11<06:15,  1.13it/s]
                                                 
{'loss': 0.6516, 'grad_norm': 1.7357397936191772, 'learning_rate': 1.387888707037643e-06, 'epoch': 0.99}

 33%|███▎      | 207/630 [03:11<06:15,  1.13it/s]
 33%|███▎      | 208/630 [03:12<06:14,  1.13it/s]
                                                 
{'loss': 0.7354, 'grad_norm': 2.9946783173993827, 'learning_rate': 1.3846153846153844e-06, 'epoch': 0.99}

 33%|███▎      | 208/630 [03:12<06:14,  1.13it/s]
 33%|███▎      | 209/630 [03:13<06:13,  1.13it/s]
                                                 
{'loss': 0.6831, 'grad_norm': 1.8336652342644124, 'learning_rate': 1.381342062193126e-06, 'epoch': 1.0}

 33%|███▎      | 209/630 [03:13<06:13,  1.13it/s]
 33%|███▎      | 210/630 [03:13<06:12,  1.13it/s]
                                                 
{'loss': 0.6009, 'grad_norm': 1.7027278025290866, 'learning_rate': 1.3780687397708675e-06, 'epoch': 1.0}

 33%|███▎      | 210/630 [03:13<06:12,  1.13it/s]
 33%|███▎      | 211/630 [03:14<06:11,  1.13it/s]
                                                 
{'loss': 0.57, 'grad_norm': 1.8706975055763542, 'learning_rate': 1.3747954173486086e-06, 'epoch': 1.0}

 33%|███▎      | 211/630 [03:14<06:11,  1.13it/s]
 34%|███▎      | 212/630 [03:15<06:10,  1.13it/s]
                                                 
{'loss': 0.5844, 'grad_norm': 1.9851246801170241, 'learning_rate': 1.3715220949263502e-06, 'epoch': 1.01}

 34%|███▎      | 212/630 [03:15<06:10,  1.13it/s]
 34%|███▍      | 213/630 [03:16<06:09,  1.13it/s]
                                                 
{'loss': 0.6328, 'grad_norm': 1.8137796672320015, 'learning_rate': 1.3682487725040917e-06, 'epoch': 1.01}

 34%|███▍      | 213/630 [03:16<06:09,  1.13it/s]
 34%|███▍      | 214/630 [03:17<06:08,  1.13it/s]
                                                 
{'loss': 0.6349, 'grad_norm': 2.1820246713033473, 'learning_rate': 1.3649754500818329e-06, 'epoch': 1.02}

 34%|███▍      | 214/630 [03:17<06:08,  1.13it/s]
 34%|███▍      | 215/630 [03:18<06:08,  1.13it/s]
                                                 
{'loss': 0.5904, 'grad_norm': 1.7822837454535827, 'learning_rate': 1.3617021276595744e-06, 'epoch': 1.02}

 34%|███▍      | 215/630 [03:18<06:08,  1.13it/s]
 34%|███▍      | 216/630 [03:19<06:08,  1.12it/s]
                                                 
{'loss': 0.5189, 'grad_norm': 1.7668955481468567, 'learning_rate': 1.358428805237316e-06, 'epoch': 1.03}

 34%|███▍      | 216/630 [03:19<06:08,  1.12it/s]
 34%|███▍      | 217/630 [03:20<06:07,  1.12it/s]
                                                 
{'loss': 0.5313, 'grad_norm': 1.6798607771608272, 'learning_rate': 1.3551554828150573e-06, 'epoch': 1.03}

 34%|███▍      | 217/630 [03:20<06:07,  1.12it/s]
 35%|███▍      | 218/630 [03:21<06:06,  1.13it/s]
                                                 
{'loss': 0.5711, 'grad_norm': 1.7604726288015153, 'learning_rate': 1.3518821603927986e-06, 'epoch': 1.04}

 35%|███▍      | 218/630 [03:21<06:06,  1.13it/s]
 35%|███▍      | 219/630 [03:21<06:04,  1.13it/s]
                                                 
{'loss': 0.579, 'grad_norm': 1.7982869938550181, 'learning_rate': 1.34860883797054e-06, 'epoch': 1.04}

 35%|███▍      | 219/630 [03:21<06:04,  1.13it/s]
 35%|███▍      | 220/630 [03:22<06:04,  1.13it/s]
                                                 
{'loss': 0.6694, 'grad_norm': 1.8858874081400268, 'learning_rate': 1.3453355155482815e-06, 'epoch': 1.05}

 35%|███▍      | 220/630 [03:22<06:04,  1.13it/s]
 35%|███▌      | 221/630 [03:23<06:03,  1.13it/s]
                                                 
{'loss': 0.6797, 'grad_norm': 2.1285672588151714, 'learning_rate': 1.3420621931260228e-06, 'epoch': 1.05}

 35%|███▌      | 221/630 [03:23<06:03,  1.13it/s]
 35%|███▌      | 222/630 [03:24<06:02,  1.13it/s]
                                                 
{'loss': 0.5808, 'grad_norm': 1.8227983612114114, 'learning_rate': 1.3387888707037642e-06, 'epoch': 1.06}

 35%|███▌      | 222/630 [03:24<06:02,  1.13it/s]
 35%|███▌      | 223/630 [03:25<06:01,  1.13it/s]
                                                 
{'loss': 0.5921, 'grad_norm': 1.780411294391336, 'learning_rate': 1.3355155482815057e-06, 'epoch': 1.06}

 35%|███▌      | 223/630 [03:25<06:01,  1.13it/s]
 36%|███▌      | 224/630 [03:26<06:00,  1.13it/s]
                                                 
{'loss': 0.6049, 'grad_norm': 1.7221970270119749, 'learning_rate': 1.3322422258592473e-06, 'epoch': 1.07}

 36%|███▌      | 224/630 [03:26<06:00,  1.13it/s]
 36%|███▌      | 225/630 [03:27<05:59,  1.13it/s]
                                                 
{'loss': 0.6507, 'grad_norm': 2.194448095535824, 'learning_rate': 1.3289689034369884e-06, 'epoch': 1.07}

 36%|███▌      | 225/630 [03:27<05:59,  1.13it/s]
 36%|███▌      | 226/630 [03:28<05:58,  1.13it/s]
                                                 
{'loss': 0.5902, 'grad_norm': 2.0985705965165966, 'learning_rate': 1.32569558101473e-06, 'epoch': 1.08}

 36%|███▌      | 226/630 [03:28<05:58,  1.13it/s]
 36%|███▌      | 227/630 [03:29<05:57,  1.13it/s]
                                                 
{'loss': 0.6154, 'grad_norm': 1.9609271870752405, 'learning_rate': 1.3224222585924713e-06, 'epoch': 1.08}

 36%|███▌      | 227/630 [03:29<05:57,  1.13it/s]
 36%|███▌      | 228/630 [03:29<05:58,  1.12it/s]
                                                 
{'loss': 0.6408, 'grad_norm': 2.192683052786367, 'learning_rate': 1.3191489361702126e-06, 'epoch': 1.09}

 36%|███▌      | 228/630 [03:29<05:58,  1.12it/s]
 36%|███▋      | 229/630 [03:30<05:58,  1.12it/s]
                                                 
{'loss': 0.6217, 'grad_norm': 2.699480637780038, 'learning_rate': 1.3158756137479541e-06, 'epoch': 1.09}

 36%|███▋      | 229/630 [03:30<05:58,  1.12it/s]
 37%|███▋      | 230/630 [03:31<05:57,  1.12it/s]
                                                 
{'loss': 0.6079, 'grad_norm': 1.6996440939753295, 'learning_rate': 1.3126022913256955e-06, 'epoch': 1.1}

 37%|███▋      | 230/630 [03:31<05:57,  1.12it/s]
 37%|███▋      | 231/630 [03:32<05:55,  1.12it/s]
                                                 
{'loss': 0.55, 'grad_norm': 1.993565159013375, 'learning_rate': 1.309328968903437e-06, 'epoch': 1.1}

 37%|███▋      | 231/630 [03:32<05:55,  1.12it/s]
 37%|███▋      | 232/630 [03:33<05:54,  1.12it/s]
                                                 
{'loss': 0.5998, 'grad_norm': 2.022690955159374, 'learning_rate': 1.3060556464811784e-06, 'epoch': 1.1}

 37%|███▋      | 232/630 [03:33<05:54,  1.12it/s]
 37%|███▋      | 233/630 [03:34<05:53,  1.12it/s]
                                                 
{'loss': 0.5415, 'grad_norm': 1.9392495615588996, 'learning_rate': 1.3027823240589197e-06, 'epoch': 1.11}

 37%|███▋      | 233/630 [03:34<05:53,  1.12it/s]
 37%|███▋      | 234/630 [03:35<05:50,  1.13it/s]
                                                 
{'loss': 0.6111, 'grad_norm': 1.9198721473248783, 'learning_rate': 1.2995090016366612e-06, 'epoch': 1.11}

 37%|███▋      | 234/630 [03:35<05:50,  1.13it/s]
 37%|███▋      | 235/630 [03:36<05:49,  1.13it/s]
                                                 
{'loss': 0.6466, 'grad_norm': 2.1876188682851456, 'learning_rate': 1.2962356792144024e-06, 'epoch': 1.12}

 37%|███▋      | 235/630 [03:36<05:49,  1.13it/s]
 37%|███▋      | 236/630 [03:37<05:48,  1.13it/s]
                                                 
{'loss': 0.6006, 'grad_norm': 1.8858788778295186, 'learning_rate': 1.292962356792144e-06, 'epoch': 1.12}

 37%|███▋      | 236/630 [03:37<05:48,  1.13it/s]
 38%|███▊      | 237/630 [03:37<05:47,  1.13it/s]
                                                 
{'loss': 0.6139, 'grad_norm': 2.0393219760521557, 'learning_rate': 1.2896890343698855e-06, 'epoch': 1.13}

 38%|███▊      | 237/630 [03:37<05:47,  1.13it/s]
 38%|███▊      | 238/630 [03:38<05:47,  1.13it/s]
                                                 
{'loss': 0.6057, 'grad_norm': 1.9734492987417784, 'learning_rate': 1.2864157119476268e-06, 'epoch': 1.13}

 38%|███▊      | 238/630 [03:38<05:47,  1.13it/s]
 38%|███▊      | 239/630 [03:39<05:46,  1.13it/s]
                                                 
{'loss': 0.6444, 'grad_norm': 2.010493774398585, 'learning_rate': 1.2831423895253681e-06, 'epoch': 1.14}

 38%|███▊      | 239/630 [03:39<05:46,  1.13it/s]
 38%|███▊      | 240/630 [03:40<05:45,  1.13it/s]
                                                 
{'loss': 0.6327, 'grad_norm': 2.185492942594252, 'learning_rate': 1.2798690671031097e-06, 'epoch': 1.14}

 38%|███▊      | 240/630 [03:40<05:45,  1.13it/s]
 38%|███▊      | 241/630 [03:41<05:44,  1.13it/s]
                                                 
{'loss': 0.5699, 'grad_norm': 1.6626176965883361, 'learning_rate': 1.276595744680851e-06, 'epoch': 1.15}

 38%|███▊      | 241/630 [03:41<05:44,  1.13it/s]
 38%|███▊      | 242/630 [03:42<05:43,  1.13it/s]
                                                 
{'loss': 0.549, 'grad_norm': 2.120895977215297, 'learning_rate': 1.2733224222585923e-06, 'epoch': 1.15}

 38%|███▊      | 242/630 [03:42<05:43,  1.13it/s]
 39%|███▊      | 243/630 [03:43<05:42,  1.13it/s]
                                                 
{'loss': 0.6442, 'grad_norm': 1.8528122544307861, 'learning_rate': 1.270049099836334e-06, 'epoch': 1.16}

 39%|███▊      | 243/630 [03:43<05:42,  1.13it/s]
 39%|███▊      | 244/630 [03:44<05:41,  1.13it/s]
                                                 
{'loss': 0.5928, 'grad_norm': 1.8782876662196537, 'learning_rate': 1.2667757774140752e-06, 'epoch': 1.16}

 39%|███▊      | 244/630 [03:44<05:41,  1.13it/s]
 39%|███▉      | 245/630 [03:44<05:40,  1.13it/s]
                                                 
{'loss': 0.5623, 'grad_norm': 2.718388361079657, 'learning_rate': 1.2635024549918168e-06, 'epoch': 1.17}

 39%|███▉      | 245/630 [03:44<05:40,  1.13it/s]
 39%|███▉      | 246/630 [03:45<05:41,  1.13it/s]
                                                 
{'loss': 0.6053, 'grad_norm': 1.97808800013477, 'learning_rate': 1.260229132569558e-06, 'epoch': 1.17}

 39%|███▉      | 246/630 [03:45<05:41,  1.13it/s]
 39%|███▉      | 247/630 [03:46<05:40,  1.12it/s]
                                                 
{'loss': 0.6098, 'grad_norm': 1.762170222155739, 'learning_rate': 1.2569558101472994e-06, 'epoch': 1.18}

 39%|███▉      | 247/630 [03:46<05:40,  1.12it/s]
 39%|███▉      | 248/630 [03:47<05:38,  1.13it/s]
                                                 
{'loss': 0.5119, 'grad_norm': 1.628519161544972, 'learning_rate': 1.253682487725041e-06, 'epoch': 1.18}

 39%|███▉      | 248/630 [03:47<05:38,  1.13it/s]
 40%|███▉      | 249/630 [03:48<05:37,  1.13it/s]
                                                 
{'loss': 0.5731, 'grad_norm': 1.9194883299631325, 'learning_rate': 1.2504091653027821e-06, 'epoch': 1.19}

 40%|███▉      | 249/630 [03:48<05:37,  1.13it/s]
 40%|███▉      | 250/630 [03:49<05:38,  1.12it/s]
                                                 
{'loss': 0.5881, 'grad_norm': 1.7915632273463111, 'learning_rate': 1.2471358428805237e-06, 'epoch': 1.19}

 40%|███▉      | 250/630 [03:49<05:38,  1.12it/s]
 40%|███▉      | 251/630 [03:50<05:37,  1.12it/s]
                                                 
{'loss': 0.5861, 'grad_norm': 1.9584673314732297, 'learning_rate': 1.2438625204582652e-06, 'epoch': 1.2}

 40%|███▉      | 251/630 [03:50<05:37,  1.12it/s]
 40%|████      | 252/630 [03:51<05:36,  1.12it/s]
                                                 
{'loss': 0.6216, 'grad_norm': 1.8049706536401289, 'learning_rate': 1.2405891980360065e-06, 'epoch': 1.2}

 40%|████      | 252/630 [03:51<05:36,  1.12it/s]
 40%|████      | 253/630 [03:52<05:34,  1.13it/s]
                                                 
{'loss': 0.628, 'grad_norm': 2.010391351245582, 'learning_rate': 1.2373158756137479e-06, 'epoch': 1.2}

 40%|████      | 253/630 [03:52<05:34,  1.13it/s]
 40%|████      | 254/630 [03:52<05:32,  1.13it/s]
                                                 
{'loss': 0.5696, 'grad_norm': 1.9598446806696252, 'learning_rate': 1.2340425531914892e-06, 'epoch': 1.21}

 40%|████      | 254/630 [03:52<05:32,  1.13it/s]
 40%|████      | 255/630 [03:53<05:32,  1.13it/s]
                                                 
{'loss': 0.5983, 'grad_norm': 1.9499194951718362, 'learning_rate': 1.2307692307692308e-06, 'epoch': 1.21}

 40%|████      | 255/630 [03:53<05:32,  1.13it/s]
 41%|████      | 256/630 [03:54<05:33,  1.12it/s]
                                                 
{'loss': 0.548, 'grad_norm': 1.9818363642279153, 'learning_rate': 1.227495908346972e-06, 'epoch': 1.22}

 41%|████      | 256/630 [03:54<05:33,  1.12it/s]
 41%|████      | 257/630 [03:55<05:31,  1.13it/s]
                                                 
{'loss': 0.61, 'grad_norm': 2.2137683688389034, 'learning_rate': 1.2242225859247134e-06, 'epoch': 1.22}

 41%|████      | 257/630 [03:55<05:31,  1.13it/s]
 41%|████      | 258/630 [03:56<05:29,  1.13it/s]
                                                 
{'loss': 0.5381, 'grad_norm': 1.6665032821388404, 'learning_rate': 1.220949263502455e-06, 'epoch': 1.23}

 41%|████      | 258/630 [03:56<05:29,  1.13it/s]
 41%|████      | 259/630 [03:57<05:28,  1.13it/s]
                                                 
{'loss': 0.6483, 'grad_norm': 1.899771821013276, 'learning_rate': 1.2176759410801965e-06, 'epoch': 1.23}

 41%|████      | 259/630 [03:57<05:28,  1.13it/s]
 41%|████▏     | 260/630 [03:58<05:28,  1.13it/s]
                                                 
{'loss': 0.5753, 'grad_norm': 1.9925685078317499, 'learning_rate': 1.2144026186579376e-06, 'epoch': 1.24}

 41%|████▏     | 260/630 [03:58<05:28,  1.13it/s]
 41%|████▏     | 261/630 [03:59<05:27,  1.13it/s]
                                                 
{'loss': 0.5882, 'grad_norm': 2.0539235982206154, 'learning_rate': 1.2111292962356792e-06, 'epoch': 1.24}

 41%|████▏     | 261/630 [03:59<05:27,  1.13it/s]
 42%|████▏     | 262/630 [04:00<05:25,  1.13it/s]
                                                 
{'loss': 0.6871, 'grad_norm': 2.0632136775554524, 'learning_rate': 1.2078559738134207e-06, 'epoch': 1.25}

 42%|████▏     | 262/630 [04:00<05:25,  1.13it/s]
 42%|████▏     | 263/630 [04:00<05:25,  1.13it/s]
                                                 
{'loss': 0.5574, 'grad_norm': 2.214292993160414, 'learning_rate': 1.2045826513911619e-06, 'epoch': 1.25}

 42%|████▏     | 263/630 [04:00<05:25,  1.13it/s]
 42%|████▏     | 264/630 [04:01<05:25,  1.13it/s]
                                                 
{'loss': 0.5592, 'grad_norm': 1.8480029238550904, 'learning_rate': 1.2013093289689034e-06, 'epoch': 1.26}

 42%|████▏     | 264/630 [04:01<05:25,  1.13it/s]
 42%|████▏     | 265/630 [04:02<05:23,  1.13it/s]
                                                 
{'loss': 0.5658, 'grad_norm': 1.8092053385830713, 'learning_rate': 1.1980360065466447e-06, 'epoch': 1.26}

 42%|████▏     | 265/630 [04:02<05:23,  1.13it/s]
 42%|████▏     | 266/630 [04:03<05:22,  1.13it/s]
                                                 
{'loss': 0.6123, 'grad_norm': 1.8590999472021328, 'learning_rate': 1.1947626841243863e-06, 'epoch': 1.27}

 42%|████▏     | 266/630 [04:03<05:22,  1.13it/s]
 42%|████▏     | 267/630 [04:04<05:21,  1.13it/s]
                                                 
{'loss': 0.5613, 'grad_norm': 1.8063163153303465, 'learning_rate': 1.1914893617021276e-06, 'epoch': 1.27}

 42%|████▏     | 267/630 [04:04<05:21,  1.13it/s]
 43%|████▎     | 268/630 [04:05<05:22,  1.12it/s]
                                                 
{'loss': 0.6168, 'grad_norm': 2.1943251165336872, 'learning_rate': 1.188216039279869e-06, 'epoch': 1.28}

 43%|████▎     | 268/630 [04:05<05:22,  1.12it/s]
 43%|████▎     | 269/630 [04:06<05:20,  1.13it/s]
                                                 
{'loss': 0.6039, 'grad_norm': 2.085020790619264, 'learning_rate': 1.1849427168576105e-06, 'epoch': 1.28}

 43%|████▎     | 269/630 [04:06<05:20,  1.13it/s]
 43%|████▎     | 270/630 [04:07<05:19,  1.13it/s]
                                                 
{'loss': 0.5302, 'grad_norm': 2.180227611049375, 'learning_rate': 1.1816693944353518e-06, 'epoch': 1.29}

 43%|████▎     | 270/630 [04:07<05:19,  1.13it/s]
 43%|████▎     | 271/630 [04:08<05:18,  1.13it/s]
                                                 
{'loss': 0.6053, 'grad_norm': 2.3216468961662726, 'learning_rate': 1.1783960720130932e-06, 'epoch': 1.29}

 43%|████▎     | 271/630 [04:08<05:18,  1.13it/s]
 43%|████▎     | 272/630 [04:08<05:17,  1.13it/s]
                                                 
{'loss': 0.6114, 'grad_norm': 2.4994511094103826, 'learning_rate': 1.1751227495908347e-06, 'epoch': 1.3}

 43%|████▎     | 272/630 [04:08<05:17,  1.13it/s]
 43%|████▎     | 273/630 [04:09<05:15,  1.13it/s]
                                                 
{'loss': 0.5807, 'grad_norm': 1.9918591326856698, 'learning_rate': 1.171849427168576e-06, 'epoch': 1.3}

 43%|████▎     | 273/630 [04:09<05:15,  1.13it/s]
 43%|████▎     | 274/630 [04:10<05:14,  1.13it/s]
                                                 
{'loss': 0.5556, 'grad_norm': 2.1286842818364664, 'learning_rate': 1.1685761047463174e-06, 'epoch': 1.3}

 43%|████▎     | 274/630 [04:10<05:14,  1.13it/s]
 44%|████▎     | 275/630 [04:11<05:52,  1.01it/s]
                                                 
{'loss': 0.6052, 'grad_norm': 2.148302610099635, 'learning_rate': 1.165302782324059e-06, 'epoch': 1.31}

 44%|████▎     | 275/630 [04:11<05:52,  1.01it/s]
 44%|████▍     | 276/630 [04:12<05:39,  1.04it/s]
                                                 
{'loss': 0.5316, 'grad_norm': 2.1773468305474566, 'learning_rate': 1.1620294599018003e-06, 'epoch': 1.31}

 44%|████▍     | 276/630 [04:12<05:39,  1.04it/s]
 44%|████▍     | 277/630 [04:13<05:30,  1.07it/s]
                                                 
{'loss': 0.5519, 'grad_norm': 1.833905101723677, 'learning_rate': 1.1587561374795416e-06, 'epoch': 1.32}

 44%|████▍     | 277/630 [04:13<05:30,  1.07it/s]
 44%|████▍     | 278/630 [04:14<05:24,  1.08it/s]
                                                 
{'loss': 0.565, 'grad_norm': 2.1686978960041956, 'learning_rate': 1.1554828150572832e-06, 'epoch': 1.32}

 44%|████▍     | 278/630 [04:14<05:24,  1.08it/s]
 44%|████▍     | 279/630 [04:15<05:21,  1.09it/s]
                                                 
{'loss': 0.5781, 'grad_norm': 1.9738263555186324, 'learning_rate': 1.1522094926350245e-06, 'epoch': 1.33}

 44%|████▍     | 279/630 [04:15<05:21,  1.09it/s]
 44%|████▍     | 280/630 [04:16<05:17,  1.10it/s]
                                                 
{'loss': 0.5836, 'grad_norm': 1.93440686262158, 'learning_rate': 1.148936170212766e-06, 'epoch': 1.33}

 44%|████▍     | 280/630 [04:16<05:17,  1.10it/s]
 45%|████▍     | 281/630 [04:17<05:15,  1.11it/s]
                                                 
{'loss': 0.5662, 'grad_norm': 1.9257206777002422, 'learning_rate': 1.1456628477905072e-06, 'epoch': 1.34}

 45%|████▍     | 281/630 [04:17<05:15,  1.11it/s]
 45%|████▍     | 282/630 [04:18<05:12,  1.11it/s]
                                                 
{'loss': 0.6249, 'grad_norm': 2.091720701346452, 'learning_rate': 1.1423895253682487e-06, 'epoch': 1.34}

 45%|████▍     | 282/630 [04:18<05:12,  1.11it/s]
 45%|████▍     | 283/630 [04:19<05:09,  1.12it/s]
                                                 
{'loss': 0.5858, 'grad_norm': 1.9807362828016675, 'learning_rate': 1.1391162029459903e-06, 'epoch': 1.35}

 45%|████▍     | 283/630 [04:19<05:09,  1.12it/s]
 45%|████▌     | 284/630 [04:19<05:08,  1.12it/s]
                                                 
{'loss': 0.5614, 'grad_norm': 1.7638013086221673, 'learning_rate': 1.1358428805237314e-06, 'epoch': 1.35}

 45%|████▌     | 284/630 [04:19<05:08,  1.12it/s]
 45%|████▌     | 285/630 [04:20<05:07,  1.12it/s]
                                                 
{'loss': 0.5475, 'grad_norm': 1.9967034567834974, 'learning_rate': 1.132569558101473e-06, 'epoch': 1.36}

 45%|████▌     | 285/630 [04:20<05:07,  1.12it/s]
 45%|████▌     | 286/630 [04:21<05:05,  1.13it/s]
                                                 
{'loss': 0.5705, 'grad_norm': 2.388462964863793, 'learning_rate': 1.1292962356792145e-06, 'epoch': 1.36}

 45%|████▌     | 286/630 [04:21<05:05,  1.13it/s]
 46%|████▌     | 287/630 [04:22<05:04,  1.13it/s]
                                                 
{'loss': 0.636, 'grad_norm': 2.001099371961074, 'learning_rate': 1.1260229132569558e-06, 'epoch': 1.37}

 46%|████▌     | 287/630 [04:22<05:04,  1.13it/s]
 46%|████▌     | 288/630 [04:23<05:03,  1.13it/s]
                                                 
{'loss': 0.5073, 'grad_norm': 1.8756494146967333, 'learning_rate': 1.1227495908346971e-06, 'epoch': 1.37}

 46%|████▌     | 288/630 [04:23<05:03,  1.13it/s]
 46%|████▌     | 289/630 [04:24<05:03,  1.12it/s]
                                                 
{'loss': 0.5998, 'grad_norm': 1.7341228890975275, 'learning_rate': 1.1194762684124387e-06, 'epoch': 1.38}

 46%|████▌     | 289/630 [04:24<05:03,  1.12it/s]
 46%|████▌     | 290/630 [04:25<05:01,  1.13it/s]
                                                 
{'loss': 0.5437, 'grad_norm': 1.8347641336782199, 'learning_rate': 1.11620294599018e-06, 'epoch': 1.38}

 46%|████▌     | 290/630 [04:25<05:01,  1.13it/s]
 46%|████▌     | 291/630 [04:26<05:00,  1.13it/s]
                                                 
{'loss': 0.5229, 'grad_norm': 2.3271296165810345, 'learning_rate': 1.1129296235679214e-06, 'epoch': 1.39}

 46%|████▌     | 291/630 [04:26<05:00,  1.13it/s]
 46%|████▋     | 292/630 [04:27<05:00,  1.12it/s]
                                                 
{'loss': 0.5652, 'grad_norm': 2.035151319728594, 'learning_rate': 1.1096563011456627e-06, 'epoch': 1.39}

 46%|████▋     | 292/630 [04:27<05:00,  1.12it/s]
 47%|████▋     | 293/630 [04:27<04:59,  1.12it/s]
                                                 
{'loss': 0.6121, 'grad_norm': 2.0352209717697987, 'learning_rate': 1.1063829787234042e-06, 'epoch': 1.4}

 47%|████▋     | 293/630 [04:27<04:59,  1.12it/s]
 47%|████▋     | 294/630 [04:28<04:57,  1.13it/s]
                                                 
{'loss': 0.5605, 'grad_norm': 1.9472902357212178, 'learning_rate': 1.1031096563011458e-06, 'epoch': 1.4}

 47%|████▋     | 294/630 [04:28<04:57,  1.13it/s]
 47%|████▋     | 295/630 [04:29<04:57,  1.13it/s]
                                                 
{'loss': 0.5357, 'grad_norm': 1.7309903418494956, 'learning_rate': 1.099836333878887e-06, 'epoch': 1.4}

 47%|████▋     | 295/630 [04:29<04:57,  1.13it/s]
 47%|████▋     | 296/630 [04:30<04:57,  1.12it/s]
                                                 
{'loss': 0.5763, 'grad_norm': 2.2964389963482743, 'learning_rate': 1.0965630114566285e-06, 'epoch': 1.41}

 47%|████▋     | 296/630 [04:30<04:57,  1.12it/s]
 47%|████▋     | 297/630 [04:31<04:57,  1.12it/s]
                                                 
{'loss': 0.5707, 'grad_norm': 1.875450406380695, 'learning_rate': 1.09328968903437e-06, 'epoch': 1.41}

 47%|████▋     | 297/630 [04:31<04:57,  1.12it/s]
 47%|████▋     | 298/630 [04:32<04:57,  1.12it/s]
                                                 
{'loss': 0.5809, 'grad_norm': 1.9603558463110613, 'learning_rate': 1.0900163666121111e-06, 'epoch': 1.42}

 47%|████▋     | 298/630 [04:32<04:57,  1.12it/s]
 47%|████▋     | 299/630 [04:33<04:55,  1.12it/s]
                                                 
{'loss': 0.5396, 'grad_norm': 1.72614083525579, 'learning_rate': 1.0867430441898527e-06, 'epoch': 1.42}

 47%|████▋     | 299/630 [04:33<04:55,  1.12it/s]
 48%|████▊     | 300/630 [04:34<04:54,  1.12it/s]
                                                 
{'loss': 0.5945, 'grad_norm': 1.8140760688131021, 'learning_rate': 1.083469721767594e-06, 'epoch': 1.43}

 48%|████▊     | 300/630 [04:34<04:54,  1.12it/s]
 48%|████▊     | 301/630 [04:35<04:52,  1.13it/s]
                                                 
{'loss': 0.5524, 'grad_norm': 1.8782128222066017, 'learning_rate': 1.0801963993453356e-06, 'epoch': 1.43}

 48%|████▊     | 301/630 [04:35<04:52,  1.13it/s]
 48%|████▊     | 302/630 [04:35<04:51,  1.13it/s]
                                                 
{'loss': 0.569, 'grad_norm': 2.2185429466730198, 'learning_rate': 1.0769230769230769e-06, 'epoch': 1.44}

 48%|████▊     | 302/630 [04:35<04:51,  1.13it/s]
 48%|████▊     | 303/630 [04:36<04:50,  1.12it/s]
                                                 
{'loss': 0.6211, 'grad_norm': 2.2675242468323518, 'learning_rate': 1.0736497545008182e-06, 'epoch': 1.44}

 48%|████▊     | 303/630 [04:36<04:50,  1.12it/s]
 48%|████▊     | 304/630 [04:37<04:50,  1.12it/s]
                                                 
{'loss': 0.5931, 'grad_norm': 2.1859422698391846, 'learning_rate': 1.0703764320785598e-06, 'epoch': 1.45}

 48%|████▊     | 304/630 [04:37<04:50,  1.12it/s]
 48%|████▊     | 305/630 [04:38<04:49,  1.12it/s]
                                                 
{'loss': 0.5706, 'grad_norm': 2.0729798104120496, 'learning_rate': 1.0671031096563011e-06, 'epoch': 1.45}

 48%|████▊     | 305/630 [04:38<04:49,  1.12it/s]
 49%|████▊     | 306/630 [04:39<04:48,  1.12it/s]
                                                 
{'loss': 0.6015, 'grad_norm': 2.597814593265579, 'learning_rate': 1.0638297872340424e-06, 'epoch': 1.46}

 49%|████▊     | 306/630 [04:39<04:48,  1.12it/s]
 49%|████▊     | 307/630 [04:40<04:47,  1.12it/s]
                                                 
{'loss': 0.5972, 'grad_norm': 2.5428287800656006, 'learning_rate': 1.060556464811784e-06, 'epoch': 1.46}

 49%|████▊     | 307/630 [04:40<04:47,  1.12it/s]
 49%|████▉     | 308/630 [04:41<04:46,  1.12it/s]
                                                 
{'loss': 0.5392, 'grad_norm': 1.917933364522579, 'learning_rate': 1.0572831423895253e-06, 'epoch': 1.47}

 49%|████▉     | 308/630 [04:41<04:46,  1.12it/s]
 49%|████▉     | 309/630 [04:42<04:45,  1.13it/s]
                                                 
{'loss': 0.5098, 'grad_norm': 2.40598871437052, 'learning_rate': 1.0540098199672667e-06, 'epoch': 1.47}

 49%|████▉     | 309/630 [04:42<04:45,  1.13it/s]
 49%|████▉     | 310/630 [04:43<04:44,  1.12it/s]
                                                 
{'loss': 0.6146, 'grad_norm': 1.943854900009343, 'learning_rate': 1.0507364975450082e-06, 'epoch': 1.48}

 49%|████▉     | 310/630 [04:43<04:44,  1.12it/s]
 49%|████▉     | 311/630 [04:43<04:44,  1.12it/s]
                                                 
{'loss': 0.6161, 'grad_norm': 2.503857057548725, 'learning_rate': 1.0474631751227495e-06, 'epoch': 1.48}

 49%|████▉     | 311/630 [04:43<04:44,  1.12it/s]
 50%|████▉     | 312/630 [04:44<04:42,  1.12it/s]
                                                 
{'loss': 0.5072, 'grad_norm': 1.8238349189572356, 'learning_rate': 1.0441898527004909e-06, 'epoch': 1.49}

 50%|████▉     | 312/630 [04:44<04:42,  1.12it/s]
 50%|████▉     | 313/630 [04:45<04:41,  1.12it/s]
                                                 
{'loss': 0.5772, 'grad_norm': 2.2047105877119306, 'learning_rate': 1.0409165302782324e-06, 'epoch': 1.49}

 50%|████▉     | 313/630 [04:45<04:41,  1.12it/s]
 50%|████▉     | 314/630 [04:46<04:40,  1.13it/s]
                                                 
{'loss': 0.5652, 'grad_norm': 1.650864375400303, 'learning_rate': 1.0376432078559738e-06, 'epoch': 1.5}

 50%|████▉     | 314/630 [04:46<04:40,  1.13it/s]
 50%|█████     | 315/630 [04:47<04:39,  1.13it/s]
                                                 
{'loss': 0.5674, 'grad_norm': 2.0236752247430583, 'learning_rate': 1.0343698854337153e-06, 'epoch': 1.5}

 50%|█████     | 315/630 [04:47<04:39,  1.13it/s]
 50%|█████     | 316/630 [04:48<04:38,  1.13it/s]
                                                 
{'loss': 0.5249, 'grad_norm': 1.6774316837060743, 'learning_rate': 1.0310965630114566e-06, 'epoch': 1.5}

 50%|█████     | 316/630 [04:48<04:38,  1.13it/s]
 50%|█████     | 317/630 [04:49<04:37,  1.13it/s]
                                                 
{'loss': 0.541, 'grad_norm': 1.7525142742225306, 'learning_rate': 1.027823240589198e-06, 'epoch': 1.51}

 50%|█████     | 317/630 [04:49<04:37,  1.13it/s]
 50%|█████     | 318/630 [04:50<04:36,  1.13it/s]
                                                 
{'loss': 0.547, 'grad_norm': 1.91091386398068, 'learning_rate': 1.0245499181669395e-06, 'epoch': 1.51}

 50%|█████     | 318/630 [04:50<04:36,  1.13it/s]
 51%|█████     | 319/630 [04:51<04:35,  1.13it/s]
                                                 
{'loss': 0.6826, 'grad_norm': 2.0598246163354923, 'learning_rate': 1.0212765957446806e-06, 'epoch': 1.52}

 51%|█████     | 319/630 [04:51<04:35,  1.13it/s]
 51%|█████     | 320/630 [04:51<04:34,  1.13it/s]
                                                 
{'loss': 0.6252, 'grad_norm': 2.2340876249302055, 'learning_rate': 1.0180032733224222e-06, 'epoch': 1.52}

 51%|█████     | 320/630 [04:51<04:34,  1.13it/s]
 51%|█████     | 321/630 [04:52<04:33,  1.13it/s]
                                                 
{'loss': 0.6149, 'grad_norm': 2.682196251659615, 'learning_rate': 1.0147299509001637e-06, 'epoch': 1.53}

 51%|█████     | 321/630 [04:52<04:33,  1.13it/s]
 51%|█████     | 322/630 [04:53<04:33,  1.12it/s]
                                                 
{'loss': 0.5747, 'grad_norm': 1.8115075404727192, 'learning_rate': 1.011456628477905e-06, 'epoch': 1.53}

 51%|█████     | 322/630 [04:53<04:33,  1.12it/s]
 51%|█████▏    | 323/630 [04:54<04:33,  1.12it/s]
                                                 
{'loss': 0.5831, 'grad_norm': 2.290123942668331, 'learning_rate': 1.0081833060556464e-06, 'epoch': 1.54}

 51%|█████▏    | 323/630 [04:54<04:33,  1.12it/s]
 51%|█████▏    | 324/630 [04:55<04:31,  1.13it/s]
                                                 
{'loss': 0.5982, 'grad_norm': 2.3126769732660013, 'learning_rate': 1.004909983633388e-06, 'epoch': 1.54}

 51%|█████▏    | 324/630 [04:55<04:31,  1.13it/s]
 52%|█████▏    | 325/630 [04:56<04:30,  1.13it/s]
                                                 
{'loss': 0.5107, 'grad_norm': 1.745531270866657, 'learning_rate': 1.0016366612111293e-06, 'epoch': 1.55}

 52%|█████▏    | 325/630 [04:56<04:30,  1.13it/s]
 52%|█████▏    | 326/630 [04:57<04:29,  1.13it/s]
                                                 
{'loss': 0.5288, 'grad_norm': 1.773623946446175, 'learning_rate': 9.983633387888706e-07, 'epoch': 1.55}

 52%|█████▏    | 326/630 [04:57<04:29,  1.13it/s]
 52%|█████▏    | 327/630 [04:58<04:28,  1.13it/s]
                                                 
{'loss': 0.5835, 'grad_norm': 2.146015742524857, 'learning_rate': 9.95090016366612e-07, 'epoch': 1.56}

 52%|█████▏    | 327/630 [04:58<04:28,  1.13it/s]
 52%|█████▏    | 328/630 [04:59<04:27,  1.13it/s]
                                                 
{'loss': 0.5277, 'grad_norm': 1.730222779553265, 'learning_rate': 9.918166939443535e-07, 'epoch': 1.56}

 52%|█████▏    | 328/630 [04:59<04:27,  1.13it/s]
 52%|█████▏    | 329/630 [04:59<04:26,  1.13it/s]
                                                 
{'loss': 0.5861, 'grad_norm': 2.0269912049370737, 'learning_rate': 9.885433715220948e-07, 'epoch': 1.57}

 52%|█████▏    | 329/630 [04:59<04:26,  1.13it/s]
 52%|█████▏    | 330/630 [05:00<04:25,  1.13it/s]
                                                 
{'loss': 0.6895, 'grad_norm': 2.6973078542409685, 'learning_rate': 9.852700490998362e-07, 'epoch': 1.57}

 52%|█████▏    | 330/630 [05:00<04:25,  1.13it/s]
 53%|█████▎    | 331/630 [05:01<04:24,  1.13it/s]
                                                 
{'loss': 0.55, 'grad_norm': 1.9128235467630554, 'learning_rate': 9.819967266775777e-07, 'epoch': 1.58}

 53%|█████▎    | 331/630 [05:01<04:24,  1.13it/s]
 53%|█████▎    | 332/630 [05:02<04:23,  1.13it/s]
                                                 
{'loss': 0.6011, 'grad_norm': 2.019901946635748, 'learning_rate': 9.78723404255319e-07, 'epoch': 1.58}

 53%|█████▎    | 332/630 [05:02<04:23,  1.13it/s]
 53%|█████▎    | 333/630 [05:03<04:22,  1.13it/s]
                                                 
{'loss': 0.5979, 'grad_norm': 1.8800398138698449, 'learning_rate': 9.754500818330606e-07, 'epoch': 1.59}

 53%|█████▎    | 333/630 [05:03<04:22,  1.13it/s]
 53%|█████▎    | 334/630 [05:04<04:21,  1.13it/s]
                                                 
{'loss': 0.5884, 'grad_norm': 1.7828227997892734, 'learning_rate': 9.72176759410802e-07, 'epoch': 1.59}

 53%|█████▎    | 334/630 [05:04<04:21,  1.13it/s]
 53%|█████▎    | 335/630 [05:05<04:21,  1.13it/s]
                                                 
{'loss': 0.5247, 'grad_norm': 1.794595020357391, 'learning_rate': 9.689034369885433e-07, 'epoch': 1.6}

 53%|█████▎    | 335/630 [05:05<04:21,  1.13it/s]
 53%|█████▎    | 336/630 [05:06<04:20,  1.13it/s]
                                                 
{'loss': 0.5705, 'grad_norm': 1.776462229505038, 'learning_rate': 9.656301145662848e-07, 'epoch': 1.6}

 53%|█████▎    | 336/630 [05:06<04:20,  1.13it/s]
 53%|█████▎    | 337/630 [05:06<04:19,  1.13it/s]
                                                 
{'loss': 0.5772, 'grad_norm': 2.3571429198335787, 'learning_rate': 9.623567921440262e-07, 'epoch': 1.6}

 53%|█████▎    | 337/630 [05:06<04:19,  1.13it/s]
 54%|█████▎    | 338/630 [05:07<04:18,  1.13it/s]
                                                 
{'loss': 0.5747, 'grad_norm': 2.068777425529516, 'learning_rate': 9.590834697217675e-07, 'epoch': 1.61}

 54%|█████▎    | 338/630 [05:07<04:18,  1.13it/s]
 54%|█████▍    | 339/630 [05:08<04:17,  1.13it/s]
                                                 
{'loss': 0.5789, 'grad_norm': 1.7640883332149362, 'learning_rate': 9.55810147299509e-07, 'epoch': 1.61}

 54%|█████▍    | 339/630 [05:08<04:17,  1.13it/s]
 54%|█████▍    | 340/630 [05:09<04:17,  1.13it/s]
                                                 
{'loss': 0.58, 'grad_norm': 1.8160107413379176, 'learning_rate': 9.525368248772504e-07, 'epoch': 1.62}

 54%|█████▍    | 340/630 [05:09<04:17,  1.13it/s]
 54%|█████▍    | 341/630 [05:10<04:16,  1.13it/s]
                                                 
{'loss': 0.5863, 'grad_norm': 2.0994277919271527, 'learning_rate': 9.492635024549918e-07, 'epoch': 1.62}

 54%|█████▍    | 341/630 [05:10<04:16,  1.13it/s]
 54%|█████▍    | 342/630 [05:11<04:15,  1.13it/s]
                                                 
{'loss': 0.5567, 'grad_norm': 2.1136088777478967, 'learning_rate': 9.459901800327333e-07, 'epoch': 1.63}

 54%|█████▍    | 342/630 [05:11<04:15,  1.13it/s]
 54%|█████▍    | 343/630 [05:12<04:14,  1.13it/s]
                                                 
{'loss': 0.5362, 'grad_norm': 1.925926750515458, 'learning_rate': 9.427168576104746e-07, 'epoch': 1.63}

 54%|█████▍    | 343/630 [05:12<04:14,  1.13it/s]
 55%|█████▍    | 344/630 [05:13<04:13,  1.13it/s]
                                                 
{'loss': 0.5432, 'grad_norm': 2.2095021296434996, 'learning_rate': 9.394435351882159e-07, 'epoch': 1.64}

 55%|█████▍    | 344/630 [05:13<04:13,  1.13it/s]
 55%|█████▍    | 345/630 [05:14<04:13,  1.13it/s]
                                                 
{'loss': 0.5761, 'grad_norm': 2.434130110448841, 'learning_rate': 9.361702127659575e-07, 'epoch': 1.64}

 55%|█████▍    | 345/630 [05:14<04:13,  1.13it/s]
 55%|█████▍    | 346/630 [05:14<04:11,  1.13it/s]
                                                 
{'loss': 0.5884, 'grad_norm': 2.4957796678920317, 'learning_rate': 9.328968903436988e-07, 'epoch': 1.65}

 55%|█████▍    | 346/630 [05:14<04:11,  1.13it/s]
 55%|█████▌    | 347/630 [05:15<04:10,  1.13it/s]
                                                 
{'loss': 0.5735, 'grad_norm': 1.8723964942732236, 'learning_rate': 9.296235679214402e-07, 'epoch': 1.65}

 55%|█████▌    | 347/630 [05:15<04:10,  1.13it/s]
 55%|█████▌    | 348/630 [05:16<04:09,  1.13it/s]
                                                 
{'loss': 0.6224, 'grad_norm': 2.591971569824478, 'learning_rate': 9.263502454991816e-07, 'epoch': 1.66}

 55%|█████▌    | 348/630 [05:16<04:09,  1.13it/s]
 55%|█████▌    | 349/630 [05:17<04:08,  1.13it/s]
                                                 
{'loss': 0.5746, 'grad_norm': 2.0220088604627273, 'learning_rate': 9.230769230769231e-07, 'epoch': 1.66}

 55%|█████▌    | 349/630 [05:17<04:08,  1.13it/s]
 56%|█████▌    | 350/630 [05:18<04:07,  1.13it/s]
                                                 
{'loss': 0.5584, 'grad_norm': 2.2940108378066464, 'learning_rate': 9.198036006546645e-07, 'epoch': 1.67}

 56%|█████▌    | 350/630 [05:18<04:07,  1.13it/s]
 56%|█████▌    | 351/630 [05:19<04:07,  1.13it/s]
                                                 
{'loss': 0.5638, 'grad_norm': 1.9946411734498095, 'learning_rate': 9.165302782324058e-07, 'epoch': 1.67}

 56%|█████▌    | 351/630 [05:19<04:07,  1.13it/s]
 56%|█████▌    | 352/630 [05:20<04:06,  1.13it/s]
                                                 
{'loss': 0.5303, 'grad_norm': 1.8050562152280518, 'learning_rate': 9.132569558101472e-07, 'epoch': 1.68}

 56%|█████▌    | 352/630 [05:20<04:06,  1.13it/s]
 56%|█████▌    | 353/630 [05:21<04:05,  1.13it/s]
                                                 
{'loss': 0.5241, 'grad_norm': 1.718793481698883, 'learning_rate': 9.099836333878887e-07, 'epoch': 1.68}

 56%|█████▌    | 353/630 [05:21<04:05,  1.13it/s]
 56%|█████▌    | 354/630 [05:22<04:04,  1.13it/s]
                                                 
{'loss': 0.5131, 'grad_norm': 2.0420148677118704, 'learning_rate': 9.067103109656301e-07, 'epoch': 1.69}

 56%|█████▌    | 354/630 [05:22<04:04,  1.13it/s]
 56%|█████▋    | 355/630 [05:22<04:03,  1.13it/s]
                                                 
{'loss': 0.5628, 'grad_norm': 1.7402944795174962, 'learning_rate': 9.034369885433715e-07, 'epoch': 1.69}

 56%|█████▋    | 355/630 [05:22<04:03,  1.13it/s]
 57%|█████▋    | 356/630 [05:23<04:02,  1.13it/s]
                                                 
{'loss': 0.4911, 'grad_norm': 1.8085606796344, 'learning_rate': 9.001636661211129e-07, 'epoch': 1.7}

 57%|█████▋    | 356/630 [05:23<04:02,  1.13it/s]
 57%|█████▋    | 357/630 [05:24<04:01,  1.13it/s]
                                                 
{'loss': 0.589, 'grad_norm': 1.8688182295818045, 'learning_rate': 8.968903436988543e-07, 'epoch': 1.7}

 57%|█████▋    | 357/630 [05:24<04:01,  1.13it/s]
 57%|█████▋    | 358/630 [05:25<04:01,  1.13it/s]
                                                 
{'loss': 0.5991, 'grad_norm': 1.9965403166363078, 'learning_rate': 8.936170212765957e-07, 'epoch': 1.7}

 57%|█████▋    | 358/630 [05:25<04:01,  1.13it/s]
 57%|█████▋    | 359/630 [05:26<04:00,  1.13it/s]
                                                 
{'loss': 0.5972, 'grad_norm': 1.87756104537681, 'learning_rate': 8.903436988543371e-07, 'epoch': 1.71}

 57%|█████▋    | 359/630 [05:26<04:00,  1.13it/s]
 57%|█████▋    | 360/630 [05:27<03:59,  1.13it/s]
                                                 
{'loss': 0.4508, 'grad_norm': 1.7283498263761923, 'learning_rate': 8.870703764320784e-07, 'epoch': 1.71}

 57%|█████▋    | 360/630 [05:27<03:59,  1.13it/s]
 57%|█████▋    | 361/630 [05:28<03:58,  1.13it/s]
                                                 
{'loss': 0.5222, 'grad_norm': 2.0313654026655312, 'learning_rate': 8.8379705400982e-07, 'epoch': 1.72}

 57%|█████▋    | 361/630 [05:28<03:58,  1.13it/s]
 57%|█████▋    | 362/630 [05:29<03:57,  1.13it/s]
                                                 
{'loss': 0.5893, 'grad_norm': 2.0456026146800252, 'learning_rate': 8.805237315875613e-07, 'epoch': 1.72}

 57%|█████▋    | 362/630 [05:29<03:57,  1.13it/s]
 58%|█████▊    | 363/630 [05:30<03:56,  1.13it/s]
                                                 
{'loss': 0.5396, 'grad_norm': 2.1005915363242034, 'learning_rate': 8.772504091653028e-07, 'epoch': 1.73}

 58%|█████▊    | 363/630 [05:30<03:56,  1.13it/s]
 58%|█████▊    | 364/630 [05:30<03:56,  1.13it/s]
                                                 
{'loss': 0.5948, 'grad_norm': 2.140748821323929, 'learning_rate': 8.739770867430442e-07, 'epoch': 1.73}

 58%|█████▊    | 364/630 [05:30<03:56,  1.13it/s]
 58%|█████▊    | 365/630 [05:31<03:55,  1.13it/s]
                                                 
{'loss': 0.5764, 'grad_norm': 1.946371367827272, 'learning_rate': 8.707037643207855e-07, 'epoch': 1.74}

 58%|█████▊    | 365/630 [05:31<03:55,  1.13it/s]
 58%|█████▊    | 366/630 [05:32<03:54,  1.13it/s]
                                                 
{'loss': 0.547, 'grad_norm': 2.0395265856503086, 'learning_rate': 8.67430441898527e-07, 'epoch': 1.74}

 58%|█████▊    | 366/630 [05:32<03:54,  1.13it/s]
 58%|█████▊    | 367/630 [05:33<03:53,  1.13it/s]
                                                 
{'loss': 0.4699, 'grad_norm': 1.7797812396239205, 'learning_rate': 8.641571194762683e-07, 'epoch': 1.75}

 58%|█████▊    | 367/630 [05:33<03:53,  1.13it/s]
 58%|█████▊    | 368/630 [05:34<03:52,  1.13it/s]
                                                 
{'loss': 0.5402, 'grad_norm': 1.7865733108458763, 'learning_rate': 8.608837970540099e-07, 'epoch': 1.75}

 58%|█████▊    | 368/630 [05:34<03:52,  1.13it/s]
 59%|█████▊    | 369/630 [05:35<03:51,  1.13it/s]
                                                 
{'loss': 0.536, 'grad_norm': 2.1001335621414676, 'learning_rate': 8.576104746317512e-07, 'epoch': 1.76}

 59%|█████▊    | 369/630 [05:35<03:51,  1.13it/s]
 59%|█████▊    | 370/630 [05:36<03:50,  1.13it/s]
                                                 
{'loss': 0.4566, 'grad_norm': 1.8430603494136466, 'learning_rate': 8.543371522094926e-07, 'epoch': 1.76}

 59%|█████▊    | 370/630 [05:36<03:50,  1.13it/s]
 59%|█████▉    | 371/630 [05:37<03:50,  1.13it/s]
                                                 
{'loss': 0.597, 'grad_norm': 1.9132291269943444, 'learning_rate': 8.51063829787234e-07, 'epoch': 1.77}

 59%|█████▉    | 371/630 [05:37<03:50,  1.13it/s]
 59%|█████▉    | 372/630 [05:38<03:48,  1.13it/s]
                                                 
{'loss': 0.5891, 'grad_norm': 1.9774135056763338, 'learning_rate': 8.477905073649754e-07, 'epoch': 1.77}

 59%|█████▉    | 372/630 [05:38<03:48,  1.13it/s]
 59%|█████▉    | 373/630 [05:38<03:47,  1.13it/s]
                                                 
{'loss': 0.5527, 'grad_norm': 1.8886895185059502, 'learning_rate': 8.445171849427169e-07, 'epoch': 1.78}

 59%|█████▉    | 373/630 [05:38<03:47,  1.13it/s]
 59%|█████▉    | 374/630 [05:39<03:46,  1.13it/s]
                                                 
{'loss': 0.5663, 'grad_norm': 1.8868917259891653, 'learning_rate': 8.412438625204582e-07, 'epoch': 1.78}

 59%|█████▉    | 374/630 [05:39<03:46,  1.13it/s]
 60%|█████▉    | 375/630 [05:40<03:45,  1.13it/s]
                                                 
{'loss': 0.5684, 'grad_norm': 2.252652602993458, 'learning_rate': 8.379705400981996e-07, 'epoch': 1.79}

 60%|█████▉    | 375/630 [05:40<03:45,  1.13it/s]
 60%|█████▉    | 376/630 [05:41<03:44,  1.13it/s]
                                                 
{'loss': 0.5716, 'grad_norm': 2.0549165049311546, 'learning_rate': 8.346972176759411e-07, 'epoch': 1.79}

 60%|█████▉    | 376/630 [05:41<03:44,  1.13it/s]
 60%|█████▉    | 377/630 [05:42<03:44,  1.13it/s]
                                                 
{'loss': 0.5522, 'grad_norm': 1.7183702264066036, 'learning_rate': 8.314238952536824e-07, 'epoch': 1.8}

 60%|█████▉    | 377/630 [05:42<03:44,  1.13it/s]
 60%|██████    | 378/630 [05:43<03:43,  1.13it/s]
                                                 
{'loss': 0.4939, 'grad_norm': 1.872289304712998, 'learning_rate': 8.281505728314238e-07, 'epoch': 1.8}

 60%|██████    | 378/630 [05:43<03:43,  1.13it/s]
 60%|██████    | 379/630 [05:44<03:42,  1.13it/s]
                                                 
{'loss': 0.5208, 'grad_norm': 1.965662056306946, 'learning_rate': 8.248772504091652e-07, 'epoch': 1.8}

 60%|██████    | 379/630 [05:44<03:42,  1.13it/s]
 60%|██████    | 380/630 [05:45<03:41,  1.13it/s]
                                                 
{'loss': 0.5827, 'grad_norm': 1.9675022679770326, 'learning_rate': 8.216039279869067e-07, 'epoch': 1.81}

 60%|██████    | 380/630 [05:45<03:41,  1.13it/s]
 60%|██████    | 381/630 [05:45<03:40,  1.13it/s]
                                                 
{'loss': 0.6154, 'grad_norm': 2.1825406792970687, 'learning_rate': 8.183306055646481e-07, 'epoch': 1.81}

 60%|██████    | 381/630 [05:45<03:40,  1.13it/s]
 61%|██████    | 382/630 [05:46<03:39,  1.13it/s]
                                                 
{'loss': 0.5327, 'grad_norm': 1.8503628388782045, 'learning_rate': 8.150572831423895e-07, 'epoch': 1.82}

 61%|██████    | 382/630 [05:46<03:39,  1.13it/s]
 61%|██████    | 383/630 [05:47<03:40,  1.12it/s]
                                                 
{'loss': 0.5159, 'grad_norm': 2.137453727168369, 'learning_rate': 8.117839607201308e-07, 'epoch': 1.82}

 61%|██████    | 383/630 [05:47<03:40,  1.12it/s]
 61%|██████    | 384/630 [05:48<03:38,  1.13it/s]
                                                 
{'loss': 0.5525, 'grad_norm': 2.629615605386305, 'learning_rate': 8.085106382978723e-07, 'epoch': 1.83}

 61%|██████    | 384/630 [05:48<03:38,  1.13it/s]
 61%|██████    | 385/630 [05:49<03:38,  1.12it/s]
                                                 
{'loss': 0.5679, 'grad_norm': 2.058154415676157, 'learning_rate': 8.052373158756137e-07, 'epoch': 1.83}

 61%|██████    | 385/630 [05:49<03:38,  1.12it/s]
 61%|██████▏   | 386/630 [05:50<03:37,  1.12it/s]
                                                 
{'loss': 0.6, 'grad_norm': 1.8402678829930228, 'learning_rate': 8.019639934533551e-07, 'epoch': 1.84}

 61%|██████▏   | 386/630 [05:50<03:37,  1.12it/s]
 61%|██████▏   | 387/630 [05:51<03:35,  1.13it/s]
                                                 
{'loss': 0.571, 'grad_norm': 2.618611703559461, 'learning_rate': 7.986906710310966e-07, 'epoch': 1.84}

 61%|██████▏   | 387/630 [05:51<03:35,  1.13it/s]
 62%|██████▏   | 388/630 [05:52<03:34,  1.13it/s]
                                                 
{'loss': 0.6128, 'grad_norm': 2.190080841667349, 'learning_rate': 7.954173486088379e-07, 'epoch': 1.85}

 62%|██████▏   | 388/630 [05:52<03:34,  1.13it/s]
 62%|██████▏   | 389/630 [05:53<03:33,  1.13it/s]
                                                 
{'loss': 0.5606, 'grad_norm': 1.8533213994295699, 'learning_rate': 7.921440261865794e-07, 'epoch': 1.85}

 62%|██████▏   | 389/630 [05:53<03:33,  1.13it/s]
 62%|██████▏   | 390/630 [05:53<03:32,  1.13it/s]
                                                 
{'loss': 0.5939, 'grad_norm': 1.9331162530172856, 'learning_rate': 7.888707037643207e-07, 'epoch': 1.86}

 62%|██████▏   | 390/630 [05:53<03:32,  1.13it/s]
 62%|██████▏   | 391/630 [05:54<03:32,  1.13it/s]
                                                 
{'loss': 0.6253, 'grad_norm': 1.8406789929581477, 'learning_rate': 7.855973813420622e-07, 'epoch': 1.86}

 62%|██████▏   | 391/630 [05:54<03:32,  1.13it/s]
 62%|██████▏   | 392/630 [05:55<03:31,  1.13it/s]
                                                 
{'loss': 0.5131, 'grad_norm': 1.9384848771506051, 'learning_rate': 7.823240589198036e-07, 'epoch': 1.87}

 62%|██████▏   | 392/630 [05:55<03:31,  1.13it/s]
 62%|██████▏   | 393/630 [05:56<03:30,  1.13it/s]
                                                 
{'loss': 0.6295, 'grad_norm': 2.0295863263073946, 'learning_rate': 7.790507364975449e-07, 'epoch': 1.87}

 62%|██████▏   | 393/630 [05:56<03:30,  1.13it/s]
 63%|██████▎   | 394/630 [05:57<03:29,  1.13it/s]
                                                 
{'loss': 0.577, 'grad_norm': 2.203717270756147, 'learning_rate': 7.757774140752864e-07, 'epoch': 1.88}

 63%|██████▎   | 394/630 [05:57<03:29,  1.13it/s]
 63%|██████▎   | 395/630 [05:58<03:28,  1.12it/s]
                                                 
{'loss': 0.5674, 'grad_norm': 2.312357628873641, 'learning_rate': 7.725040916530278e-07, 'epoch': 1.88}

 63%|██████▎   | 395/630 [05:58<03:28,  1.12it/s]
 63%|██████▎   | 396/630 [05:59<03:27,  1.13it/s]
                                                 
{'loss': 0.5425, 'grad_norm': 2.368066498044638, 'learning_rate': 7.692307692307693e-07, 'epoch': 1.89}

 63%|██████▎   | 396/630 [05:59<03:27,  1.13it/s]
 63%|██████▎   | 397/630 [06:00<03:26,  1.13it/s]
                                                 
{'loss': 0.5093, 'grad_norm': 1.7769622933454858, 'learning_rate': 7.659574468085106e-07, 'epoch': 1.89}

 63%|██████▎   | 397/630 [06:00<03:26,  1.13it/s]
 63%|██████▎   | 398/630 [06:01<03:25,  1.13it/s]
                                                 
{'loss': 0.554, 'grad_norm': 1.9144357002285568, 'learning_rate': 7.626841243862519e-07, 'epoch': 1.9}

 63%|██████▎   | 398/630 [06:01<03:25,  1.13it/s]
 63%|██████▎   | 399/630 [06:01<03:24,  1.13it/s]
                                                 
{'loss': 0.603, 'grad_norm': 2.1676109578155867, 'learning_rate': 7.594108019639935e-07, 'epoch': 1.9}

 63%|██████▎   | 399/630 [06:01<03:24,  1.13it/s]
 63%|██████▎   | 400/630 [06:02<03:24,  1.13it/s]
                                                 
{'loss': 0.4967, 'grad_norm': 1.9191729731070988, 'learning_rate': 7.561374795417348e-07, 'epoch': 1.9}

 63%|██████▎   | 400/630 [06:02<03:24,  1.13it/s]
 64%|██████▎   | 401/630 [06:03<03:23,  1.13it/s]
                                                 
{'loss': 0.5883, 'grad_norm': 1.9509608546129193, 'learning_rate': 7.528641571194762e-07, 'epoch': 1.91}

 64%|██████▎   | 401/630 [06:03<03:23,  1.13it/s]
 64%|██████▍   | 402/630 [06:04<03:22,  1.13it/s]
                                                 
{'loss': 0.5362, 'grad_norm': 1.7908309979081802, 'learning_rate': 7.495908346972176e-07, 'epoch': 1.91}

 64%|██████▍   | 402/630 [06:04<03:22,  1.13it/s]
 64%|██████▍   | 403/630 [06:05<03:21,  1.13it/s]
                                                 
{'loss': 0.5407, 'grad_norm': 2.1127053674892187, 'learning_rate': 7.463175122749591e-07, 'epoch': 1.92}

 64%|██████▍   | 403/630 [06:05<03:21,  1.13it/s]
 64%|██████▍   | 404/630 [06:06<03:20,  1.13it/s]
                                                 
{'loss': 0.4949, 'grad_norm': 1.7968503889247853, 'learning_rate': 7.430441898527005e-07, 'epoch': 1.92}

 64%|██████▍   | 404/630 [06:06<03:20,  1.13it/s]
 64%|██████▍   | 405/630 [06:07<03:19,  1.13it/s]
                                                 
{'loss': 0.5519, 'grad_norm': 2.131974723992675, 'learning_rate': 7.397708674304418e-07, 'epoch': 1.93}

 64%|██████▍   | 405/630 [06:07<03:19,  1.13it/s]
 64%|██████▍   | 406/630 [06:08<03:18,  1.13it/s]
                                                 
{'loss': 0.5738, 'grad_norm': 1.9791727599456683, 'learning_rate': 7.364975450081832e-07, 'epoch': 1.93}

 64%|██████▍   | 406/630 [06:08<03:18,  1.13it/s]
 65%|██████▍   | 407/630 [06:09<03:17,  1.13it/s]
                                                 
{'loss': 0.5585, 'grad_norm': 1.9397759482038246, 'learning_rate': 7.332242225859247e-07, 'epoch': 1.94}

 65%|██████▍   | 407/630 [06:09<03:17,  1.13it/s]
 65%|██████▍   | 408/630 [06:09<03:16,  1.13it/s]
                                                 
{'loss': 0.5345, 'grad_norm': 1.7786337581743354, 'learning_rate': 7.299509001636661e-07, 'epoch': 1.94}

 65%|██████▍   | 408/630 [06:09<03:16,  1.13it/s]
 65%|██████▍   | 409/630 [06:10<03:15,  1.13it/s]
                                                 
{'loss': 0.5153, 'grad_norm': 2.0134245516426255, 'learning_rate': 7.266775777414075e-07, 'epoch': 1.95}

 65%|██████▍   | 409/630 [06:10<03:15,  1.13it/s]
 65%|██████▌   | 410/630 [06:11<03:14,  1.13it/s]
                                                 
{'loss': 0.5946, 'grad_norm': 2.2233187693307306, 'learning_rate': 7.23404255319149e-07, 'epoch': 1.95}

 65%|██████▌   | 410/630 [06:11<03:14,  1.13it/s]
 65%|██████▌   | 411/630 [06:12<03:13,  1.13it/s]
                                                 
{'loss': 0.5769, 'grad_norm': 2.5423011116939187, 'learning_rate': 7.201309328968903e-07, 'epoch': 1.96}

 65%|██████▌   | 411/630 [06:12<03:13,  1.13it/s]
 65%|██████▌   | 412/630 [06:13<03:12,  1.13it/s]
                                                 
{'loss': 0.5163, 'grad_norm': 1.8177529266756, 'learning_rate': 7.168576104746317e-07, 'epoch': 1.96}

 65%|██████▌   | 412/630 [06:13<03:12,  1.13it/s]
 66%|██████▌   | 413/630 [06:14<03:12,  1.13it/s]
                                                 
{'loss': 0.586, 'grad_norm': 1.9627584719980817, 'learning_rate': 7.135842880523731e-07, 'epoch': 1.97}

 66%|██████▌   | 413/630 [06:14<03:12,  1.13it/s]
 66%|██████▌   | 414/630 [06:15<03:11,  1.13it/s]
                                                 
{'loss': 0.5357, 'grad_norm': 1.8255365106601553, 'learning_rate': 7.103109656301146e-07, 'epoch': 1.97}

 66%|██████▌   | 414/630 [06:15<03:11,  1.13it/s]
 66%|██████▌   | 415/630 [06:16<03:10,  1.13it/s]
                                                 
{'loss': 0.5216, 'grad_norm': 2.0662446115177766, 'learning_rate': 7.07037643207856e-07, 'epoch': 1.98}

 66%|██████▌   | 415/630 [06:16<03:10,  1.13it/s]
 66%|██████▌   | 416/630 [06:17<03:09,  1.13it/s]
                                                 
{'loss': 0.5297, 'grad_norm': 1.7815474721917481, 'learning_rate': 7.037643207855973e-07, 'epoch': 1.98}

 66%|██████▌   | 416/630 [06:17<03:09,  1.13it/s]
 66%|██████▌   | 417/630 [06:17<03:08,  1.13it/s]
                                                 
{'loss': 0.5697, 'grad_norm': 2.246192935316456, 'learning_rate': 7.004909983633388e-07, 'epoch': 1.99}

 66%|██████▌   | 417/630 [06:17<03:08,  1.13it/s]
 66%|██████▋   | 418/630 [06:18<03:07,  1.13it/s]
                                                 
{'loss': 0.5662, 'grad_norm': 1.9570763781269451, 'learning_rate': 6.972176759410802e-07, 'epoch': 1.99}

 66%|██████▋   | 418/630 [06:18<03:07,  1.13it/s]
 67%|██████▋   | 419/630 [06:19<03:08,  1.12it/s]
                                                 
{'loss': 0.5139, 'grad_norm': 2.4498507726785093, 'learning_rate': 6.939443535188215e-07, 'epoch': 2.0}

 67%|██████▋   | 419/630 [06:19<03:08,  1.12it/s]
 67%|██████▋   | 420/630 [06:20<03:07,  1.12it/s]
                                                 
{'loss': 0.4684, 'grad_norm': 1.9704972412700195, 'learning_rate': 6.90671031096563e-07, 'epoch': 2.0}

 67%|██████▋   | 420/630 [06:20<03:07,  1.12it/s]
 67%|██████▋   | 421/630 [06:21<03:06,  1.12it/s]
                                                 
{'loss': 0.4942, 'grad_norm': 1.829179754115815, 'learning_rate': 6.873977086743043e-07, 'epoch': 2.0}

 67%|██████▋   | 421/630 [06:21<03:06,  1.12it/s]
 67%|██████▋   | 422/630 [06:22<03:05,  1.12it/s]
                                                 
{'loss': 0.4831, 'grad_norm': 1.7907619891454847, 'learning_rate': 6.841243862520459e-07, 'epoch': 2.01}

 67%|██████▋   | 422/630 [06:22<03:05,  1.12it/s]
 67%|██████▋   | 423/630 [06:23<03:04,  1.12it/s]
                                                 
{'loss': 0.4211, 'grad_norm': 1.8261832233758568, 'learning_rate': 6.808510638297872e-07, 'epoch': 2.01}

 67%|██████▋   | 423/630 [06:23<03:04,  1.12it/s]
 67%|██████▋   | 424/630 [06:24<03:02,  1.13it/s]
                                                 
{'loss': 0.5138, 'grad_norm': 2.2409027370711145, 'learning_rate': 6.775777414075286e-07, 'epoch': 2.02}

 67%|██████▋   | 424/630 [06:24<03:02,  1.13it/s]
 67%|██████▋   | 425/630 [06:25<03:02,  1.13it/s]
                                                 
{'loss': 0.5332, 'grad_norm': 2.3351689687918857, 'learning_rate': 6.7430441898527e-07, 'epoch': 2.02}

 67%|██████▋   | 425/630 [06:25<03:02,  1.13it/s]
 68%|██████▊   | 426/630 [06:25<03:01,  1.12it/s]
                                                 
{'loss': 0.4636, 'grad_norm': 1.7147695986144191, 'learning_rate': 6.710310965630114e-07, 'epoch': 2.03}

 68%|██████▊   | 426/630 [06:25<03:01,  1.12it/s]
 68%|██████▊   | 427/630 [06:26<03:00,  1.12it/s]
                                                 
{'loss': 0.5035, 'grad_norm': 2.4771637606405914, 'learning_rate': 6.677577741407529e-07, 'epoch': 2.03}

 68%|██████▊   | 427/630 [06:26<03:00,  1.12it/s]
 68%|██████▊   | 428/630 [06:27<02:59,  1.13it/s]
                                                 
{'loss': 0.5207, 'grad_norm': 1.8638658696521846, 'learning_rate': 6.644844517184942e-07, 'epoch': 2.04}

 68%|██████▊   | 428/630 [06:27<02:59,  1.13it/s]
 68%|██████▊   | 429/630 [06:28<02:58,  1.12it/s]
                                                 
{'loss': 0.4965, 'grad_norm': 1.867880153232834, 'learning_rate': 6.612111292962356e-07, 'epoch': 2.04}

 68%|██████▊   | 429/630 [06:28<02:58,  1.12it/s]
 68%|██████▊   | 430/630 [06:29<02:57,  1.13it/s]
                                                 
{'loss': 0.48, 'grad_norm': 2.050550443375546, 'learning_rate': 6.579378068739771e-07, 'epoch': 2.05}

 68%|██████▊   | 430/630 [06:29<02:57,  1.13it/s]
 68%|██████▊   | 431/630 [06:30<02:56,  1.13it/s]
                                                 
{'loss': 0.5598, 'grad_norm': 2.0558336374028445, 'learning_rate': 6.546644844517185e-07, 'epoch': 2.05}

 68%|██████▊   | 431/630 [06:30<02:56,  1.13it/s]
 69%|██████▊   | 432/630 [06:31<02:56,  1.12it/s]
                                                 
{'loss': 0.5027, 'grad_norm': 1.9804853753189549, 'learning_rate': 6.513911620294599e-07, 'epoch': 2.06}

 69%|██████▊   | 432/630 [06:31<02:56,  1.12it/s]
 69%|██████▊   | 433/630 [06:32<02:55,  1.12it/s]
                                                 
{'loss': 0.4302, 'grad_norm': 1.9872044470580539, 'learning_rate': 6.481178396072012e-07, 'epoch': 2.06}

 69%|██████▊   | 433/630 [06:32<02:55,  1.12it/s]
 69%|██████▉   | 434/630 [06:33<02:54,  1.12it/s]
                                                 
{'loss': 0.4716, 'grad_norm': 2.1938889862153923, 'learning_rate': 6.448445171849427e-07, 'epoch': 2.07}

 69%|██████▉   | 434/630 [06:33<02:54,  1.12it/s]
 69%|██████▉   | 435/630 [06:33<02:54,  1.12it/s]
                                                 
{'loss': 0.5069, 'grad_norm': 2.3043967461278783, 'learning_rate': 6.415711947626841e-07, 'epoch': 2.07}

 69%|██████▉   | 435/630 [06:33<02:54,  1.12it/s]
 69%|██████▉   | 436/630 [06:34<02:52,  1.12it/s]
                                                 
{'loss': 0.4916, 'grad_norm': 2.138624897495506, 'learning_rate': 6.382978723404255e-07, 'epoch': 2.08}

 69%|██████▉   | 436/630 [06:34<02:52,  1.12it/s]
 69%|██████▉   | 437/630 [06:35<02:51,  1.12it/s]
                                                 
{'loss': 0.6362, 'grad_norm': 3.0409932070359433, 'learning_rate': 6.35024549918167e-07, 'epoch': 2.08}

 69%|██████▉   | 437/630 [06:35<02:51,  1.12it/s]
 70%|██████▉   | 438/630 [06:36<02:50,  1.13it/s]
                                                 
{'loss': 0.4467, 'grad_norm': 1.7876178271363097, 'learning_rate': 6.317512274959084e-07, 'epoch': 2.09}

 70%|██████▉   | 438/630 [06:36<02:50,  1.13it/s]
 70%|██████▉   | 439/630 [06:37<02:49,  1.12it/s]
                                                 
{'loss': 0.4676, 'grad_norm': 2.658326665068003, 'learning_rate': 6.284779050736497e-07, 'epoch': 2.09}

 70%|██████▉   | 439/630 [06:37<02:49,  1.12it/s]
 70%|██████▉   | 440/630 [06:38<02:48,  1.13it/s]
                                                 
{'loss': 0.5148, 'grad_norm': 1.9777452356402114, 'learning_rate': 6.252045826513911e-07, 'epoch': 2.1}

 70%|██████▉   | 440/630 [06:38<02:48,  1.13it/s]
 70%|███████   | 441/630 [06:39<02:47,  1.13it/s]
                                                 
{'loss': 0.4996, 'grad_norm': 2.0167694418423894, 'learning_rate': 6.219312602291326e-07, 'epoch': 2.1}

 70%|███████   | 441/630 [06:39<02:47,  1.13it/s]
 70%|███████   | 442/630 [06:40<02:46,  1.13it/s]
                                                 
{'loss': 0.5279, 'grad_norm': 2.255342594354761, 'learning_rate': 6.186579378068739e-07, 'epoch': 2.1}

 70%|███████   | 442/630 [06:40<02:46,  1.13it/s]
 70%|███████   | 443/630 [06:41<02:45,  1.13it/s]
                                                 
{'loss': 0.5616, 'grad_norm': 2.1534317620928545, 'learning_rate': 6.153846153846154e-07, 'epoch': 2.11}

 70%|███████   | 443/630 [06:41<02:45,  1.13it/s]
 70%|███████   | 444/630 [06:41<02:45,  1.12it/s]
                                                 
{'loss': 0.4912, 'grad_norm': 2.1099099901119964, 'learning_rate': 6.121112929623567e-07, 'epoch': 2.11}

 70%|███████   | 444/630 [06:41<02:45,  1.12it/s]
 71%|███████   | 445/630 [06:42<02:44,  1.13it/s]
                                                 
{'loss': 0.4671, 'grad_norm': 2.3980567679986717, 'learning_rate': 6.088379705400983e-07, 'epoch': 2.12}

 71%|███████   | 445/630 [06:42<02:44,  1.13it/s]
 71%|███████   | 446/630 [06:43<02:42,  1.13it/s]
                                                 
{'loss': 0.5824, 'grad_norm': 2.043401629984876, 'learning_rate': 6.055646481178396e-07, 'epoch': 2.12}

 71%|███████   | 446/630 [06:43<02:42,  1.13it/s]
 71%|███████   | 447/630 [06:44<02:42,  1.13it/s]
                                                 
{'loss': 0.585, 'grad_norm': 2.273699005487469, 'learning_rate': 6.022913256955809e-07, 'epoch': 2.13}

 71%|███████   | 447/630 [06:44<02:42,  1.13it/s]
 71%|███████   | 448/630 [06:45<02:41,  1.13it/s]
                                                 
{'loss': 0.4923, 'grad_norm': 2.0889970571042524, 'learning_rate': 5.990180032733224e-07, 'epoch': 2.13}

 71%|███████   | 448/630 [06:45<02:41,  1.13it/s]
 71%|███████▏  | 449/630 [06:46<02:40,  1.13it/s]
                                                 
{'loss': 0.4439, 'grad_norm': 1.9481857225306836, 'learning_rate': 5.957446808510638e-07, 'epoch': 2.14}

 71%|███████▏  | 449/630 [06:46<02:40,  1.13it/s]
 71%|███████▏  | 450/630 [06:47<02:40,  1.12it/s]
                                                 
{'loss': 0.5263, 'grad_norm': 1.9791585389481068, 'learning_rate': 5.924713584288053e-07, 'epoch': 2.14}

 71%|███████▏  | 450/630 [06:47<02:40,  1.12it/s]
 72%|███████▏  | 451/630 [06:48<02:38,  1.13it/s]
                                                 
{'loss': 0.5071, 'grad_norm': 2.02295680528349, 'learning_rate': 5.891980360065466e-07, 'epoch': 2.15}

 72%|███████▏  | 451/630 [06:48<02:38,  1.13it/s]
 72%|███████▏  | 452/630 [06:49<02:38,  1.13it/s]
                                                 
{'loss': 0.5084, 'grad_norm': 2.0374724876361228, 'learning_rate': 5.85924713584288e-07, 'epoch': 2.15}

 72%|███████▏  | 452/630 [06:49<02:38,  1.13it/s]
 72%|███████▏  | 453/630 [06:49<02:37,  1.12it/s]
                                                 
{'loss': 0.4666, 'grad_norm': 1.8816449447646602, 'learning_rate': 5.826513911620295e-07, 'epoch': 2.16}

 72%|███████▏  | 453/630 [06:49<02:37,  1.12it/s]
 72%|███████▏  | 454/630 [06:50<02:36,  1.13it/s]
                                                 
{'loss': 0.4974, 'grad_norm': 2.1041275410187112, 'learning_rate': 5.793780687397708e-07, 'epoch': 2.16}

 72%|███████▏  | 454/630 [06:50<02:36,  1.13it/s]
 72%|███████▏  | 455/630 [06:51<02:35,  1.13it/s]
                                                 
{'loss': 0.5212, 'grad_norm': 2.0549394210814, 'learning_rate': 5.761047463175122e-07, 'epoch': 2.17}

 72%|███████▏  | 455/630 [06:51<02:35,  1.13it/s]
 72%|███████▏  | 456/630 [06:52<02:34,  1.13it/s]
                                                 
{'loss': 0.4813, 'grad_norm': 1.8532752799944978, 'learning_rate': 5.728314238952536e-07, 'epoch': 2.17}

 72%|███████▏  | 456/630 [06:52<02:34,  1.13it/s]
 73%|███████▎  | 457/630 [06:53<02:33,  1.13it/s]
                                                 
{'loss': 0.4967, 'grad_norm': 2.251955350652549, 'learning_rate': 5.695581014729951e-07, 'epoch': 2.18}

 73%|███████▎  | 457/630 [06:53<02:33,  1.13it/s]
 73%|███████▎  | 458/630 [06:54<02:31,  1.13it/s]
                                                 
{'loss': 0.4811, 'grad_norm': 1.8954394054986061, 'learning_rate': 5.662847790507365e-07, 'epoch': 2.18}

 73%|███████▎  | 458/630 [06:54<02:31,  1.13it/s]
 73%|███████▎  | 459/630 [06:55<02:31,  1.13it/s]
                                                 
{'loss': 0.4398, 'grad_norm': 2.1741770588059093, 'learning_rate': 5.630114566284779e-07, 'epoch': 2.19}

 73%|███████▎  | 459/630 [06:55<02:31,  1.13it/s]
 73%|███████▎  | 460/630 [06:56<02:31,  1.13it/s]
                                                 
{'loss': 0.5569, 'grad_norm': 2.2665526447991984, 'learning_rate': 5.597381342062193e-07, 'epoch': 2.19}

 73%|███████▎  | 460/630 [06:56<02:31,  1.13it/s]
 73%|███████▎  | 461/630 [06:57<02:30,  1.13it/s]
                                                 
{'loss': 0.4442, 'grad_norm': 1.6921937527319466, 'learning_rate': 5.564648117839607e-07, 'epoch': 2.2}

 73%|███████▎  | 461/630 [06:57<02:30,  1.13it/s]
 73%|███████▎  | 462/630 [06:57<02:29,  1.13it/s]
                                                 
{'loss': 0.6006, 'grad_norm': 2.163096020927105, 'learning_rate': 5.531914893617021e-07, 'epoch': 2.2}

 73%|███████▎  | 462/630 [06:57<02:29,  1.13it/s]
 73%|███████▎  | 463/630 [06:58<02:28,  1.13it/s]
                                                 
{'loss': 0.4827, 'grad_norm': 2.0740894092666355, 'learning_rate': 5.499181669394435e-07, 'epoch': 2.2}

 73%|███████▎  | 463/630 [06:58<02:28,  1.13it/s]
 74%|███████▎  | 464/630 [06:59<02:27,  1.13it/s]
                                                 
{'loss': 0.493, 'grad_norm': 1.877952597202284, 'learning_rate': 5.46644844517185e-07, 'epoch': 2.21}

 74%|███████▎  | 464/630 [06:59<02:27,  1.13it/s]
 74%|███████▍  | 465/630 [07:00<02:26,  1.13it/s]
                                                 
{'loss': 0.4974, 'grad_norm': 2.1065730823318587, 'learning_rate': 5.433715220949263e-07, 'epoch': 2.21}

 74%|███████▍  | 465/630 [07:00<02:26,  1.13it/s]
 74%|███████▍  | 466/630 [07:01<02:25,  1.13it/s]
                                                 
{'loss': 0.4857, 'grad_norm': 2.0234730122331714, 'learning_rate': 5.400981996726678e-07, 'epoch': 2.22}

 74%|███████▍  | 466/630 [07:01<02:25,  1.13it/s]
 74%|███████▍  | 467/630 [07:02<02:24,  1.13it/s]
                                                 
{'loss': 0.4963, 'grad_norm': 2.025778959539414, 'learning_rate': 5.368248772504091e-07, 'epoch': 2.22}

 74%|███████▍  | 467/630 [07:02<02:24,  1.13it/s]
 74%|███████▍  | 468/630 [07:03<02:23,  1.13it/s]
                                                 
{'loss': 0.4852, 'grad_norm': 1.8074635899116767, 'learning_rate': 5.335515548281506e-07, 'epoch': 2.23}

 74%|███████▍  | 468/630 [07:03<02:23,  1.13it/s]
 74%|███████▍  | 469/630 [07:04<02:23,  1.13it/s]
                                                 
{'loss': 0.4424, 'grad_norm': 2.136103679821945, 'learning_rate': 5.30278232405892e-07, 'epoch': 2.23}

 74%|███████▍  | 469/630 [07:04<02:23,  1.13it/s]
 75%|███████▍  | 470/630 [07:04<02:22,  1.13it/s]
                                                 
{'loss': 0.4707, 'grad_norm': 1.7473716125519894, 'learning_rate': 5.270049099836333e-07, 'epoch': 2.24}

 75%|███████▍  | 470/630 [07:04<02:22,  1.13it/s]
 75%|███████▍  | 471/630 [07:05<02:21,  1.12it/s]
                                                 
{'loss': 0.5381, 'grad_norm': 1.9432127682449645, 'learning_rate': 5.237315875613748e-07, 'epoch': 2.24}

 75%|███████▍  | 471/630 [07:05<02:21,  1.12it/s]
 75%|███████▍  | 472/630 [07:06<02:20,  1.12it/s]
                                                 
{'loss': 0.4838, 'grad_norm': 2.0071238838414724, 'learning_rate': 5.204582651391162e-07, 'epoch': 2.25}

 75%|███████▍  | 472/630 [07:06<02:20,  1.12it/s]
 75%|███████▌  | 473/630 [07:07<02:19,  1.13it/s]
                                                 
{'loss': 0.4737, 'grad_norm': 1.9314769799071176, 'learning_rate': 5.171849427168577e-07, 'epoch': 2.25}

 75%|███████▌  | 473/630 [07:07<02:19,  1.13it/s]
 75%|███████▌  | 474/630 [07:08<02:18,  1.12it/s]
                                                 
{'loss': 0.495, 'grad_norm': 1.8950673931238744, 'learning_rate': 5.13911620294599e-07, 'epoch': 2.26}

 75%|███████▌  | 474/630 [07:08<02:18,  1.12it/s]
 75%|███████▌  | 475/630 [07:09<02:17,  1.13it/s]
                                                 
{'loss': 0.5273, 'grad_norm': 2.052227178256184, 'learning_rate': 5.106382978723403e-07, 'epoch': 2.26}

 75%|███████▌  | 475/630 [07:09<02:17,  1.13it/s]
 76%|███████▌  | 476/630 [07:10<02:16,  1.13it/s]
                                                 
{'loss': 0.4192, 'grad_norm': 1.6326625663797896, 'learning_rate': 5.073649754500819e-07, 'epoch': 2.27}

 76%|███████▌  | 476/630 [07:10<02:16,  1.13it/s]
 76%|███████▌  | 477/630 [07:11<02:15,  1.13it/s]
                                                 
{'loss': 0.4522, 'grad_norm': 1.822022388807368, 'learning_rate': 5.040916530278232e-07, 'epoch': 2.27}

 76%|███████▌  | 477/630 [07:11<02:15,  1.13it/s]
 76%|███████▌  | 478/630 [07:12<02:15,  1.12it/s]
                                                 
{'loss': 0.4635, 'grad_norm': 2.0717486866254857, 'learning_rate': 5.008183306055646e-07, 'epoch': 2.28}

 76%|███████▌  | 478/630 [07:12<02:15,  1.12it/s]
 76%|███████▌  | 479/630 [07:12<02:14,  1.12it/s]
                                                 
{'loss': 0.4781, 'grad_norm': 1.9312976633448782, 'learning_rate': 4.97545008183306e-07, 'epoch': 2.28}

 76%|███████▌  | 479/630 [07:12<02:14,  1.12it/s]
 76%|███████▌  | 480/630 [07:13<02:13,  1.12it/s]
                                                 
{'loss': 0.5343, 'grad_norm': 1.9557931675083253, 'learning_rate': 4.942716857610474e-07, 'epoch': 2.29}

 76%|███████▌  | 480/630 [07:13<02:13,  1.12it/s]
 76%|███████▋  | 481/630 [07:14<02:12,  1.12it/s]
                                                 
{'loss': 0.5154, 'grad_norm': 1.8542306012617995, 'learning_rate': 4.909983633387889e-07, 'epoch': 2.29}

 76%|███████▋  | 481/630 [07:14<02:12,  1.12it/s]
 77%|███████▋  | 482/630 [07:15<02:11,  1.12it/s]
                                                 
{'loss': 0.4953, 'grad_norm': 2.2889243940864983, 'learning_rate': 4.877250409165303e-07, 'epoch': 2.3}

 77%|███████▋  | 482/630 [07:15<02:11,  1.12it/s]
 77%|███████▋  | 483/630 [07:16<02:10,  1.13it/s]
                                                 
{'loss': 0.4772, 'grad_norm': 2.3141921564241152, 'learning_rate': 4.844517184942716e-07, 'epoch': 2.3}

 77%|███████▋  | 483/630 [07:16<02:10,  1.13it/s]
 77%|███████▋  | 484/630 [07:17<02:09,  1.13it/s]
                                                 
{'loss': 0.4585, 'grad_norm': 2.200309618575596, 'learning_rate': 4.811783960720131e-07, 'epoch': 2.3}

 77%|███████▋  | 484/630 [07:17<02:09,  1.13it/s]
 77%|███████▋  | 485/630 [07:18<02:08,  1.13it/s]
                                                 
{'loss': 0.4359, 'grad_norm': 2.2016903723169436, 'learning_rate': 4.779050736497545e-07, 'epoch': 2.31}

 77%|███████▋  | 485/630 [07:18<02:08,  1.13it/s]
 77%|███████▋  | 486/630 [07:19<02:07,  1.13it/s]
                                                 
{'loss': 0.5052, 'grad_norm': 2.0123627773279704, 'learning_rate': 4.746317512274959e-07, 'epoch': 2.31}

 77%|███████▋  | 486/630 [07:19<02:07,  1.13it/s]
 77%|███████▋  | 487/630 [07:20<02:06,  1.13it/s]
                                                 
{'loss': 0.4541, 'grad_norm': 2.1896920637954413, 'learning_rate': 4.713584288052373e-07, 'epoch': 2.32}

 77%|███████▋  | 487/630 [07:20<02:06,  1.13it/s]
 77%|███████▋  | 488/630 [07:20<02:05,  1.13it/s]
                                                 
{'loss': 0.4584, 'grad_norm': 1.9508221916280508, 'learning_rate': 4.6808510638297873e-07, 'epoch': 2.32}

 77%|███████▋  | 488/630 [07:20<02:05,  1.13it/s]
 78%|███████▊  | 489/630 [07:21<02:05,  1.13it/s]
                                                 
{'loss': 0.4296, 'grad_norm': 2.390760620777384, 'learning_rate': 4.648117839607201e-07, 'epoch': 2.33}

 78%|███████▊  | 489/630 [07:21<02:05,  1.13it/s]
 78%|███████▊  | 490/630 [07:22<02:04,  1.12it/s]
                                                 
{'loss': 0.5942, 'grad_norm': 2.0578176580066585, 'learning_rate': 4.6153846153846156e-07, 'epoch': 2.33}

 78%|███████▊  | 490/630 [07:22<02:04,  1.12it/s]
 78%|███████▊  | 491/630 [07:23<02:04,  1.12it/s]
                                                 
{'loss': 0.5021, 'grad_norm': 2.184471809342526, 'learning_rate': 4.582651391162029e-07, 'epoch': 2.34}

 78%|███████▊  | 491/630 [07:23<02:04,  1.12it/s]
 78%|███████▊  | 492/630 [07:24<02:02,  1.12it/s]
                                                 
{'loss': 0.51, 'grad_norm': 2.148891648765226, 'learning_rate': 4.5499181669394434e-07, 'epoch': 2.34}

 78%|███████▊  | 492/630 [07:24<02:02,  1.12it/s]
 78%|███████▊  | 493/630 [07:25<02:01,  1.12it/s]
                                                 
{'loss': 0.4792, 'grad_norm': 2.0490889563731858, 'learning_rate': 4.517184942716857e-07, 'epoch': 2.35}

 78%|███████▊  | 493/630 [07:25<02:01,  1.12it/s]
 78%|███████▊  | 494/630 [07:26<02:00,  1.13it/s]
                                                 
{'loss': 0.5745, 'grad_norm': 2.234569803223158, 'learning_rate': 4.4844517184942717e-07, 'epoch': 2.35}

 78%|███████▊  | 494/630 [07:26<02:00,  1.13it/s]
 79%|███████▊  | 495/630 [07:27<01:59,  1.13it/s]
                                                 
{'loss': 0.4255, 'grad_norm': 2.2412270209531555, 'learning_rate': 4.4517184942716855e-07, 'epoch': 2.36}

 79%|███████▊  | 495/630 [07:27<01:59,  1.13it/s]
 79%|███████▊  | 496/630 [07:28<01:59,  1.13it/s]
                                                 
{'loss': 0.4695, 'grad_norm': 1.8842723301428597, 'learning_rate': 4.4189852700491e-07, 'epoch': 2.36}

 79%|███████▊  | 496/630 [07:28<01:59,  1.13it/s]
 79%|███████▉  | 497/630 [07:28<01:58,  1.13it/s]
                                                 
{'loss': 0.496, 'grad_norm': 1.8225188961404268, 'learning_rate': 4.386252045826514e-07, 'epoch': 2.37}

 79%|███████▉  | 497/630 [07:28<01:58,  1.13it/s]
 79%|███████▉  | 498/630 [07:29<01:57,  1.13it/s]
                                                 
{'loss': 0.485, 'grad_norm': 1.819211526801383, 'learning_rate': 4.3535188216039277e-07, 'epoch': 2.37}

 79%|███████▉  | 498/630 [07:29<01:57,  1.13it/s]
 79%|███████▉  | 499/630 [07:30<01:56,  1.13it/s]
                                                 
{'loss': 0.4655, 'grad_norm': 2.1954944360522095, 'learning_rate': 4.3207855973813416e-07, 'epoch': 2.38}

 79%|███████▉  | 499/630 [07:30<01:56,  1.13it/s]
 79%|███████▉  | 500/630 [07:31<01:55,  1.12it/s]
                                                 
{'loss': 0.5273, 'grad_norm': 2.171944902479379, 'learning_rate': 4.288052373158756e-07, 'epoch': 2.38}

 79%|███████▉  | 500/630 [07:31<01:55,  1.12it/s][rank1]: Traceback (most recent call last):
[rank1]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/serialization.py", line 628, in save
[rank1]:     _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
[rank1]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/serialization.py", line 862, in _save
[rank1]:     zip_file.write_record(name, storage, num_bytes)
[rank1]: RuntimeError: [enforce fail at inline_container.cc:764] . PytorchStreamWriter failed writing file data/0: file write failed

[rank1]: During handling of the above exception, another exception occurred:

[rank1]: Traceback (most recent call last):
[rank1]:   File "/opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/llm-attacks/stanford_alpaca/train.py", line 222, in <module>
[rank1]:     train()
[rank1]:   File "/opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/llm-attacks/stanford_alpaca/train.py", line 216, in train
[rank1]:     trainer.train()
[rank1]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/trainer.py", line 1885, in train
[rank1]:     return inner_training_loop(
[rank1]:            ^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/trainer.py", line 2291, in _inner_training_loop
[rank1]:     self._maybe_log_save_evaluate(tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)
[rank1]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/trainer.py", line 2732, in _maybe_log_save_evaluate
[rank1]:     self._save_checkpoint(model, trial, metrics=metrics)
[rank1]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/trainer.py", line 2815, in _save_checkpoint
[rank1]:     self._save_optimizer_and_scheduler(output_dir)
[rank1]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/trainer.py", line 2914, in _save_optimizer_and_scheduler
[rank1]:     self.model_wrapped.save_checkpoint(output_dir)
[rank1]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 3112, in save_checkpoint
[rank1]:     self._save_zero_checkpoint(save_dir, tag)
[rank1]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 3473, in _save_zero_checkpoint
[rank1]:     self.checkpoint_engine.save(zero_sd, zero_checkpoint_name)
[rank1]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/deepspeed/runtime/checkpoint_engine/torch_checkpoint_engine.py", line 22, in save
[rank1]:     torch.save(state_dict, path)
[rank1]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/serialization.py", line 627, in save
[rank1]:     with _open_zipfile_writer(f) as opened_zipfile:
[rank1]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/serialization.py", line 475, in __exit__
[rank1]:     self.file_like.write_end_of_file()
[rank1]: RuntimeError: [enforce fail at inline_container.cc:595] . unexpected pos 2432 vs 2326
[rank2]: Traceback (most recent call last):
[rank2]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/serialization.py", line 628, in save
[rank2]:     _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
[rank2]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/serialization.py", line 862, in _save
[rank2]:     zip_file.write_record(name, storage, num_bytes)
[rank2]: RuntimeError: [enforce fail at inline_container.cc:764] . PytorchStreamWriter failed writing file data/0: file write failed

[rank2]: During handling of the above exception, another exception occurred:

[rank2]: Traceback (most recent call last):
[rank2]:   File "/opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/llm-attacks/stanford_alpaca/train.py", line 222, in <module>
[rank2]:     train()
[rank2]:   File "/opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/llm-attacks/stanford_alpaca/train.py", line 216, in train
[rank2]:     trainer.train()
[rank2]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/trainer.py", line 1885, in train
[rank2]:     return inner_training_loop(
[rank2]:            ^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/trainer.py", line 2291, in _inner_training_loop
[rank2]:     self._maybe_log_save_evaluate(tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)
[rank2]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/trainer.py", line 2732, in _maybe_log_save_evaluate
[rank2]:     self._save_checkpoint(model, trial, metrics=metrics)
[rank2]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/trainer.py", line 2815, in _save_checkpoint
[rank2]:     self._save_optimizer_and_scheduler(output_dir)
[rank2]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/trainer.py", line 2914, in _save_optimizer_and_scheduler
[rank2]:     self.model_wrapped.save_checkpoint(output_dir)
[rank2]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 3112, in save_checkpoint
[rank2]:     self._save_zero_checkpoint(save_dir, tag)
[rank2]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 3473, in _save_zero_checkpoint
[rank2]:     self.checkpoint_engine.save(zero_sd, zero_checkpoint_name)
[rank2]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/deepspeed/runtime/checkpoint_engine/torch_checkpoint_engine.py", line 22, in save
[rank2]:     torch.save(state_dict, path)
[rank2]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/serialization.py", line 627, in save
[rank2]:     with _open_zipfile_writer(f) as opened_zipfile:
[rank2]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/serialization.py", line 475, in __exit__
[rank2]:     self.file_like.write_end_of_file()
[rank2]: RuntimeError: [enforce fail at inline_container.cc:595] . unexpected pos 2432 vs 2326
[rank3]: Traceback (most recent call last):
[rank3]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/serialization.py", line 628, in save
[rank3]:     _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
[rank3]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/serialization.py", line 862, in _save
[rank3]:     zip_file.write_record(name, storage, num_bytes)
[rank3]: RuntimeError: [enforce fail at inline_container.cc:764] . PytorchStreamWriter failed writing file data/0: file write failed

[rank3]: During handling of the above exception, another exception occurred:

[rank3]: Traceback (most recent call last):
[rank3]:   File "/opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/llm-attacks/stanford_alpaca/train.py", line 222, in <module>
[rank3]:     train()
[rank3]:   File "/opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/llm-attacks/stanford_alpaca/train.py", line 216, in train
[rank3]:     trainer.train()
[rank3]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/trainer.py", line 1885, in train
[rank3]:     return inner_training_loop(
[rank3]:            ^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/trainer.py", line 2291, in _inner_training_loop
[rank3]:     self._maybe_log_save_evaluate(tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)
[rank3]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/trainer.py", line 2732, in _maybe_log_save_evaluate
[rank3]:     self._save_checkpoint(model, trial, metrics=metrics)
[rank3]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/trainer.py", line 2815, in _save_checkpoint
[rank3]:     self._save_optimizer_and_scheduler(output_dir)
[rank3]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/trainer.py", line 2914, in _save_optimizer_and_scheduler
[rank3]:     self.model_wrapped.save_checkpoint(output_dir)
[rank3]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 3112, in save_checkpoint
[rank3]:     self._save_zero_checkpoint(save_dir, tag)
[rank3]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 3473, in _save_zero_checkpoint
[rank3]:     self.checkpoint_engine.save(zero_sd, zero_checkpoint_name)
[rank3]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/deepspeed/runtime/checkpoint_engine/torch_checkpoint_engine.py", line 22, in save
[rank3]:     torch.save(state_dict, path)
[rank3]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/serialization.py", line 627, in save
[rank3]:     with _open_zipfile_writer(f) as opened_zipfile:
[rank3]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/serialization.py", line 475, in __exit__
[rank3]:     self.file_like.write_end_of_file()
[rank3]: RuntimeError: [enforce fail at inline_container.cc:595] . unexpected pos 2432 vs 2326
[rank7]: Traceback (most recent call last):
[rank7]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/serialization.py", line 628, in save
[rank7]:     _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
[rank7]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/serialization.py", line 862, in _save
[rank7]:     zip_file.write_record(name, storage, num_bytes)
[rank7]: RuntimeError: [enforce fail at inline_container.cc:764] . PytorchStreamWriter failed writing file data/0: file write failed

[rank7]: During handling of the above exception, another exception occurred:

[rank7]: Traceback (most recent call last):
[rank7]:   File "/opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/llm-attacks/stanford_alpaca/train.py", line 222, in <module>
[rank7]:     train()
[rank7]:   File "/opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/llm-attacks/stanford_alpaca/train.py", line 216, in train
[rank7]:     trainer.train()
[rank7]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/trainer.py", line 1885, in train
[rank7]:     return inner_training_loop(
[rank7]:            ^^^^^^^^^^^^^^^^^^^^
[rank7]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/trainer.py", line 2291, in _inner_training_loop
[rank7]:     self._maybe_log_save_evaluate(tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)
[rank7]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/trainer.py", line 2732, in _maybe_log_save_evaluate
[rank7]:     self._save_checkpoint(model, trial, metrics=metrics)
[rank7]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/trainer.py", line 2815, in _save_checkpoint
[rank7]:     self._save_optimizer_and_scheduler(output_dir)
[rank7]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/trainer.py", line 2914, in _save_optimizer_and_scheduler
[rank7]:     self.model_wrapped.save_checkpoint(output_dir)
[rank7]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 3112, in save_checkpoint
[rank7]:     self._save_zero_checkpoint(save_dir, tag)
[rank7]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 3473, in _save_zero_checkpoint
[rank7]:     self.checkpoint_engine.save(zero_sd, zero_checkpoint_name)
[rank7]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/deepspeed/runtime/checkpoint_engine/torch_checkpoint_engine.py", line 22, in save
[rank7]:     torch.save(state_dict, path)
[rank7]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/serialization.py", line 627, in save
[rank7]:     with _open_zipfile_writer(f) as opened_zipfile:
[rank7]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/serialization.py", line 475, in __exit__
[rank7]:     self.file_like.write_end_of_file()
[rank7]: RuntimeError: [enforce fail at inline_container.cc:595] . unexpected pos 2432 vs 2326
[rank6]: Traceback (most recent call last):
[rank6]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/serialization.py", line 628, in save
[rank6]:     _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
[rank6]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/serialization.py", line 862, in _save
[rank6]:     zip_file.write_record(name, storage, num_bytes)
[rank6]: RuntimeError: [enforce fail at inline_container.cc:764] . PytorchStreamWriter failed writing file data/0: file write failed

[rank6]: During handling of the above exception, another exception occurred:

[rank6]: Traceback (most recent call last):
[rank6]:   File "/opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/llm-attacks/stanford_alpaca/train.py", line 222, in <module>
[rank6]:     train()
[rank6]:   File "/opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/llm-attacks/stanford_alpaca/train.py", line 216, in train
[rank6]:     trainer.train()
[rank6]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/trainer.py", line 1885, in train
[rank6]:     return inner_training_loop(
[rank6]:            ^^^^^^^^^^^^^^^^^^^^
[rank6]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/trainer.py", line 2291, in _inner_training_loop
[rank6]:     self._maybe_log_save_evaluate(tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)
[rank6]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/trainer.py", line 2732, in _maybe_log_save_evaluate
[rank6]:     self._save_checkpoint(model, trial, metrics=metrics)
[rank6]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/trainer.py", line 2815, in _save_checkpoint
[rank6]:     self._save_optimizer_and_scheduler(output_dir)
[rank6]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/trainer.py", line 2914, in _save_optimizer_and_scheduler
[rank6]:     self.model_wrapped.save_checkpoint(output_dir)
[rank6]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 3112, in save_checkpoint
[rank6]:     self._save_zero_checkpoint(save_dir, tag)
[rank6]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 3473, in _save_zero_checkpoint
[rank6]:     self.checkpoint_engine.save(zero_sd, zero_checkpoint_name)
[rank6]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/deepspeed/runtime/checkpoint_engine/torch_checkpoint_engine.py", line 22, in save
[rank6]:     torch.save(state_dict, path)
[rank6]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/serialization.py", line 627, in save
[rank6]:     with _open_zipfile_writer(f) as opened_zipfile:
[rank6]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/serialization.py", line 475, in __exit__
[rank6]:     self.file_like.write_end_of_file()
[rank6]: RuntimeError: [enforce fail at inline_container.cc:595] . unexpected pos 2432 vs 2326
[rank0]: Traceback (most recent call last):
[rank0]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/serialization.py", line 628, in save
[rank0]:     _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
[rank0]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/serialization.py", line 862, in _save
[rank0]:     zip_file.write_record(name, storage, num_bytes)
[rank0]: RuntimeError: [enforce fail at inline_container.cc:764] . PytorchStreamWriter failed writing file data/0: file write failed

[rank0]: During handling of the above exception, another exception occurred:

[rank0]: Traceback (most recent call last):
[rank0]:   File "/opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/llm-attacks/stanford_alpaca/train.py", line 222, in <module>
[rank0]:     train()
[rank0]:   File "/opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/llm-attacks/stanford_alpaca/train.py", line 216, in train
[rank0]:     trainer.train()
[rank0]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/trainer.py", line 1885, in train
[rank0]:     return inner_training_loop(
[rank0]:            ^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/trainer.py", line 2291, in _inner_training_loop
[rank0]:     self._maybe_log_save_evaluate(tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)
[rank0]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/trainer.py", line 2732, in _maybe_log_save_evaluate
[rank0]:     self._save_checkpoint(model, trial, metrics=metrics)
[rank0]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/trainer.py", line 2815, in _save_checkpoint
[rank0]:     self._save_optimizer_and_scheduler(output_dir)
[rank0]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/trainer.py", line 2914, in _save_optimizer_and_scheduler
[rank0]:     self.model_wrapped.save_checkpoint(output_dir)
[rank0]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 3112, in save_checkpoint
[rank0]:     self._save_zero_checkpoint(save_dir, tag)
[rank0]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 3473, in _save_zero_checkpoint
[rank0]:     self.checkpoint_engine.save(zero_sd, zero_checkpoint_name)
[rank0]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/deepspeed/runtime/checkpoint_engine/torch_checkpoint_engine.py", line 22, in save
[rank0]:     torch.save(state_dict, path)
[rank0]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/serialization.py", line 627, in save
[rank0]:     with _open_zipfile_writer(f) as opened_zipfile:
[rank0]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/serialization.py", line 475, in __exit__
[rank0]:     self.file_like.write_end_of_file()
[rank0]: RuntimeError: [enforce fail at inline_container.cc:595] . unexpected pos 2432 vs 2326
[rank4]: Traceback (most recent call last):
[rank4]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/serialization.py", line 628, in save
[rank4]:     _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
[rank4]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/serialization.py", line 862, in _save
[rank4]:     zip_file.write_record(name, storage, num_bytes)
[rank4]: RuntimeError: [enforce fail at inline_container.cc:764] . PytorchStreamWriter failed writing file data/0: file write failed

[rank4]: During handling of the above exception, another exception occurred:

[rank4]: Traceback (most recent call last):
[rank4]:   File "/opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/llm-attacks/stanford_alpaca/train.py", line 222, in <module>
[rank4]:     train()
[rank4]:   File "/opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/llm-attacks/stanford_alpaca/train.py", line 216, in train
[rank4]:     trainer.train()
[rank4]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/trainer.py", line 1885, in train
[rank4]:     return inner_training_loop(
[rank4]:            ^^^^^^^^^^^^^^^^^^^^
[rank4]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/trainer.py", line 2291, in _inner_training_loop
[rank4]:     self._maybe_log_save_evaluate(tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)
[rank4]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/trainer.py", line 2732, in _maybe_log_save_evaluate
[rank4]:     self._save_checkpoint(model, trial, metrics=metrics)
[rank4]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/trainer.py", line 2815, in _save_checkpoint
[rank4]:     self._save_optimizer_and_scheduler(output_dir)
[rank4]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/trainer.py", line 2914, in _save_optimizer_and_scheduler
[rank4]:     self.model_wrapped.save_checkpoint(output_dir)
[rank4]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 3112, in save_checkpoint
[rank4]:     self._save_zero_checkpoint(save_dir, tag)
[rank4]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 3473, in _save_zero_checkpoint
[rank4]:     self.checkpoint_engine.save(zero_sd, zero_checkpoint_name)
[rank4]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/deepspeed/runtime/checkpoint_engine/torch_checkpoint_engine.py", line 22, in save
[rank4]:     torch.save(state_dict, path)
[rank4]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/serialization.py", line 627, in save
[rank4]:     with _open_zipfile_writer(f) as opened_zipfile:
[rank4]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/serialization.py", line 475, in __exit__
[rank4]:     self.file_like.write_end_of_file()
[rank4]: RuntimeError: [enforce fail at inline_container.cc:595] . unexpected pos 2432 vs 2326
[rank5]: Traceback (most recent call last):
[rank5]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/serialization.py", line 628, in save
[rank5]:     _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
[rank5]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/serialization.py", line 862, in _save
[rank5]:     zip_file.write_record(name, storage, num_bytes)
[rank5]: RuntimeError: [enforce fail at inline_container.cc:764] . PytorchStreamWriter failed writing file data/0: file write failed

[rank5]: During handling of the above exception, another exception occurred:

[rank5]: Traceback (most recent call last):
[rank5]:   File "/opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/llm-attacks/stanford_alpaca/train.py", line 222, in <module>
[rank5]:     train()
[rank5]:   File "/opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/llm-attacks/stanford_alpaca/train.py", line 216, in train
[rank5]:     trainer.train()
[rank5]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/trainer.py", line 1885, in train
[rank5]:     return inner_training_loop(
[rank5]:            ^^^^^^^^^^^^^^^^^^^^
[rank5]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/trainer.py", line 2291, in _inner_training_loop
[rank5]:     self._maybe_log_save_evaluate(tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)
[rank5]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/trainer.py", line 2732, in _maybe_log_save_evaluate
[rank5]:     self._save_checkpoint(model, trial, metrics=metrics)
[rank5]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/trainer.py", line 2815, in _save_checkpoint
[rank5]:     self._save_optimizer_and_scheduler(output_dir)
[rank5]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/trainer.py", line 2914, in _save_optimizer_and_scheduler
[rank5]:     self.model_wrapped.save_checkpoint(output_dir)
[rank5]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 3112, in save_checkpoint
[rank5]:     self._save_zero_checkpoint(save_dir, tag)
[rank5]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 3473, in _save_zero_checkpoint
[rank5]:     self.checkpoint_engine.save(zero_sd, zero_checkpoint_name)
[rank5]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/deepspeed/runtime/checkpoint_engine/torch_checkpoint_engine.py", line 22, in save
[rank5]:     torch.save(state_dict, path)
[rank5]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/serialization.py", line 627, in save
[rank5]:     with _open_zipfile_writer(f) as opened_zipfile:
[rank5]:   File "/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/serialization.py", line 475, in __exit__
[rank5]:     self.file_like.write_end_of_file()
[rank5]: RuntimeError: [enforce fail at inline_container.cc:595] . unexpected pos 2432 vs 2326

 79%|███████▉  | 500/630 [08:00<02:04,  1.04it/s]
[2024-07-12 05:07:09,274] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 2148478
[2024-07-12 05:07:09,275] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 2148479
[2024-07-12 05:07:09,308] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 2148480
[2024-07-12 05:07:09,337] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 2148481
[2024-07-12 05:07:09,365] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 2148482
[2024-07-12 05:07:09,391] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 2148483
[2024-07-12 05:07:09,416] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 2148484
[2024-07-12 05:07:09,492] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 2148485
[2024-07-12 05:07:09,518] [ERROR] [launch.py:325:sigkill_handler] ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=7', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Llama-2-7b-hf', '--data_path', '../data/stanford_alpaca/sharegpt_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_meta-llama_sharegpt_tuned', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1'] exits with return code = 1
