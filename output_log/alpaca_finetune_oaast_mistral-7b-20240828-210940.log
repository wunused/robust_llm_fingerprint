Namespace(mode=['alpaca'], base_model='meta-llama/Meta-Llama-3-8B', template_name='barebone', total_bsz=64, epoch=3, lr=2e-05, data_path='', task_name='oasst1', tuned_dir='./cache', use_peft=False, lora_r=16, lora_alpha=32)
num gpus:  8
Running 1/1: deepspeed --num_gpus=8 train.py --deepspeed ../deepspeed_config/zero3.json --bf16 --tf32=True
        --model_name_or_path meta-llama/Meta-Llama-3-8B --data_path ../data/stanford_alpaca/oasst1_data.json
        --output_dir /fsx-project/yunyun/models/meta-llama_Meta-Llama-3-8B_oasst1_tuned
        --num_train_epochs 3
        --per_device_train_batch_size 10
        --per_device_eval_batch_size 4
        --gradient_accumulation_steps 1
        --gradient_checkpointing=True
        --evaluation_strategy=no
        --save_strategy=steps
        --save_steps 500
        --save_total_limit 1
        --learning_rate 2e-6
        --weight_decay 0.
        --report_to tensorboard
        --warmup_ratio 0.03
        --lr_scheduler_type=cosine
        --logging_steps 1
        --use_peft False 
        --lora_r 16 --lora_alpha 32
['deepspeed', '--num_gpus=8', 'train.py', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Meta-Llama-3-8B', '--data_path', '../data/stanford_alpaca/oasst1_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_Meta-Llama-3-8B_oasst1_tuned', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1', '--use_peft', 'False', '--lora_r', '16', '--lora_alpha', '32']
[2024-08-28 21:09:54,053] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2024-08-28 21:10:02,051] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2024-08-28 21:10:02,052] [INFO] [runner.py:568:main] cmd = /data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNSwgNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None train.py --deepspeed ../deepspeed_config/zero3.json --bf16 --tf32=True --model_name_or_path meta-llama/Meta-Llama-3-8B --data_path ../data/stanford_alpaca/oasst1_data.json --output_dir /fsx-project/yunyun/models/meta-llama_Meta-Llama-3-8B_oasst1_tuned --num_train_epochs 3 --per_device_train_batch_size 10 --per_device_eval_batch_size 4 --gradient_accumulation_steps 1 --gradient_checkpointing=True --evaluation_strategy=no --save_strategy=steps --save_steps 500 --save_total_limit 1 --learning_rate 2e-6 --weight_decay 0. --report_to tensorboard --warmup_ratio 0.03 --lr_scheduler_type=cosine --logging_steps 1 --use_peft False --lora_r 16 --lora_alpha 32
[2024-08-28 21:10:04,605] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2024-08-28 21:10:08,062] [INFO] [launch.py:139:main] 0 NCCL_ASYNC_ERROR_HANDLING=1
[2024-08-28 21:10:08,062] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]}
[2024-08-28 21:10:08,062] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=8, node_rank=0
[2024-08-28 21:10:08,062] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]})
[2024-08-28 21:10:08,062] [INFO] [launch.py:164:main] dist_world_size=8
[2024-08-28 21:10:08,062] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
[2024-08-28 21:10:08,063] [INFO] [launch.py:256:main] process 3774641 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=0', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Meta-Llama-3-8B', '--data_path', '../data/stanford_alpaca/oasst1_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_Meta-Llama-3-8B_oasst1_tuned', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1', '--use_peft', 'False', '--lora_r', '16', '--lora_alpha', '32']
[2024-08-28 21:10:08,064] [INFO] [launch.py:256:main] process 3774642 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=1', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Meta-Llama-3-8B', '--data_path', '../data/stanford_alpaca/oasst1_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_Meta-Llama-3-8B_oasst1_tuned', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1', '--use_peft', 'False', '--lora_r', '16', '--lora_alpha', '32']
[2024-08-28 21:10:08,064] [INFO] [launch.py:256:main] process 3774643 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=2', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Meta-Llama-3-8B', '--data_path', '../data/stanford_alpaca/oasst1_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_Meta-Llama-3-8B_oasst1_tuned', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1', '--use_peft', 'False', '--lora_r', '16', '--lora_alpha', '32']
[2024-08-28 21:10:08,065] [INFO] [launch.py:256:main] process 3774644 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=3', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Meta-Llama-3-8B', '--data_path', '../data/stanford_alpaca/oasst1_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_Meta-Llama-3-8B_oasst1_tuned', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1', '--use_peft', 'False', '--lora_r', '16', '--lora_alpha', '32']
[2024-08-28 21:10:08,065] [INFO] [launch.py:256:main] process 3774645 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=4', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Meta-Llama-3-8B', '--data_path', '../data/stanford_alpaca/oasst1_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_Meta-Llama-3-8B_oasst1_tuned', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1', '--use_peft', 'False', '--lora_r', '16', '--lora_alpha', '32']
[2024-08-28 21:10:08,066] [INFO] [launch.py:256:main] process 3774646 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=5', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Meta-Llama-3-8B', '--data_path', '../data/stanford_alpaca/oasst1_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_Meta-Llama-3-8B_oasst1_tuned', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1', '--use_peft', 'False', '--lora_r', '16', '--lora_alpha', '32']
[2024-08-28 21:10:08,066] [INFO] [launch.py:256:main] process 3774647 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=6', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Meta-Llama-3-8B', '--data_path', '../data/stanford_alpaca/oasst1_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_Meta-Llama-3-8B_oasst1_tuned', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1', '--use_peft', 'False', '--lora_r', '16', '--lora_alpha', '32']
[2024-08-28 21:10:08,067] [INFO] [launch.py:256:main] process 3774648 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=7', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Meta-Llama-3-8B', '--data_path', '../data/stanford_alpaca/oasst1_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_Meta-Llama-3-8B_oasst1_tuned', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1', '--use_peft', 'False', '--lora_r', '16', '--lora_alpha', '32']
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
2024-08-28 21:10:20.074999: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-08-28 21:10:20.075008: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-08-28 21:10:20.075019: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-08-28 21:10:20.075000: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-08-28 21:10:20.075015: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-08-28 21:10:20.075011: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-08-28 21:10:20.075016: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-08-28 21:10:20.075009: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-08-28 21:10:20.266417: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-08-28 21:10:20.266420: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-08-28 21:10:20.266439: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-08-28 21:10:20.266434: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-08-28 21:10:20.266435: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-08-28 21:10:20.266445: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-08-28 21:10:20.266459: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-08-28 21:10:20.266460: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-08-28 21:10:20.315726: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-08-28 21:10:20.315726: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-08-28 21:10:20.315734: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-08-28 21:10:20.315740: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-08-28 21:10:20.315741: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-08-28 21:10:20.315745: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-08-28 21:10:20.315747: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-08-28 21:10:20.315750: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-08-28 21:10:20.727785: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-08-28 21:10:20.727782: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-08-28 21:10:20.727791: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-08-28 21:10:20.727799: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-08-28 21:10:20.727799: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-08-28 21:10:20.727808: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-08-28 21:10:20.727808: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-08-28 21:10:20.727813: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-08-28 21:10:25.832775: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-08-28 21:10:25.832783: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-08-28 21:10:25.832871: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-08-28 21:10:25.832868: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-08-28 21:10:25.832884: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-08-28 21:10:25.832915: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-08-28 21:10:25.832952: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-08-28 21:10:25.832976: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
[2024-08-28 21:10:43,492] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
[2024-08-28 21:10:43,562] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-28 21:10:43,657] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-28 21:10:43,659] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-28 21:10:43,663] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-28 21:10:43,664] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-28 21:10:43,665] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-28 21:10:43,673] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2024-08-28 21:10:44,258] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-08-28 21:10:44,290] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-08-28 21:10:44,381] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-08-28 21:10:44,381] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-08-28 21:10:44,388] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-08-28 21:10:44,403] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-08-28 21:10:44,433] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-08-28 21:10:44,437] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-08-28 21:10:44,437] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 477.66it/s]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 1589.35it/s]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 1317.20it/s]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 1500.65it/s]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 1255.12it/s]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 1514.33it/s]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 1480.78it/s]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 1517.75it/s]
[2024-08-28 21:10:56,035] [INFO] [partition_parameters.py:345:__exit__] finished initializing model - num_params = 291, num_elems = 8.03B
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:01,  2.11it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:01,  1.99it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:01,  1.98it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:01,  2.09it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:01,  1.96it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:01,  1.97it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:01,  1.95it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:10<00:30, 10.23s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:10<00:12,  6.19s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:10<00:12,  6.19s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:10<00:12,  6.19s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:10<00:12,  6.19s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:10<00:12,  6.18s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:10<00:12,  6.19s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:10<00:12,  6.20s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:19<00:07,  7.46s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:19<00:07,  7.48s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:19<00:07,  7.48s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:19<00:07,  7.48s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:19<00:07,  7.48s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:19<00:07,  7.48s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:19<00:07,  7.48s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:19<00:19,  9.96s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:20<00:00,  4.97s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:20<00:00,  5.20s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:20<00:00,  4.97s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:20<00:00,  5.20s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:20<00:00,  4.97s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:20<00:00,  5.20s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:20<00:00,  4.97s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:20<00:00,  5.20s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:20<00:00,  4.98s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:20<00:00,  5.21s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:20<00:00,  4.98s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:20<00:00,  5.21s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:20<00:00,  4.98s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:20<00:00,  5.20s/it]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 2850.36it/s]
Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 2244.74it/s]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 1509.96it/s]
Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 2001.10it/s]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 1606.86it/s]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 1660.62it/s]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 1616.93it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:29<00:09,  9.75s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:31<00:00,  6.81s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:31<00:00,  7.95s/it]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 1547.57it/s]
[2024-08-28 21:11:28,110] [INFO] [partition_parameters.py:345:__exit__] finished initializing model - num_params = 582, num_elems = 16.06B
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.17s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.18s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.18s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.19s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.19s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.19s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.20s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.98s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:07<00:06,  3.47s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:07<00:06,  3.47s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:07<00:06,  3.48s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:07<00:06,  3.47s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:07<00:06,  3.48s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:07<00:06,  3.48s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:07<00:06,  3.48s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:09<00:02,  2.92s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:09<00:02,  2.92s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:09<00:02,  2.92s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:09<00:02,  2.92s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:09<00:02,  2.92s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:09<00:02,  2.92s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:09<00:02,  2.93s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:06,  3.01s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.13s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.13s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.13s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.13s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.13s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.13s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.13s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.59s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.59s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.59s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.59s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.58s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.59s/it]




Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.59s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:09<00:03,  3.01s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.16s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.47s/it]
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
[rank6]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[rank5]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[rank4]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[rank2]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[rank7]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[rank0]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[rank1]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[rank3]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...



Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121/fused_adam/build.ninja...
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/cpp_extension.py:1967: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Loading extension module fused_adam...
Loading extension module fused_adam...
Loading extension module fused_adam...
Loading extension module fused_adam...
Loading extension module fused_adam...
Loading extension module fused_adam...Loading extension module fused_adam...

Time to load fused_adam op: 3.1928534507751465 secondsTime to load fused_adam op: 3.1929666996002197 secondsTime to load fused_adam op: 3.1929283142089844 secondsTime to load fused_adam op: 3.192939281463623 seconds

Time to load fused_adam op: 3.1929712295532227 secondsTime to load fused_adam op: 3.1930112838745117 secondsTime to load fused_adam op: 3.193002462387085 secondsTime to load fused_adam op: 3.1930086612701416 seconds





Parameter Offload: Total persistent parameters: 268701696 in 129 params
  0%|          | 0/189 [00:00<?, ?it/s]/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
  1%|          | 1/189 [00:08<26:58,  8.61s/it]                                               {'loss': 1.2257, 'grad_norm': 5.958296448949026, 'learning_rate': 0.0, 'epoch': 0.02}
  1%|          | 1/189 [00:08<26:58,  8.61s/it]  1%|          | 2/189 [00:09<13:03,  4.19s/it]                                               {'loss': 1.1617, 'grad_norm': 6.1147791205485875, 'learning_rate': 7.73705614469083e-07, 'epoch': 0.03}
  1%|          | 2/189 [00:09<13:03,  4.19s/it]  2%|▏         | 3/189 [00:10<08:27,  2.73s/it]                                               {'loss': 1.1477, 'grad_norm': 6.197111687060211, 'learning_rate': 1.2262943855309167e-06, 'epoch': 0.05}
  2%|▏         | 3/189 [00:10<08:27,  2.73s/it]  2%|▏         | 4/189 [00:11<06:14,  2.02s/it]                                               {'loss': 1.1172, 'grad_norm': 5.826956116602967, 'learning_rate': 1.547411228938166e-06, 'epoch': 0.06}
  2%|▏         | 4/189 [00:11<06:14,  2.02s/it]  3%|▎         | 5/189 [00:12<05:00,  1.63s/it]                                               {'loss': 1.0895, 'grad_norm': 4.906063271091837, 'learning_rate': 1.796488803407854e-06, 'epoch': 0.08}
  3%|▎         | 5/189 [00:12<05:00,  1.63s/it]  3%|▎         | 6/189 [00:13<04:16,  1.40s/it]                                               {'loss': 1.1574, 'grad_norm': 5.312807281353493, 'learning_rate': 1.9999999999999995e-06, 'epoch': 0.1}
  3%|▎         | 6/189 [00:13<04:16,  1.40s/it]  4%|▎         | 7/189 [00:14<03:48,  1.26s/it]                                               {'loss': 1.0753, 'grad_norm': 4.991065553328, 'learning_rate': 2e-06, 'epoch': 0.11}
  4%|▎         | 7/189 [00:14<03:48,  1.26s/it]  4%|▍         | 8/189 [00:15<03:29,  1.16s/it]                                               {'loss': 1.103, 'grad_norm': 4.000688996836919, 'learning_rate': 1.989071038251366e-06, 'epoch': 0.13}
  4%|▍         | 8/189 [00:15<03:29,  1.16s/it]  5%|▍         | 9/189 [00:16<03:16,  1.09s/it]                                               {'loss': 1.1197, 'grad_norm': 4.019025189873786, 'learning_rate': 1.978142076502732e-06, 'epoch': 0.14}
  5%|▍         | 9/189 [00:16<03:16,  1.09s/it]  5%|▌         | 10/189 [00:17<03:06,  1.04s/it]                                                {'loss': 1.0541, 'grad_norm': 3.540006914663764, 'learning_rate': 1.967213114754098e-06, 'epoch': 0.16}
  5%|▌         | 10/189 [00:17<03:06,  1.04s/it]  6%|▌         | 11/189 [00:18<03:00,  1.02s/it]                                                {'loss': 1.1248, 'grad_norm': 3.6287647328767463, 'learning_rate': 1.9562841530054644e-06, 'epoch': 0.17}
  6%|▌         | 11/189 [00:18<03:00,  1.02s/it]  6%|▋         | 12/189 [00:19<02:57,  1.00s/it]                                                {'loss': 1.185, 'grad_norm': 3.776093292870055, 'learning_rate': 1.9453551912568304e-06, 'epoch': 0.19}
  6%|▋         | 12/189 [00:19<02:57,  1.00s/it]  7%|▋         | 13/189 [00:20<02:53,  1.01it/s]                                                {'loss': 1.0993, 'grad_norm': 3.6937399344756807, 'learning_rate': 1.9344262295081967e-06, 'epoch': 0.21}
  7%|▋         | 13/189 [00:20<02:53,  1.01it/s]  7%|▋         | 14/189 [00:21<02:50,  1.03it/s]                                                {'loss': 1.0609, 'grad_norm': 3.724049880159483, 'learning_rate': 1.9234972677595626e-06, 'epoch': 0.22}
  7%|▋         | 14/189 [00:21<02:50,  1.03it/s]  8%|▊         | 15/189 [00:22<02:48,  1.03it/s]                                                {'loss': 1.1423, 'grad_norm': 3.6982044402002665, 'learning_rate': 1.912568306010929e-06, 'epoch': 0.24}
  8%|▊         | 15/189 [00:22<02:48,  1.03it/s]  8%|▊         | 16/189 [00:23<02:46,  1.04it/s]                                                {'loss': 1.0463, 'grad_norm': 3.97902204636459, 'learning_rate': 1.901639344262295e-06, 'epoch': 0.25}
  8%|▊         | 16/189 [00:23<02:46,  1.04it/s]  9%|▉         | 17/189 [00:24<02:45,  1.04it/s]                                                {'loss': 1.1255, 'grad_norm': 3.4028695519675405, 'learning_rate': 1.8907103825136612e-06, 'epoch': 0.27}
  9%|▉         | 17/189 [00:24<02:45,  1.04it/s] 10%|▉         | 18/189 [00:24<02:43,  1.05it/s]                                                {'loss': 1.2292, 'grad_norm': 3.3880957351857353, 'learning_rate': 1.8797814207650274e-06, 'epoch': 0.29}
 10%|▉         | 18/189 [00:24<02:43,  1.05it/s] 10%|█         | 19/189 [00:25<02:42,  1.05it/s]                                                {'loss': 1.1441, 'grad_norm': 3.3352953323502192, 'learning_rate': 1.8688524590163935e-06, 'epoch': 0.3}
 10%|█         | 19/189 [00:25<02:42,  1.05it/s] 11%|█         | 20/189 [00:26<02:40,  1.05it/s]                                                {'loss': 1.1803, 'grad_norm': 3.5298401803197317, 'learning_rate': 1.8579234972677596e-06, 'epoch': 0.32}
 11%|█         | 20/189 [00:26<02:40,  1.05it/s] 11%|█         | 21/189 [00:27<02:39,  1.05it/s]                                                {'loss': 0.9909, 'grad_norm': 3.169334578375398, 'learning_rate': 1.8469945355191256e-06, 'epoch': 0.33}
 11%|█         | 21/189 [00:27<02:39,  1.05it/s] 12%|█▏        | 22/189 [00:28<02:38,  1.05it/s]                                                {'loss': 1.1704, 'grad_norm': 3.4366916086763304, 'learning_rate': 1.8360655737704917e-06, 'epoch': 0.35}
 12%|█▏        | 22/189 [00:28<02:38,  1.05it/s] 12%|█▏        | 23/189 [00:29<02:36,  1.06it/s]                                                {'loss': 1.0123, 'grad_norm': 3.386698958911682, 'learning_rate': 1.8251366120218578e-06, 'epoch': 0.37}
 12%|█▏        | 23/189 [00:29<02:36,  1.06it/s] 13%|█▎        | 24/189 [00:30<02:35,  1.06it/s]                                                {'loss': 0.9691, 'grad_norm': 3.425970493172859, 'learning_rate': 1.814207650273224e-06, 'epoch': 0.38}
 13%|█▎        | 24/189 [00:30<02:35,  1.06it/s] 13%|█▎        | 25/189 [00:31<02:34,  1.06it/s]                                                {'loss': 1.0977, 'grad_norm': 3.189748592886648, 'learning_rate': 1.80327868852459e-06, 'epoch': 0.4}
 13%|█▎        | 25/189 [00:31<02:34,  1.06it/s] 14%|█▍        | 26/189 [00:32<02:33,  1.06it/s]                                                {'loss': 1.0815, 'grad_norm': 3.4419643767893344, 'learning_rate': 1.7923497267759562e-06, 'epoch': 0.41}
 14%|█▍        | 26/189 [00:32<02:33,  1.06it/s] 14%|█▍        | 27/189 [00:33<02:33,  1.06it/s]                                                {'loss': 0.9703, 'grad_norm': 3.2142130212853837, 'learning_rate': 1.7814207650273224e-06, 'epoch': 0.43}
 14%|█▍        | 27/189 [00:33<02:33,  1.06it/s] 15%|█▍        | 28/189 [00:34<02:32,  1.05it/s]                                                {'loss': 1.0338, 'grad_norm': 3.1761068165488084, 'learning_rate': 1.7704918032786885e-06, 'epoch': 0.44}
 15%|█▍        | 28/189 [00:34<02:32,  1.05it/s] 15%|█▌        | 29/189 [00:35<02:31,  1.05it/s]                                                {'loss': 0.9322, 'grad_norm': 3.2260407444444676, 'learning_rate': 1.7595628415300544e-06, 'epoch': 0.46}
 15%|█▌        | 29/189 [00:35<02:31,  1.05it/s] 16%|█▌        | 30/189 [00:36<02:30,  1.06it/s]                                                {'loss': 1.161, 'grad_norm': 3.49678827281147, 'learning_rate': 1.7486338797814206e-06, 'epoch': 0.48}
 16%|█▌        | 30/189 [00:36<02:30,  1.06it/s] 16%|█▋        | 31/189 [00:37<02:29,  1.06it/s]                                                {'loss': 1.0463, 'grad_norm': 3.651034181289756, 'learning_rate': 1.7377049180327867e-06, 'epoch': 0.49}
 16%|█▋        | 31/189 [00:37<02:29,  1.06it/s] 17%|█▋        | 32/189 [00:38<02:28,  1.06it/s]                                                {'loss': 0.9119, 'grad_norm': 3.210233634093211, 'learning_rate': 1.7267759562841528e-06, 'epoch': 0.51}
 17%|█▋        | 32/189 [00:38<02:28,  1.06it/s] 17%|█▋        | 33/189 [00:39<02:27,  1.06it/s]                                                {'loss': 1.003, 'grad_norm': 3.873057234803757, 'learning_rate': 1.715846994535519e-06, 'epoch': 0.52}
 17%|█▋        | 33/189 [00:39<02:27,  1.06it/s] 18%|█▊        | 34/189 [00:40<02:27,  1.05it/s]                                                {'loss': 0.8546, 'grad_norm': 3.1519065897327136, 'learning_rate': 1.704918032786885e-06, 'epoch': 0.54}
 18%|█▊        | 34/189 [00:40<02:27,  1.05it/s] 19%|█▊        | 35/189 [00:41<02:25,  1.06it/s]                                                {'loss': 0.9744, 'grad_norm': 3.4007439933263153, 'learning_rate': 1.6939890710382514e-06, 'epoch': 0.56}
 19%|█▊        | 35/189 [00:41<02:25,  1.06it/s] 19%|█▉        | 36/189 [00:41<02:24,  1.06it/s]                                                {'loss': 0.9135, 'grad_norm': 3.220891384405558, 'learning_rate': 1.6830601092896176e-06, 'epoch': 0.57}
 19%|█▉        | 36/189 [00:41<02:24,  1.06it/s] 20%|█▉        | 37/189 [00:42<02:23,  1.06it/s]                                                {'loss': 1.0437, 'grad_norm': 3.42171838767954, 'learning_rate': 1.6721311475409837e-06, 'epoch': 0.59}
 20%|█▉        | 37/189 [00:42<02:23,  1.06it/s] 20%|██        | 38/189 [00:43<02:22,  1.06it/s]                                                {'loss': 1.0679, 'grad_norm': 3.619063452972144, 'learning_rate': 1.6612021857923496e-06, 'epoch': 0.6}
 20%|██        | 38/189 [00:43<02:22,  1.06it/s] 21%|██        | 39/189 [00:44<02:22,  1.05it/s]                                                {'loss': 1.0039, 'grad_norm': 3.6203192519528637, 'learning_rate': 1.6502732240437158e-06, 'epoch': 0.62}
 21%|██        | 39/189 [00:44<02:22,  1.05it/s] 21%|██        | 40/189 [00:45<02:22,  1.05it/s]                                                {'loss': 1.0185, 'grad_norm': 3.225764050987721, 'learning_rate': 1.6393442622950819e-06, 'epoch': 0.63}
 21%|██        | 40/189 [00:45<02:22,  1.05it/s] 22%|██▏       | 41/189 [00:46<02:20,  1.05it/s]                                                {'loss': 0.9868, 'grad_norm': 3.32428030619549, 'learning_rate': 1.628415300546448e-06, 'epoch': 0.65}
 22%|██▏       | 41/189 [00:46<02:20,  1.05it/s] 22%|██▏       | 42/189 [00:47<02:19,  1.05it/s]                                                {'loss': 0.9111, 'grad_norm': 3.367072658396338, 'learning_rate': 1.6174863387978142e-06, 'epoch': 0.67}
 22%|██▏       | 42/189 [00:47<02:19,  1.05it/s] 23%|██▎       | 43/189 [00:48<02:19,  1.05it/s]                                                {'loss': 0.9778, 'grad_norm': 3.3602326117663828, 'learning_rate': 1.6065573770491803e-06, 'epoch': 0.68}
 23%|██▎       | 43/189 [00:48<02:19,  1.05it/s] 23%|██▎       | 44/189 [00:49<02:18,  1.05it/s]                                                {'loss': 1.014, 'grad_norm': 3.2749524104035226, 'learning_rate': 1.5956284153005464e-06, 'epoch': 0.7}
 23%|██▎       | 44/189 [00:49<02:18,  1.05it/s] 24%|██▍       | 45/189 [00:50<02:16,  1.06it/s]                                                {'loss': 1.0119, 'grad_norm': 3.4402648844352743, 'learning_rate': 1.5846994535519126e-06, 'epoch': 0.71}
 24%|██▍       | 45/189 [00:50<02:16,  1.06it/s] 24%|██▍       | 46/189 [00:51<02:15,  1.05it/s]                                                {'loss': 1.0964, 'grad_norm': 4.0051051508513655, 'learning_rate': 1.5737704918032787e-06, 'epoch': 0.73}
 24%|██▍       | 46/189 [00:51<02:15,  1.05it/s] 25%|██▍       | 47/189 [00:52<02:14,  1.05it/s]                                                {'loss': 0.9565, 'grad_norm': 3.411354112589994, 'learning_rate': 1.5628415300546446e-06, 'epoch': 0.75}
 25%|██▍       | 47/189 [00:52<02:14,  1.05it/s] 25%|██▌       | 48/189 [00:53<02:14,  1.05it/s]                                                {'loss': 0.9008, 'grad_norm': 3.2895341294006197, 'learning_rate': 1.5519125683060107e-06, 'epoch': 0.76}
 25%|██▌       | 48/189 [00:53<02:14,  1.05it/s] 26%|██▌       | 49/189 [00:54<02:13,  1.05it/s]                                                {'loss': 1.0696, 'grad_norm': 3.2963526980192053, 'learning_rate': 1.5409836065573769e-06, 'epoch': 0.78}
 26%|██▌       | 49/189 [00:54<02:13,  1.05it/s] 26%|██▋       | 50/189 [00:55<02:12,  1.05it/s]                                                {'loss': 0.961, 'grad_norm': 3.2458508810618882, 'learning_rate': 1.530054644808743e-06, 'epoch': 0.79}
 26%|██▋       | 50/189 [00:55<02:12,  1.05it/s] 27%|██▋       | 51/189 [00:56<02:11,  1.05it/s]                                                {'loss': 0.9498, 'grad_norm': 3.144359228571307, 'learning_rate': 1.5191256830601091e-06, 'epoch': 0.81}
 27%|██▋       | 51/189 [00:56<02:11,  1.05it/s] 28%|██▊       | 52/189 [00:57<02:10,  1.05it/s]                                                {'loss': 0.9266, 'grad_norm': 3.6784081989865203, 'learning_rate': 1.5081967213114753e-06, 'epoch': 0.83}
 28%|██▊       | 52/189 [00:57<02:10,  1.05it/s] 28%|██▊       | 53/189 [00:58<02:09,  1.05it/s]                                                {'loss': 0.9613, 'grad_norm': 3.4592220893791605, 'learning_rate': 1.4972677595628416e-06, 'epoch': 0.84}
 28%|██▊       | 53/189 [00:58<02:09,  1.05it/s] 29%|██▊       | 54/189 [00:59<02:08,  1.05it/s]                                                {'loss': 0.967, 'grad_norm': 3.2613517884890757, 'learning_rate': 1.4863387978142078e-06, 'epoch': 0.86}
 29%|██▊       | 54/189 [00:59<02:08,  1.05it/s] 29%|██▉       | 55/189 [01:00<02:07,  1.05it/s]                                                {'loss': 0.8784, 'grad_norm': 3.467995482199697, 'learning_rate': 1.4754098360655739e-06, 'epoch': 0.87}
 29%|██▉       | 55/189 [01:00<02:07,  1.05it/s] 30%|██▉       | 56/189 [01:00<02:06,  1.05it/s]                                                {'loss': 0.9228, 'grad_norm': 3.658437766380817, 'learning_rate': 1.4644808743169398e-06, 'epoch': 0.89}
 30%|██▉       | 56/189 [01:00<02:06,  1.05it/s] 30%|███       | 57/189 [01:01<02:06,  1.04it/s]                                                {'loss': 0.8625, 'grad_norm': 3.3638245743937114, 'learning_rate': 1.453551912568306e-06, 'epoch': 0.9}
 30%|███       | 57/189 [01:01<02:06,  1.04it/s] 31%|███       | 58/189 [01:02<02:05,  1.04it/s]                                                {'loss': 0.8921, 'grad_norm': 3.7846450366711513, 'learning_rate': 1.442622950819672e-06, 'epoch': 0.92}
 31%|███       | 58/189 [01:02<02:05,  1.04it/s] 31%|███       | 59/189 [01:03<02:05,  1.04it/s]                                                {'loss': 1.053, 'grad_norm': 3.772780381037798, 'learning_rate': 1.4316939890710382e-06, 'epoch': 0.94}
 31%|███       | 59/189 [01:03<02:05,  1.04it/s] 32%|███▏      | 60/189 [01:04<02:03,  1.04it/s]                                                {'loss': 0.8897, 'grad_norm': 3.621141808909916, 'learning_rate': 1.4207650273224043e-06, 'epoch': 0.95}
 32%|███▏      | 60/189 [01:04<02:03,  1.04it/s] 32%|███▏      | 61/189 [01:05<02:02,  1.04it/s]                                                {'loss': 0.946, 'grad_norm': 3.5426737735804195, 'learning_rate': 1.4098360655737705e-06, 'epoch': 0.97}
 32%|███▏      | 61/189 [01:05<02:02,  1.04it/s] 33%|███▎      | 62/189 [01:06<02:01,  1.04it/s]                                                {'loss': 0.8875, 'grad_norm': 3.6194121022185852, 'learning_rate': 1.3989071038251366e-06, 'epoch': 0.98}
 33%|███▎      | 62/189 [01:06<02:01,  1.04it/s] 33%|███▎      | 63/189 [01:07<02:00,  1.04it/s]                                                {'loss': 0.865, 'grad_norm': 3.683089796118103, 'learning_rate': 1.3879781420765027e-06, 'epoch': 1.0}
 33%|███▎      | 63/189 [01:07<02:00,  1.04it/s] 34%|███▍      | 64/189 [01:08<01:59,  1.04it/s]                                                {'loss': 0.7016, 'grad_norm': 3.8376368785921007, 'learning_rate': 1.3770491803278687e-06, 'epoch': 1.02}
 34%|███▍      | 64/189 [01:08<01:59,  1.04it/s] 34%|███▍      | 65/189 [01:09<01:58,  1.04it/s]                                                {'loss': 0.7135, 'grad_norm': 3.7779588231241923, 'learning_rate': 1.3661202185792348e-06, 'epoch': 1.03}
 34%|███▍      | 65/189 [01:09<01:58,  1.04it/s] 35%|███▍      | 66/189 [01:10<01:57,  1.04it/s]                                                {'loss': 0.7705, 'grad_norm': 5.583634692742766, 'learning_rate': 1.355191256830601e-06, 'epoch': 1.05}
 35%|███▍      | 66/189 [01:10<01:57,  1.04it/s] 35%|███▌      | 67/189 [01:11<01:56,  1.04it/s]                                                {'loss': 0.74, 'grad_norm': 3.9995060695066766, 'learning_rate': 1.344262295081967e-06, 'epoch': 1.06}
 35%|███▌      | 67/189 [01:11<01:56,  1.04it/s] 36%|███▌      | 68/189 [01:12<01:55,  1.05it/s]                                                {'loss': 0.7617, 'grad_norm': 3.6212633971147303, 'learning_rate': 1.3333333333333332e-06, 'epoch': 1.08}
 36%|███▌      | 68/189 [01:12<01:55,  1.05it/s] 37%|███▋      | 69/189 [01:13<01:54,  1.05it/s]                                                {'loss': 0.7706, 'grad_norm': 3.6320222081773452, 'learning_rate': 1.3224043715846993e-06, 'epoch': 1.1}
 37%|███▋      | 69/189 [01:13<01:54,  1.05it/s] 37%|███▋      | 70/189 [01:14<01:53,  1.05it/s]                                                {'loss': 0.7885, 'grad_norm': 3.652434406405114, 'learning_rate': 1.3114754098360655e-06, 'epoch': 1.11}
 37%|███▋      | 70/189 [01:14<01:53,  1.05it/s] 38%|███▊      | 71/189 [01:15<01:52,  1.05it/s]                                                {'loss': 0.7353, 'grad_norm': 3.7781677713631296, 'learning_rate': 1.3005464480874316e-06, 'epoch': 1.13}
 38%|███▊      | 71/189 [01:15<01:52,  1.05it/s] 38%|███▊      | 72/189 [01:16<01:51,  1.05it/s]                                                {'loss': 0.8679, 'grad_norm': 3.5053358483329036, 'learning_rate': 1.289617486338798e-06, 'epoch': 1.14}
 38%|███▊      | 72/189 [01:16<01:51,  1.05it/s] 39%|███▊      | 73/189 [01:17<01:50,  1.05it/s]                                                {'loss': 0.7395, 'grad_norm': 3.4742417123867333, 'learning_rate': 1.2786885245901639e-06, 'epoch': 1.16}
 39%|███▊      | 73/189 [01:17<01:50,  1.05it/s] 39%|███▉      | 74/189 [01:18<01:49,  1.05it/s]                                                {'loss': 0.7035, 'grad_norm': 3.156073185500826, 'learning_rate': 1.26775956284153e-06, 'epoch': 1.17}
 39%|███▉      | 74/189 [01:18<01:49,  1.05it/s] 40%|███▉      | 75/189 [01:19<01:48,  1.05it/s]                                                {'loss': 0.8243, 'grad_norm': 4.162780116618935, 'learning_rate': 1.2568306010928961e-06, 'epoch': 1.19}
 40%|███▉      | 75/189 [01:19<01:48,  1.05it/s] 40%|████      | 76/189 [01:20<01:47,  1.05it/s]                                                {'loss': 0.6737, 'grad_norm': 3.371785249640896, 'learning_rate': 1.2459016393442623e-06, 'epoch': 1.21}
 40%|████      | 76/189 [01:20<01:47,  1.05it/s] 41%|████      | 77/189 [01:21<01:47,  1.05it/s]                                                {'loss': 0.6684, 'grad_norm': 3.631034564277565, 'learning_rate': 1.2349726775956284e-06, 'epoch': 1.22}
 41%|████      | 77/189 [01:21<01:47,  1.05it/s] 41%|████▏     | 78/189 [01:22<01:46,  1.05it/s]                                                {'loss': 0.7048, 'grad_norm': 3.4403083684837656, 'learning_rate': 1.2240437158469945e-06, 'epoch': 1.24}
 41%|████▏     | 78/189 [01:22<01:46,  1.05it/s] 42%|████▏     | 79/189 [01:23<01:44,  1.05it/s]                                                {'loss': 0.7148, 'grad_norm': 3.448700125585782, 'learning_rate': 1.2131147540983607e-06, 'epoch': 1.25}
 42%|████▏     | 79/189 [01:23<01:44,  1.05it/s] 42%|████▏     | 80/189 [01:23<01:43,  1.05it/s]                                                {'loss': 0.7368, 'grad_norm': 3.8115201837587698, 'learning_rate': 1.2021857923497268e-06, 'epoch': 1.27}
 42%|████▏     | 80/189 [01:23<01:43,  1.05it/s] 43%|████▎     | 81/189 [01:24<01:43,  1.05it/s]                                                {'loss': 0.6825, 'grad_norm': 4.926204834444067, 'learning_rate': 1.191256830601093e-06, 'epoch': 1.29}
 43%|████▎     | 81/189 [01:24<01:43,  1.05it/s] 43%|████▎     | 82/189 [01:25<01:42,  1.05it/s]                                                {'loss': 0.6993, 'grad_norm': 3.7709567401058854, 'learning_rate': 1.1803278688524589e-06, 'epoch': 1.3}
 43%|████▎     | 82/189 [01:25<01:42,  1.05it/s] 44%|████▍     | 83/189 [01:26<01:40,  1.05it/s]                                                {'loss': 0.7216, 'grad_norm': 3.696763310244905, 'learning_rate': 1.169398907103825e-06, 'epoch': 1.32}
 44%|████▍     | 83/189 [01:26<01:40,  1.05it/s] 44%|████▍     | 84/189 [01:27<01:40,  1.05it/s]                                                {'loss': 0.6758, 'grad_norm': 3.2973741515967743, 'learning_rate': 1.1584699453551911e-06, 'epoch': 1.33}
 44%|████▍     | 84/189 [01:27<01:40,  1.05it/s] 45%|████▍     | 85/189 [01:28<01:39,  1.05it/s]                                                {'loss': 0.7768, 'grad_norm': 3.3541541023216195, 'learning_rate': 1.1475409836065573e-06, 'epoch': 1.35}
 45%|████▍     | 85/189 [01:28<01:39,  1.05it/s] 46%|████▌     | 86/189 [01:29<01:38,  1.05it/s]                                                {'loss': 0.634, 'grad_norm': 3.2770569901619844, 'learning_rate': 1.1366120218579234e-06, 'epoch': 1.37}
 46%|████▌     | 86/189 [01:29<01:38,  1.05it/s] 46%|████▌     | 87/189 [01:30<01:37,  1.05it/s]                                                {'loss': 0.6574, 'grad_norm': 3.6740011526909706, 'learning_rate': 1.1256830601092895e-06, 'epoch': 1.38}
 46%|████▌     | 87/189 [01:30<01:37,  1.05it/s] 47%|████▋     | 88/189 [01:31<01:36,  1.05it/s]                                                {'loss': 0.5154, 'grad_norm': 3.3519236733758664, 'learning_rate': 1.1147540983606557e-06, 'epoch': 1.4}
 47%|████▋     | 88/189 [01:31<01:36,  1.05it/s] 47%|████▋     | 89/189 [01:32<01:35,  1.05it/s]                                                {'loss': 0.7702, 'grad_norm': 3.512465944901381, 'learning_rate': 1.1038251366120218e-06, 'epoch': 1.41}
 47%|████▋     | 89/189 [01:32<01:35,  1.05it/s] 48%|████▊     | 90/189 [01:33<01:34,  1.05it/s]                                                {'loss': 0.65, 'grad_norm': 3.3553723710414793, 'learning_rate': 1.092896174863388e-06, 'epoch': 1.43}
 48%|████▊     | 90/189 [01:33<01:34,  1.05it/s] 48%|████▊     | 91/189 [01:34<01:33,  1.05it/s]                                                {'loss': 0.6365, 'grad_norm': 3.7547048969969676, 'learning_rate': 1.081967213114754e-06, 'epoch': 1.44}
 48%|████▊     | 91/189 [01:34<01:33,  1.05it/s] 49%|████▊     | 92/189 [01:35<01:32,  1.05it/s]                                                {'loss': 0.6301, 'grad_norm': 3.5777783326165418, 'learning_rate': 1.0710382513661202e-06, 'epoch': 1.46}
 49%|████▊     | 92/189 [01:35<01:32,  1.05it/s] 49%|████▉     | 93/189 [01:36<01:31,  1.05it/s]                                                {'loss': 0.8912, 'grad_norm': 4.3036463075328735, 'learning_rate': 1.0601092896174863e-06, 'epoch': 1.48}
 49%|████▉     | 93/189 [01:36<01:31,  1.05it/s] 50%|████▉     | 94/189 [01:37<01:30,  1.05it/s]                                                {'loss': 0.595, 'grad_norm': 3.6732922749892367, 'learning_rate': 1.0491803278688525e-06, 'epoch': 1.49}
 50%|████▉     | 94/189 [01:37<01:30,  1.05it/s] 50%|█████     | 95/189 [01:38<01:29,  1.05it/s]                                                {'loss': 0.5034, 'grad_norm': 3.5847924732476018, 'learning_rate': 1.0382513661202186e-06, 'epoch': 1.51}
 50%|█████     | 95/189 [01:38<01:29,  1.05it/s] 51%|█████     | 96/189 [01:39<01:28,  1.05it/s]                                                {'loss': 0.6686, 'grad_norm': 3.6271526149788915, 'learning_rate': 1.0273224043715847e-06, 'epoch': 1.52}
 51%|█████     | 96/189 [01:39<01:28,  1.05it/s] 51%|█████▏    | 97/189 [01:40<01:27,  1.05it/s]                                                {'loss': 0.6492, 'grad_norm': 4.036033857219388, 'learning_rate': 1.0163934426229509e-06, 'epoch': 1.54}
 51%|█████▏    | 97/189 [01:40<01:27,  1.05it/s] 52%|█████▏    | 98/189 [01:41<01:26,  1.05it/s]                                                {'loss': 0.5073, 'grad_norm': 3.7010553980034993, 'learning_rate': 1.005464480874317e-06, 'epoch': 1.56}
 52%|█████▏    | 98/189 [01:41<01:26,  1.05it/s] 52%|█████▏    | 99/189 [01:42<01:25,  1.05it/s]                                                {'loss': 0.7478, 'grad_norm': 4.092074895537266, 'learning_rate': 9.94535519125683e-07, 'epoch': 1.57}
 52%|█████▏    | 99/189 [01:42<01:25,  1.05it/s] 53%|█████▎    | 100/189 [01:43<01:24,  1.05it/s]                                                 {'loss': 0.715, 'grad_norm': 4.410831622934191, 'learning_rate': 9.83606557377049e-07, 'epoch': 1.59}
 53%|█████▎    | 100/189 [01:43<01:24,  1.05it/s] 53%|█████▎    | 101/189 [01:43<01:23,  1.05it/s]                                                 {'loss': 0.5593, 'grad_norm': 3.3034038794504204, 'learning_rate': 9.726775956284152e-07, 'epoch': 1.6}
 53%|█████▎    | 101/189 [01:43<01:23,  1.05it/s] 54%|█████▍    | 102/189 [01:44<01:22,  1.05it/s]                                                 {'loss': 0.6209, 'grad_norm': 3.8327017108311856, 'learning_rate': 9.617486338797813e-07, 'epoch': 1.62}
 54%|█████▍    | 102/189 [01:44<01:22,  1.05it/s] 54%|█████▍    | 103/189 [01:45<01:22,  1.05it/s]                                                 {'loss': 0.6305, 'grad_norm': 3.6490502284381128, 'learning_rate': 9.508196721311474e-07, 'epoch': 1.63}
 54%|█████▍    | 103/189 [01:45<01:22,  1.05it/s] 55%|█████▌    | 104/189 [01:46<01:21,  1.05it/s]                                                 {'loss': 0.624, 'grad_norm': 4.866980134750877, 'learning_rate': 9.398907103825137e-07, 'epoch': 1.65}
 55%|█████▌    | 104/189 [01:46<01:21,  1.05it/s] 56%|█████▌    | 105/189 [01:47<01:20,  1.04it/s]                                                 {'loss': 0.6165, 'grad_norm': 3.9259626997892045, 'learning_rate': 9.289617486338798e-07, 'epoch': 1.67}
 56%|█████▌    | 105/189 [01:47<01:20,  1.04it/s] 56%|█████▌    | 106/189 [01:48<01:19,  1.05it/s]                                                 {'loss': 0.6506, 'grad_norm': 4.1293886931404975, 'learning_rate': 9.180327868852458e-07, 'epoch': 1.68}
 56%|█████▌    | 106/189 [01:48<01:19,  1.05it/s] 57%|█████▋    | 107/189 [01:49<01:18,  1.05it/s]                                                 {'loss': 0.6202, 'grad_norm': 4.042662471238298, 'learning_rate': 9.07103825136612e-07, 'epoch': 1.7}
 57%|█████▋    | 107/189 [01:49<01:18,  1.05it/s] 57%|█████▋    | 108/189 [01:50<01:17,  1.05it/s]                                                 {'loss': 0.6493, 'grad_norm': 3.62332319440553, 'learning_rate': 8.961748633879781e-07, 'epoch': 1.71}
 57%|█████▋    | 108/189 [01:50<01:17,  1.05it/s] 58%|█████▊    | 109/189 [01:51<01:16,  1.05it/s]                                                 {'loss': 0.6509, 'grad_norm': 3.9807722538059602, 'learning_rate': 8.852459016393443e-07, 'epoch': 1.73}
 58%|█████▊    | 109/189 [01:51<01:16,  1.05it/s] 58%|█████▊    | 110/189 [01:52<01:15,  1.05it/s]                                                 {'loss': 0.5833, 'grad_norm': 3.8425566756893788, 'learning_rate': 8.743169398907103e-07, 'epoch': 1.75}
 58%|█████▊    | 110/189 [01:52<01:15,  1.05it/s] 59%|█████▊    | 111/189 [01:53<01:14,  1.05it/s]                                                 {'loss': 0.5967, 'grad_norm': 3.954157729489438, 'learning_rate': 8.633879781420764e-07, 'epoch': 1.76}
 59%|█████▊    | 111/189 [01:53<01:14,  1.05it/s] 59%|█████▉    | 112/189 [01:54<01:13,  1.05it/s]                                                 {'loss': 0.586, 'grad_norm': 3.7857771934822066, 'learning_rate': 8.524590163934425e-07, 'epoch': 1.78}
 59%|█████▉    | 112/189 [01:54<01:13,  1.05it/s] 60%|█████▉    | 113/189 [01:55<01:12,  1.05it/s]                                                 {'loss': 0.5528, 'grad_norm': 4.029002836439377, 'learning_rate': 8.415300546448088e-07, 'epoch': 1.79}
 60%|█████▉    | 113/189 [01:55<01:12,  1.05it/s] 60%|██████    | 114/189 [01:56<01:11,  1.05it/s]                                                 {'loss': 0.7667, 'grad_norm': 3.56503693079006, 'learning_rate': 8.306010928961748e-07, 'epoch': 1.81}
 60%|██████    | 114/189 [01:56<01:11,  1.05it/s] 61%|██████    | 115/189 [01:57<01:10,  1.05it/s]                                                 {'loss': 0.7034, 'grad_norm': 3.606996934706717, 'learning_rate': 8.196721311475409e-07, 'epoch': 1.83}
 61%|██████    | 115/189 [01:57<01:10,  1.05it/s] 61%|██████▏   | 116/189 [01:58<01:09,  1.05it/s]                                                 {'loss': 0.5839, 'grad_norm': 4.156735819932782, 'learning_rate': 8.087431693989071e-07, 'epoch': 1.84}
 61%|██████▏   | 116/189 [01:58<01:09,  1.05it/s] 62%|██████▏   | 117/189 [01:59<01:08,  1.05it/s]                                                 {'loss': 0.675, 'grad_norm': 5.076698772077065, 'learning_rate': 7.978142076502732e-07, 'epoch': 1.86}
 62%|██████▏   | 117/189 [01:59<01:08,  1.05it/s] 62%|██████▏   | 118/189 [02:00<01:07,  1.05it/s]                                                 {'loss': 0.6412, 'grad_norm': 3.6824168893061957, 'learning_rate': 7.868852459016393e-07, 'epoch': 1.87}
 62%|██████▏   | 118/189 [02:00<01:07,  1.05it/s] 63%|██████▎   | 119/189 [02:01<01:06,  1.05it/s]                                                 {'loss': 0.5871, 'grad_norm': 3.9022292208471465, 'learning_rate': 7.759562841530054e-07, 'epoch': 1.89}
 63%|██████▎   | 119/189 [02:01<01:06,  1.05it/s] 63%|██████▎   | 120/189 [02:02<01:05,  1.05it/s]                                                 {'loss': 0.5375, 'grad_norm': 3.75070710528015, 'learning_rate': 7.650273224043715e-07, 'epoch': 1.9}
 63%|██████▎   | 120/189 [02:02<01:05,  1.05it/s] 64%|██████▍   | 121/189 [02:03<01:05,  1.04it/s]                                                 {'loss': 0.6778, 'grad_norm': 3.930349934368674, 'learning_rate': 7.540983606557376e-07, 'epoch': 1.92}
 64%|██████▍   | 121/189 [02:03<01:05,  1.04it/s] 65%|██████▍   | 122/189 [02:04<01:04,  1.04it/s]                                                 {'loss': 0.5524, 'grad_norm': 3.871751706062423, 'learning_rate': 7.431693989071039e-07, 'epoch': 1.94}
 65%|██████▍   | 122/189 [02:04<01:04,  1.04it/s] 65%|██████▌   | 123/189 [02:04<01:03,  1.05it/s]                                                 {'loss': 0.5316, 'grad_norm': 3.645792675583711, 'learning_rate': 7.322404371584699e-07, 'epoch': 1.95}
 65%|██████▌   | 123/189 [02:04<01:03,  1.05it/s] 66%|██████▌   | 124/189 [02:05<01:01,  1.05it/s]                                                 {'loss': 0.5514, 'grad_norm': 3.6496282091400736, 'learning_rate': 7.21311475409836e-07, 'epoch': 1.97}
 66%|██████▌   | 124/189 [02:05<01:01,  1.05it/s] 66%|██████▌   | 125/189 [02:06<01:01,  1.05it/s]                                                 {'loss': 0.6174, 'grad_norm': 4.1389605618202845, 'learning_rate': 7.103825136612022e-07, 'epoch': 1.98}
 66%|██████▌   | 125/189 [02:06<01:01,  1.05it/s] 67%|██████▋   | 126/189 [02:07<01:02,  1.01it/s]                                                 {'loss': 0.5824, 'grad_norm': 3.9972655012936955, 'learning_rate': 6.994535519125683e-07, 'epoch': 2.0}
 67%|██████▋   | 126/189 [02:07<01:02,  1.01it/s] 67%|██████▋   | 127/189 [02:09<01:03,  1.02s/it]                                                 {'loss': 0.4982, 'grad_norm': 3.6680796660996937, 'learning_rate': 6.885245901639343e-07, 'epoch': 2.02}
 67%|██████▋   | 127/189 [02:09<01:03,  1.02s/it] 68%|██████▊   | 128/189 [02:09<01:00,  1.00it/s]                                                 {'loss': 0.5911, 'grad_norm': 4.074682867661168, 'learning_rate': 6.775956284153005e-07, 'epoch': 2.03}
 68%|██████▊   | 128/189 [02:09<01:00,  1.00it/s] 68%|██████▊   | 129/189 [02:10<00:58,  1.02it/s]                                                 {'loss': 0.5227, 'grad_norm': 3.4608849612969466, 'learning_rate': 6.666666666666666e-07, 'epoch': 2.05}
 68%|██████▊   | 129/189 [02:10<00:58,  1.02it/s] 69%|██████▉   | 130/189 [02:11<00:57,  1.03it/s]                                                 {'loss': 0.5268, 'grad_norm': 4.01023207924231, 'learning_rate': 6.557377049180327e-07, 'epoch': 2.06}
 69%|██████▉   | 130/189 [02:11<00:57,  1.03it/s] 69%|██████▉   | 131/189 [02:12<00:56,  1.03it/s]                                                 {'loss': 0.4906, 'grad_norm': 4.420012135175157, 'learning_rate': 6.44808743169399e-07, 'epoch': 2.08}
 69%|██████▉   | 131/189 [02:12<00:56,  1.03it/s] 70%|██████▉   | 132/189 [02:13<00:55,  1.03it/s]                                                 {'loss': 0.6449, 'grad_norm': 4.256509896726898, 'learning_rate': 6.33879781420765e-07, 'epoch': 2.1}
 70%|██████▉   | 132/189 [02:13<00:55,  1.03it/s] 70%|███████   | 133/189 [02:14<00:53,  1.04it/s]                                                 {'loss': 0.5796, 'grad_norm': 4.535418466782426, 'learning_rate': 6.229508196721311e-07, 'epoch': 2.11}
 70%|███████   | 133/189 [02:14<00:53,  1.04it/s] 71%|███████   | 134/189 [02:15<00:52,  1.05it/s]                                                 {'loss': 0.5005, 'grad_norm': 4.019942499710216, 'learning_rate': 6.120218579234973e-07, 'epoch': 2.13}
 71%|███████   | 134/189 [02:15<00:52,  1.05it/s] 71%|███████▏  | 135/189 [02:16<00:51,  1.05it/s]                                                 {'loss': 0.4607, 'grad_norm': 5.559493700505198, 'learning_rate': 6.010928961748634e-07, 'epoch': 2.14}
 71%|███████▏  | 135/189 [02:16<00:51,  1.05it/s] 72%|███████▏  | 136/189 [02:17<00:50,  1.05it/s]                                                 {'loss': 0.3797, 'grad_norm': 3.4386763290232722, 'learning_rate': 5.901639344262294e-07, 'epoch': 2.16}
 72%|███████▏  | 136/189 [02:17<00:50,  1.05it/s] 72%|███████▏  | 137/189 [02:18<00:49,  1.04it/s]                                                 {'loss': 0.3981, 'grad_norm': 3.183809021775622, 'learning_rate': 5.792349726775956e-07, 'epoch': 2.17}
 72%|███████▏  | 137/189 [02:18<00:49,  1.04it/s] 73%|███████▎  | 138/189 [02:19<00:48,  1.05it/s]                                                 {'loss': 0.4922, 'grad_norm': 4.605555024031493, 'learning_rate': 5.683060109289617e-07, 'epoch': 2.19}
 73%|███████▎  | 138/189 [02:19<00:48,  1.05it/s] 74%|███████▎  | 139/189 [02:20<00:47,  1.05it/s]                                                 {'loss': 0.4262, 'grad_norm': 4.098172013140172, 'learning_rate': 5.573770491803278e-07, 'epoch': 2.21}
 74%|███████▎  | 139/189 [02:20<00:47,  1.05it/s] 74%|███████▍  | 140/189 [02:21<00:46,  1.05it/s]                                                 {'loss': 0.4869, 'grad_norm': 4.178732256696778, 'learning_rate': 5.46448087431694e-07, 'epoch': 2.22}
 74%|███████▍  | 140/189 [02:21<00:46,  1.05it/s] 75%|███████▍  | 141/189 [02:22<00:46,  1.04it/s]                                                 {'loss': 0.4936, 'grad_norm': 3.8295937434075653, 'learning_rate': 5.355191256830601e-07, 'epoch': 2.24}
 75%|███████▍  | 141/189 [02:22<00:46,  1.04it/s] 75%|███████▌  | 142/189 [02:23<00:44,  1.05it/s]                                                 {'loss': 0.5047, 'grad_norm': 4.322388894579684, 'learning_rate': 5.245901639344262e-07, 'epoch': 2.25}
 75%|███████▌  | 142/189 [02:23<00:44,  1.05it/s] 76%|███████▌  | 143/189 [02:24<00:43,  1.05it/s]                                                 {'loss': 0.476, 'grad_norm': 3.843045560685772, 'learning_rate': 5.136612021857924e-07, 'epoch': 2.27}
 76%|███████▌  | 143/189 [02:24<00:43,  1.05it/s] 76%|███████▌  | 144/189 [02:25<00:43,  1.04it/s]                                                 {'loss': 0.5622, 'grad_norm': 4.9459377993660025, 'learning_rate': 5.027322404371585e-07, 'epoch': 2.29}
 76%|███████▌  | 144/189 [02:25<00:43,  1.04it/s] 77%|███████▋  | 145/189 [02:26<00:42,  1.05it/s]                                                 {'loss': 0.3748, 'grad_norm': 3.44326743303423, 'learning_rate': 4.918032786885245e-07, 'epoch': 2.3}
 77%|███████▋  | 145/189 [02:26<00:42,  1.05it/s] 77%|███████▋  | 146/189 [02:27<00:41,  1.05it/s]                                                 {'loss': 0.4231, 'grad_norm': 4.023368009918379, 'learning_rate': 4.808743169398907e-07, 'epoch': 2.32}
 77%|███████▋  | 146/189 [02:27<00:41,  1.05it/s] 78%|███████▊  | 147/189 [02:28<00:40,  1.05it/s]                                                 {'loss': 0.399, 'grad_norm': 4.08834258165251, 'learning_rate': 4.6994535519125684e-07, 'epoch': 2.33}
 78%|███████▊  | 147/189 [02:28<00:40,  1.05it/s] 78%|███████▊  | 148/189 [02:29<00:39,  1.05it/s]                                                 {'loss': 0.5915, 'grad_norm': 4.894155192092387, 'learning_rate': 4.590163934426229e-07, 'epoch': 2.35}
 78%|███████▊  | 148/189 [02:29<00:39,  1.05it/s] 79%|███████▉  | 149/189 [02:30<00:38,  1.05it/s]                                                 {'loss': 0.3533, 'grad_norm': 4.377069338264372, 'learning_rate': 4.4808743169398906e-07, 'epoch': 2.37}
 79%|███████▉  | 149/189 [02:30<00:38,  1.05it/s] 79%|███████▉  | 150/189 [02:30<00:37,  1.05it/s]                                                 {'loss': 0.5065, 'grad_norm': 3.907322171868102, 'learning_rate': 4.3715846994535514e-07, 'epoch': 2.38}
 79%|███████▉  | 150/189 [02:30<00:37,  1.05it/s] 80%|███████▉  | 151/189 [02:31<00:36,  1.05it/s]                                                 {'loss': 0.4972, 'grad_norm': 4.898513368212926, 'learning_rate': 4.2622950819672127e-07, 'epoch': 2.4}
 80%|███████▉  | 151/189 [02:31<00:36,  1.05it/s] 80%|████████  | 152/189 [02:32<00:35,  1.05it/s]                                                 {'loss': 0.5031, 'grad_norm': 3.8186997219604266, 'learning_rate': 4.153005464480874e-07, 'epoch': 2.41}
 80%|████████  | 152/189 [02:32<00:35,  1.05it/s] 81%|████████  | 153/189 [02:33<00:33,  1.06it/s]                                                 {'loss': 0.6045, 'grad_norm': 4.427479468835859, 'learning_rate': 4.0437158469945354e-07, 'epoch': 2.43}
 81%|████████  | 153/189 [02:33<00:33,  1.06it/s] 81%|████████▏ | 154/189 [02:34<00:33,  1.05it/s]                                                 {'loss': 0.472, 'grad_norm': 3.6914077205246985, 'learning_rate': 3.9344262295081967e-07, 'epoch': 2.44}
 81%|████████▏ | 154/189 [02:34<00:33,  1.05it/s] 82%|████████▏ | 155/189 [02:35<00:32,  1.05it/s]                                                 {'loss': 0.5479, 'grad_norm': 4.991166199742266, 'learning_rate': 3.8251366120218575e-07, 'epoch': 2.46}
 82%|████████▏ | 155/189 [02:35<00:32,  1.05it/s] 83%|████████▎ | 156/189 [02:36<00:31,  1.05it/s]                                                 {'loss': 0.5777, 'grad_norm': 6.08667704645078, 'learning_rate': 3.7158469945355194e-07, 'epoch': 2.48}
 83%|████████▎ | 156/189 [02:36<00:31,  1.05it/s] 83%|████████▎ | 157/189 [02:37<00:30,  1.05it/s]                                                 {'loss': 0.5242, 'grad_norm': 3.5102525262325677, 'learning_rate': 3.60655737704918e-07, 'epoch': 2.49}
 83%|████████▎ | 157/189 [02:37<00:30,  1.05it/s] 84%|████████▎ | 158/189 [02:38<00:29,  1.05it/s]                                                 {'loss': 0.3427, 'grad_norm': 2.9040548419766874, 'learning_rate': 3.4972677595628415e-07, 'epoch': 2.51}
 84%|████████▎ | 158/189 [02:38<00:29,  1.05it/s] 84%|████████▍ | 159/189 [02:39<00:28,  1.05it/s]                                                 {'loss': 0.5127, 'grad_norm': 4.602854052476924, 'learning_rate': 3.3879781420765023e-07, 'epoch': 2.52}
 84%|████████▍ | 159/189 [02:39<00:28,  1.05it/s] 85%|████████▍ | 160/189 [02:40<00:27,  1.04it/s]                                                 {'loss': 0.4094, 'grad_norm': 4.740444543510451, 'learning_rate': 3.2786885245901637e-07, 'epoch': 2.54}
 85%|████████▍ | 160/189 [02:40<00:27,  1.04it/s] 85%|████████▌ | 161/189 [02:41<00:26,  1.04it/s]                                                 {'loss': 0.4247, 'grad_norm': 3.9959215811080675, 'learning_rate': 3.169398907103825e-07, 'epoch': 2.56}
 85%|████████▌ | 161/189 [02:41<00:26,  1.04it/s] 86%|████████▌ | 162/189 [02:42<00:25,  1.04it/s]                                                 {'loss': 0.4567, 'grad_norm': 4.238623834699119, 'learning_rate': 3.0601092896174863e-07, 'epoch': 2.57}
 86%|████████▌ | 162/189 [02:42<00:25,  1.04it/s] 86%|████████▌ | 163/189 [02:43<00:24,  1.05it/s]                                                 {'loss': 0.3938, 'grad_norm': 3.53653638834974, 'learning_rate': 2.950819672131147e-07, 'epoch': 2.59}
 86%|████████▌ | 163/189 [02:43<00:24,  1.05it/s] 87%|████████▋ | 164/189 [02:44<00:23,  1.05it/s]                                                 {'loss': 0.3868, 'grad_norm': 3.783890470218832, 'learning_rate': 2.8415300546448085e-07, 'epoch': 2.6}
 87%|████████▋ | 164/189 [02:44<00:23,  1.05it/s] 87%|████████▋ | 165/189 [02:45<00:22,  1.05it/s]                                                 {'loss': 0.375, 'grad_norm': 3.5095669163711505, 'learning_rate': 2.73224043715847e-07, 'epoch': 2.62}
 87%|████████▋ | 165/189 [02:45<00:22,  1.05it/s] 88%|████████▊ | 166/189 [02:46<00:22,  1.04it/s]                                                 {'loss': 0.6182, 'grad_norm': 4.33939909989276, 'learning_rate': 2.622950819672131e-07, 'epoch': 2.63}
 88%|████████▊ | 166/189 [02:46<00:22,  1.04it/s] 88%|████████▊ | 167/189 [02:47<00:21,  1.04it/s]                                                 {'loss': 0.4261, 'grad_norm': 3.9976938512276776, 'learning_rate': 2.5136612021857925e-07, 'epoch': 2.65}
 88%|████████▊ | 167/189 [02:47<00:21,  1.04it/s] 89%|████████▉ | 168/189 [02:48<00:19,  1.05it/s]                                                 {'loss': 0.4594, 'grad_norm': 4.383387746482641, 'learning_rate': 2.4043715846994533e-07, 'epoch': 2.67}
 89%|████████▉ | 168/189 [02:48<00:19,  1.05it/s] 89%|████████▉ | 169/189 [02:49<00:18,  1.05it/s]                                                 {'loss': 0.4595, 'grad_norm': 5.314710199527769, 'learning_rate': 2.2950819672131146e-07, 'epoch': 2.68}
 89%|████████▉ | 169/189 [02:49<00:18,  1.05it/s] 90%|████████▉ | 170/189 [02:50<00:18,  1.05it/s]                                                 {'loss': 0.4596, 'grad_norm': 4.581219689396056, 'learning_rate': 2.1857923497267757e-07, 'epoch': 2.7}
 90%|████████▉ | 170/189 [02:50<00:18,  1.05it/s] 90%|█████████ | 171/189 [02:51<00:17,  1.05it/s]                                                 {'loss': 0.3471, 'grad_norm': 3.126372275550887, 'learning_rate': 2.076502732240437e-07, 'epoch': 2.71}
 90%|█████████ | 171/189 [02:51<00:17,  1.05it/s] 91%|█████████ | 172/189 [02:51<00:16,  1.05it/s]                                                 {'loss': 0.4524, 'grad_norm': 7.966732956788376, 'learning_rate': 1.9672131147540984e-07, 'epoch': 2.73}
 91%|█████████ | 172/189 [02:51<00:16,  1.05it/s] 92%|█████████▏| 173/189 [02:52<00:15,  1.05it/s]                                                 {'loss': 0.6324, 'grad_norm': 6.4090061071775954, 'learning_rate': 1.8579234972677597e-07, 'epoch': 2.75}
 92%|█████████▏| 173/189 [02:52<00:15,  1.05it/s] 92%|█████████▏| 174/189 [02:53<00:14,  1.05it/s]                                                 {'loss': 0.4323, 'grad_norm': 3.8598342440259086, 'learning_rate': 1.7486338797814208e-07, 'epoch': 2.76}
 92%|█████████▏| 174/189 [02:53<00:14,  1.05it/s] 93%|█████████▎| 175/189 [02:54<00:13,  1.05it/s]                                                 {'loss': 0.4699, 'grad_norm': 4.97912701919016, 'learning_rate': 1.6393442622950818e-07, 'epoch': 2.78}
 93%|█████████▎| 175/189 [02:54<00:13,  1.05it/s] 93%|█████████▎| 176/189 [02:55<00:12,  1.05it/s]                                                 {'loss': 0.3778, 'grad_norm': 4.4642128510469075, 'learning_rate': 1.5300546448087432e-07, 'epoch': 2.79}
 93%|█████████▎| 176/189 [02:55<00:12,  1.05it/s] 94%|█████████▎| 177/189 [02:56<00:11,  1.05it/s]                                                 {'loss': 0.4104, 'grad_norm': 3.762333928171326, 'learning_rate': 1.4207650273224042e-07, 'epoch': 2.81}
 94%|█████████▎| 177/189 [02:56<00:11,  1.05it/s] 94%|█████████▍| 178/189 [02:57<00:10,  1.05it/s]                                                 {'loss': 0.4279, 'grad_norm': 3.7649376584743544, 'learning_rate': 1.3114754098360656e-07, 'epoch': 2.83}
 94%|█████████▍| 178/189 [02:57<00:10,  1.05it/s] 95%|█████████▍| 179/189 [02:58<00:09,  1.05it/s]                                                 {'loss': 0.5314, 'grad_norm': 3.571312808526371, 'learning_rate': 1.2021857923497266e-07, 'epoch': 2.84}
 95%|█████████▍| 179/189 [02:58<00:09,  1.05it/s] 95%|█████████▌| 180/189 [02:59<00:08,  1.04it/s]                                                 {'loss': 0.4263, 'grad_norm': 3.614843743716289, 'learning_rate': 1.0928961748633878e-07, 'epoch': 2.86}
 95%|█████████▌| 180/189 [02:59<00:08,  1.04it/s] 96%|█████████▌| 181/189 [03:00<00:07,  1.05it/s]                                                 {'loss': 0.3775, 'grad_norm': 4.383116713742891, 'learning_rate': 9.836065573770492e-08, 'epoch': 2.87}
 96%|█████████▌| 181/189 [03:00<00:07,  1.05it/s] 96%|█████████▋| 182/189 [03:01<00:06,  1.05it/s]                                                 {'loss': 0.396, 'grad_norm': 3.573023468665696, 'learning_rate': 8.743169398907104e-08, 'epoch': 2.89}
 96%|█████████▋| 182/189 [03:01<00:06,  1.05it/s] 97%|█████████▋| 183/189 [03:02<00:05,  1.05it/s]                                                 {'loss': 0.3771, 'grad_norm': 3.9690302688243126, 'learning_rate': 7.650273224043716e-08, 'epoch': 2.9}
 97%|█████████▋| 183/189 [03:02<00:05,  1.05it/s] 97%|█████████▋| 184/189 [03:03<00:04,  1.05it/s]                                                 {'loss': 0.5511, 'grad_norm': 4.351624459292693, 'learning_rate': 6.557377049180328e-08, 'epoch': 2.92}
 97%|█████████▋| 184/189 [03:03<00:04,  1.05it/s] 98%|█████████▊| 185/189 [03:04<00:03,  1.05it/s]                                                 {'loss': 0.401, 'grad_norm': 3.4047917972869004, 'learning_rate': 5.464480874316939e-08, 'epoch': 2.94}
 98%|█████████▊| 185/189 [03:04<00:03,  1.05it/s] 98%|█████████▊| 186/189 [03:05<00:02,  1.05it/s]                                                 {'loss': 0.2581, 'grad_norm': 4.2513057207315175, 'learning_rate': 4.371584699453552e-08, 'epoch': 2.95}
 98%|█████████▊| 186/189 [03:05<00:02,  1.05it/s] 99%|█████████▉| 187/189 [03:06<00:01,  1.05it/s]                                                 {'loss': 0.4867, 'grad_norm': 3.5548294807114003, 'learning_rate': 3.278688524590164e-08, 'epoch': 2.97}
 99%|█████████▉| 187/189 [03:06<00:01,  1.05it/s] 99%|█████████▉| 188/189 [03:07<00:00,  1.05it/s]                                                 {'loss': 0.3894, 'grad_norm': 3.468836696225184, 'learning_rate': 2.185792349726776e-08, 'epoch': 2.98}
 99%|█████████▉| 188/189 [03:07<00:00,  1.05it/s]100%|██████████| 189/189 [03:08<00:00,  1.05it/s]                                                 {'loss': 0.4008, 'grad_norm': 3.613434553870176, 'learning_rate': 1.092896174863388e-08, 'epoch': 3.0}
100%|██████████| 189/189 [03:08<00:00,  1.05it/s]                                                 {'train_runtime': 259.1205, 'train_samples_per_second': 57.738, 'train_steps_per_second': 0.729, 'train_loss': 0.7190748765355065, 'epoch': 3.0}
100%|██████████| 189/189 [04:19<00:00,  1.05it/s]100%|██████████| 189/189 [04:19<00:00,  1.37s/it]
[2024-08-28 21:16:24,110] [INFO] [launch.py:351:main] Process 3774643 exits successfully.
[2024-08-28 21:16:24,110] [INFO] [launch.py:351:main] Process 3774648 exits successfully.
[2024-08-28 21:16:25,110] [INFO] [launch.py:351:main] Process 3774647 exits successfully.
[2024-08-28 21:16:25,111] [INFO] [launch.py:351:main] Process 3774642 exits successfully.
[2024-08-28 21:16:25,111] [INFO] [launch.py:351:main] Process 3774646 exits successfully.
[2024-08-28 21:16:25,111] [INFO] [launch.py:351:main] Process 3774645 exits successfully.
[2024-08-28 21:16:25,111] [INFO] [launch.py:351:main] Process 3774644 exits successfully.
[2024-08-28 21:16:52,114] [INFO] [launch.py:351:main] Process 3774641 exits successfully.
