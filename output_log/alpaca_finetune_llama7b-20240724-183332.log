Namespace(mode=['alpaca'], base_model='meta-llama/Llama-2-7b-hf', task_name='roleplay', tuned_dir='./cache')
num gpus:  8
Running 1/1: deepspeed --num_gpus=8 train.py --deepspeed ../deepspeed_config/zero3.json --bf16 --tf32=True
    --model_name_or_path meta-llama/Llama-2-7b-hf --data_path ../data/stanford_alpaca/roleplay_data.json
    --output_dir /fsx-project/yunyun/models/meta-llama_meta-llama_roleplay_tuned
    --num_train_epochs 3
    --per_device_train_batch_size 10
    --per_device_eval_batch_size 4
    --gradient_accumulation_steps 1
    --gradient_checkpointing=True
    --evaluation_strategy=no
    --save_strategy=steps
    --save_steps 500
    --save_total_limit 1
    --learning_rate 2e-6
    --weight_decay 0.
    --report_to tensorboard
    --warmup_ratio 0.03
    --lr_scheduler_type=cosine
    --logging_steps 1
['deepspeed', '--num_gpus=8', 'train.py', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Llama-2-7b-hf', '--data_path', '../data/stanford_alpaca/roleplay_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_meta-llama_roleplay_tuned', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1']
[2024-07-24 18:33:37,382] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2024-07-24 18:33:41,342] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2024-07-24 18:33:41,342] [INFO] [runner.py:568:main] cmd = /data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNSwgNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None train.py --deepspeed ../deepspeed_config/zero3.json --bf16 --tf32=True --model_name_or_path meta-llama/Llama-2-7b-hf --data_path ../data/stanford_alpaca/roleplay_data.json --output_dir /fsx-project/yunyun/models/meta-llama_meta-llama_roleplay_tuned --num_train_epochs 3 --per_device_train_batch_size 10 --per_device_eval_batch_size 4 --gradient_accumulation_steps 1 --gradient_checkpointing=True --evaluation_strategy=no --save_strategy=steps --save_steps 500 --save_total_limit 1 --learning_rate 2e-6 --weight_decay 0. --report_to tensorboard --warmup_ratio 0.03 --lr_scheduler_type=cosine --logging_steps 1
[2024-07-24 18:33:44,061] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2024-07-24 18:33:48,514] [INFO] [launch.py:139:main] 0 NCCL_ASYNC_ERROR_HANDLING=1
[2024-07-24 18:33:48,514] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]}
[2024-07-24 18:33:48,514] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=8, node_rank=0
[2024-07-24 18:33:48,514] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]})
[2024-07-24 18:33:48,514] [INFO] [launch.py:164:main] dist_world_size=8
[2024-07-24 18:33:48,514] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
[2024-07-24 18:33:48,515] [INFO] [launch.py:256:main] process 289208 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=0', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Llama-2-7b-hf', '--data_path', '../data/stanford_alpaca/roleplay_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_meta-llama_roleplay_tuned', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1']
[2024-07-24 18:33:48,516] [INFO] [launch.py:256:main] process 289209 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=1', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Llama-2-7b-hf', '--data_path', '../data/stanford_alpaca/roleplay_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_meta-llama_roleplay_tuned', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1']
[2024-07-24 18:33:48,517] [INFO] [launch.py:256:main] process 289210 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=2', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Llama-2-7b-hf', '--data_path', '../data/stanford_alpaca/roleplay_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_meta-llama_roleplay_tuned', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1']
[2024-07-24 18:33:48,517] [INFO] [launch.py:256:main] process 289211 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=3', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Llama-2-7b-hf', '--data_path', '../data/stanford_alpaca/roleplay_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_meta-llama_roleplay_tuned', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1']
[2024-07-24 18:33:48,518] [INFO] [launch.py:256:main] process 289212 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=4', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Llama-2-7b-hf', '--data_path', '../data/stanford_alpaca/roleplay_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_meta-llama_roleplay_tuned', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1']
[2024-07-24 18:33:48,518] [INFO] [launch.py:256:main] process 289213 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=5', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Llama-2-7b-hf', '--data_path', '../data/stanford_alpaca/roleplay_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_meta-llama_roleplay_tuned', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1']
[2024-07-24 18:33:48,519] [INFO] [launch.py:256:main] process 289214 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=6', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Llama-2-7b-hf', '--data_path', '../data/stanford_alpaca/roleplay_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_meta-llama_roleplay_tuned', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1']
[2024-07-24 18:33:48,520] [INFO] [launch.py:256:main] process 289215 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=7', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', 'meta-llama/Llama-2-7b-hf', '--data_path', '../data/stanford_alpaca/roleplay_data.json', '--output_dir', '/fsx-project/yunyun/models/meta-llama_meta-llama_roleplay_tuned', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1']
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
[2024-07-24 18:34:02,632] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-24 18:34:02,650] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
[2024-07-24 18:34:02,868] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible

[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2024-07-24 18:34:02,942] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-24 18:34:03,008] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-24 18:34:03,009] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-24 18:34:03,033] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-24 18:34:03,040] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.

[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2024-07-24 18:34:03,421] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-24 18:34:03,421] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-24 18:34:03,630] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-24 18:34:03,716] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-24 18:34:03,754] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-24 18:34:03,754] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-07-24 18:34:03,768] [INFO] [comm.py:637:init_distributed] cdb=None
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[2024-07-24 18:34:03,791] [INFO] [comm.py:637:init_distributed] cdb=None
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[2024-07-24 18:34:03,823] [INFO] [comm.py:637:init_distributed] cdb=None
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 2/2 [00:00<00:00, 792.87it/s]
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 2/2 [00:00<00:00, 1376.31it/s]
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 2/2 [00:00<00:00, 1285.81it/s]
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 2/2 [00:00<00:00, 1352.13it/s]
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 2/2 [00:00<00:00, 1320.83it/s]
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 2/2 [00:00<00:00, 1240.92it/s]
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 2/2 [00:00<00:00, 1323.12it/s]
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 2/2 [00:00<00:00, 1333.85it/s]
[2024-07-24 18:34:14,667] [INFO] [partition_parameters.py:345:__exit__] finished initializing model - num_params = 291, num_elems = 6.74B
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.84s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.85s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.85s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.87s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.86s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.88s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.89s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.14s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.25s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.15s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.25s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.14s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.25s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.15s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.25s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.14s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.25s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.15s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.26s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.15s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.26s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.62s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.45s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.77s/it]
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
[rank0]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121/fused_adam/build.ninja...
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/cpp_extension.py:1967: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
[rank1]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
[rank5]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
[rank2]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
[rank4]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
[rank6]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.26876020431518555 seconds
[rank3]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
[rank7]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121/fused_adam/build.ninja...
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/cpp_extension.py:1967: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Parameter Offload: Total persistent parameters: 266240 in 65 params
Loading extension module fused_adam...
Time to load fused_adam op: 0.24246788024902344 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.3032057285308838 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.40381288528442383 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.30372190475463867 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.5035991668701172 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.5032663345336914 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.4038376808166504 seconds
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  0%|          | 0/270 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
  0%|          | 1/270 [00:08<37:17,  8.32s/it]                                               {'loss': 1.8355, 'grad_norm': 8.028565093233594, 'learning_rate': 0.0, 'epoch': 0.01}
  0%|          | 1/270 [00:08<37:17,  8.32s/it]  1%|          | 2/270 [00:09<18:43,  4.19s/it]                                               {'loss': 1.8099, 'grad_norm': 8.589387095939154, 'learning_rate': 6.309297535714572e-07, 'epoch': 0.02}
  1%|          | 2/270 [00:09<18:43,  4.19s/it]  1%|          | 3/270 [00:10<12:49,  2.88s/it]                                               {'loss': 1.8299, 'grad_norm': 7.778869194053287, 'learning_rate': 1e-06, 'epoch': 0.03}
  1%|          | 3/270 [00:10<12:49,  2.88s/it]  1%|▏         | 4/270 [00:12<09:35,  2.16s/it]                                               {'loss': 1.7773, 'grad_norm': 7.5515583746337045, 'learning_rate': 1.2618595071429145e-06, 'epoch': 0.04}
  1%|▏         | 4/270 [00:12<09:35,  2.16s/it]  2%|▏         | 5/270 [00:12<07:38,  1.73s/it]                                               {'loss': 1.8264, 'grad_norm': 9.016687423551621, 'learning_rate': 1.4649735207179267e-06, 'epoch': 0.06}
  2%|▏         | 5/270 [00:12<07:38,  1.73s/it]  2%|▏         | 6/270 [00:13<06:20,  1.44s/it]                                               {'loss': 1.8427, 'grad_norm': 7.471718442560663, 'learning_rate': 1.6309297535714573e-06, 'epoch': 0.07}
  2%|▏         | 6/270 [00:13<06:20,  1.44s/it]  3%|▎         | 7/270 [00:14<05:47,  1.32s/it]                                               {'loss': 1.7002, 'grad_norm': 6.040352733141577, 'learning_rate': 1.771243749161422e-06, 'epoch': 0.08}
  3%|▎         | 7/270 [00:14<05:47,  1.32s/it]  3%|▎         | 8/270 [00:16<05:41,  1.30s/it]                                               {'loss': 1.7269, 'grad_norm': 6.540714233372399, 'learning_rate': 1.8927892607143718e-06, 'epoch': 0.09}
  3%|▎         | 8/270 [00:16<05:41,  1.30s/it]  3%|▎         | 9/270 [00:17<05:20,  1.23s/it]                                               {'loss': 1.64, 'grad_norm': 6.554345513954555, 'learning_rate': 2e-06, 'epoch': 0.1}
  3%|▎         | 9/270 [00:17<05:20,  1.23s/it]  4%|▎         | 10/270 [00:18<04:50,  1.12s/it]                                                {'loss': 1.6456, 'grad_norm': 6.128711239591645, 'learning_rate': 2e-06, 'epoch': 0.11}
  4%|▎         | 10/270 [00:18<04:50,  1.12s/it]  4%|▍         | 11/270 [00:19<05:09,  1.19s/it]                                                {'loss': 1.5992, 'grad_norm': 6.203690912378274, 'learning_rate': 1.9923371647509575e-06, 'epoch': 0.12}
  4%|▍         | 11/270 [00:19<05:09,  1.19s/it]  4%|▍         | 12/270 [00:20<04:54,  1.14s/it]                                                {'loss': 1.5596, 'grad_norm': 3.580058835094679, 'learning_rate': 1.984674329501916e-06, 'epoch': 0.13}
  4%|▍         | 12/270 [00:20<04:54,  1.14s/it]  5%|▍         | 13/270 [00:21<04:39,  1.09s/it]                                                {'loss': 1.5231, 'grad_norm': 3.6578067016748737, 'learning_rate': 1.9770114942528735e-06, 'epoch': 0.14}
  5%|▍         | 13/270 [00:21<04:39,  1.09s/it]  5%|▌         | 14/270 [00:22<04:23,  1.03s/it]                                                {'loss': 1.4985, 'grad_norm': 3.1751092539732486, 'learning_rate': 1.9693486590038315e-06, 'epoch': 0.16}
  5%|▌         | 14/270 [00:22<04:23,  1.03s/it]  6%|▌         | 15/270 [00:23<04:13,  1.00it/s]                                                {'loss': 1.5337, 'grad_norm': 3.7047996781537513, 'learning_rate': 1.961685823754789e-06, 'epoch': 0.17}
  6%|▌         | 15/270 [00:23<04:13,  1.00it/s]  6%|▌         | 16/270 [00:24<04:31,  1.07s/it]                                                {'loss': 1.4221, 'grad_norm': 3.190409698759802, 'learning_rate': 1.954022988505747e-06, 'epoch': 0.18}
  6%|▌         | 16/270 [00:24<04:31,  1.07s/it]  6%|▋         | 17/270 [00:25<04:19,  1.03s/it]                                                {'loss': 1.4239, 'grad_norm': 2.7855261209003355, 'learning_rate': 1.946360153256705e-06, 'epoch': 0.19}
  6%|▋         | 17/270 [00:25<04:19,  1.03s/it]  7%|▋         | 18/270 [00:26<04:12,  1.00s/it]                                                {'loss': 1.4787, 'grad_norm': 2.7447540962243537, 'learning_rate': 1.9386973180076627e-06, 'epoch': 0.2}
  7%|▋         | 18/270 [00:26<04:12,  1.00s/it]  7%|▋         | 19/270 [00:27<04:09,  1.01it/s]                                                {'loss': 1.4913, 'grad_norm': 2.6931578042300086, 'learning_rate': 1.9310344827586207e-06, 'epoch': 0.21}
  7%|▋         | 19/270 [00:27<04:09,  1.01it/s]  7%|▋         | 20/270 [00:28<04:45,  1.14s/it]                                                {'loss': 1.4888, 'grad_norm': 2.965577875093602, 'learning_rate': 1.9233716475095787e-06, 'epoch': 0.22}
  7%|▋         | 20/270 [00:28<04:45,  1.14s/it]  8%|▊         | 21/270 [00:29<04:23,  1.06s/it]                                                {'loss': 1.4657, 'grad_norm': 2.863994008836082, 'learning_rate': 1.9157088122605362e-06, 'epoch': 0.23}
  8%|▊         | 21/270 [00:29<04:23,  1.06s/it]  8%|▊         | 22/270 [00:30<04:10,  1.01s/it]                                                {'loss': 1.4133, 'grad_norm': 2.719557684241511, 'learning_rate': 1.9080459770114942e-06, 'epoch': 0.24}
  8%|▊         | 22/270 [00:30<04:10,  1.01s/it]  9%|▊         | 23/270 [00:31<03:59,  1.03it/s]                                                {'loss': 1.492, 'grad_norm': 2.821964968563364, 'learning_rate': 1.900383141762452e-06, 'epoch': 0.26}
  9%|▊         | 23/270 [00:31<03:59,  1.03it/s]  9%|▉         | 24/270 [00:33<04:46,  1.17s/it]                                                {'loss': 1.455, 'grad_norm': 2.5712103807159736, 'learning_rate': 1.8927203065134098e-06, 'epoch': 0.27}
  9%|▉         | 24/270 [00:33<04:46,  1.17s/it]  9%|▉         | 25/270 [00:34<04:27,  1.09s/it]                                                {'loss': 1.4907, 'grad_norm': 2.6302402708505666, 'learning_rate': 1.8850574712643676e-06, 'epoch': 0.28}
  9%|▉         | 25/270 [00:34<04:27,  1.09s/it] 10%|▉         | 26/270 [00:34<04:13,  1.04s/it]                                                {'loss': 1.3403, 'grad_norm': 2.4117563611544792, 'learning_rate': 1.8773946360153256e-06, 'epoch': 0.29}
 10%|▉         | 26/270 [00:34<04:13,  1.04s/it] 10%|█         | 27/270 [00:35<04:00,  1.01it/s]                                                {'loss': 1.4746, 'grad_norm': 2.5731951058312443, 'learning_rate': 1.8697318007662834e-06, 'epoch': 0.3}
 10%|█         | 27/270 [00:35<04:00,  1.01it/s] 10%|█         | 28/270 [00:36<03:57,  1.02it/s]                                                {'loss': 1.4521, 'grad_norm': 2.945370477706129, 'learning_rate': 1.8620689655172412e-06, 'epoch': 0.31}
 10%|█         | 28/270 [00:36<03:57,  1.02it/s] 11%|█         | 29/270 [00:37<04:05,  1.02s/it]                                                {'loss': 1.5331, 'grad_norm': 2.72071593193823, 'learning_rate': 1.8544061302681992e-06, 'epoch': 0.32}
 11%|█         | 29/270 [00:37<04:05,  1.02s/it] 11%|█         | 30/270 [00:38<04:03,  1.02s/it]                                                {'loss': 1.3407, 'grad_norm': 2.542668842872952, 'learning_rate': 1.846743295019157e-06, 'epoch': 0.33}
 11%|█         | 30/270 [00:38<04:03,  1.02s/it] 11%|█▏        | 31/270 [00:39<03:55,  1.02it/s]                                                {'loss': 1.3949, 'grad_norm': 2.6032850384531243, 'learning_rate': 1.8390804597701148e-06, 'epoch': 0.34}
 11%|█▏        | 31/270 [00:39<03:55,  1.02it/s] 12%|█▏        | 32/270 [00:40<04:08,  1.04s/it]                                                {'loss': 1.4199, 'grad_norm': 2.743009135566448, 'learning_rate': 1.8314176245210726e-06, 'epoch': 0.36}
 12%|█▏        | 32/270 [00:41<04:08,  1.04s/it] 12%|█▏        | 33/270 [00:42<04:20,  1.10s/it]                                                {'loss': 1.4904, 'grad_norm': 2.4829246145485118, 'learning_rate': 1.8237547892720306e-06, 'epoch': 0.37}
 12%|█▏        | 33/270 [00:42<04:20,  1.10s/it] 13%|█▎        | 34/270 [00:43<04:07,  1.05s/it]                                                {'loss': 1.4609, 'grad_norm': 2.67449286171498, 'learning_rate': 1.8160919540229884e-06, 'epoch': 0.38}
 13%|█▎        | 34/270 [00:43<04:07,  1.05s/it] 13%|█▎        | 35/270 [00:44<03:55,  1.00s/it]                                                {'loss': 1.4079, 'grad_norm': 2.4794460295289147, 'learning_rate': 1.8084291187739462e-06, 'epoch': 0.39}
 13%|█▎        | 35/270 [00:44<03:55,  1.00s/it] 13%|█▎        | 36/270 [00:45<04:33,  1.17s/it]                                                {'loss': 1.3842, 'grad_norm': 2.5526974649653056, 'learning_rate': 1.8007662835249042e-06, 'epoch': 0.4}
 13%|█▎        | 36/270 [00:45<04:33,  1.17s/it] 14%|█▎        | 37/270 [00:46<04:25,  1.14s/it]                                                {'loss': 1.4316, 'grad_norm': 2.3414732678416947, 'learning_rate': 1.793103448275862e-06, 'epoch': 0.41}
 14%|█▎        | 37/270 [00:46<04:25,  1.14s/it] 14%|█▍        | 38/270 [00:47<04:09,  1.08s/it]                                                {'loss': 1.3921, 'grad_norm': 2.322788345063451, 'learning_rate': 1.7854406130268197e-06, 'epoch': 0.42}
 14%|█▍        | 38/270 [00:47<04:09,  1.08s/it] 14%|█▍        | 39/270 [00:48<03:55,  1.02s/it]                                                {'loss': 1.3963, 'grad_norm': 2.452261874366234, 'learning_rate': 1.7777777777777775e-06, 'epoch': 0.43}
 14%|█▍        | 39/270 [00:48<03:55,  1.02s/it] 15%|█▍        | 40/270 [00:49<04:23,  1.14s/it]                                                {'loss': 1.4172, 'grad_norm': 2.4697278935512528, 'learning_rate': 1.7701149425287355e-06, 'epoch': 0.44}
 15%|█▍        | 40/270 [00:49<04:23,  1.14s/it] 15%|█▌        | 41/270 [00:50<04:15,  1.12s/it]                                                {'loss': 1.3622, 'grad_norm': 2.3965251050783727, 'learning_rate': 1.7624521072796933e-06, 'epoch': 0.46}
 15%|█▌        | 41/270 [00:50<04:15,  1.12s/it] 16%|█▌        | 42/270 [00:51<03:58,  1.05s/it]                                                {'loss': 1.4326, 'grad_norm': 2.4977458139372737, 'learning_rate': 1.7547892720306511e-06, 'epoch': 0.47}
 16%|█▌        | 42/270 [00:51<03:58,  1.05s/it] 16%|█▌        | 43/270 [00:52<03:48,  1.01s/it]                                                {'loss': 1.4632, 'grad_norm': 2.4315613752041076, 'learning_rate': 1.7471264367816091e-06, 'epoch': 0.48}
 16%|█▌        | 43/270 [00:52<03:48,  1.01s/it] 16%|█▋        | 44/270 [00:54<04:14,  1.13s/it]                                                {'loss': 1.4496, 'grad_norm': 2.3370514488609575, 'learning_rate': 1.739463601532567e-06, 'epoch': 0.49}
 16%|█▋        | 44/270 [00:54<04:14,  1.13s/it] 17%|█▋        | 45/270 [00:55<04:07,  1.10s/it]                                                {'loss': 1.4032, 'grad_norm': 2.306174416282441, 'learning_rate': 1.7318007662835247e-06, 'epoch': 0.5}
 17%|█▋        | 45/270 [00:55<04:07,  1.10s/it] 17%|█▋        | 46/270 [00:56<03:55,  1.05s/it]                                                {'loss': 1.4625, 'grad_norm': 2.4432817311520387, 'learning_rate': 1.7241379310344825e-06, 'epoch': 0.51}
 17%|█▋        | 46/270 [00:56<03:55,  1.05s/it] 17%|█▋        | 47/270 [00:57<03:44,  1.01s/it]                                                {'loss': 1.4249, 'grad_norm': 2.609573627026463, 'learning_rate': 1.7164750957854405e-06, 'epoch': 0.52}
 17%|█▋        | 47/270 [00:57<03:44,  1.01s/it] 18%|█▊        | 48/270 [00:58<04:02,  1.09s/it]                                                {'loss': 1.4651, 'grad_norm': 2.527961197740835, 'learning_rate': 1.7088122605363983e-06, 'epoch': 0.53}
 18%|█▊        | 48/270 [00:58<04:02,  1.09s/it] 18%|█▊        | 49/270 [00:59<04:03,  1.10s/it]                                                {'loss': 1.4079, 'grad_norm': 2.4496241775969296, 'learning_rate': 1.701149425287356e-06, 'epoch': 0.54}
 18%|█▊        | 49/270 [00:59<04:03,  1.10s/it] 19%|█▊        | 50/270 [01:00<03:50,  1.05s/it]                                                {'loss': 1.4355, 'grad_norm': 2.6694803171278263, 'learning_rate': 1.693486590038314e-06, 'epoch': 0.56}
 19%|█▊        | 50/270 [01:00<03:50,  1.05s/it] 19%|█▉        | 51/270 [01:01<03:40,  1.01s/it]                                                {'loss': 1.379, 'grad_norm': 2.4705265360217656, 'learning_rate': 1.6858237547892719e-06, 'epoch': 0.57}
 19%|█▉        | 51/270 [01:01<03:40,  1.01s/it] 19%|█▉        | 52/270 [01:02<03:54,  1.08s/it]                                                {'loss': 1.3854, 'grad_norm': 2.414148167920947, 'learning_rate': 1.6781609195402297e-06, 'epoch': 0.58}
 19%|█▉        | 52/270 [01:02<03:54,  1.08s/it] 20%|█▉        | 53/270 [01:03<03:52,  1.07s/it]                                                {'loss': 1.4652, 'grad_norm': 2.5593249400649434, 'learning_rate': 1.6704980842911879e-06, 'epoch': 0.59}
 20%|█▉        | 53/270 [01:03<03:52,  1.07s/it] 20%|██        | 54/270 [01:04<03:46,  1.05s/it]                                                {'loss': 1.3701, 'grad_norm': 2.4285074106578377, 'learning_rate': 1.6628352490421457e-06, 'epoch': 0.6}
 20%|██        | 54/270 [01:04<03:46,  1.05s/it] 20%|██        | 55/270 [01:05<03:39,  1.02s/it]                                                {'loss': 1.4147, 'grad_norm': 2.469023827705143, 'learning_rate': 1.6551724137931035e-06, 'epoch': 0.61}
 20%|██        | 55/270 [01:05<03:39,  1.02s/it] 21%|██        | 56/270 [01:06<03:41,  1.03s/it]                                                {'loss': 1.434, 'grad_norm': 3.2875083102173237, 'learning_rate': 1.6475095785440612e-06, 'epoch': 0.62}
 21%|██        | 56/270 [01:06<03:41,  1.03s/it] 21%|██        | 57/270 [01:07<03:53,  1.10s/it]                                                {'loss': 1.3843, 'grad_norm': 2.4797391274877234, 'learning_rate': 1.6398467432950192e-06, 'epoch': 0.63}
 21%|██        | 57/270 [01:07<03:53,  1.10s/it] 21%|██▏       | 58/270 [01:08<03:39,  1.03s/it]                                                {'loss': 1.3765, 'grad_norm': 2.4986830841698575, 'learning_rate': 1.632183908045977e-06, 'epoch': 0.64}
 21%|██▏       | 58/270 [01:08<03:39,  1.03s/it] 22%|██▏       | 59/270 [01:09<03:30,  1.00it/s]                                                {'loss': 1.4179, 'grad_norm': 2.2030654972643253, 'learning_rate': 1.6245210727969348e-06, 'epoch': 0.66}
 22%|██▏       | 59/270 [01:09<03:30,  1.00it/s] 22%|██▏       | 60/270 [01:10<03:33,  1.02s/it]                                                {'loss': 1.3082, 'grad_norm': 2.455087035695211, 'learning_rate': 1.6168582375478928e-06, 'epoch': 0.67}
 22%|██▏       | 60/270 [01:10<03:33,  1.02s/it] 23%|██▎       | 61/270 [01:12<03:57,  1.14s/it]                                                {'loss': 1.3851, 'grad_norm': 2.3259376619150305, 'learning_rate': 1.6091954022988506e-06, 'epoch': 0.68}
 23%|██▎       | 61/270 [01:12<03:57,  1.14s/it] 23%|██▎       | 62/270 [01:13<03:40,  1.06s/it]                                                {'loss': 1.3299, 'grad_norm': 2.346339842795094, 'learning_rate': 1.6015325670498084e-06, 'epoch': 0.69}
 23%|██▎       | 62/270 [01:13<03:40,  1.06s/it] 23%|██▎       | 63/270 [01:13<03:28,  1.01s/it]                                                {'loss': 1.3937, 'grad_norm': 2.5687929143730286, 'learning_rate': 1.5938697318007662e-06, 'epoch': 0.7}
 23%|██▎       | 63/270 [01:13<03:28,  1.01s/it] 24%|██▎       | 64/270 [01:14<03:22,  1.02it/s]                                                {'loss': 1.4293, 'grad_norm': 2.5277946621648635, 'learning_rate': 1.5862068965517242e-06, 'epoch': 0.71}
 24%|██▎       | 64/270 [01:14<03:22,  1.02it/s] 24%|██▍       | 65/270 [01:16<03:38,  1.06s/it]                                                {'loss': 1.376, 'grad_norm': 2.4478904541842805, 'learning_rate': 1.578544061302682e-06, 'epoch': 0.72}
 24%|██▍       | 65/270 [01:16<03:38,  1.06s/it] 24%|██▍       | 66/270 [01:17<03:32,  1.04s/it]                                                {'loss': 1.3673, 'grad_norm': 2.5699130896734825, 'learning_rate': 1.5708812260536398e-06, 'epoch': 0.73}
 24%|██▍       | 66/270 [01:17<03:32,  1.04s/it] 25%|██▍       | 67/270 [01:18<03:24,  1.01s/it]                                                {'loss': 1.3764, 'grad_norm': 2.5162026455364503, 'learning_rate': 1.5632183908045978e-06, 'epoch': 0.74}
 25%|██▍       | 67/270 [01:18<03:24,  1.01s/it] 25%|██▌       | 68/270 [01:18<03:18,  1.02it/s]                                                {'loss': 1.3629, 'grad_norm': 2.445672923898866, 'learning_rate': 1.5555555555555556e-06, 'epoch': 0.76}
 25%|██▌       | 68/270 [01:18<03:18,  1.02it/s] 26%|██▌       | 69/270 [01:20<03:27,  1.03s/it]                                                {'loss': 1.4477, 'grad_norm': 2.881879810017528, 'learning_rate': 1.5478927203065134e-06, 'epoch': 0.77}
 26%|██▌       | 69/270 [01:20<03:27,  1.03s/it] 26%|██▌       | 70/270 [01:21<03:31,  1.06s/it]                                                {'loss': 1.3463, 'grad_norm': 2.2914817335891664, 'learning_rate': 1.5402298850574712e-06, 'epoch': 0.78}
 26%|██▌       | 70/270 [01:21<03:31,  1.06s/it] 26%|██▋       | 71/270 [01:22<03:23,  1.02s/it]                                                {'loss': 1.3899, 'grad_norm': 2.569047588057268, 'learning_rate': 1.5325670498084292e-06, 'epoch': 0.79}
 26%|██▋       | 71/270 [01:22<03:23,  1.02s/it] 27%|██▋       | 72/270 [01:23<03:13,  1.02it/s]                                                {'loss': 1.4529, 'grad_norm': 2.510800924675087, 'learning_rate': 1.524904214559387e-06, 'epoch': 0.8}
 27%|██▋       | 72/270 [01:23<03:13,  1.02it/s] 27%|██▋       | 73/270 [01:23<03:09,  1.04it/s]                                                {'loss': 1.3812, 'grad_norm': 2.36287808030295, 'learning_rate': 1.5172413793103447e-06, 'epoch': 0.81}
 27%|██▋       | 73/270 [01:23<03:09,  1.04it/s] 27%|██▋       | 74/270 [01:25<03:22,  1.03s/it]                                                {'loss': 1.3368, 'grad_norm': 2.5484430515596643, 'learning_rate': 1.5095785440613027e-06, 'epoch': 0.82}
 27%|██▋       | 74/270 [01:25<03:22,  1.03s/it] 28%|██▊       | 75/270 [01:26<03:14,  1.00it/s]                                                {'loss': 1.4195, 'grad_norm': 2.3419022392682436, 'learning_rate': 1.5019157088122605e-06, 'epoch': 0.83}
 28%|██▊       | 75/270 [01:26<03:14,  1.00it/s] 28%|██▊       | 76/270 [01:27<03:17,  1.02s/it]                                                {'loss': 1.3699, 'grad_norm': 2.4481725252986104, 'learning_rate': 1.4942528735632183e-06, 'epoch': 0.84}
 28%|██▊       | 76/270 [01:27<03:17,  1.02s/it] 29%|██▊       | 77/270 [01:27<03:07,  1.03it/s]                                                {'loss': 1.3648, 'grad_norm': 2.258292788659026, 'learning_rate': 1.4865900383141763e-06, 'epoch': 0.86}
 29%|██▊       | 77/270 [01:27<03:07,  1.03it/s] 29%|██▉       | 78/270 [01:29<03:30,  1.10s/it]                                                {'loss': 1.3699, 'grad_norm': 2.4556784285197075, 'learning_rate': 1.4789272030651341e-06, 'epoch': 0.87}
 29%|██▉       | 78/270 [01:29<03:30,  1.10s/it] 29%|██▉       | 79/270 [01:30<03:18,  1.04s/it]                                                {'loss': 1.4015, 'grad_norm': 2.7440737234367116, 'learning_rate': 1.471264367816092e-06, 'epoch': 0.88}
 29%|██▉       | 79/270 [01:30<03:18,  1.04s/it] 30%|██▉       | 80/270 [01:31<03:11,  1.01s/it]                                                {'loss': 1.4034, 'grad_norm': 2.5577457769550715, 'learning_rate': 1.4636015325670497e-06, 'epoch': 0.89}
 30%|██▉       | 80/270 [01:31<03:11,  1.01s/it] 30%|███       | 81/270 [01:32<03:04,  1.02it/s]                                                {'loss': 1.5335, 'grad_norm': 2.5573426987661665, 'learning_rate': 1.4559386973180077e-06, 'epoch': 0.9}
 30%|███       | 81/270 [01:32<03:04,  1.02it/s] 30%|███       | 82/270 [01:33<03:26,  1.10s/it]                                                {'loss': 1.3448, 'grad_norm': 2.4601565314004104, 'learning_rate': 1.4482758620689655e-06, 'epoch': 0.91}
 30%|███       | 82/270 [01:33<03:26,  1.10s/it] 31%|███       | 83/270 [01:34<03:18,  1.06s/it]                                                {'loss': 1.3657, 'grad_norm': 2.494095712178211, 'learning_rate': 1.4406130268199233e-06, 'epoch': 0.92}
 31%|███       | 83/270 [01:34<03:18,  1.06s/it] 31%|███       | 84/270 [01:35<03:10,  1.02s/it]                                                {'loss': 1.3739, 'grad_norm': 2.5114369924857454, 'learning_rate': 1.4329501915708813e-06, 'epoch': 0.93}
 31%|███       | 84/270 [01:35<03:10,  1.02s/it] 31%|███▏      | 85/270 [01:36<03:04,  1.00it/s]                                                {'loss': 1.3103, 'grad_norm': 2.4367245009550578, 'learning_rate': 1.425287356321839e-06, 'epoch': 0.94}
 31%|███▏      | 85/270 [01:36<03:04,  1.00it/s] 32%|███▏      | 86/270 [01:37<03:10,  1.04s/it]                                                {'loss': 1.4846, 'grad_norm': 2.598158020265072, 'learning_rate': 1.4176245210727969e-06, 'epoch': 0.96}
 32%|███▏      | 86/270 [01:37<03:10,  1.04s/it] 32%|███▏      | 87/270 [01:38<03:31,  1.16s/it]                                                {'loss': 1.4358, 'grad_norm': 2.570387997822008, 'learning_rate': 1.4099616858237547e-06, 'epoch': 0.97}
 32%|███▏      | 87/270 [01:38<03:31,  1.16s/it] 33%|███▎      | 88/270 [01:39<03:22,  1.11s/it]                                                {'loss': 1.3937, 'grad_norm': 2.568233003314015, 'learning_rate': 1.4022988505747127e-06, 'epoch': 0.98}
 33%|███▎      | 88/270 [01:39<03:22,  1.11s/it] 33%|███▎      | 89/270 [01:40<03:10,  1.05s/it]                                                {'loss': 1.3415, 'grad_norm': 2.5196742957953964, 'learning_rate': 1.3946360153256705e-06, 'epoch': 0.99}
 33%|███▎      | 89/270 [01:40<03:10,  1.05s/it] 33%|███▎      | 90/270 [01:41<03:13,  1.08s/it]                                                {'loss': 1.3675, 'grad_norm': 2.374219113372208, 'learning_rate': 1.3869731800766282e-06, 'epoch': 1.0}
 33%|███▎      | 90/270 [01:41<03:13,  1.08s/it] 34%|███▎      | 91/270 [01:43<03:29,  1.17s/it]                                                {'loss': 1.3479, 'grad_norm': 2.4847013597096974, 'learning_rate': 1.3793103448275862e-06, 'epoch': 1.01}
 34%|███▎      | 91/270 [01:43<03:29,  1.17s/it] 34%|███▍      | 92/270 [01:44<03:16,  1.10s/it]                                                {'loss': 1.2867, 'grad_norm': 2.2884725313850303, 'learning_rate': 1.371647509578544e-06, 'epoch': 1.02}
 34%|███▍      | 92/270 [01:44<03:16,  1.10s/it] 34%|███▍      | 93/270 [01:45<03:08,  1.06s/it]                                                {'loss': 1.2496, 'grad_norm': 2.418017446344888, 'learning_rate': 1.3639846743295018e-06, 'epoch': 1.03}
 34%|███▍      | 93/270 [01:45<03:08,  1.06s/it] 35%|███▍      | 94/270 [01:46<02:55,  1.00it/s]                                                {'loss': 1.3003, 'grad_norm': 2.798384204410316, 'learning_rate': 1.3563218390804596e-06, 'epoch': 1.04}
 35%|███▍      | 94/270 [01:46<02:55,  1.00it/s] 35%|███▌      | 95/270 [01:47<03:04,  1.06s/it]                                                {'loss': 1.3132, 'grad_norm': 2.4336643371141995, 'learning_rate': 1.3486590038314176e-06, 'epoch': 1.06}
 35%|███▌      | 95/270 [01:47<03:04,  1.06s/it] 36%|███▌      | 96/270 [01:48<03:03,  1.05s/it]                                                {'loss': 1.2914, 'grad_norm': 2.320996269111617, 'learning_rate': 1.3409961685823754e-06, 'epoch': 1.07}
 36%|███▌      | 96/270 [01:48<03:03,  1.05s/it] 36%|███▌      | 97/270 [01:49<02:57,  1.03s/it]                                                {'loss': 1.3045, 'grad_norm': 2.254215397795707, 'learning_rate': 1.3333333333333332e-06, 'epoch': 1.08}
 36%|███▌      | 97/270 [01:49<02:57,  1.03s/it] 36%|███▋      | 98/270 [01:50<02:48,  1.02it/s]                                                {'loss': 1.311, 'grad_norm': 2.349290568967793, 'learning_rate': 1.3256704980842912e-06, 'epoch': 1.09}
 36%|███▋      | 98/270 [01:50<02:48,  1.02it/s] 37%|███▋      | 99/270 [01:51<03:06,  1.09s/it]                                                {'loss': 1.2052, 'grad_norm': 2.333936765992317, 'learning_rate': 1.318007662835249e-06, 'epoch': 1.1}
 37%|███▋      | 99/270 [01:51<03:06,  1.09s/it] 37%|███▋      | 100/270 [01:52<03:06,  1.10s/it]                                                 {'loss': 1.2398, 'grad_norm': 2.5422202944722514, 'learning_rate': 1.3103448275862068e-06, 'epoch': 1.11}
 37%|███▋      | 100/270 [01:52<03:06,  1.10s/it] 37%|███▋      | 101/270 [01:53<02:59,  1.06s/it]                                                 {'loss': 1.3166, 'grad_norm': 2.4062441352849646, 'learning_rate': 1.3026819923371646e-06, 'epoch': 1.12}
 37%|███▋      | 101/270 [01:53<02:59,  1.06s/it] 38%|███▊      | 102/270 [01:54<02:52,  1.03s/it]                                                 {'loss': 1.3345, 'grad_norm': 2.5474894401546484, 'learning_rate': 1.2950191570881226e-06, 'epoch': 1.13}
 38%|███▊      | 102/270 [01:54<02:52,  1.03s/it] 38%|███▊      | 103/270 [01:55<02:59,  1.08s/it]                                                 {'loss': 1.3329, 'grad_norm': 2.5322329821765686, 'learning_rate': 1.2873563218390804e-06, 'epoch': 1.14}
 38%|███▊      | 103/270 [01:55<02:59,  1.08s/it] 39%|███▊      | 104/270 [01:56<03:00,  1.08s/it]                                                 {'loss': 1.3431, 'grad_norm': 2.4462970353721967, 'learning_rate': 1.2796934865900382e-06, 'epoch': 1.16}
 39%|███▊      | 104/270 [01:56<03:00,  1.08s/it] 39%|███▉      | 105/270 [01:57<02:54,  1.06s/it]                                                 {'loss': 1.2307, 'grad_norm': 2.4751862055338267, 'learning_rate': 1.2720306513409962e-06, 'epoch': 1.17}
 39%|███▉      | 105/270 [01:57<02:54,  1.06s/it] 39%|███▉      | 106/270 [01:58<02:45,  1.01s/it]                                                 {'loss': 1.2884, 'grad_norm': 2.4425771150929965, 'learning_rate': 1.264367816091954e-06, 'epoch': 1.18}
 39%|███▉      | 106/270 [01:58<02:45,  1.01s/it] 40%|███▉      | 107/270 [01:59<02:46,  1.02s/it]                                                 {'loss': 1.3242, 'grad_norm': 2.499372227543908, 'learning_rate': 1.2567049808429117e-06, 'epoch': 1.19}
 40%|███▉      | 107/270 [01:59<02:46,  1.02s/it] 40%|████      | 108/270 [02:00<02:50,  1.05s/it]                                                 {'loss': 1.276, 'grad_norm': 2.7007950863255172, 'learning_rate': 1.2490421455938697e-06, 'epoch': 1.2}
 40%|████      | 108/270 [02:00<02:50,  1.05s/it] 40%|████      | 109/270 [02:01<02:41,  1.00s/it]                                                 {'loss': 1.1849, 'grad_norm': 2.5222659521223867, 'learning_rate': 1.2413793103448275e-06, 'epoch': 1.21}
 40%|████      | 109/270 [02:01<02:41,  1.00s/it] 41%|████      | 110/270 [02:02<02:35,  1.03it/s]                                                 {'loss': 1.3275, 'grad_norm': 2.566505663027439, 'learning_rate': 1.2337164750957853e-06, 'epoch': 1.22}
 41%|████      | 110/270 [02:02<02:35,  1.03it/s] 41%|████      | 111/270 [02:03<02:28,  1.07it/s]                                                 {'loss': 1.3038, 'grad_norm': 2.499439608925108, 'learning_rate': 1.2260536398467431e-06, 'epoch': 1.23}
 41%|████      | 111/270 [02:03<02:28,  1.07it/s] 41%|████▏     | 112/270 [02:05<02:59,  1.14s/it]                                                 {'loss': 1.2569, 'grad_norm': 2.329475301450698, 'learning_rate': 1.2183908045977011e-06, 'epoch': 1.24}
 41%|████▏     | 112/270 [02:05<02:59,  1.14s/it] 42%|████▏     | 113/270 [02:06<02:46,  1.06s/it]                                                 {'loss': 1.3151, 'grad_norm': 2.458407878212052, 'learning_rate': 1.210727969348659e-06, 'epoch': 1.26}
 42%|████▏     | 113/270 [02:06<02:46,  1.06s/it] 42%|████▏     | 114/270 [02:06<02:37,  1.01s/it]                                                 {'loss': 1.2652, 'grad_norm': 2.478964268275959, 'learning_rate': 1.2030651340996167e-06, 'epoch': 1.27}
 42%|████▏     | 114/270 [02:06<02:37,  1.01s/it] 43%|████▎     | 115/270 [02:07<02:30,  1.03it/s]                                                 {'loss': 1.3266, 'grad_norm': 2.5755840410832778, 'learning_rate': 1.1954022988505747e-06, 'epoch': 1.28}
 43%|████▎     | 115/270 [02:07<02:30,  1.03it/s] 43%|████▎     | 116/270 [02:09<03:01,  1.18s/it]                                                 {'loss': 1.2206, 'grad_norm': 2.5653580060301944, 'learning_rate': 1.1877394636015325e-06, 'epoch': 1.29}
 43%|████▎     | 116/270 [02:09<03:01,  1.18s/it] 43%|████▎     | 117/270 [02:10<02:45,  1.08s/it]                                                 {'loss': 1.2664, 'grad_norm': 2.4251927553916723, 'learning_rate': 1.1800766283524903e-06, 'epoch': 1.3}
 43%|████▎     | 117/270 [02:10<02:45,  1.08s/it] 44%|████▎     | 118/270 [02:11<02:36,  1.03s/it]                                                 {'loss': 1.2373, 'grad_norm': 2.505528980621217, 'learning_rate': 1.172413793103448e-06, 'epoch': 1.31}
 44%|████▎     | 118/270 [02:11<02:36,  1.03s/it] 44%|████▍     | 119/270 [02:12<02:29,  1.01it/s]                                                 {'loss': 1.2457, 'grad_norm': 2.6006251191412333, 'learning_rate': 1.164750957854406e-06, 'epoch': 1.32}
 44%|████▍     | 119/270 [02:12<02:29,  1.01it/s] 44%|████▍     | 120/270 [02:13<02:41,  1.08s/it]                                                 {'loss': 1.2956, 'grad_norm': 2.4990712542158886, 'learning_rate': 1.1570881226053639e-06, 'epoch': 1.33}
 44%|████▍     | 120/270 [02:13<02:41,  1.08s/it] 45%|████▍     | 121/270 [02:14<02:33,  1.03s/it]                                                 {'loss': 1.2353, 'grad_norm': 2.6334601733952927, 'learning_rate': 1.1494252873563217e-06, 'epoch': 1.34}
 45%|████▍     | 121/270 [02:14<02:33,  1.03s/it] 45%|████▌     | 122/270 [02:15<02:26,  1.01it/s]                                                 {'loss': 1.2625, 'grad_norm': 2.505899063733689, 'learning_rate': 1.1417624521072797e-06, 'epoch': 1.36}
 45%|████▌     | 122/270 [02:15<02:26,  1.01it/s] 46%|████▌     | 123/270 [02:16<02:21,  1.04it/s]                                                 {'loss': 1.2418, 'grad_norm': 2.558321688751343, 'learning_rate': 1.1340996168582375e-06, 'epoch': 1.37}
 46%|████▌     | 123/270 [02:16<02:21,  1.04it/s] 46%|████▌     | 124/270 [02:17<02:37,  1.08s/it]                                                 {'loss': 1.3071, 'grad_norm': 2.6289460224255357, 'learning_rate': 1.1264367816091952e-06, 'epoch': 1.38}
 46%|████▌     | 124/270 [02:17<02:37,  1.08s/it] 46%|████▋     | 125/270 [02:18<02:41,  1.11s/it]                                                 {'loss': 1.2393, 'grad_norm': 2.4292477674438384, 'learning_rate': 1.118773946360153e-06, 'epoch': 1.39}
 46%|████▋     | 125/270 [02:18<02:41,  1.11s/it] 47%|████▋     | 126/270 [02:19<02:31,  1.05s/it]                                                 {'loss': 1.2489, 'grad_norm': 2.480392255504322, 'learning_rate': 1.111111111111111e-06, 'epoch': 1.4}
 47%|████▋     | 126/270 [02:19<02:31,  1.05s/it] 47%|████▋     | 127/270 [02:20<02:21,  1.01it/s]                                                 {'loss': 1.3038, 'grad_norm': 2.6557444135071573, 'learning_rate': 1.1034482758620688e-06, 'epoch': 1.41}
 47%|████▋     | 127/270 [02:20<02:21,  1.01it/s] 47%|████▋     | 128/270 [02:21<02:28,  1.05s/it]                                                 {'loss': 1.2802, 'grad_norm': 2.595278506898984, 'learning_rate': 1.0957854406130266e-06, 'epoch': 1.42}
 47%|████▋     | 128/270 [02:21<02:28,  1.05s/it] 48%|████▊     | 129/270 [02:22<02:32,  1.08s/it]                                                 {'loss': 1.2767, 'grad_norm': 2.510642950512104, 'learning_rate': 1.0881226053639846e-06, 'epoch': 1.43}
 48%|████▊     | 129/270 [02:22<02:32,  1.08s/it] 48%|████▊     | 130/270 [02:23<02:26,  1.04s/it]                                                 {'loss': 1.226, 'grad_norm': 2.335344907932342, 'learning_rate': 1.0804597701149424e-06, 'epoch': 1.44}
 48%|████▊     | 130/270 [02:23<02:26,  1.04s/it] 49%|████▊     | 131/270 [02:24<02:19,  1.00s/it]                                                 {'loss': 1.2304, 'grad_norm': 2.3891706978651674, 'learning_rate': 1.0727969348659002e-06, 'epoch': 1.46}
 49%|████▊     | 131/270 [02:24<02:19,  1.00s/it] 49%|████▉     | 132/270 [02:25<02:29,  1.08s/it]                                                 {'loss': 1.2805, 'grad_norm': 2.6506497961393864, 'learning_rate': 1.0651340996168582e-06, 'epoch': 1.47}
 49%|████▉     | 132/270 [02:25<02:29,  1.08s/it] 49%|████▉     | 133/270 [02:26<02:24,  1.05s/it]                                                 {'loss': 1.306, 'grad_norm': 2.5171204841066146, 'learning_rate': 1.057471264367816e-06, 'epoch': 1.48}
 49%|████▉     | 133/270 [02:26<02:24,  1.05s/it] 50%|████▉     | 134/270 [02:27<02:16,  1.01s/it]                                                 {'loss': 1.2999, 'grad_norm': 2.4803434629501355, 'learning_rate': 1.0498084291187738e-06, 'epoch': 1.49}
 50%|████▉     | 134/270 [02:27<02:16,  1.01s/it] 50%|█████     | 135/270 [02:28<02:11,  1.03it/s]                                                 {'loss': 1.2916, 'grad_norm': 2.494062873486269, 'learning_rate': 1.0421455938697316e-06, 'epoch': 1.5}
 50%|█████     | 135/270 [02:28<02:11,  1.03it/s] 50%|█████     | 136/270 [02:29<02:09,  1.03it/s]                                                 {'loss': 1.2436, 'grad_norm': 2.4346271651139184, 'learning_rate': 1.0344827586206896e-06, 'epoch': 1.51}
 50%|█████     | 136/270 [02:29<02:09,  1.03it/s] 51%|█████     | 137/270 [02:30<02:15,  1.02s/it]                                                 {'loss': 1.2694, 'grad_norm': 2.4416783225213523, 'learning_rate': 1.0268199233716474e-06, 'epoch': 1.52}
 51%|█████     | 137/270 [02:30<02:15,  1.02s/it] 51%|█████     | 138/270 [02:31<02:10,  1.01it/s]                                                 {'loss': 1.2829, 'grad_norm': 2.4385468290563597, 'learning_rate': 1.0191570881226052e-06, 'epoch': 1.53}
 51%|█████     | 138/270 [02:31<02:10,  1.01it/s] 51%|█████▏    | 139/270 [02:32<02:06,  1.03it/s]                                                 {'loss': 1.3386, 'grad_norm': 2.455714237396726, 'learning_rate': 1.0114942528735634e-06, 'epoch': 1.54}
 51%|█████▏    | 139/270 [02:32<02:06,  1.03it/s] 52%|█████▏    | 140/270 [02:33<02:02,  1.06it/s]                                                 {'loss': 1.2446, 'grad_norm': 2.4293169886157573, 'learning_rate': 1.0038314176245212e-06, 'epoch': 1.56}
 52%|█████▏    | 140/270 [02:33<02:02,  1.06it/s] 52%|█████▏    | 141/270 [02:34<02:20,  1.09s/it]                                                 {'loss': 1.2838, 'grad_norm': 2.665319966438503, 'learning_rate': 9.961685823754787e-07, 'epoch': 1.57}
 52%|█████▏    | 141/270 [02:34<02:20,  1.09s/it] 53%|█████▎    | 142/270 [02:35<02:14,  1.05s/it]                                                 {'loss': 1.2388, 'grad_norm': 2.553525955560642, 'learning_rate': 9.885057471264367e-07, 'epoch': 1.58}
 53%|█████▎    | 142/270 [02:35<02:14,  1.05s/it] 53%|█████▎    | 143/270 [02:36<02:09,  1.02s/it]                                                 {'loss': 1.2925, 'grad_norm': 2.4099324630292687, 'learning_rate': 9.808429118773945e-07, 'epoch': 1.59}
 53%|█████▎    | 143/270 [02:36<02:09,  1.02s/it] 53%|█████▎    | 144/270 [02:37<02:04,  1.01it/s]                                                 {'loss': 1.2417, 'grad_norm': 2.5184047977144965, 'learning_rate': 9.731800766283525e-07, 'epoch': 1.6}
 53%|█████▎    | 144/270 [02:37<02:04,  1.01it/s] 54%|█████▎    | 145/270 [02:39<02:19,  1.12s/it]                                                 {'loss': 1.2841, 'grad_norm': 2.450214992337625, 'learning_rate': 9.655172413793103e-07, 'epoch': 1.61}
 54%|█████▎    | 145/270 [02:39<02:19,  1.12s/it] 54%|█████▍    | 146/270 [02:40<02:11,  1.06s/it]                                                 {'loss': 1.3168, 'grad_norm': 2.6128735299386667, 'learning_rate': 9.578544061302681e-07, 'epoch': 1.62}
 54%|█████▍    | 146/270 [02:40<02:11,  1.06s/it] 54%|█████▍    | 147/270 [02:41<02:10,  1.06s/it]                                                 {'loss': 1.2547, 'grad_norm': 2.6805047968886093, 'learning_rate': 9.50191570881226e-07, 'epoch': 1.63}
 54%|█████▍    | 147/270 [02:41<02:10,  1.06s/it] 55%|█████▍    | 148/270 [02:42<02:03,  1.01s/it]                                                 {'loss': 1.2253, 'grad_norm': 2.6129102383419895, 'learning_rate': 9.425287356321838e-07, 'epoch': 1.64}
 55%|█████▍    | 148/270 [02:42<02:03,  1.01s/it] 55%|█████▌    | 149/270 [02:43<02:17,  1.14s/it]                                                 {'loss': 1.2713, 'grad_norm': 2.733611782892076, 'learning_rate': 9.348659003831417e-07, 'epoch': 1.66}
 55%|█████▌    | 149/270 [02:43<02:17,  1.14s/it] 56%|█████▌    | 150/270 [02:44<02:09,  1.08s/it]                                                 {'loss': 1.2713, 'grad_norm': 2.364592531069079, 'learning_rate': 9.272030651340996e-07, 'epoch': 1.67}
 56%|█████▌    | 150/270 [02:44<02:09,  1.08s/it] 56%|█████▌    | 151/270 [02:45<01:59,  1.01s/it]                                                 {'loss': 1.2247, 'grad_norm': 2.9103913737524167, 'learning_rate': 9.195402298850574e-07, 'epoch': 1.68}
 56%|█████▌    | 151/270 [02:45<01:59,  1.01s/it] 56%|█████▋    | 152/270 [02:46<01:56,  1.02it/s]                                                 {'loss': 1.2624, 'grad_norm': 2.515538676656284, 'learning_rate': 9.118773946360153e-07, 'epoch': 1.69}
 56%|█████▋    | 152/270 [02:46<01:56,  1.02it/s] 57%|█████▋    | 153/270 [02:47<02:04,  1.07s/it]                                                 {'loss': 1.2786, 'grad_norm': 2.543796166751141, 'learning_rate': 9.042145593869731e-07, 'epoch': 1.7}
 57%|█████▋    | 153/270 [02:47<02:04,  1.07s/it] 57%|█████▋    | 154/270 [02:48<02:07,  1.10s/it]                                                 {'loss': 1.2695, 'grad_norm': 2.4926840812414173, 'learning_rate': 8.96551724137931e-07, 'epoch': 1.71}
 57%|█████▋    | 154/270 [02:48<02:07,  1.10s/it] 57%|█████▋    | 155/270 [02:49<02:02,  1.06s/it]                                                 {'loss': 1.2469, 'grad_norm': 2.542922215969186, 'learning_rate': 8.888888888888888e-07, 'epoch': 1.72}
 57%|█████▋    | 155/270 [02:49<02:02,  1.06s/it] 58%|█████▊    | 156/270 [02:50<01:57,  1.03s/it]                                                 {'loss': 1.178, 'grad_norm': 2.303214743982984, 'learning_rate': 8.812260536398467e-07, 'epoch': 1.73}
 58%|█████▊    | 156/270 [02:50<01:57,  1.03s/it] 58%|█████▊    | 157/270 [02:51<01:57,  1.04s/it]                                                 {'loss': 1.2389, 'grad_norm': 2.643963425206785, 'learning_rate': 8.735632183908046e-07, 'epoch': 1.74}
 58%|█████▊    | 157/270 [02:51<01:57,  1.04s/it] 59%|█████▊    | 158/270 [02:52<02:02,  1.09s/it]                                                 {'loss': 1.3341, 'grad_norm': 2.59414860261595, 'learning_rate': 8.659003831417624e-07, 'epoch': 1.76}
 59%|█████▊    | 158/270 [02:52<02:02,  1.09s/it] 59%|█████▉    | 159/270 [02:53<01:59,  1.07s/it]                                                 {'loss': 1.2327, 'grad_norm': 2.4882296437874944, 'learning_rate': 8.582375478927202e-07, 'epoch': 1.77}
 59%|█████▉    | 159/270 [02:53<01:59,  1.07s/it] 59%|█████▉    | 160/270 [02:54<01:52,  1.02s/it]                                                 {'loss': 1.3135, 'grad_norm': 2.468046937144121, 'learning_rate': 8.50574712643678e-07, 'epoch': 1.78}
 59%|█████▉    | 160/270 [02:54<01:52,  1.02s/it] 60%|█████▉    | 161/270 [02:56<02:00,  1.10s/it]                                                 {'loss': 1.2282, 'grad_norm': 2.4251101687554444, 'learning_rate': 8.429118773946359e-07, 'epoch': 1.79}
 60%|█████▉    | 161/270 [02:56<02:00,  1.10s/it] 60%|██████    | 162/270 [02:57<01:59,  1.10s/it]                                                 {'loss': 1.2507, 'grad_norm': 2.459518312128514, 'learning_rate': 8.352490421455939e-07, 'epoch': 1.8}
 60%|██████    | 162/270 [02:57<01:59,  1.10s/it] 60%|██████    | 163/270 [02:58<01:52,  1.05s/it]                                                 {'loss': 1.3181, 'grad_norm': 2.3772550548750293, 'learning_rate': 8.275862068965517e-07, 'epoch': 1.81}
 60%|██████    | 163/270 [02:58<01:52,  1.05s/it] 61%|██████    | 164/270 [02:59<01:46,  1.00s/it]                                                 {'loss': 1.2518, 'grad_norm': 2.510984853967346, 'learning_rate': 8.199233716475096e-07, 'epoch': 1.82}
 61%|██████    | 164/270 [02:59<01:46,  1.00s/it] 61%|██████    | 165/270 [03:00<01:58,  1.12s/it]                                                 {'loss': 1.2436, 'grad_norm': 2.3209053632240884, 'learning_rate': 8.122605363984674e-07, 'epoch': 1.83}
 61%|██████    | 165/270 [03:00<01:58,  1.12s/it] 61%|██████▏   | 166/270 [03:01<01:52,  1.08s/it]                                                 {'loss': 1.2201, 'grad_norm': 2.6646118221729647, 'learning_rate': 8.045977011494253e-07, 'epoch': 1.84}
 61%|██████▏   | 166/270 [03:01<01:52,  1.08s/it] 62%|██████▏   | 167/270 [03:02<01:49,  1.06s/it]                                                 {'loss': 1.2146, 'grad_norm': 2.357198767127536, 'learning_rate': 7.969348659003831e-07, 'epoch': 1.86}
 62%|██████▏   | 167/270 [03:02<01:49,  1.06s/it] 62%|██████▏   | 168/270 [03:03<01:42,  1.00s/it]                                                 {'loss': 1.2569, 'grad_norm': 2.502855804593657, 'learning_rate': 7.89272030651341e-07, 'epoch': 1.87}
 62%|██████▏   | 168/270 [03:03<01:42,  1.00s/it] 63%|██████▎   | 169/270 [03:04<01:44,  1.03s/it]                                                 {'loss': 1.3042, 'grad_norm': 2.5989398990257744, 'learning_rate': 7.816091954022989e-07, 'epoch': 1.88}
 63%|██████▎   | 169/270 [03:04<01:44,  1.03s/it] 63%|██████▎   | 170/270 [03:05<01:43,  1.04s/it]                                                 {'loss': 1.2999, 'grad_norm': 2.4900556531022215, 'learning_rate': 7.739463601532567e-07, 'epoch': 1.89}
 63%|██████▎   | 170/270 [03:05<01:43,  1.04s/it] 63%|██████▎   | 171/270 [03:06<01:39,  1.00s/it]                                                 {'loss': 1.2119, 'grad_norm': 2.5864873286280927, 'learning_rate': 7.662835249042146e-07, 'epoch': 1.9}
 63%|██████▎   | 171/270 [03:06<01:39,  1.00s/it] 64%|██████▎   | 172/270 [03:07<01:35,  1.03it/s]                                                 {'loss': 1.2257, 'grad_norm': 2.6118471475098444, 'learning_rate': 7.586206896551724e-07, 'epoch': 1.91}
 64%|██████▎   | 172/270 [03:07<01:35,  1.03it/s] 64%|██████▍   | 173/270 [03:08<01:47,  1.11s/it]                                                 {'loss': 1.2698, 'grad_norm': 2.710363938285183, 'learning_rate': 7.509578544061303e-07, 'epoch': 1.92}
 64%|██████▍   | 173/270 [03:08<01:47,  1.11s/it] 64%|██████▍   | 174/270 [03:09<01:45,  1.10s/it]                                                 {'loss': 1.2694, 'grad_norm': 2.5473269646206993, 'learning_rate': 7.432950191570882e-07, 'epoch': 1.93}
 64%|██████▍   | 174/270 [03:09<01:45,  1.10s/it] 65%|██████▍   | 175/270 [03:10<01:40,  1.05s/it]                                                 {'loss': 1.2541, 'grad_norm': 2.5918635627625135, 'learning_rate': 7.35632183908046e-07, 'epoch': 1.94}
 65%|██████▍   | 175/270 [03:10<01:40,  1.05s/it] 65%|██████▌   | 176/270 [03:11<01:35,  1.01s/it]                                                 {'loss': 1.2743, 'grad_norm': 2.520293109649324, 'learning_rate': 7.279693486590039e-07, 'epoch': 1.96}
 65%|██████▌   | 176/270 [03:11<01:35,  1.01s/it] 66%|██████▌   | 177/270 [03:12<01:42,  1.10s/it]                                                 {'loss': 1.3091, 'grad_norm': 2.5343804261575924, 'learning_rate': 7.203065134099616e-07, 'epoch': 1.97}
 66%|██████▌   | 177/270 [03:12<01:42,  1.10s/it] 66%|██████▌   | 178/270 [03:14<01:42,  1.11s/it]                                                 {'loss': 1.202, 'grad_norm': 2.526469123207233, 'learning_rate': 7.126436781609195e-07, 'epoch': 1.98}
 66%|██████▌   | 178/270 [03:14<01:42,  1.11s/it] 66%|██████▋   | 179/270 [03:14<01:35,  1.05s/it]                                                 {'loss': 1.2789, 'grad_norm': 2.864699560832335, 'learning_rate': 7.049808429118773e-07, 'epoch': 1.99}
 66%|██████▋   | 179/270 [03:14<01:35,  1.05s/it] 67%|██████▋   | 180/270 [03:15<01:31,  1.02s/it]                                                 {'loss': 1.292, 'grad_norm': 2.5016142037207754, 'learning_rate': 6.973180076628352e-07, 'epoch': 2.0}
 67%|██████▋   | 180/270 [03:15<01:31,  1.02s/it] 67%|██████▋   | 181/270 [03:17<01:32,  1.04s/it]                                                 {'loss': 1.2114, 'grad_norm': 2.462372475900435, 'learning_rate': 6.896551724137931e-07, 'epoch': 2.01}
 67%|██████▋   | 181/270 [03:17<01:32,  1.04s/it] 67%|██████▋   | 182/270 [03:18<01:33,  1.06s/it]                                                 {'loss': 1.1658, 'grad_norm': 2.5566879779056, 'learning_rate': 6.819923371647509e-07, 'epoch': 2.02}
 67%|██████▋   | 182/270 [03:18<01:33,  1.06s/it] 68%|██████▊   | 183/270 [03:19<01:30,  1.04s/it]                                                 {'loss': 1.131, 'grad_norm': 2.5130319311837757, 'learning_rate': 6.743295019157088e-07, 'epoch': 2.03}
 68%|██████▊   | 183/270 [03:19<01:30,  1.04s/it] 68%|██████▊   | 184/270 [03:20<01:25,  1.00it/s]                                                 {'loss': 1.2367, 'grad_norm': 2.605373264662125, 'learning_rate': 6.666666666666666e-07, 'epoch': 2.04}
 68%|██████▊   | 184/270 [03:20<01:25,  1.00it/s] 69%|██████▊   | 185/270 [03:20<01:21,  1.04it/s]                                                 {'loss': 1.1408, 'grad_norm': 2.4514153218689616, 'learning_rate': 6.590038314176245e-07, 'epoch': 2.06}
 69%|██████▊   | 185/270 [03:20<01:21,  1.04it/s] 69%|██████▉   | 186/270 [03:22<01:27,  1.05s/it]                                                 {'loss': 1.2878, 'grad_norm': 2.444621820080907, 'learning_rate': 6.513409961685823e-07, 'epoch': 2.07}
 69%|██████▉   | 186/270 [03:22<01:27,  1.05s/it] 69%|██████▉   | 187/270 [03:23<01:24,  1.01s/it]                                                 {'loss': 1.1942, 'grad_norm': 2.3315160467081846, 'learning_rate': 6.436781609195402e-07, 'epoch': 2.08}
 69%|██████▉   | 187/270 [03:23<01:24,  1.01s/it] 70%|██████▉   | 188/270 [03:23<01:20,  1.02it/s]                                                 {'loss': 1.1733, 'grad_norm': 2.5246787430321014, 'learning_rate': 6.360153256704981e-07, 'epoch': 2.09}
 70%|██████▉   | 188/270 [03:23<01:20,  1.02it/s] 70%|███████   | 189/270 [03:24<01:16,  1.05it/s]                                                 {'loss': 1.1802, 'grad_norm': 2.652927135012973, 'learning_rate': 6.283524904214559e-07, 'epoch': 2.1}
 70%|███████   | 189/270 [03:24<01:16,  1.05it/s] 70%|███████   | 190/270 [03:26<01:25,  1.07s/it]                                                 {'loss': 1.2425, 'grad_norm': 2.4731397205969254, 'learning_rate': 6.206896551724138e-07, 'epoch': 2.11}
 70%|███████   | 190/270 [03:26<01:25,  1.07s/it] 71%|███████   | 191/270 [03:27<01:27,  1.11s/it]                                                 {'loss': 1.2433, 'grad_norm': 2.5102327496389867, 'learning_rate': 6.130268199233716e-07, 'epoch': 2.12}
 71%|███████   | 191/270 [03:27<01:27,  1.11s/it] 71%|███████   | 192/270 [03:28<01:23,  1.07s/it]                                                 {'loss': 1.257, 'grad_norm': 2.543806552962651, 'learning_rate': 6.053639846743295e-07, 'epoch': 2.13}
 71%|███████   | 192/270 [03:28<01:23,  1.07s/it] 71%|███████▏  | 193/270 [03:29<01:18,  1.02s/it]                                                 {'loss': 1.1821, 'grad_norm': 2.6011431488290437, 'learning_rate': 5.977011494252874e-07, 'epoch': 2.14}
 71%|███████▏  | 193/270 [03:29<01:18,  1.02s/it] 72%|███████▏  | 194/270 [03:30<01:21,  1.07s/it]                                                 {'loss': 1.1605, 'grad_norm': 2.40434352710687, 'learning_rate': 5.900383141762451e-07, 'epoch': 2.16}
 72%|███████▏  | 194/270 [03:30<01:21,  1.07s/it] 72%|███████▏  | 195/270 [03:31<01:19,  1.05s/it]                                                 {'loss': 1.164, 'grad_norm': 2.809104968807954, 'learning_rate': 5.82375478927203e-07, 'epoch': 2.17}
 72%|███████▏  | 195/270 [03:31<01:19,  1.05s/it] 73%|███████▎  | 196/270 [03:32<01:15,  1.02s/it]                                                 {'loss': 1.1983, 'grad_norm': 2.631087624685651, 'learning_rate': 5.747126436781608e-07, 'epoch': 2.18}
 73%|███████▎  | 196/270 [03:32<01:15,  1.02s/it] 73%|███████▎  | 197/270 [03:33<01:13,  1.00s/it]                                                 {'loss': 1.2151, 'grad_norm': 2.5371893227050166, 'learning_rate': 5.670498084291187e-07, 'epoch': 2.19}
 73%|███████▎  | 197/270 [03:33<01:13,  1.00s/it] 73%|███████▎  | 198/270 [03:34<01:11,  1.00it/s]                                                 {'loss': 1.2057, 'grad_norm': 2.608429622463191, 'learning_rate': 5.593869731800765e-07, 'epoch': 2.2}
 73%|███████▎  | 198/270 [03:34<01:11,  1.00it/s] 74%|███████▎  | 199/270 [03:35<01:21,  1.15s/it]                                                 {'loss': 1.1164, 'grad_norm': 2.529816759147702, 'learning_rate': 5.517241379310344e-07, 'epoch': 2.21}
 74%|███████▎  | 199/270 [03:35<01:21,  1.15s/it] 74%|███████▍  | 200/270 [03:36<01:16,  1.09s/it]                                                 {'loss': 1.187, 'grad_norm': 2.5146582050027835, 'learning_rate': 5.440613026819923e-07, 'epoch': 2.22}
 74%|███████▍  | 200/270 [03:36<01:16,  1.09s/it] 74%|███████▍  | 201/270 [03:37<01:10,  1.01s/it]                                                 {'loss': 1.1607, 'grad_norm': 2.7054311937905053, 'learning_rate': 5.363984674329501e-07, 'epoch': 2.23}
 74%|███████▍  | 201/270 [03:37<01:10,  1.01s/it] 75%|███████▍  | 202/270 [03:38<01:05,  1.04it/s]                                                 {'loss': 1.1831, 'grad_norm': 2.744418960769212, 'learning_rate': 5.28735632183908e-07, 'epoch': 2.24}
 75%|███████▍  | 202/270 [03:38<01:05,  1.04it/s] 75%|███████▌  | 203/270 [03:39<01:11,  1.06s/it]                                                 {'loss': 1.2473, 'grad_norm': 2.687427510474699, 'learning_rate': 5.210727969348658e-07, 'epoch': 2.26}
 75%|███████▌  | 203/270 [03:39<01:11,  1.06s/it] 76%|███████▌  | 204/270 [03:40<01:08,  1.04s/it]                                                 {'loss': 1.1871, 'grad_norm': 2.6655855529654913, 'learning_rate': 5.134099616858237e-07, 'epoch': 2.27}
 76%|███████▌  | 204/270 [03:40<01:08,  1.04s/it] 76%|███████▌  | 205/270 [03:41<01:06,  1.02s/it]                                                 {'loss': 1.2023, 'grad_norm': 2.4745897271430928, 'learning_rate': 5.057471264367817e-07, 'epoch': 2.28}
 76%|███████▌  | 205/270 [03:41<01:06,  1.02s/it] 76%|███████▋  | 206/270 [03:42<01:02,  1.02it/s]                                                 {'loss': 1.2076, 'grad_norm': 2.6335887008388323, 'learning_rate': 4.980842911877394e-07, 'epoch': 2.29}
 76%|███████▋  | 206/270 [03:42<01:02,  1.02it/s] 77%|███████▋  | 207/270 [03:43<01:08,  1.09s/it]                                                 {'loss': 1.0841, 'grad_norm': 2.5212762175008163, 'learning_rate': 4.904214559386973e-07, 'epoch': 2.3}
 77%|███████▋  | 207/270 [03:43<01:08,  1.09s/it] 77%|███████▋  | 208/270 [03:44<01:03,  1.03s/it]                                                 {'loss': 1.1975, 'grad_norm': 2.5587403210011552, 'learning_rate': 4.827586206896552e-07, 'epoch': 2.31}
 77%|███████▋  | 208/270 [03:44<01:03,  1.03s/it] 77%|███████▋  | 209/270 [03:45<01:04,  1.05s/it]                                                 {'loss': 1.2251, 'grad_norm': 2.5600070749584662, 'learning_rate': 4.75095785440613e-07, 'epoch': 2.32}
 77%|███████▋  | 209/270 [03:45<01:04,  1.05s/it] 78%|███████▊  | 210/270 [03:46<00:58,  1.02it/s]                                                 {'loss': 1.1972, 'grad_norm': 2.888805760632505, 'learning_rate': 4.6743295019157085e-07, 'epoch': 2.33}
 78%|███████▊  | 210/270 [03:46<00:58,  1.02it/s] 78%|███████▊  | 211/270 [03:48<01:08,  1.16s/it]                                                 {'loss': 1.1993, 'grad_norm': 2.832134519685119, 'learning_rate': 4.597701149425287e-07, 'epoch': 2.34}
 78%|███████▊  | 211/270 [03:48<01:08,  1.16s/it] 79%|███████▊  | 212/270 [03:49<01:03,  1.09s/it]                                                 {'loss': 1.1542, 'grad_norm': 2.426562742058939, 'learning_rate': 4.5210727969348654e-07, 'epoch': 2.36}
 79%|███████▊  | 212/270 [03:49<01:03,  1.09s/it] 79%|███████▉  | 213/270 [03:50<00:59,  1.04s/it]                                                 {'loss': 1.1852, 'grad_norm': 2.6709598880995324, 'learning_rate': 4.444444444444444e-07, 'epoch': 2.37}
 79%|███████▉  | 213/270 [03:50<00:59,  1.04s/it] 79%|███████▉  | 214/270 [03:51<00:55,  1.01it/s]                                                 {'loss': 1.1587, 'grad_norm': 2.5632010362073605, 'learning_rate': 4.367816091954023e-07, 'epoch': 2.38}
 79%|███████▉  | 214/270 [03:51<00:55,  1.01it/s] 80%|███████▉  | 215/270 [03:52<01:05,  1.20s/it]                                                 {'loss': 1.1981, 'grad_norm': 2.532599582316627, 'learning_rate': 4.291187739463601e-07, 'epoch': 2.39}
 80%|███████▉  | 215/270 [03:52<01:05,  1.20s/it] 80%|████████  | 216/270 [03:53<00:59,  1.11s/it]                                                 {'loss': 1.2086, 'grad_norm': 2.539597479074178, 'learning_rate': 4.2145593869731797e-07, 'epoch': 2.4}
 80%|████████  | 216/270 [03:53<00:59,  1.11s/it] 80%|████████  | 217/270 [03:54<00:55,  1.04s/it]                                                 {'loss': 1.1845, 'grad_norm': 2.5976913114270657, 'learning_rate': 4.1379310344827586e-07, 'epoch': 2.41}
 80%|████████  | 217/270 [03:54<00:55,  1.04s/it] 81%|████████  | 218/270 [03:55<00:51,  1.00it/s]                                                 {'loss': 1.1208, 'grad_norm': 2.676113506365565, 'learning_rate': 4.061302681992337e-07, 'epoch': 2.42}
 81%|████████  | 218/270 [03:55<00:51,  1.00it/s] 81%|████████  | 219/270 [03:56<00:51,  1.00s/it]                                                 {'loss': 1.21, 'grad_norm': 2.733600283606448, 'learning_rate': 3.9846743295019155e-07, 'epoch': 2.43}
 81%|████████  | 219/270 [03:56<00:51,  1.00s/it] 81%|████████▏ | 220/270 [03:57<00:51,  1.03s/it]                                                 {'loss': 1.1915, 'grad_norm': 2.5199954131508555, 'learning_rate': 3.9080459770114945e-07, 'epoch': 2.44}
 81%|████████▏ | 220/270 [03:57<00:51,  1.03s/it] 82%|████████▏ | 221/270 [03:58<00:48,  1.01it/s]                                                 {'loss': 1.1323, 'grad_norm': 2.512829504128284, 'learning_rate': 3.831417624521073e-07, 'epoch': 2.46}
 82%|████████▏ | 221/270 [03:58<00:48,  1.01it/s] 82%|████████▏ | 222/270 [03:59<00:45,  1.07it/s]                                                 {'loss': 1.1747, 'grad_norm': 2.708297652534264, 'learning_rate': 3.7547892720306513e-07, 'epoch': 2.47}
 82%|████████▏ | 222/270 [03:59<00:45,  1.07it/s] 83%|████████▎ | 223/270 [04:00<00:52,  1.12s/it]                                                 {'loss': 1.1888, 'grad_norm': 2.7424199941272214, 'learning_rate': 3.67816091954023e-07, 'epoch': 2.48}
 83%|████████▎ | 223/270 [04:00<00:52,  1.12s/it] 83%|████████▎ | 224/270 [04:01<00:50,  1.11s/it]                                                 {'loss': 1.1866, 'grad_norm': 2.3853318467070346, 'learning_rate': 3.601532567049808e-07, 'epoch': 2.49}
 83%|████████▎ | 224/270 [04:01<00:50,  1.11s/it] 83%|████████▎ | 225/270 [04:02<00:48,  1.07s/it]                                                 {'loss': 1.2216, 'grad_norm': 2.5710741702739806, 'learning_rate': 3.5249042145593867e-07, 'epoch': 2.5}
 83%|████████▎ | 225/270 [04:02<00:48,  1.07s/it] 84%|████████▎ | 226/270 [04:03<00:44,  1.01s/it]                                                 {'loss': 1.1953, 'grad_norm': 2.6187890924985253, 'learning_rate': 3.4482758620689656e-07, 'epoch': 2.51}
 84%|████████▎ | 226/270 [04:03<00:44,  1.01s/it] 84%|████████▍ | 227/270 [04:05<00:50,  1.17s/it]                                                 {'loss': 1.1666, 'grad_norm': 2.5540667292557178, 'learning_rate': 3.371647509578544e-07, 'epoch': 2.52}
 84%|████████▍ | 227/270 [04:05<00:50,  1.17s/it] 84%|████████▍ | 228/270 [04:06<00:45,  1.09s/it]                                                 {'loss': 1.1285, 'grad_norm': 2.434397054298534, 'learning_rate': 3.2950191570881225e-07, 'epoch': 2.53}
 84%|████████▍ | 228/270 [04:06<00:45,  1.09s/it] 85%|████████▍ | 229/270 [04:07<00:42,  1.04s/it]                                                 {'loss': 1.1577, 'grad_norm': 2.4884690228705524, 'learning_rate': 3.218390804597701e-07, 'epoch': 2.54}
 85%|████████▍ | 229/270 [04:07<00:42,  1.04s/it] 85%|████████▌ | 230/270 [04:08<00:39,  1.00it/s]                                                 {'loss': 1.2019, 'grad_norm': 2.6513806474205226, 'learning_rate': 3.1417624521072794e-07, 'epoch': 2.56}
 85%|████████▌ | 230/270 [04:08<00:39,  1.00it/s] 86%|████████▌ | 231/270 [04:09<00:46,  1.18s/it]                                                 {'loss': 1.2013, 'grad_norm': 2.620035452731562, 'learning_rate': 3.065134099616858e-07, 'epoch': 2.57}
 86%|████████▌ | 231/270 [04:09<00:46,  1.18s/it] 86%|████████▌ | 232/270 [04:10<00:41,  1.09s/it]                                                 {'loss': 1.168, 'grad_norm': 2.6711547868782457, 'learning_rate': 2.988505747126437e-07, 'epoch': 2.58}
 86%|████████▌ | 232/270 [04:10<00:41,  1.09s/it] 86%|████████▋ | 233/270 [04:11<00:38,  1.03s/it]                                                 {'loss': 1.1413, 'grad_norm': 2.571413224832656, 'learning_rate': 2.911877394636015e-07, 'epoch': 2.59}
 86%|████████▋ | 233/270 [04:11<00:38,  1.03s/it] 87%|████████▋ | 234/270 [04:12<00:35,  1.01it/s]                                                 {'loss': 1.2165, 'grad_norm': 2.725262774242607, 'learning_rate': 2.8352490421455936e-07, 'epoch': 2.6}
 87%|████████▋ | 234/270 [04:12<00:35,  1.01it/s] 87%|████████▋ | 235/270 [04:14<00:42,  1.21s/it]                                                 {'loss': 1.1455, 'grad_norm': 2.529223501271989, 'learning_rate': 2.758620689655172e-07, 'epoch': 2.61}
 87%|████████▋ | 235/270 [04:14<00:42,  1.21s/it] 87%|████████▋ | 236/270 [04:14<00:37,  1.11s/it]                                                 {'loss': 1.2025, 'grad_norm': 2.48286153009333, 'learning_rate': 2.6819923371647505e-07, 'epoch': 2.62}
 87%|████████▋ | 236/270 [04:14<00:37,  1.11s/it] 88%|████████▊ | 237/270 [04:15<00:34,  1.05s/it]                                                 {'loss': 1.1439, 'grad_norm': 2.5748633295282906, 'learning_rate': 2.605363984674329e-07, 'epoch': 2.63}
 88%|████████▊ | 237/270 [04:15<00:34,  1.05s/it] 88%|████████▊ | 238/270 [04:16<00:31,  1.00it/s]                                                 {'loss': 1.1612, 'grad_norm': 2.6613201340029073, 'learning_rate': 2.5287356321839084e-07, 'epoch': 2.64}
 88%|████████▊ | 238/270 [04:16<00:31,  1.00it/s] 89%|████████▊ | 239/270 [04:17<00:32,  1.05s/it]                                                 {'loss': 1.1761, 'grad_norm': 2.479144622066204, 'learning_rate': 2.4521072796934863e-07, 'epoch': 2.66}
 89%|████████▊ | 239/270 [04:17<00:32,  1.05s/it] 89%|████████▉ | 240/270 [04:18<00:31,  1.05s/it]                                                 {'loss': 1.1732, 'grad_norm': 2.4148607625957323, 'learning_rate': 2.375478927203065e-07, 'epoch': 2.67}
 89%|████████▉ | 240/270 [04:18<00:31,  1.05s/it] 89%|████████▉ | 241/270 [04:19<00:30,  1.04s/it]                                                 {'loss': 1.1351, 'grad_norm': 2.5247310976562995, 'learning_rate': 2.2988505747126435e-07, 'epoch': 2.68}
 89%|████████▉ | 241/270 [04:19<00:30,  1.04s/it] 90%|████████▉ | 242/270 [04:20<00:27,  1.01it/s]                                                 {'loss': 1.217, 'grad_norm': 2.479083279438681, 'learning_rate': 2.222222222222222e-07, 'epoch': 2.69}
 90%|████████▉ | 242/270 [04:20<00:27,  1.01it/s] 90%|█████████ | 243/270 [04:22<00:32,  1.19s/it]                                                 {'loss': 1.1165, 'grad_norm': 2.616924207412492, 'learning_rate': 2.1455938697318006e-07, 'epoch': 2.7}
 90%|█████████ | 243/270 [04:22<00:32,  1.19s/it] 90%|█████████ | 244/270 [04:23<00:28,  1.11s/it]                                                 {'loss': 1.1365, 'grad_norm': 2.420563457667625, 'learning_rate': 2.0689655172413793e-07, 'epoch': 2.71}
 90%|█████████ | 244/270 [04:23<00:28,  1.11s/it] 91%|█████████ | 245/270 [04:24<00:25,  1.04s/it]                                                 {'loss': 1.1244, 'grad_norm': 2.708566700831198, 'learning_rate': 1.9923371647509578e-07, 'epoch': 2.72}
 91%|█████████ | 245/270 [04:24<00:25,  1.04s/it] 91%|█████████ | 246/270 [04:25<00:23,  1.01it/s]                                                 {'loss': 1.1675, 'grad_norm': 2.6360260077516893, 'learning_rate': 1.9157088122605365e-07, 'epoch': 2.73}
 91%|█████████ | 246/270 [04:25<00:23,  1.01it/s] 91%|█████████▏| 247/270 [04:26<00:24,  1.08s/it]                                                 {'loss': 1.1468, 'grad_norm': 2.525772572573886, 'learning_rate': 1.839080459770115e-07, 'epoch': 2.74}
 91%|█████████▏| 247/270 [04:26<00:24,  1.08s/it] 92%|█████████▏| 248/270 [04:27<00:23,  1.06s/it]                                                 {'loss': 1.1339, 'grad_norm': 2.747046094685963, 'learning_rate': 1.7624521072796933e-07, 'epoch': 2.76}
 92%|█████████▏| 248/270 [04:27<00:23,  1.06s/it] 92%|█████████▏| 249/270 [04:28<00:22,  1.06s/it]                                                 {'loss': 1.1925, 'grad_norm': 2.5984515406141018, 'learning_rate': 1.685823754789272e-07, 'epoch': 2.77}
 92%|█████████▏| 249/270 [04:28<00:22,  1.06s/it] 93%|█████████▎| 250/270 [04:29<00:20,  1.02s/it]                                                 {'loss': 1.1612, 'grad_norm': 2.5970066903930333, 'learning_rate': 1.6091954022988505e-07, 'epoch': 2.78}
 93%|█████████▎| 250/270 [04:29<00:20,  1.02s/it] 93%|█████████▎| 251/270 [04:30<00:22,  1.19s/it]                                                 {'loss': 1.1516, 'grad_norm': 2.620597728520328, 'learning_rate': 1.532567049808429e-07, 'epoch': 2.79}
 93%|█████████▎| 251/270 [04:30<00:22,  1.19s/it] 93%|█████████▎| 252/270 [04:31<00:19,  1.11s/it]                                                 {'loss': 1.22, 'grad_norm': 2.583038093339439, 'learning_rate': 1.4559386973180076e-07, 'epoch': 2.8}
 93%|█████████▎| 252/270 [04:31<00:19,  1.11s/it] 94%|█████████▎| 253/270 [04:32<00:17,  1.04s/it]                                                 {'loss': 1.1678, 'grad_norm': 2.7137686680218236, 'learning_rate': 1.379310344827586e-07, 'epoch': 2.81}
 94%|█████████▎| 253/270 [04:32<00:17,  1.04s/it] 94%|█████████▍| 254/270 [04:33<00:15,  1.01it/s]                                                 {'loss': 1.1323, 'grad_norm': 2.735468421726129, 'learning_rate': 1.3026819923371645e-07, 'epoch': 2.82}
 94%|█████████▍| 254/270 [04:33<00:15,  1.01it/s] 94%|█████████▍| 255/270 [04:35<00:16,  1.13s/it]                                                 {'loss': 1.217, 'grad_norm': 2.513232919642744, 'learning_rate': 1.2260536398467432e-07, 'epoch': 2.83}
 94%|█████████▍| 255/270 [04:35<00:16,  1.13s/it] 95%|█████████▍| 256/270 [04:35<00:14,  1.05s/it]                                                 {'loss': 1.2062, 'grad_norm': 2.498287109854708, 'learning_rate': 1.1494252873563217e-07, 'epoch': 2.84}
 95%|█████████▍| 256/270 [04:35<00:14,  1.05s/it] 95%|█████████▌| 257/270 [04:37<00:13,  1.06s/it]                                                 {'loss': 1.1497, 'grad_norm': 2.4702592288726715, 'learning_rate': 1.0727969348659003e-07, 'epoch': 2.86}
 95%|█████████▌| 257/270 [04:37<00:13,  1.06s/it] 96%|█████████▌| 258/270 [04:37<00:12,  1.00s/it]                                                 {'loss': 1.2015, 'grad_norm': 2.4980940807429484, 'learning_rate': 9.961685823754789e-08, 'epoch': 2.87}
 96%|█████████▌| 258/270 [04:37<00:12,  1.00s/it] 96%|█████████▌| 259/270 [04:39<00:13,  1.23s/it]                                                 {'loss': 1.1727, 'grad_norm': 2.6638675080288197, 'learning_rate': 9.195402298850574e-08, 'epoch': 2.88}
 96%|█████████▌| 259/270 [04:39<00:13,  1.23s/it] 96%|█████████▋| 260/270 [04:40<00:11,  1.13s/it]                                                 {'loss': 1.1736, 'grad_norm': 2.6222566442076425, 'learning_rate': 8.42911877394636e-08, 'epoch': 2.89}
 96%|█████████▋| 260/270 [04:40<00:11,  1.13s/it] 97%|█████████▋| 261/270 [04:41<00:09,  1.07s/it]                                                 {'loss': 1.1632, 'grad_norm': 2.569439857333257, 'learning_rate': 7.662835249042144e-08, 'epoch': 2.9}
 97%|█████████▋| 261/270 [04:41<00:09,  1.07s/it] 97%|█████████▋| 262/270 [04:42<00:08,  1.00s/it]                                                 {'loss': 1.1695, 'grad_norm': 2.5754017504099984, 'learning_rate': 6.89655172413793e-08, 'epoch': 2.91}
 97%|█████████▋| 262/270 [04:42<00:08,  1.00s/it] 97%|█████████▋| 263/270 [04:43<00:07,  1.14s/it]                                                 {'loss': 1.1744, 'grad_norm': 2.672931943921604, 'learning_rate': 6.130268199233716e-08, 'epoch': 2.92}
 97%|█████████▋| 263/270 [04:43<00:07,  1.14s/it] 98%|█████████▊| 264/270 [04:44<00:06,  1.06s/it]                                                 {'loss': 1.2144, 'grad_norm': 2.6634123135003502, 'learning_rate': 5.3639846743295015e-08, 'epoch': 2.93}
 98%|█████████▊| 264/270 [04:44<00:06,  1.06s/it] 98%|█████████▊| 265/270 [04:45<00:05,  1.03s/it]                                                 {'loss': 1.2119, 'grad_norm': 2.6749887131739642, 'learning_rate': 4.597701149425287e-08, 'epoch': 2.94}
 98%|█████████▊| 265/270 [04:45<00:05,  1.03s/it] 99%|█████████▊| 266/270 [04:46<00:03,  1.02it/s]                                                 {'loss': 1.2086, 'grad_norm': 2.6204822072647875, 'learning_rate': 3.831417624521072e-08, 'epoch': 2.96}
 99%|█████████▊| 266/270 [04:46<00:03,  1.02it/s] 99%|█████████▉| 267/270 [04:47<00:03,  1.08s/it]                                                 {'loss': 1.2296, 'grad_norm': 2.675678284926724, 'learning_rate': 3.065134099616858e-08, 'epoch': 2.97}
 99%|█████████▉| 267/270 [04:47<00:03,  1.08s/it] 99%|█████████▉| 268/270 [04:48<00:02,  1.06s/it]                                                 {'loss': 1.1705, 'grad_norm': 2.510459976654927, 'learning_rate': 2.2988505747126436e-08, 'epoch': 2.98}
 99%|█████████▉| 268/270 [04:48<00:02,  1.06s/it]100%|█████████▉| 269/270 [04:49<00:01,  1.05s/it]                                                 {'loss': 1.1885, 'grad_norm': 2.579318116553478, 'learning_rate': 1.532567049808429e-08, 'epoch': 2.99}
100%|█████████▉| 269/270 [04:49<00:01,  1.05s/it]100%|██████████| 270/270 [04:50<00:00,  1.00it/s]                                                 {'loss': 1.1678, 'grad_norm': 2.551379125370135, 'learning_rate': 7.662835249042145e-09, 'epoch': 3.0}
100%|██████████| 270/270 [04:50<00:00,  1.00it/s]                                                 {'train_runtime': 290.7565, 'train_samples_per_second': 74.165, 'train_steps_per_second': 0.929, 'train_loss': 1.3033003303739759, 'epoch': 3.0}
100%|██████████| 270/270 [04:50<00:00,  1.00it/s]100%|██████████| 270/270 [04:50<00:00,  1.08s/it]
[2024-07-24 18:39:39,562] [INFO] [launch.py:351:main] Process 289209 exits successfully.
[2024-07-24 18:39:39,563] [INFO] [launch.py:351:main] Process 289212 exits successfully.
[2024-07-24 18:39:39,563] [INFO] [launch.py:351:main] Process 289213 exits successfully.
[2024-07-24 18:39:39,563] [INFO] [launch.py:351:main] Process 289215 exits successfully.
[2024-07-24 18:39:40,563] [INFO] [launch.py:351:main] Process 289210 exits successfully.
[2024-07-24 18:39:40,564] [INFO] [launch.py:351:main] Process 289211 exits successfully.
[2024-07-24 18:39:40,564] [INFO] [launch.py:351:main] Process 289214 exits successfully.
[2024-07-24 18:40:08,567] [INFO] [launch.py:351:main] Process 289208 exits successfully.
