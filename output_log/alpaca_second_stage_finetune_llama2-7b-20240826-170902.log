Namespace(mode=['alpaca'], base_model='/fsx-project/yunyun/models/meta-llama_Llama2-7b_sharegpt_tuned', template_name='barebone', total_bsz=64, epoch=3, lr=2e-05, data_path='', task_name='ni', tuned_dir='./cache', use_peft=False, lora_r=16, lora_alpha=32)
num gpus:  8
Running 1/1: deepspeed --num_gpus=8 train.py --deepspeed ../deepspeed_config/zero3.json --bf16 --tf32=True
        --model_name_or_path /fsx-project/yunyun/models/meta-llama_Llama2-7b_sharegpt_tuned --data_path ../data/stanford_alpaca/ni_data.json
        --output_dir /fsx-project/yunyun/models/_fsx-project_ni_tuned
        --num_train_epochs 3
        --per_device_train_batch_size 10
        --per_device_eval_batch_size 4
        --gradient_accumulation_steps 1
        --gradient_checkpointing=True
        --evaluation_strategy=no
        --save_strategy=steps
        --save_steps 500
        --save_total_limit 1
        --learning_rate 2e-6
        --weight_decay 0.
        --report_to tensorboard
        --warmup_ratio 0.03
        --lr_scheduler_type=cosine
        --logging_steps 1
        --use_peft False 
        --lora_r 16 --lora_alpha 32
['deepspeed', '--num_gpus=8', 'train.py', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', '/fsx-project/yunyun/models/meta-llama_Llama2-7b_sharegpt_tuned', '--data_path', '../data/stanford_alpaca/ni_data.json', '--output_dir', '/fsx-project/yunyun/models/_fsx-project_ni_tuned', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1', '--use_peft', 'False', '--lora_r', '16', '--lora_alpha', '32']
[2024-08-26 17:09:17,053] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2024-08-26 17:09:25,121] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2024-08-26 17:09:25,122] [INFO] [runner.py:568:main] cmd = /data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNSwgNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None train.py --deepspeed ../deepspeed_config/zero3.json --bf16 --tf32=True --model_name_or_path /fsx-project/yunyun/models/meta-llama_Llama2-7b_sharegpt_tuned --data_path ../data/stanford_alpaca/ni_data.json --output_dir /fsx-project/yunyun/models/_fsx-project_ni_tuned --num_train_epochs 3 --per_device_train_batch_size 10 --per_device_eval_batch_size 4 --gradient_accumulation_steps 1 --gradient_checkpointing=True --evaluation_strategy=no --save_strategy=steps --save_steps 500 --save_total_limit 1 --learning_rate 2e-6 --weight_decay 0. --report_to tensorboard --warmup_ratio 0.03 --lr_scheduler_type=cosine --logging_steps 1 --use_peft False --lora_r 16 --lora_alpha 32
[2024-08-26 17:09:27,710] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2024-08-26 17:09:31,149] [INFO] [launch.py:139:main] 0 NCCL_ASYNC_ERROR_HANDLING=1
[2024-08-26 17:09:31,150] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]}
[2024-08-26 17:09:31,150] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=8, node_rank=0
[2024-08-26 17:09:31,150] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]})
[2024-08-26 17:09:31,150] [INFO] [launch.py:164:main] dist_world_size=8
[2024-08-26 17:09:31,150] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
[2024-08-26 17:09:31,151] [INFO] [launch.py:256:main] process 951604 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=0', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', '/fsx-project/yunyun/models/meta-llama_Llama2-7b_sharegpt_tuned', '--data_path', '../data/stanford_alpaca/ni_data.json', '--output_dir', '/fsx-project/yunyun/models/_fsx-project_ni_tuned', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1', '--use_peft', 'False', '--lora_r', '16', '--lora_alpha', '32']
[2024-08-26 17:09:31,151] [INFO] [launch.py:256:main] process 951605 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=1', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', '/fsx-project/yunyun/models/meta-llama_Llama2-7b_sharegpt_tuned', '--data_path', '../data/stanford_alpaca/ni_data.json', '--output_dir', '/fsx-project/yunyun/models/_fsx-project_ni_tuned', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1', '--use_peft', 'False', '--lora_r', '16', '--lora_alpha', '32']
[2024-08-26 17:09:31,152] [INFO] [launch.py:256:main] process 951606 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=2', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', '/fsx-project/yunyun/models/meta-llama_Llama2-7b_sharegpt_tuned', '--data_path', '../data/stanford_alpaca/ni_data.json', '--output_dir', '/fsx-project/yunyun/models/_fsx-project_ni_tuned', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1', '--use_peft', 'False', '--lora_r', '16', '--lora_alpha', '32']
[2024-08-26 17:09:31,152] [INFO] [launch.py:256:main] process 951607 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=3', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', '/fsx-project/yunyun/models/meta-llama_Llama2-7b_sharegpt_tuned', '--data_path', '../data/stanford_alpaca/ni_data.json', '--output_dir', '/fsx-project/yunyun/models/_fsx-project_ni_tuned', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1', '--use_peft', 'False', '--lora_r', '16', '--lora_alpha', '32']
[2024-08-26 17:09:31,153] [INFO] [launch.py:256:main] process 951608 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=4', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', '/fsx-project/yunyun/models/meta-llama_Llama2-7b_sharegpt_tuned', '--data_path', '../data/stanford_alpaca/ni_data.json', '--output_dir', '/fsx-project/yunyun/models/_fsx-project_ni_tuned', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1', '--use_peft', 'False', '--lora_r', '16', '--lora_alpha', '32']
[2024-08-26 17:09:31,153] [INFO] [launch.py:256:main] process 951609 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=5', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', '/fsx-project/yunyun/models/meta-llama_Llama2-7b_sharegpt_tuned', '--data_path', '../data/stanford_alpaca/ni_data.json', '--output_dir', '/fsx-project/yunyun/models/_fsx-project_ni_tuned', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1', '--use_peft', 'False', '--lora_r', '16', '--lora_alpha', '32']
[2024-08-26 17:09:31,154] [INFO] [launch.py:256:main] process 951610 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=6', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', '/fsx-project/yunyun/models/meta-llama_Llama2-7b_sharegpt_tuned', '--data_path', '../data/stanford_alpaca/ni_data.json', '--output_dir', '/fsx-project/yunyun/models/_fsx-project_ni_tuned', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1', '--use_peft', 'False', '--lora_r', '16', '--lora_alpha', '32']
[2024-08-26 17:09:31,155] [INFO] [launch.py:256:main] process 951611 spawned with command: ['/data/home/yunyun/miniconda3/envs/newtorch2/bin/python3.11', '-u', 'train.py', '--local_rank=7', '--deepspeed', '../deepspeed_config/zero3.json', '--bf16', '--tf32=True', '--model_name_or_path', '/fsx-project/yunyun/models/meta-llama_Llama2-7b_sharegpt_tuned', '--data_path', '../data/stanford_alpaca/ni_data.json', '--output_dir', '/fsx-project/yunyun/models/_fsx-project_ni_tuned', '--num_train_epochs', '3', '--per_device_train_batch_size', '10', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--gradient_checkpointing=True', '--evaluation_strategy=no', '--save_strategy=steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-6', '--weight_decay', '0.', '--report_to', 'tensorboard', '--warmup_ratio', '0.03', '--lr_scheduler_type=cosine', '--logging_steps', '1', '--use_peft', 'False', '--lora_r', '16', '--lora_alpha', '32']
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
[2024-08-26 17:09:47,351] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-26 17:09:47,367] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-26 17:09:47,433] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-26 17:09:47,441] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-26 17:09:47,478] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-26 17:09:47,486] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-26 17:09:47,521] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-26 17:09:47,529] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2024-08-26 17:09:48,122] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-08-26 17:09:48,126] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-08-26 17:09:48,179] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-08-26 17:09:48,196] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-08-26 17:09:48,196] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-08-26 17:09:48,234] [INFO] [comm.py:637:init_distributed] cdb=None
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[2024-08-26 17:09:48,239] [INFO] [comm.py:637:init_distributed] cdb=None
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[2024-08-26 17:09:48,265] [INFO] [comm.py:637:init_distributed] cdb=None
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[2024-08-26 17:09:48,268] [INFO] [comm.py:637:init_distributed] cdb=None
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[2024-08-26 17:09:59,264] [INFO] [partition_parameters.py:345:__exit__] finished initializing model - num_params = 291, num_elems = 6.74B
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:01,  1.38it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:01,  1.37it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:01,  1.37it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:01,  1.35it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:01,  1.36it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:01,  1.35it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:01,  1.35it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:07<00:14,  7.03s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:07<00:04,  4.38s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:07<00:04,  4.38s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:07<00:04,  4.39s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:07<00:04,  4.39s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:07<00:04,  4.40s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:07<00:04,  4.40s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:07<00:04,  4.40s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:12<00:00,  4.49s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:12<00:00,  4.10s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [00:12<00:00,  4.49s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:12<00:00,  4.10s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [00:12<00:00,  4.49s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:12<00:00,  4.10s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [00:12<00:00,  4.50s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:12<00:00,  4.10s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [00:12<00:00,  4.50s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:12<00:00,  4.11s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [00:12<00:00,  4.51s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:12<00:00,  4.11s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [00:12<00:00,  4.51s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:12<00:00,  4.11s/it]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:13<00:06,  6.99s/it]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 3/3 [00:19<00:00,  6.14s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:19<00:00,  6.37s/it]
[2024-08-26 17:10:18,543] [INFO] [partition_parameters.py:345:__exit__] finished initializing model - num_params = 582, num_elems = 13.48B
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:03,  1.79s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:03,  1.79s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:03,  1.79s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:03,  1.79s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:03,  1.81s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:03,  1.81s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:03,  1.81s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:02<00:05,  2.89s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:04<00:02,  2.41s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:04<00:02,  2.42s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:04<00:02,  2.42s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:04<00:02,  2.42s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:04<00:02,  2.42s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:04<00:02,  2.42s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:04<00:02,  2.42s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:06<00:00,  2.08s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:06<00:00,  2.11s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [00:06<00:00,  2.08s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:06<00:00,  2.11s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [00:06<00:00,  2.08s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:06<00:00,  2.11s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [00:06<00:00,  2.08s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:06<00:00,  2.08s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:06<00:00,  2.08s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:06<00:00,  2.11s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [00:06<00:00,  2.11s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [00:06<00:00,  2.11s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [00:06<00:00,  2.08s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:06<00:00,  2.11s/it]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:05<00:02,  2.90s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:07<00:00,  2.52s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:07<00:00,  2.62s/it]
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Loading data...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
[rank0]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[rank2]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...

[rank1]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
[rank3]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
[rank4]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
[rank6]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121/fused_adam/build.ninja...
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/cpp_extension.py:1967: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
[rank7]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
[rank5]:[W Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
Using /opt/hpcaas/.mounts/fs-0301404b74c8d22fd/home/yunyun/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
[1/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output multi_tensor_adam.cuda.o.d -DTORCH_EXTENSION_NAME=fused_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/deepspeed/ops/csrc/includes -I/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/deepspeed/ops/csrc/adam -isystem /data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/include -isystem /data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/include/TH -isystem /data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /data/home/yunyun/miniconda3/envs/newtorch2/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_90,code=compute_90 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -lineinfo --use_fast_math -gencode=arch=compute_90,code=sm_90 -gencode=arch=compute_90,code=compute_90 -DBF16_AVAILABLE -U__CUDA_NO_BFLOAT16_OPERATORS__ -U__CUDA_NO_BFLOAT162_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ -std=c++17 -c /data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/deepspeed/ops/csrc/adam/multi_tensor_adam.cu -o multi_tensor_adam.cuda.o 
[2/3] c++ -MMD -MF fused_adam_frontend.o.d -DTORCH_EXTENSION_NAME=fused_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/deepspeed/ops/csrc/includes -I/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/deepspeed/ops/csrc/adam -isystem /data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/include -isystem /data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/include/TH -isystem /data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /data/home/yunyun/miniconda3/envs/newtorch2/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -std=c++17 -g -Wno-reorder -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DBF16_AVAILABLE -c /data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/deepspeed/ops/csrc/adam/fused_adam_frontend.cpp -o fused_adam_frontend.o 
[3/3] c++ fused_adam_frontend.o multi_tensor_adam.cuda.o -shared -L/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o fused_adam.so
Loading extension module fused_adam...
Loading extension module fused_adam...
Loading extension module fused_adam...
Loading extension module fused_adam...
Loading extension module fused_adam...
Loading extension module fused_adam...
Loading extension module fused_adam...
Loading extension module fused_adam...
Time to load fused_adam op: 35.03131985664368 secondsTime to load fused_adam op: 35.11078643798828 secondsTime to load fused_adam op: 35.27247929573059 secondsTime to load fused_adam op: 35.21887016296387 seconds
Time to load fused_adam op: 35.26964449882507 secondsTime to load fused_adam op: 35.361626863479614 secondsTime to load fused_adam op: 35.31957030296326 seconds

Time to load fused_adam op: 35.36163020133972 seconds




Parameter Offload: Total persistent parameters: 266240 in 65 params
  0%|          | 0/564 [00:00<?, ?it/s]/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
  0%|          | 1/564 [00:09<1:27:34,  9.33s/it]                                                 {'loss': 2.5183, 'grad_norm': 55.34342786689222, 'learning_rate': 0.0, 'epoch': 0.01}
  0%|          | 1/564 [00:09<1:27:34,  9.33s/it]  0%|          | 2/564 [00:10<41:44,  4.46s/it]                                                 {'loss': 2.6638, 'grad_norm': 58.12841763780357, 'learning_rate': 4.89301084236452e-07, 'epoch': 0.01}
  0%|          | 2/564 [00:10<41:44,  4.46s/it]  1%|          | 3/564 [00:10<25:16,  2.70s/it]                                               {'loss': 2.4766, 'grad_norm': 54.73143617948329, 'learning_rate': 7.755238700769802e-07, 'epoch': 0.02}
  1%|          | 3/564 [00:10<25:16,  2.70s/it]  1%|          | 4/564 [00:11<17:30,  1.88s/it]                                               {'loss': 2.4697, 'grad_norm': 49.4634851507429, 'learning_rate': 9.78602168472904e-07, 'epoch': 0.02}
  1%|          | 4/564 [00:11<17:30,  1.88s/it]  1%|          | 5/564 [00:12<13:14,  1.42s/it]                                               {'loss': 2.7581, 'grad_norm': 54.27661699317952, 'learning_rate': 1.1361219343474658e-06, 'epoch': 0.03}
  1%|          | 5/564 [00:12<13:14,  1.42s/it]  1%|          | 6/564 [00:12<10:37,  1.14s/it]                                               {'loss': 2.2171, 'grad_norm': 44.67702784815407, 'learning_rate': 1.264824954313432e-06, 'epoch': 0.03}
  1%|          | 6/564 [00:12<10:37,  1.14s/it]  1%|          | 7/564 [00:13<09:40,  1.04s/it]                                               {'loss': 2.1477, 'grad_norm': 41.04721250607924, 'learning_rate': 1.373641807199326e-06, 'epoch': 0.04}
  1%|          | 7/564 [00:13<09:40,  1.04s/it]  1%|▏         | 8/564 [00:14<08:21,  1.11it/s]                                               {'loss': 2.1059, 'grad_norm': 46.67268232691676, 'learning_rate': 1.4679032527093559e-06, 'epoch': 0.04}
  1%|▏         | 8/564 [00:14<08:21,  1.11it/s]  2%|▏         | 9/564 [00:14<07:27,  1.24it/s]                                               {'loss': 1.6904, 'grad_norm': 41.35014673450374, 'learning_rate': 1.5510477401539603e-06, 'epoch': 0.05}
  2%|▏         | 9/564 [00:14<07:27,  1.24it/s]  2%|▏         | 10/564 [00:15<07:06,  1.30it/s]                                                {'loss': 1.4826, 'grad_norm': 20.040342022274803, 'learning_rate': 1.625423018583918e-06, 'epoch': 0.05}
  2%|▏         | 10/564 [00:15<07:06,  1.30it/s]  2%|▏         | 11/564 [00:16<06:40,  1.38it/s]                                                {'loss': 1.3939, 'grad_norm': 26.397791822555103, 'learning_rate': 1.6927036418410939e-06, 'epoch': 0.06}
  2%|▏         | 11/564 [00:16<06:40,  1.38it/s]  2%|▏         | 12/564 [00:16<06:41,  1.38it/s]                                                {'loss': 1.4836, 'grad_norm': 21.902970476240398, 'learning_rate': 1.7541260385498841e-06, 'epoch': 0.06}
  2%|▏         | 12/564 [00:16<06:41,  1.38it/s]  2%|▏         | 13/564 [00:17<06:18,  1.46it/s]                                                {'loss': 1.2261, 'grad_norm': 19.30823543651799, 'learning_rate': 1.8106291662380673e-06, 'epoch': 0.07}
  2%|▏         | 13/564 [00:17<06:18,  1.46it/s]  2%|▏         | 14/564 [00:18<06:40,  1.37it/s]                                                {'loss': 1.0742, 'grad_norm': 14.179601083466428, 'learning_rate': 1.862942891435778e-06, 'epoch': 0.07}
  2%|▏         | 14/564 [00:18<06:40,  1.37it/s]  3%|▎         | 15/564 [00:19<06:54,  1.32it/s]                                                {'loss': 0.8151, 'grad_norm': 15.901880295272802, 'learning_rate': 1.911645804424446e-06, 'epoch': 0.08}
  3%|▎         | 15/564 [00:19<06:54,  1.32it/s]  3%|▎         | 16/564 [00:19<07:04,  1.29it/s]                                                {'loss': 0.9775, 'grad_norm': 11.410287048409037, 'learning_rate': 1.957204336945808e-06, 'epoch': 0.09}
  3%|▎         | 16/564 [00:19<07:04,  1.29it/s]  3%|▎         | 17/564 [00:20<07:14,  1.26it/s]                                                {'loss': 0.7953, 'grad_norm': 9.374791210366805, 'learning_rate': 2e-06, 'epoch': 0.09}
  3%|▎         | 17/564 [00:20<07:14,  1.26it/s]  3%|▎         | 18/564 [00:21<06:40,  1.36it/s]                                                {'loss': 0.8963, 'grad_norm': 10.386929016134221, 'learning_rate': 2e-06, 'epoch': 0.1}
  3%|▎         | 18/564 [00:21<06:40,  1.36it/s]  3%|▎         | 19/564 [00:22<06:56,  1.31it/s]                                                {'loss': 0.8665, 'grad_norm': 9.876531374726309, 'learning_rate': 1.996343692870201e-06, 'epoch': 0.1}
  3%|▎         | 19/564 [00:22<06:56,  1.31it/s]  4%|▎         | 20/564 [00:23<07:06,  1.28it/s]                                                {'loss': 0.6565, 'grad_norm': 8.091962355273242, 'learning_rate': 1.992687385740402e-06, 'epoch': 0.11}
  4%|▎         | 20/564 [00:23<07:06,  1.28it/s]  4%|▎         | 21/564 [00:23<07:13,  1.25it/s]                                                {'loss': 0.8837, 'grad_norm': 9.563117774254668, 'learning_rate': 1.9890310786106034e-06, 'epoch': 0.11}
  4%|▎         | 21/564 [00:23<07:13,  1.25it/s]  4%|▍         | 22/564 [00:24<07:15,  1.24it/s]                                                {'loss': 0.844, 'grad_norm': 7.7553001711304885, 'learning_rate': 1.9853747714808044e-06, 'epoch': 0.12}
  4%|▍         | 22/564 [00:24<07:15,  1.24it/s]  4%|▍         | 23/564 [00:25<06:42,  1.35it/s]                                                {'loss': 0.9634, 'grad_norm': 8.115570949265818, 'learning_rate': 1.9817184643510055e-06, 'epoch': 0.12}
  4%|▍         | 23/564 [00:25<06:42,  1.35it/s]  4%|▍         | 24/564 [00:25<06:17,  1.43it/s]                                                {'loss': 0.7505, 'grad_norm': 10.973378295683249, 'learning_rate': 1.9780621572212065e-06, 'epoch': 0.13}
  4%|▍         | 24/564 [00:25<06:17,  1.43it/s]  4%|▍         | 25/564 [00:26<06:18,  1.43it/s]                                                {'loss': 0.6836, 'grad_norm': 7.0115570007454515, 'learning_rate': 1.9744058500914075e-06, 'epoch': 0.13}
  4%|▍         | 25/564 [00:26<06:18,  1.43it/s]  5%|▍         | 26/564 [00:27<06:39,  1.35it/s]                                                {'loss': 0.6196, 'grad_norm': 7.935159941029201, 'learning_rate': 1.970749542961609e-06, 'epoch': 0.14}
  5%|▍         | 26/564 [00:27<06:39,  1.35it/s]  5%|▍         | 27/564 [00:28<06:50,  1.31it/s]                                                {'loss': 0.6685, 'grad_norm': 6.402143160500479, 'learning_rate': 1.9670932358318095e-06, 'epoch': 0.14}
  5%|▍         | 27/564 [00:28<06:50,  1.31it/s]  5%|▍         | 28/564 [00:29<07:00,  1.27it/s]                                                {'loss': 0.6826, 'grad_norm': 8.373696644402697, 'learning_rate': 1.963436928702011e-06, 'epoch': 0.15}
  5%|▍         | 28/564 [00:29<07:00,  1.27it/s]  5%|▌         | 29/564 [00:29<06:29,  1.37it/s]                                                {'loss': 0.6995, 'grad_norm': 6.846205035834098, 'learning_rate': 1.959780621572212e-06, 'epoch': 0.15}
  5%|▌         | 29/564 [00:29<06:29,  1.37it/s]  5%|▌         | 30/564 [00:30<06:45,  1.32it/s]                                                {'loss': 0.6338, 'grad_norm': 8.611992390289334, 'learning_rate': 1.956124314442413e-06, 'epoch': 0.16}
  5%|▌         | 30/564 [00:30<06:45,  1.32it/s]  5%|▌         | 31/564 [00:31<06:18,  1.41it/s]                                                {'loss': 0.8017, 'grad_norm': 6.421768788084578, 'learning_rate': 1.952468007312614e-06, 'epoch': 0.16}
  5%|▌         | 31/564 [00:31<06:18,  1.41it/s]  6%|▌         | 32/564 [00:31<06:34,  1.35it/s]                                                {'loss': 0.6886, 'grad_norm': 8.565717496407622, 'learning_rate': 1.948811700182815e-06, 'epoch': 0.17}
  6%|▌         | 32/564 [00:31<06:34,  1.35it/s]  6%|▌         | 33/564 [00:32<06:50,  1.29it/s]                                                {'loss': 0.5203, 'grad_norm': 6.967561228013649, 'learning_rate': 1.9451553930530165e-06, 'epoch': 0.18}
  6%|▌         | 33/564 [00:32<06:50,  1.29it/s]  6%|▌         | 34/564 [00:33<06:57,  1.27it/s]                                                {'loss': 0.7089, 'grad_norm': 6.975949571547782, 'learning_rate': 1.9414990859232176e-06, 'epoch': 0.18}
  6%|▌         | 34/564 [00:33<06:57,  1.27it/s]  6%|▌         | 35/564 [00:34<06:26,  1.37it/s]                                                {'loss': 0.6932, 'grad_norm': 6.834452826798572, 'learning_rate': 1.9378427787934186e-06, 'epoch': 0.19}
  6%|▌         | 35/564 [00:34<06:26,  1.37it/s]  6%|▋         | 36/564 [00:34<06:38,  1.33it/s]                                                {'loss': 0.6994, 'grad_norm': 6.6392284452900485, 'learning_rate': 1.9341864716636196e-06, 'epoch': 0.19}
  6%|▋         | 36/564 [00:35<06:38,  1.33it/s]  7%|▋         | 37/564 [00:35<06:12,  1.42it/s]                                                {'loss': 0.537, 'grad_norm': 6.875744024895729, 'learning_rate': 1.9305301645338206e-06, 'epoch': 0.2}
  7%|▋         | 37/564 [00:35<06:12,  1.42it/s]  7%|▋         | 38/564 [00:36<05:53,  1.49it/s]                                                {'loss': 0.6438, 'grad_norm': 5.949731567212996, 'learning_rate': 1.9268738574040217e-06, 'epoch': 0.2}
  7%|▋         | 38/564 [00:36<05:53,  1.49it/s]  7%|▋         | 39/564 [00:36<05:40,  1.54it/s]                                                {'loss': 0.6661, 'grad_norm': 6.94931483218078, 'learning_rate': 1.923217550274223e-06, 'epoch': 0.21}
  7%|▋         | 39/564 [00:36<05:40,  1.54it/s]  7%|▋         | 40/564 [00:37<05:57,  1.47it/s]                                                {'loss': 0.5706, 'grad_norm': 6.960561943248225, 'learning_rate': 1.919561243144424e-06, 'epoch': 0.21}
  7%|▋         | 40/564 [00:37<05:57,  1.47it/s]  7%|▋         | 41/564 [00:38<05:42,  1.53it/s]                                                {'loss': 0.7127, 'grad_norm': 6.309918865980663, 'learning_rate': 1.915904936014625e-06, 'epoch': 0.22}
  7%|▋         | 41/564 [00:38<05:42,  1.53it/s]  7%|▋         | 42/564 [00:38<05:31,  1.57it/s]                                                {'loss': 0.7685, 'grad_norm': 7.816025422024489, 'learning_rate': 1.912248628884826e-06, 'epoch': 0.22}
  7%|▋         | 42/564 [00:38<05:31,  1.57it/s]  8%|▊         | 43/564 [00:39<05:28,  1.58it/s]                                                {'loss': 0.6618, 'grad_norm': 6.227781425705141, 'learning_rate': 1.908592321755027e-06, 'epoch': 0.23}
  8%|▊         | 43/564 [00:39<05:28,  1.58it/s]  8%|▊         | 44/564 [00:39<05:23,  1.61it/s]                                                {'loss': 0.6407, 'grad_norm': 7.165880731992237, 'learning_rate': 1.9049360146252284e-06, 'epoch': 0.23}
  8%|▊         | 44/564 [00:39<05:23,  1.61it/s]  8%|▊         | 45/564 [00:40<05:19,  1.63it/s]                                                {'loss': 0.6861, 'grad_norm': 6.92704673456414, 'learning_rate': 1.9012797074954294e-06, 'epoch': 0.24}
  8%|▊         | 45/564 [00:40<05:19,  1.63it/s]  8%|▊         | 46/564 [00:41<05:49,  1.48it/s]                                                {'loss': 0.4779, 'grad_norm': 6.202482191713289, 'learning_rate': 1.8976234003656307e-06, 'epoch': 0.24}
  8%|▊         | 46/564 [00:41<05:49,  1.48it/s]  8%|▊         | 47/564 [00:41<05:36,  1.54it/s]                                                {'loss': 0.6081, 'grad_norm': 5.713669640269602, 'learning_rate': 1.8939670932358317e-06, 'epoch': 0.25}
  8%|▊         | 47/564 [00:41<05:36,  1.54it/s]  9%|▊         | 48/564 [00:42<06:04,  1.42it/s]                                                {'loss': 0.6674, 'grad_norm': 5.685258254663658, 'learning_rate': 1.8903107861060327e-06, 'epoch': 0.26}
  9%|▊         | 48/564 [00:42<06:04,  1.42it/s]  9%|▊         | 49/564 [00:43<06:00,  1.43it/s]                                                {'loss': 0.6089, 'grad_norm': 6.46963597966769, 'learning_rate': 1.886654478976234e-06, 'epoch': 0.26}
  9%|▊         | 49/564 [00:43<06:00,  1.43it/s]  9%|▉         | 50/564 [00:44<05:44,  1.49it/s]                                                {'loss': 0.7341, 'grad_norm': 6.23068210819088, 'learning_rate': 1.882998171846435e-06, 'epoch': 0.27}
  9%|▉         | 50/564 [00:44<05:44,  1.49it/s]  9%|▉         | 51/564 [00:44<05:33,  1.54it/s]                                                {'loss': 0.7154, 'grad_norm': 8.354343232577387, 'learning_rate': 1.8793418647166362e-06, 'epoch': 0.27}
  9%|▉         | 51/564 [00:44<05:33,  1.54it/s]  9%|▉         | 52/564 [00:45<05:24,  1.58it/s]                                                {'loss': 0.8014, 'grad_norm': 6.345049933247526, 'learning_rate': 1.875685557586837e-06, 'epoch': 0.28}
  9%|▉         | 52/564 [00:45<05:24,  1.58it/s]  9%|▉         | 53/564 [00:45<05:18,  1.61it/s]                                                {'loss': 0.6078, 'grad_norm': 6.19760107815886, 'learning_rate': 1.8720292504570383e-06, 'epoch': 0.28}
  9%|▉         | 53/564 [00:45<05:18,  1.61it/s] 10%|▉         | 54/564 [00:46<05:48,  1.46it/s]                                                {'loss': 0.7, 'grad_norm': 6.1133729669825465, 'learning_rate': 1.8683729433272395e-06, 'epoch': 0.29}
 10%|▉         | 54/564 [00:46<05:48,  1.46it/s] 10%|▉         | 55/564 [00:47<05:35,  1.52it/s]                                                {'loss': 0.6358, 'grad_norm': 6.106947756604316, 'learning_rate': 1.8647166361974405e-06, 'epoch': 0.29}
 10%|▉         | 55/564 [00:47<05:35,  1.52it/s] 10%|▉         | 56/564 [00:47<05:25,  1.56it/s]                                                {'loss': 0.6568, 'grad_norm': 7.775082236395389, 'learning_rate': 1.8610603290676416e-06, 'epoch': 0.3}
 10%|▉         | 56/564 [00:47<05:25,  1.56it/s] 10%|█         | 57/564 [00:48<05:17,  1.60it/s]                                                {'loss': 0.6091, 'grad_norm': 5.788726214495001, 'learning_rate': 1.8574040219378426e-06, 'epoch': 0.3}
 10%|█         | 57/564 [00:48<05:17,  1.60it/s] 10%|█         | 58/564 [00:49<05:46,  1.46it/s]                                                {'loss': 0.6338, 'grad_norm': 6.172341495908643, 'learning_rate': 1.8537477148080438e-06, 'epoch': 0.31}
 10%|█         | 58/564 [00:49<05:46,  1.46it/s] 10%|█         | 59/564 [00:49<05:32,  1.52it/s]                                                {'loss': 0.5504, 'grad_norm': 7.768480579349901, 'learning_rate': 1.8500914076782448e-06, 'epoch': 0.31}
 10%|█         | 59/564 [00:49<05:32,  1.52it/s] 11%|█         | 60/564 [00:50<05:56,  1.42it/s]                                                {'loss': 0.6319, 'grad_norm': 5.841245633005583, 'learning_rate': 1.846435100548446e-06, 'epoch': 0.32}
 11%|█         | 60/564 [00:50<05:56,  1.42it/s] 11%|█         | 61/564 [00:51<05:39,  1.48it/s]                                                {'loss': 0.6289, 'grad_norm': 5.417071521445071, 'learning_rate': 1.842778793418647e-06, 'epoch': 0.32}
 11%|█         | 61/564 [00:51<05:39,  1.48it/s] 11%|█         | 62/564 [00:52<05:51,  1.43it/s]                                                {'loss': 0.5903, 'grad_norm': 6.452609308579973, 'learning_rate': 1.8391224862888481e-06, 'epoch': 0.33}
 11%|█         | 62/564 [00:52<05:51,  1.43it/s] 11%|█         | 63/564 [00:52<05:36,  1.49it/s]                                                {'loss': 0.5234, 'grad_norm': 6.7549748697053715, 'learning_rate': 1.8354661791590494e-06, 'epoch': 0.34}
 11%|█         | 63/564 [00:52<05:36,  1.49it/s] 11%|█▏        | 64/564 [00:53<05:58,  1.39it/s]                                                {'loss': 0.6066, 'grad_norm': 7.347367498695581, 'learning_rate': 1.8318098720292504e-06, 'epoch': 0.34}
 11%|█▏        | 64/564 [00:53<05:58,  1.39it/s] 12%|█▏        | 65/564 [00:54<06:02,  1.38it/s]                                                {'loss': 0.5966, 'grad_norm': 8.154120131738294, 'learning_rate': 1.8281535648994514e-06, 'epoch': 0.35}
 12%|█▏        | 65/564 [00:54<06:02,  1.38it/s] 12%|█▏        | 66/564 [00:55<06:16,  1.32it/s]                                                {'loss': 0.7361, 'grad_norm': 7.395775061832422, 'learning_rate': 1.8244972577696524e-06, 'epoch': 0.35}
 12%|█▏        | 66/564 [00:55<06:16,  1.32it/s] 12%|█▏        | 67/564 [00:55<05:52,  1.41it/s]                                                {'loss': 0.4774, 'grad_norm': 6.234793651545834, 'learning_rate': 1.8208409506398537e-06, 'epoch': 0.36}
 12%|█▏        | 67/564 [00:55<05:52,  1.41it/s] 12%|█▏        | 68/564 [00:56<06:07,  1.35it/s]                                                {'loss': 0.6043, 'grad_norm': 7.0776511569680345, 'learning_rate': 1.817184643510055e-06, 'epoch': 0.36}
 12%|█▏        | 68/564 [00:56<06:07,  1.35it/s] 12%|█▏        | 69/564 [00:57<06:21,  1.30it/s]                                                {'loss': 0.6108, 'grad_norm': 7.373873211254854, 'learning_rate': 1.813528336380256e-06, 'epoch': 0.37}
 12%|█▏        | 69/564 [00:57<06:21,  1.30it/s] 12%|█▏        | 70/564 [00:58<06:29,  1.27it/s]                                                {'loss': 0.5684, 'grad_norm': 6.334569653544429, 'learning_rate': 1.809872029250457e-06, 'epoch': 0.37}
 12%|█▏        | 70/564 [00:58<06:29,  1.27it/s] 13%|█▎        | 71/564 [00:58<06:33,  1.25it/s]                                                {'loss': 0.574, 'grad_norm': 5.957107149142628, 'learning_rate': 1.806215722120658e-06, 'epoch': 0.38}
 13%|█▎        | 71/564 [00:58<06:33,  1.25it/s] 13%|█▎        | 72/564 [00:59<06:35,  1.25it/s]                                                {'loss': 0.5986, 'grad_norm': 7.562702418018892, 'learning_rate': 1.8025594149908592e-06, 'epoch': 0.38}
 13%|█▎        | 72/564 [00:59<06:35,  1.25it/s] 13%|█▎        | 73/564 [01:00<06:04,  1.35it/s]                                                {'loss': 0.5935, 'grad_norm': 5.018182444141347, 'learning_rate': 1.7989031078610602e-06, 'epoch': 0.39}
 13%|█▎        | 73/564 [01:00<06:04,  1.35it/s] 13%|█▎        | 74/564 [01:01<06:13,  1.31it/s]                                                {'loss': 0.5144, 'grad_norm': 5.009117728191673, 'learning_rate': 1.7952468007312612e-06, 'epoch': 0.39}
 13%|█▎        | 74/564 [01:01<06:13,  1.31it/s] 13%|█▎        | 75/564 [01:01<05:48,  1.40it/s]                                                {'loss': 0.5476, 'grad_norm': 6.100441409670815, 'learning_rate': 1.7915904936014625e-06, 'epoch': 0.4}
 13%|█▎        | 75/564 [01:01<05:48,  1.40it/s] 13%|█▎        | 76/564 [01:02<05:31,  1.47it/s]                                                {'loss': 0.6969, 'grad_norm': 6.4179140045545395, 'learning_rate': 1.7879341864716635e-06, 'epoch': 0.4}
 13%|█▎        | 76/564 [01:02<05:31,  1.47it/s] 14%|█▎        | 77/564 [01:03<05:50,  1.39it/s]                                                {'loss': 0.6528, 'grad_norm': 7.813008963573982, 'learning_rate': 1.7842778793418647e-06, 'epoch': 0.41}
 14%|█▎        | 77/564 [01:03<05:50,  1.39it/s] 14%|█▍        | 78/564 [01:03<05:31,  1.47it/s]                                                {'loss': 0.608, 'grad_norm': 5.695627991034596, 'learning_rate': 1.7806215722120656e-06, 'epoch': 0.41}
 14%|█▍        | 78/564 [01:03<05:31,  1.47it/s] 14%|█▍        | 79/564 [01:04<05:50,  1.38it/s]                                                {'loss': 0.7009, 'grad_norm': 7.345029542903371, 'learning_rate': 1.7769652650822668e-06, 'epoch': 0.42}
 14%|█▍        | 79/564 [01:04<05:50,  1.38it/s] 14%|█▍        | 80/564 [01:05<06:04,  1.33it/s]                                                {'loss': 0.7907, 'grad_norm': 6.5236702269703155, 'learning_rate': 1.7733089579524678e-06, 'epoch': 0.43}
 14%|█▍        | 80/564 [01:05<06:04,  1.33it/s] 14%|█▍        | 81/564 [01:06<06:06,  1.32it/s]                                                {'loss': 0.5711, 'grad_norm': 5.338688936202134, 'learning_rate': 1.769652650822669e-06, 'epoch': 0.43}
 14%|█▍        | 81/564 [01:06<06:06,  1.32it/s] 15%|█▍        | 82/564 [01:07<06:14,  1.29it/s]                                                {'loss': 0.4913, 'grad_norm': 5.0788432895134505, 'learning_rate': 1.7659963436928703e-06, 'epoch': 0.44}
 15%|█▍        | 82/564 [01:07<06:14,  1.29it/s] 15%|█▍        | 83/564 [01:07<06:13,  1.29it/s]                                                {'loss': 0.6608, 'grad_norm': 6.078109784702179, 'learning_rate': 1.762340036563071e-06, 'epoch': 0.44}
 15%|█▍        | 83/564 [01:07<06:13,  1.29it/s] 15%|█▍        | 84/564 [01:08<06:21,  1.26it/s]                                                {'loss': 0.6926, 'grad_norm': 6.072332904943778, 'learning_rate': 1.7586837294332723e-06, 'epoch': 0.45}
 15%|█▍        | 84/564 [01:08<06:21,  1.26it/s] 15%|█▌        | 85/564 [01:09<05:53,  1.36it/s]                                                {'loss': 0.7988, 'grad_norm': 7.171974841226731, 'learning_rate': 1.7550274223034734e-06, 'epoch': 0.45}
 15%|█▌        | 85/564 [01:09<05:53,  1.36it/s] 15%|█▌        | 86/564 [01:10<06:06,  1.30it/s]                                                {'loss': 0.6064, 'grad_norm': 5.950197286953974, 'learning_rate': 1.7513711151736746e-06, 'epoch': 0.46}
 15%|█▌        | 86/564 [01:10<06:06,  1.30it/s] 15%|█▌        | 87/564 [01:10<06:14,  1.27it/s]                                                {'loss': 0.6074, 'grad_norm': 7.314050542661513, 'learning_rate': 1.7477148080438754e-06, 'epoch': 0.46}
 15%|█▌        | 87/564 [01:10<06:14,  1.27it/s] 16%|█▌        | 88/564 [01:11<06:09,  1.29it/s]                                                {'loss': 0.5858, 'grad_norm': 5.969203442139983, 'learning_rate': 1.7440585009140766e-06, 'epoch': 0.47}
 16%|█▌        | 88/564 [01:11<06:09,  1.29it/s] 16%|█▌        | 89/564 [01:12<06:15,  1.27it/s]                                                {'loss': 0.4884, 'grad_norm': 5.672335370347314, 'learning_rate': 1.7404021937842779e-06, 'epoch': 0.47}
 16%|█▌        | 89/564 [01:12<06:15,  1.27it/s] 16%|█▌        | 90/564 [01:13<06:18,  1.25it/s]                                                {'loss': 0.6564, 'grad_norm': 6.651667488759537, 'learning_rate': 1.736745886654479e-06, 'epoch': 0.48}
 16%|█▌        | 90/564 [01:13<06:18,  1.25it/s] 16%|█▌        | 91/564 [01:14<06:20,  1.24it/s]                                                {'loss': 0.6143, 'grad_norm': 6.54888727731282, 'learning_rate': 1.7330895795246801e-06, 'epoch': 0.48}
 16%|█▌        | 91/564 [01:14<06:20,  1.24it/s] 16%|█▋        | 92/564 [01:14<05:51,  1.34it/s]                                                {'loss': 0.6928, 'grad_norm': 5.445090550987161, 'learning_rate': 1.729433272394881e-06, 'epoch': 0.49}
 16%|█▋        | 92/564 [01:14<05:51,  1.34it/s] 16%|█▋        | 93/564 [01:15<05:29,  1.43it/s]                                                {'loss': 0.5671, 'grad_norm': 5.519539898335777, 'learning_rate': 1.7257769652650822e-06, 'epoch': 0.49}
 16%|█▋        | 93/564 [01:15<05:29,  1.43it/s] 17%|█▋        | 94/564 [01:16<05:45,  1.36it/s]                                                {'loss': 0.5757, 'grad_norm': 5.857827992049108, 'learning_rate': 1.7221206581352832e-06, 'epoch': 0.5}
 17%|█▋        | 94/564 [01:16<05:45,  1.36it/s] 17%|█▋        | 95/564 [01:16<05:25,  1.44it/s]                                                {'loss': 0.6735, 'grad_norm': 6.35175553449181, 'learning_rate': 1.7184643510054844e-06, 'epoch': 0.51}
 17%|█▋        | 95/564 [01:16<05:25,  1.44it/s] 17%|█▋        | 96/564 [01:17<05:40,  1.38it/s]                                                {'loss': 0.6815, 'grad_norm': 6.555884337440913, 'learning_rate': 1.7148080438756855e-06, 'epoch': 0.51}
 17%|█▋        | 96/564 [01:17<05:40,  1.38it/s] 17%|█▋        | 97/564 [01:18<05:21,  1.45it/s]                                                {'loss': 0.6026, 'grad_norm': 7.3565459701432285, 'learning_rate': 1.7111517367458865e-06, 'epoch': 0.52}
 17%|█▋        | 97/564 [01:18<05:21,  1.45it/s] 17%|█▋        | 98/564 [01:18<05:40,  1.37it/s]                                                {'loss': 0.6022, 'grad_norm': 5.44241721120257, 'learning_rate': 1.7074954296160877e-06, 'epoch': 0.52}
 17%|█▋        | 98/564 [01:18<05:40,  1.37it/s] 18%|█▊        | 99/564 [01:19<05:22,  1.44it/s]                                                {'loss': 0.6428, 'grad_norm': 6.049024359822165, 'learning_rate': 1.7038391224862887e-06, 'epoch': 0.53}
 18%|█▊        | 99/564 [01:19<05:22,  1.44it/s] 18%|█▊        | 100/564 [01:20<05:08,  1.50it/s]                                                 {'loss': 0.6493, 'grad_norm': 5.696020952327468, 'learning_rate': 1.70018281535649e-06, 'epoch': 0.53}
 18%|█▊        | 100/564 [01:20<05:08,  1.50it/s] 18%|█▊        | 101/564 [01:20<04:59,  1.55it/s]                                                 {'loss': 0.5054, 'grad_norm': 6.291776223209915, 'learning_rate': 1.6965265082266908e-06, 'epoch': 0.54}
 18%|█▊        | 101/564 [01:20<04:59,  1.55it/s] 18%|█▊        | 102/564 [01:21<05:23,  1.43it/s]                                                 {'loss': 0.5313, 'grad_norm': 6.426948873329298, 'learning_rate': 1.692870201096892e-06, 'epoch': 0.54}
 18%|█▊        | 102/564 [01:21<05:23,  1.43it/s] 18%|█▊        | 103/564 [01:22<05:08,  1.50it/s]                                                 {'loss': 0.5475, 'grad_norm': 5.851604499117154, 'learning_rate': 1.6892138939670933e-06, 'epoch': 0.55}
 18%|█▊        | 103/564 [01:22<05:08,  1.50it/s] 18%|█▊        | 104/564 [01:23<05:28,  1.40it/s]                                                 {'loss': 0.5774, 'grad_norm': 6.461211649174201, 'learning_rate': 1.6855575868372943e-06, 'epoch': 0.55}
 18%|█▊        | 104/564 [01:23<05:28,  1.40it/s] 19%|█▊        | 105/564 [01:23<05:44,  1.33it/s]                                                 {'loss': 0.5443, 'grad_norm': 5.901440134244839, 'learning_rate': 1.6819012797074953e-06, 'epoch': 0.56}
 19%|█▊        | 105/564 [01:23<05:44,  1.33it/s] 19%|█▉        | 106/564 [01:24<05:22,  1.42it/s]                                                 {'loss': 0.4674, 'grad_norm': 4.888071126017875, 'learning_rate': 1.6782449725776963e-06, 'epoch': 0.56}
 19%|█▉        | 106/564 [01:24<05:22,  1.42it/s] 19%|█▉        | 107/564 [01:25<05:36,  1.36it/s]                                                 {'loss': 0.7501, 'grad_norm': 6.134714163120406, 'learning_rate': 1.6745886654478976e-06, 'epoch': 0.57}
 19%|█▉        | 107/564 [01:25<05:36,  1.36it/s] 19%|█▉        | 108/564 [01:26<05:46,  1.32it/s]                                                 {'loss': 0.7031, 'grad_norm': 6.01062907524935, 'learning_rate': 1.6709323583180986e-06, 'epoch': 0.57}
 19%|█▉        | 108/564 [01:26<05:46,  1.32it/s] 19%|█▉        | 109/564 [01:26<05:55,  1.28it/s]                                                 {'loss': 0.6376, 'grad_norm': 6.6282063075746915, 'learning_rate': 1.6672760511882998e-06, 'epoch': 0.58}
 19%|█▉        | 109/564 [01:26<05:55,  1.28it/s] 20%|█▉        | 110/564 [01:27<05:29,  1.38it/s]                                                 {'loss': 0.4849, 'grad_norm': 6.497787307500317, 'learning_rate': 1.6636197440585008e-06, 'epoch': 0.59}
 20%|█▉        | 110/564 [01:27<05:29,  1.38it/s] 20%|█▉        | 111/564 [01:28<05:42,  1.32it/s]                                                 {'loss': 0.5773, 'grad_norm': 5.1350145083994265, 'learning_rate': 1.6599634369287019e-06, 'epoch': 0.59}
 20%|█▉        | 111/564 [01:28<05:42,  1.32it/s] 20%|█▉        | 112/564 [01:29<05:50,  1.29it/s]                                                 {'loss': 0.5373, 'grad_norm': 6.215100747630921, 'learning_rate': 1.6563071297989031e-06, 'epoch': 0.6}
 20%|█▉        | 112/564 [01:29<05:50,  1.29it/s] 20%|██        | 113/564 [01:29<05:30,  1.36it/s]                                                 {'loss': 0.6119, 'grad_norm': 6.798233793237987, 'learning_rate': 1.6526508226691041e-06, 'epoch': 0.6}
 20%|██        | 113/564 [01:29<05:30,  1.36it/s] 20%|██        | 114/564 [01:30<05:11,  1.44it/s]                                                 {'loss': 0.679, 'grad_norm': 6.8914347747690075, 'learning_rate': 1.6489945155393052e-06, 'epoch': 0.61}
 20%|██        | 114/564 [01:30<05:11,  1.44it/s] 20%|██        | 115/564 [01:31<04:58,  1.50it/s]                                                 {'loss': 0.6153, 'grad_norm': 6.623876384914948, 'learning_rate': 1.6453382084095064e-06, 'epoch': 0.61}
 20%|██        | 115/564 [01:31<04:58,  1.50it/s] 21%|██        | 116/564 [01:31<04:50,  1.54it/s]                                                 {'loss': 0.7169, 'grad_norm': 6.583704262428754, 'learning_rate': 1.6416819012797074e-06, 'epoch': 0.62}
 21%|██        | 116/564 [01:31<04:50,  1.54it/s] 21%|██        | 117/564 [01:32<05:16,  1.41it/s]                                                 {'loss': 0.6626, 'grad_norm': 6.174954508119542, 'learning_rate': 1.6380255941499086e-06, 'epoch': 0.62}
 21%|██        | 117/564 [01:32<05:16,  1.41it/s] 21%|██        | 118/564 [01:33<05:30,  1.35it/s]                                                 {'loss': 0.5843, 'grad_norm': 5.364862492621586, 'learning_rate': 1.6343692870201097e-06, 'epoch': 0.63}
 21%|██        | 118/564 [01:33<05:30,  1.35it/s] 21%|██        | 119/564 [01:34<05:39,  1.31it/s]                                                 {'loss': 0.4324, 'grad_norm': 6.641981169811066, 'learning_rate': 1.6307129798903107e-06, 'epoch': 0.63}
 21%|██        | 119/564 [01:34<05:39,  1.31it/s] 21%|██▏       | 120/564 [01:34<05:18,  1.39it/s]                                                 {'loss': 0.7956, 'grad_norm': 6.696825700177417, 'learning_rate': 1.6270566727605117e-06, 'epoch': 0.64}
 21%|██▏       | 120/564 [01:34<05:18,  1.39it/s] 21%|██▏       | 121/564 [01:35<05:31,  1.34it/s]                                                 {'loss': 0.5821, 'grad_norm': 7.904428485102853, 'learning_rate': 1.623400365630713e-06, 'epoch': 0.64}
 21%|██▏       | 121/564 [01:35<05:31,  1.34it/s] 22%|██▏       | 122/564 [01:36<05:10,  1.42it/s]                                                 {'loss': 0.4873, 'grad_norm': 5.785851336905551, 'learning_rate': 1.6197440585009142e-06, 'epoch': 0.65}
 22%|██▏       | 122/564 [01:36<05:10,  1.42it/s] 22%|██▏       | 123/564 [01:36<05:26,  1.35it/s]                                                 {'loss': 0.5264, 'grad_norm': 5.683830024056525, 'learning_rate': 1.616087751371115e-06, 'epoch': 0.65}
 22%|██▏       | 123/564 [01:36<05:26,  1.35it/s] 22%|██▏       | 124/564 [01:37<05:08,  1.43it/s]                                                 {'loss': 0.5488, 'grad_norm': 6.727566143145708, 'learning_rate': 1.6124314442413162e-06, 'epoch': 0.66}
 22%|██▏       | 124/564 [01:37<05:08,  1.43it/s] 22%|██▏       | 125/564 [01:38<04:54,  1.49it/s]                                                 {'loss': 0.636, 'grad_norm': 5.125610022931198, 'learning_rate': 1.6087751371115173e-06, 'epoch': 0.66}
 22%|██▏       | 125/564 [01:38<04:54,  1.49it/s] 22%|██▏       | 126/564 [01:38<05:15,  1.39it/s]                                                 {'loss': 0.8535, 'grad_norm': 7.415979614986869, 'learning_rate': 1.6051188299817185e-06, 'epoch': 0.67}
 22%|██▏       | 126/564 [01:38<05:15,  1.39it/s] 23%|██▎       | 127/564 [01:39<05:27,  1.33it/s]                                                 {'loss': 0.5332, 'grad_norm': 6.777289783211665, 'learning_rate': 1.6014625228519193e-06, 'epoch': 0.68}
 23%|██▎       | 127/564 [01:39<05:27,  1.33it/s] 23%|██▎       | 128/564 [01:40<05:38,  1.29it/s]                                                 {'loss': 0.6666, 'grad_norm': 8.103756164942128, 'learning_rate': 1.5978062157221205e-06, 'epoch': 0.68}
 23%|██▎       | 128/564 [01:40<05:38,  1.29it/s] 23%|██▎       | 129/564 [01:41<05:43,  1.27it/s]                                                 {'loss': 0.6528, 'grad_norm': 6.276066970311756, 'learning_rate': 1.5941499085923218e-06, 'epoch': 0.69}
 23%|██▎       | 129/564 [01:41<05:43,  1.27it/s] 23%|██▎       | 130/564 [01:42<05:17,  1.37it/s]                                                 {'loss': 0.6416, 'grad_norm': 7.047252768259384, 'learning_rate': 1.5904936014625228e-06, 'epoch': 0.69}
 23%|██▎       | 130/564 [01:42<05:17,  1.37it/s] 23%|██▎       | 131/564 [01:42<04:59,  1.45it/s]                                                 {'loss': 0.4576, 'grad_norm': 5.600676396054604, 'learning_rate': 1.586837294332724e-06, 'epoch': 0.7}
 23%|██▎       | 131/564 [01:42<04:59,  1.45it/s] 23%|██▎       | 132/564 [01:43<05:11,  1.39it/s]                                                 {'loss': 0.5844, 'grad_norm': 7.135294420368595, 'learning_rate': 1.5831809872029248e-06, 'epoch': 0.7}
 23%|██▎       | 132/564 [01:43<05:11,  1.39it/s] 24%|██▎       | 133/564 [01:44<05:24,  1.33it/s]                                                 {'loss': 0.6052, 'grad_norm': 6.162520913320314, 'learning_rate': 1.579524680073126e-06, 'epoch': 0.71}
 24%|██▎       | 133/564 [01:44<05:24,  1.33it/s] 24%|██▍       | 134/564 [01:44<05:03,  1.42it/s]                                                 {'loss': 0.6374, 'grad_norm': 7.3076451371958235, 'learning_rate': 1.5758683729433271e-06, 'epoch': 0.71}
 24%|██▍       | 134/564 [01:44<05:03,  1.42it/s] 24%|██▍       | 135/564 [01:45<04:49,  1.48it/s]                                                 {'loss': 0.6747, 'grad_norm': 5.913127743247372, 'learning_rate': 1.5722120658135283e-06, 'epoch': 0.72}
 24%|██▍       | 135/564 [01:45<04:49,  1.48it/s] 24%|██▍       | 136/564 [01:46<04:38,  1.53it/s]                                                 {'loss': 0.5698, 'grad_norm': 5.091695448120692, 'learning_rate': 1.5685557586837294e-06, 'epoch': 0.72}
 24%|██▍       | 136/564 [01:46<04:38,  1.53it/s] 24%|██▍       | 137/564 [01:46<04:58,  1.43it/s]                                                 {'loss': 0.511, 'grad_norm': 5.1895215824390855, 'learning_rate': 1.5648994515539304e-06, 'epoch': 0.73}
 24%|██▍       | 137/564 [01:46<04:58,  1.43it/s] 24%|██▍       | 138/564 [01:47<05:13,  1.36it/s]                                                 {'loss': 0.5194, 'grad_norm': 5.686935570359544, 'learning_rate': 1.5612431444241316e-06, 'epoch': 0.73}
 24%|██▍       | 138/564 [01:47<05:13,  1.36it/s] 25%|██▍       | 139/564 [01:48<05:24,  1.31it/s]                                                 {'loss': 0.719, 'grad_norm': 7.096947302796814, 'learning_rate': 1.5575868372943326e-06, 'epoch': 0.74}
 25%|██▍       | 139/564 [01:48<05:24,  1.31it/s] 25%|██▍       | 140/564 [01:49<05:05,  1.39it/s]                                                 {'loss': 0.5677, 'grad_norm': 5.415270646675324, 'learning_rate': 1.5539305301645339e-06, 'epoch': 0.74}
 25%|██▍       | 140/564 [01:49<05:05,  1.39it/s] 25%|██▌       | 141/564 [01:49<05:16,  1.33it/s]                                                 {'loss': 0.7211, 'grad_norm': 7.2236937989161145, 'learning_rate': 1.5502742230347347e-06, 'epoch': 0.75}
 25%|██▌       | 141/564 [01:49<05:16,  1.33it/s] 25%|██▌       | 142/564 [01:50<05:18,  1.32it/s]                                                 {'loss': 0.597, 'grad_norm': 6.532854815362072, 'learning_rate': 1.546617915904936e-06, 'epoch': 0.76}
 25%|██▌       | 142/564 [01:50<05:18,  1.32it/s] 25%|██▌       | 143/564 [01:51<05:05,  1.38it/s]                                                 {'loss': 0.5229, 'grad_norm': 5.091174168394867, 'learning_rate': 1.5429616087751372e-06, 'epoch': 0.76}
 25%|██▌       | 143/564 [01:51<05:05,  1.38it/s] 26%|██▌       | 144/564 [01:52<04:51,  1.44it/s]                                                 {'loss': 0.5746, 'grad_norm': 5.398522629080138, 'learning_rate': 1.5393053016453382e-06, 'epoch': 0.77}
 26%|██▌       | 144/564 [01:52<04:51,  1.44it/s] 26%|██▌       | 145/564 [01:52<05:08,  1.36it/s]                                                 {'loss': 0.6737, 'grad_norm': 6.763706062115368, 'learning_rate': 1.5356489945155392e-06, 'epoch': 0.77}
 26%|██▌       | 145/564 [01:52<05:08,  1.36it/s] 26%|██▌       | 146/564 [01:53<05:18,  1.31it/s]                                                 {'loss': 0.62, 'grad_norm': 6.4124784196219835, 'learning_rate': 1.5319926873857402e-06, 'epoch': 0.78}
 26%|██▌       | 146/564 [01:53<05:18,  1.31it/s] 26%|██▌       | 147/564 [01:54<05:26,  1.28it/s]                                                 {'loss': 0.4976, 'grad_norm': 5.391887977172145, 'learning_rate': 1.5283363802559415e-06, 'epoch': 0.78}
 26%|██▌       | 147/564 [01:54<05:26,  1.28it/s] 26%|██▌       | 148/564 [01:55<05:21,  1.29it/s]                                                 {'loss': 0.6111, 'grad_norm': 5.9509853925243785, 'learning_rate': 1.5246800731261425e-06, 'epoch': 0.79}
 26%|██▌       | 148/564 [01:55<05:21,  1.29it/s] 26%|██▋       | 149/564 [01:56<05:27,  1.27it/s]                                                 {'loss': 0.6683, 'grad_norm': 5.603060377952301, 'learning_rate': 1.5210237659963437e-06, 'epoch': 0.79}
 26%|██▋       | 149/564 [01:56<05:27,  1.27it/s] 27%|██▋       | 150/564 [01:56<05:04,  1.36it/s]                                                 {'loss': 0.5488, 'grad_norm': 5.947776250710277, 'learning_rate': 1.5173674588665448e-06, 'epoch': 0.8}
 27%|██▋       | 150/564 [01:56<05:04,  1.36it/s] 27%|██▋       | 151/564 [01:57<04:49,  1.43it/s]                                                 {'loss': 0.6311, 'grad_norm': 5.311036329829682, 'learning_rate': 1.5137111517367458e-06, 'epoch': 0.8}
 27%|██▋       | 151/564 [01:57<04:49,  1.43it/s] 27%|██▋       | 152/564 [01:58<05:04,  1.35it/s]                                                 {'loss': 0.6559, 'grad_norm': 6.704632433179601, 'learning_rate': 1.510054844606947e-06, 'epoch': 0.81}
 27%|██▋       | 152/564 [01:58<05:04,  1.35it/s] 27%|██▋       | 153/564 [01:58<05:10,  1.33it/s]                                                 {'loss': 0.4473, 'grad_norm': 6.097515831910383, 'learning_rate': 1.506398537477148e-06, 'epoch': 0.81}
 27%|██▋       | 153/564 [01:58<05:10,  1.33it/s] 27%|██▋       | 154/564 [01:59<05:05,  1.34it/s]                                                 {'loss': 0.4764, 'grad_norm': 4.830023659359645, 'learning_rate': 1.502742230347349e-06, 'epoch': 0.82}
 27%|██▋       | 154/564 [01:59<05:05,  1.34it/s] 27%|██▋       | 155/564 [02:00<04:46,  1.43it/s]                                                 {'loss': 0.7332, 'grad_norm': 6.477704060618464, 'learning_rate': 1.49908592321755e-06, 'epoch': 0.82}
 27%|██▋       | 155/564 [02:00<04:46,  1.43it/s] 28%|██▊       | 156/564 [02:00<04:33,  1.49it/s]                                                 {'loss': 0.775, 'grad_norm': 6.575339789493284, 'learning_rate': 1.4954296160877513e-06, 'epoch': 0.83}
 28%|██▊       | 156/564 [02:00<04:33,  1.49it/s] 28%|██▊       | 157/564 [02:01<04:24,  1.54it/s]                                                 {'loss': 0.6013, 'grad_norm': 5.084639992944748, 'learning_rate': 1.4917733089579526e-06, 'epoch': 0.84}
 28%|██▊       | 157/564 [02:01<04:24,  1.54it/s] 28%|██▊       | 158/564 [02:02<04:38,  1.46it/s]                                                 {'loss': 0.5762, 'grad_norm': 5.551116834508123, 'learning_rate': 1.4881170018281536e-06, 'epoch': 0.84}
 28%|██▊       | 158/564 [02:02<04:38,  1.46it/s] 28%|██▊       | 159/564 [02:02<04:27,  1.52it/s]                                                 {'loss': 0.5172, 'grad_norm': 7.0780772279990645, 'learning_rate': 1.4844606946983546e-06, 'epoch': 0.85}
 28%|██▊       | 159/564 [02:02<04:27,  1.52it/s] 28%|██▊       | 160/564 [02:03<04:22,  1.54it/s]                                                 {'loss': 0.5093, 'grad_norm': 5.640927839424567, 'learning_rate': 1.4808043875685556e-06, 'epoch': 0.85}
 28%|██▊       | 160/564 [02:03<04:22,  1.54it/s] 29%|██▊       | 161/564 [02:04<04:17,  1.56it/s]                                                 {'loss': 0.5398, 'grad_norm': 6.66821184622434, 'learning_rate': 1.4771480804387569e-06, 'epoch': 0.86}
 29%|██▊       | 161/564 [02:04<04:17,  1.56it/s] 29%|██▊       | 162/564 [02:04<04:41,  1.43it/s]                                                 {'loss': 0.3882, 'grad_norm': 5.266826606543413, 'learning_rate': 1.4734917733089579e-06, 'epoch': 0.86}
 29%|██▊       | 162/564 [02:04<04:41,  1.43it/s] 29%|██▉       | 163/564 [02:05<05:02,  1.33it/s]                                                 {'loss': 0.5295, 'grad_norm': 6.129819633368761, 'learning_rate': 1.469835466179159e-06, 'epoch': 0.87}
 29%|██▉       | 163/564 [02:05<05:02,  1.33it/s] 29%|██▉       | 164/564 [02:06<05:14,  1.27it/s]                                                 {'loss': 0.5538, 'grad_norm': 6.096451662679806, 'learning_rate': 1.4661791590493601e-06, 'epoch': 0.87}
 29%|██▉       | 164/564 [02:06<05:14,  1.27it/s] 29%|██▉       | 165/564 [02:07<05:19,  1.25it/s]                                                 {'loss': 0.5159, 'grad_norm': 6.706803836267644, 'learning_rate': 1.4625228519195612e-06, 'epoch': 0.88}
 29%|██▉       | 165/564 [02:07<05:19,  1.25it/s] 29%|██▉       | 166/564 [02:08<05:28,  1.21it/s]                                                 {'loss': 0.6869, 'grad_norm': 6.893320917323689, 'learning_rate': 1.4588665447897624e-06, 'epoch': 0.88}
 29%|██▉       | 166/564 [02:08<05:28,  1.21it/s] 30%|██▉       | 167/564 [02:09<05:14,  1.26it/s]                                                 {'loss': 0.4755, 'grad_norm': 5.935894991364712, 'learning_rate': 1.4552102376599632e-06, 'epoch': 0.89}
 30%|██▉       | 167/564 [02:09<05:14,  1.26it/s] 30%|██▉       | 168/564 [02:09<05:23,  1.22it/s]                                                 {'loss': 0.6767, 'grad_norm': 6.721785364598125, 'learning_rate': 1.4515539305301644e-06, 'epoch': 0.89}
 30%|██▉       | 168/564 [02:09<05:23,  1.22it/s] 30%|██▉       | 169/564 [02:10<05:11,  1.27it/s]                                                 {'loss': 0.5168, 'grad_norm': 6.294217455327802, 'learning_rate': 1.4478976234003655e-06, 'epoch': 0.9}
 30%|██▉       | 169/564 [02:10<05:11,  1.27it/s] 30%|███       | 170/564 [02:11<05:02,  1.30it/s]                                                 {'loss': 0.6998, 'grad_norm': 6.459315933024051, 'learning_rate': 1.4442413162705667e-06, 'epoch': 0.9}
 30%|███       | 170/564 [02:11<05:02,  1.30it/s] 30%|███       | 171/564 [02:12<05:15,  1.25it/s]                                                 {'loss': 0.4727, 'grad_norm': 5.689584873485658, 'learning_rate': 1.440585009140768e-06, 'epoch': 0.91}
 30%|███       | 171/564 [02:12<05:15,  1.25it/s] 30%|███       | 172/564 [02:13<05:17,  1.24it/s]                                                 {'loss': 0.7369, 'grad_norm': 6.255625074416652, 'learning_rate': 1.4369287020109688e-06, 'epoch': 0.91}
 30%|███       | 172/564 [02:13<05:17,  1.24it/s] 31%|███       | 173/564 [02:13<05:23,  1.21it/s]                                                 {'loss': 0.6433, 'grad_norm': 6.301091511114213, 'learning_rate': 1.43327239488117e-06, 'epoch': 0.92}
 31%|███       | 173/564 [02:13<05:23,  1.21it/s] 31%|███       | 174/564 [02:14<05:27,  1.19it/s]                                                 {'loss': 0.4697, 'grad_norm': 5.173746473362815, 'learning_rate': 1.429616087751371e-06, 'epoch': 0.93}
 31%|███       | 174/564 [02:14<05:27,  1.19it/s] 31%|███       | 175/564 [02:15<05:29,  1.18it/s]                                                 {'loss': 0.476, 'grad_norm': 6.114324940360039, 'learning_rate': 1.4259597806215722e-06, 'epoch': 0.93}
 31%|███       | 175/564 [02:15<05:29,  1.18it/s] 31%|███       | 176/564 [02:16<05:24,  1.20it/s]                                                 {'loss': 0.6057, 'grad_norm': 7.888823232211539, 'learning_rate': 1.422303473491773e-06, 'epoch': 0.94}
 31%|███       | 176/564 [02:16<05:24,  1.20it/s] 31%|███▏      | 177/564 [02:17<05:28,  1.18it/s]                                                 {'loss': 0.577, 'grad_norm': 4.909747172787389, 'learning_rate': 1.4186471663619743e-06, 'epoch': 0.94}
 31%|███▏      | 177/564 [02:17<05:28,  1.18it/s] 32%|███▏      | 178/564 [02:18<05:29,  1.17it/s]                                                 {'loss': 0.7042, 'grad_norm': 7.023236800038048, 'learning_rate': 1.4149908592321755e-06, 'epoch': 0.95}
 32%|███▏      | 178/564 [02:18<05:29,  1.17it/s] 32%|███▏      | 179/564 [02:19<05:23,  1.19it/s]                                                 {'loss': 0.5897, 'grad_norm': 6.917795809435846, 'learning_rate': 1.4113345521023766e-06, 'epoch': 0.95}
 32%|███▏      | 179/564 [02:19<05:23,  1.19it/s] 32%|███▏      | 180/564 [02:19<05:07,  1.25it/s]                                                 {'loss': 0.5992, 'grad_norm': 5.689641175601461, 'learning_rate': 1.4076782449725778e-06, 'epoch': 0.96}
 32%|███▏      | 180/564 [02:19<05:07,  1.25it/s] 32%|███▏      | 181/564 [02:20<04:57,  1.29it/s]                                                 {'loss': 0.6185, 'grad_norm': 5.361738369003182, 'learning_rate': 1.4040219378427786e-06, 'epoch': 0.96}
 32%|███▏      | 181/564 [02:20<04:57,  1.29it/s] 32%|███▏      | 182/564 [02:21<05:06,  1.25it/s]                                                 {'loss': 0.558, 'grad_norm': 6.128397649031835, 'learning_rate': 1.4003656307129798e-06, 'epoch': 0.97}
 32%|███▏      | 182/564 [02:21<05:06,  1.25it/s] 32%|███▏      | 183/564 [02:22<04:56,  1.29it/s]                                                 {'loss': 0.6596, 'grad_norm': 6.197278619101124, 'learning_rate': 1.3967093235831809e-06, 'epoch': 0.97}
 32%|███▏      | 183/564 [02:22<04:56,  1.29it/s] 33%|███▎      | 184/564 [02:22<04:45,  1.33it/s]                                                 {'loss': 0.8256, 'grad_norm': 7.72812581607853, 'learning_rate': 1.393053016453382e-06, 'epoch': 0.98}
 33%|███▎      | 184/564 [02:22<04:45,  1.33it/s] 33%|███▎      | 185/564 [02:23<04:52,  1.29it/s]                                                 {'loss': 0.6284, 'grad_norm': 5.635618280359261, 'learning_rate': 1.3893967093235831e-06, 'epoch': 0.98}
 33%|███▎      | 185/564 [02:23<04:52,  1.29it/s] 33%|███▎      | 186/564 [02:24<04:51,  1.30it/s]                                                 {'loss': 0.4967, 'grad_norm': 5.355122658431933, 'learning_rate': 1.3857404021937841e-06, 'epoch': 0.99}
 33%|███▎      | 186/564 [02:24<04:51,  1.30it/s] 33%|███▎      | 187/564 [02:25<04:37,  1.36it/s]                                                 {'loss': 0.6885, 'grad_norm': 6.406551635596405, 'learning_rate': 1.3820840950639854e-06, 'epoch': 0.99}
 33%|███▎      | 187/564 [02:25<04:37,  1.36it/s] 33%|███▎      | 188/564 [02:25<04:27,  1.40it/s]                                                 {'loss': 0.5627, 'grad_norm': 5.646673415948038, 'learning_rate': 1.3784277879341864e-06, 'epoch': 1.0}
 33%|███▎      | 188/564 [02:25<04:27,  1.40it/s] 34%|███▎      | 189/564 [02:26<04:45,  1.31it/s]                                                 {'loss': 0.5706, 'grad_norm': 7.583614014797978, 'learning_rate': 1.3747714808043876e-06, 'epoch': 1.01}
 34%|███▎      | 189/564 [02:26<04:45,  1.31it/s] 34%|███▎      | 190/564 [02:27<04:33,  1.37it/s]                                                 {'loss': 0.4055, 'grad_norm': 5.621532687100521, 'learning_rate': 1.3711151736745884e-06, 'epoch': 1.01}
 34%|███▎      | 190/564 [02:27<04:33,  1.37it/s] 34%|███▍      | 191/564 [02:28<04:45,  1.31it/s]                                                 {'loss': 0.4438, 'grad_norm': 4.937865316585051, 'learning_rate': 1.3674588665447897e-06, 'epoch': 1.02}
 34%|███▍      | 191/564 [02:28<04:45,  1.31it/s] 34%|███▍      | 192/564 [02:28<04:32,  1.37it/s]                                                 {'loss': 0.5197, 'grad_norm': 4.5059748451372315, 'learning_rate': 1.363802559414991e-06, 'epoch': 1.02}
 34%|███▍      | 192/564 [02:28<04:32,  1.37it/s] 34%|███▍      | 193/564 [02:29<04:23,  1.41it/s]                                                 {'loss': 0.4366, 'grad_norm': 4.7008441398769385, 'learning_rate': 1.360146252285192e-06, 'epoch': 1.03}
 34%|███▍      | 193/564 [02:29<04:23,  1.41it/s] 34%|███▍      | 194/564 [02:30<04:37,  1.33it/s]                                                 {'loss': 0.5963, 'grad_norm': 4.653036513692104, 'learning_rate': 1.356489945155393e-06, 'epoch': 1.03}
 34%|███▍      | 194/564 [02:30<04:37,  1.33it/s] 35%|███▍      | 195/564 [02:31<04:45,  1.29it/s]                                                 {'loss': 0.4238, 'grad_norm': 4.586519199681013, 'learning_rate': 1.352833638025594e-06, 'epoch': 1.04}
 35%|███▍      | 195/564 [02:31<04:45,  1.29it/s] 35%|███▍      | 196/564 [02:31<04:57,  1.24it/s]                                                 {'loss': 0.4183, 'grad_norm': 5.146947387750939, 'learning_rate': 1.3491773308957952e-06, 'epoch': 1.04}
 35%|███▍      | 196/564 [02:31<04:57,  1.24it/s] 35%|███▍      | 197/564 [02:32<04:50,  1.26it/s]                                                 {'loss': 0.4374, 'grad_norm': 4.542628441759825, 'learning_rate': 1.3455210237659962e-06, 'epoch': 1.05}
 35%|███▍      | 197/564 [02:32<04:50,  1.26it/s] 35%|███▌      | 198/564 [02:33<04:59,  1.22it/s]                                                 {'loss': 0.4121, 'grad_norm': 4.169400592084772, 'learning_rate': 1.3418647166361975e-06, 'epoch': 1.05}
 35%|███▌      | 198/564 [02:33<04:59,  1.22it/s] 35%|███▌      | 199/564 [02:34<05:01,  1.21it/s]                                                 {'loss': 0.5867, 'grad_norm': 4.462665826459139, 'learning_rate': 1.3382084095063985e-06, 'epoch': 1.06}
 35%|███▌      | 199/564 [02:34<05:01,  1.21it/s] 35%|███▌      | 200/564 [02:35<05:07,  1.18it/s]                                                 {'loss': 0.482, 'grad_norm': 5.441070734308827, 'learning_rate': 1.3345521023765995e-06, 'epoch': 1.06}
 35%|███▌      | 200/564 [02:35<05:07,  1.18it/s] 36%|███▌      | 201/564 [02:36<04:58,  1.22it/s]                                                 {'loss': 0.4833, 'grad_norm': 5.178359068634962, 'learning_rate': 1.3308957952468008e-06, 'epoch': 1.07}
 36%|███▌      | 201/564 [02:36<04:58,  1.22it/s] 36%|███▌      | 202/564 [02:36<05:05,  1.18it/s]                                                 {'loss': 0.5108, 'grad_norm': 5.246949589590615, 'learning_rate': 1.3272394881170018e-06, 'epoch': 1.07}
 36%|███▌      | 202/564 [02:36<05:05,  1.18it/s] 36%|███▌      | 203/564 [02:38<05:55,  1.02it/s]                                                 {'loss': 0.3949, 'grad_norm': 4.209916772508979, 'learning_rate': 1.3235831809872028e-06, 'epoch': 1.08}
 36%|███▌      | 203/564 [02:38<05:55,  1.02it/s] 36%|███▌      | 204/564 [02:39<05:31,  1.09it/s]                                                 {'loss': 0.3663, 'grad_norm': 4.95845463680927, 'learning_rate': 1.3199268738574038e-06, 'epoch': 1.09}
 36%|███▌      | 204/564 [02:39<05:31,  1.09it/s] 36%|███▋      | 205/564 [02:39<05:17,  1.13it/s]                                                 {'loss': 0.3982, 'grad_norm': 4.23544773473815, 'learning_rate': 1.316270566727605e-06, 'epoch': 1.09}
 36%|███▋      | 205/564 [02:39<05:17,  1.13it/s] 37%|███▋      | 206/564 [02:40<05:08,  1.16it/s]                                                 {'loss': 0.4447, 'grad_norm': 4.1172860431694165, 'learning_rate': 1.3126142595978063e-06, 'epoch': 1.1}
 37%|███▋      | 206/564 [02:40<05:08,  1.16it/s] 37%|███▋      | 207/564 [02:41<05:02,  1.18it/s]                                                 {'loss': 0.4548, 'grad_norm': 4.479059143231123, 'learning_rate': 1.3089579524680071e-06, 'epoch': 1.1}
 37%|███▋      | 207/564 [02:41<05:02,  1.18it/s] 37%|███▋      | 208/564 [02:42<04:51,  1.22it/s]                                                 {'loss': 0.4031, 'grad_norm': 4.8967174552250965, 'learning_rate': 1.3053016453382084e-06, 'epoch': 1.11}
 37%|███▋      | 208/564 [02:42<04:51,  1.22it/s] 37%|███▋      | 209/564 [02:43<04:59,  1.19it/s]                                                 {'loss': 0.5567, 'grad_norm': 5.732329768346301, 'learning_rate': 1.3016453382084094e-06, 'epoch': 1.11}
 37%|███▋      | 209/564 [02:43<04:59,  1.19it/s] 37%|███▋      | 210/564 [02:43<04:50,  1.22it/s]                                                 {'loss': 0.3962, 'grad_norm': 4.809277798604976, 'learning_rate': 1.2979890310786106e-06, 'epoch': 1.12}
 37%|███▋      | 210/564 [02:43<04:50,  1.22it/s] 37%|███▋      | 211/564 [02:44<04:59,  1.18it/s]                                                 {'loss': 0.5119, 'grad_norm': 4.8444609746857985, 'learning_rate': 1.2943327239488116e-06, 'epoch': 1.12}
 37%|███▋      | 211/564 [02:44<04:59,  1.18it/s] 38%|███▊      | 212/564 [02:45<04:50,  1.21it/s]                                                 {'loss': 0.4764, 'grad_norm': 4.3949305636324, 'learning_rate': 1.2906764168190127e-06, 'epoch': 1.13}
 38%|███▊      | 212/564 [02:45<04:50,  1.21it/s] 38%|███▊      | 213/564 [02:47<06:26,  1.10s/it]                                                 {'loss': 0.4388, 'grad_norm': 5.412814693936469, 'learning_rate': 1.2870201096892139e-06, 'epoch': 1.13}
 38%|███▊      | 213/564 [02:47<06:26,  1.10s/it] 38%|███▊      | 214/564 [02:48<05:56,  1.02s/it]                                                 {'loss': 0.4087, 'grad_norm': 4.968145731078966, 'learning_rate': 1.283363802559415e-06, 'epoch': 1.14}
 38%|███▊      | 214/564 [02:48<05:56,  1.02s/it] 38%|███▊      | 215/564 [02:48<05:35,  1.04it/s]                                                 {'loss': 0.4864, 'grad_norm': 5.006954254760422, 'learning_rate': 1.2797074954296162e-06, 'epoch': 1.14}
 38%|███▊      | 215/564 [02:48<05:35,  1.04it/s] 38%|███▊      | 216/564 [02:49<05:05,  1.14it/s]                                                 {'loss': 0.4366, 'grad_norm': 4.740324257002188, 'learning_rate': 1.276051188299817e-06, 'epoch': 1.15}
 38%|███▊      | 216/564 [02:49<05:05,  1.14it/s] 38%|███▊      | 217/564 [02:50<04:52,  1.19it/s]                                                 {'loss': 0.4358, 'grad_norm': 5.580047954776196, 'learning_rate': 1.2723948811700182e-06, 'epoch': 1.15}
 38%|███▊      | 217/564 [02:50<04:52,  1.19it/s] 39%|███▊      | 218/564 [02:51<04:26,  1.30it/s]                                                 {'loss': 0.3675, 'grad_norm': 4.813878642287608, 'learning_rate': 1.2687385740402192e-06, 'epoch': 1.16}
 39%|███▊      | 218/564 [02:51<04:26,  1.30it/s] 39%|███▉      | 219/564 [02:51<04:10,  1.38it/s]                                                 {'loss': 0.5113, 'grad_norm': 5.012359266198857, 'learning_rate': 1.2650822669104205e-06, 'epoch': 1.16}
 39%|███▉      | 219/564 [02:51<04:10,  1.38it/s] 39%|███▉      | 220/564 [02:52<03:57,  1.45it/s]                                                 {'loss': 0.4341, 'grad_norm': 5.021889162201749, 'learning_rate': 1.2614259597806217e-06, 'epoch': 1.17}
 39%|███▉      | 220/564 [02:52<03:57,  1.45it/s] 39%|███▉      | 221/564 [02:53<04:11,  1.37it/s]                                                 {'loss': 0.4706, 'grad_norm': 4.78433764307538, 'learning_rate': 1.2577696526508225e-06, 'epoch': 1.18}
 39%|███▉      | 221/564 [02:53<04:11,  1.37it/s] 39%|███▉      | 222/564 [02:53<04:16,  1.34it/s]                                                 {'loss': 0.49, 'grad_norm': 4.498612396729927, 'learning_rate': 1.2541133455210237e-06, 'epoch': 1.18}
 39%|███▉      | 222/564 [02:53<04:16,  1.34it/s] 40%|███▉      | 223/564 [02:54<03:59,  1.42it/s]                                                 {'loss': 0.3896, 'grad_norm': 5.693109510048767, 'learning_rate': 1.2504570383912248e-06, 'epoch': 1.19}
 40%|███▉      | 223/564 [02:54<03:59,  1.42it/s] 40%|███▉      | 224/564 [02:55<04:12,  1.35it/s]                                                 {'loss': 0.3572, 'grad_norm': 4.574645540095461, 'learning_rate': 1.246800731261426e-06, 'epoch': 1.19}
 40%|███▉      | 224/564 [02:55<04:12,  1.35it/s] 40%|███▉      | 225/564 [02:55<04:06,  1.38it/s]                                                 {'loss': 0.4203, 'grad_norm': 5.570500506723484, 'learning_rate': 1.2431444241316268e-06, 'epoch': 1.2}
 40%|███▉      | 225/564 [02:55<04:06,  1.38it/s] 40%|████      | 226/564 [02:57<05:10,  1.09it/s]                                                 {'loss': 0.4489, 'grad_norm': 5.11227258230203, 'learning_rate': 1.239488117001828e-06, 'epoch': 1.2}
 40%|████      | 226/564 [02:57<05:10,  1.09it/s] 40%|████      | 227/564 [02:58<04:59,  1.13it/s]                                                 {'loss': 0.3972, 'grad_norm': 4.927371014003623, 'learning_rate': 1.2358318098720293e-06, 'epoch': 1.21}
 40%|████      | 227/564 [02:58<04:59,  1.13it/s] 40%|████      | 228/564 [02:58<04:50,  1.16it/s]                                                 {'loss': 0.3669, 'grad_norm': 5.40844562054306, 'learning_rate': 1.2321755027422303e-06, 'epoch': 1.21}
 40%|████      | 228/564 [02:58<04:50,  1.16it/s] 41%|████      | 229/564 [02:59<04:37,  1.21it/s]                                                 {'loss': 0.4324, 'grad_norm': 5.200598073716274, 'learning_rate': 1.2285191956124315e-06, 'epoch': 1.22}
 41%|████      | 229/564 [02:59<04:37,  1.21it/s] 41%|████      | 230/564 [03:00<04:47,  1.16it/s]                                                 {'loss': 0.3849, 'grad_norm': 5.3401033007731895, 'learning_rate': 1.2248628884826324e-06, 'epoch': 1.22}
 41%|████      | 230/564 [03:00<04:47,  1.16it/s] 41%|████      | 231/564 [03:01<04:50,  1.14it/s]                                                 {'loss': 0.4292, 'grad_norm': 4.900253803509958, 'learning_rate': 1.2212065813528336e-06, 'epoch': 1.23}
 41%|████      | 231/564 [03:01<04:50,  1.14it/s] 41%|████      | 232/564 [03:02<04:52,  1.14it/s]                                                 {'loss': 0.3797, 'grad_norm': 4.751841503816641, 'learning_rate': 1.2175502742230346e-06, 'epoch': 1.23}
 41%|████      | 232/564 [03:02<04:52,  1.14it/s] 41%|████▏     | 233/564 [03:03<04:45,  1.16it/s]                                                 {'loss': 0.426, 'grad_norm': 5.932964819781488, 'learning_rate': 1.2138939670932358e-06, 'epoch': 1.24}
 41%|████▏     | 233/564 [03:03<04:45,  1.16it/s] 41%|████▏     | 234/564 [03:04<04:48,  1.14it/s]                                                 {'loss': 0.3665, 'grad_norm': 5.085594643366055, 'learning_rate': 1.2102376599634369e-06, 'epoch': 1.24}
 41%|████▏     | 234/564 [03:04<04:48,  1.14it/s] 42%|████▏     | 235/564 [03:04<04:35,  1.19it/s]                                                 {'loss': 0.3793, 'grad_norm': 4.969727948608078, 'learning_rate': 1.2065813528336379e-06, 'epoch': 1.25}
 42%|████▏     | 235/564 [03:04<04:35,  1.19it/s] 42%|████▏     | 236/564 [03:05<04:42,  1.16it/s]                                                 {'loss': 0.4127, 'grad_norm': 4.371668624022945, 'learning_rate': 1.2029250457038391e-06, 'epoch': 1.26}
 42%|████▏     | 236/564 [03:05<04:42,  1.16it/s] 42%|████▏     | 237/564 [03:06<04:44,  1.15it/s]                                                 {'loss': 0.422, 'grad_norm': 4.541626018764338, 'learning_rate': 1.1992687385740402e-06, 'epoch': 1.26}
 42%|████▏     | 237/564 [03:06<04:44,  1.15it/s] 42%|████▏     | 238/564 [03:07<04:33,  1.19it/s]                                                 {'loss': 0.3871, 'grad_norm': 5.568161216716745, 'learning_rate': 1.1956124314442414e-06, 'epoch': 1.27}
 42%|████▏     | 238/564 [03:07<04:33,  1.19it/s] 42%|████▏     | 239/564 [03:08<04:25,  1.22it/s]                                                 {'loss': 0.5115, 'grad_norm': 6.074237806445787, 'learning_rate': 1.1919561243144422e-06, 'epoch': 1.27}
 42%|████▏     | 239/564 [03:08<04:25,  1.22it/s] 43%|████▎     | 240/564 [03:09<04:32,  1.19it/s]                                                 {'loss': 0.3551, 'grad_norm': 4.73794053121549, 'learning_rate': 1.1882998171846434e-06, 'epoch': 1.28}
 43%|████▎     | 240/564 [03:09<04:32,  1.19it/s] 43%|████▎     | 241/564 [03:09<04:25,  1.21it/s]                                                 {'loss': 0.3329, 'grad_norm': 5.887638515506033, 'learning_rate': 1.1846435100548447e-06, 'epoch': 1.28}
 43%|████▎     | 241/564 [03:09<04:25,  1.21it/s] 43%|████▎     | 242/564 [03:11<05:54,  1.10s/it]                                                 {'loss': 0.3644, 'grad_norm': 5.451160206654742, 'learning_rate': 1.1809872029250457e-06, 'epoch': 1.29}
 43%|████▎     | 242/564 [03:11<05:54,  1.10s/it] 43%|████▎     | 243/564 [03:12<05:33,  1.04s/it]                                                 {'loss': 0.4192, 'grad_norm': 5.295345766688913, 'learning_rate': 1.1773308957952467e-06, 'epoch': 1.29}
 43%|████▎     | 243/564 [03:12<05:33,  1.04s/it] 43%|████▎     | 244/564 [03:13<04:52,  1.10it/s]                                                 {'loss': 0.391, 'grad_norm': 4.638254638525378, 'learning_rate': 1.1736745886654477e-06, 'epoch': 1.3}
 43%|████▎     | 244/564 [03:13<04:52,  1.10it/s] 43%|████▎     | 245/564 [03:14<04:43,  1.13it/s]                                                 {'loss': 0.3962, 'grad_norm': 4.335144441446559, 'learning_rate': 1.170018281535649e-06, 'epoch': 1.3}
 43%|████▎     | 245/564 [03:14<04:43,  1.13it/s] 44%|████▎     | 246/564 [03:14<04:37,  1.15it/s]                                                 {'loss': 0.3446, 'grad_norm': 4.134286405459118, 'learning_rate': 1.16636197440585e-06, 'epoch': 1.31}
 44%|████▎     | 246/564 [03:14<04:37,  1.15it/s] 44%|████▍     | 247/564 [03:15<04:33,  1.16it/s]                                                 {'loss': 0.3539, 'grad_norm': 4.643435190460634, 'learning_rate': 1.1627056672760512e-06, 'epoch': 1.31}
 44%|████▍     | 247/564 [03:15<04:33,  1.16it/s] 44%|████▍     | 248/564 [03:16<04:26,  1.18it/s]                                                 {'loss': 0.4556, 'grad_norm': 4.6075792449970185, 'learning_rate': 1.1590493601462523e-06, 'epoch': 1.32}
 44%|████▍     | 248/564 [03:16<04:26,  1.18it/s] 44%|████▍     | 249/564 [03:17<04:25,  1.19it/s]                                                 {'loss': 0.4425, 'grad_norm': 4.922158181522561, 'learning_rate': 1.1553930530164533e-06, 'epoch': 1.32}
 44%|████▍     | 249/564 [03:17<04:25,  1.19it/s] 44%|████▍     | 250/564 [03:17<04:02,  1.30it/s]                                                 {'loss': 0.4107, 'grad_norm': 5.04403118552774, 'learning_rate': 1.1517367458866545e-06, 'epoch': 1.33}
 44%|████▍     | 250/564 [03:17<04:02,  1.30it/s] 45%|████▍     | 251/564 [03:18<04:08,  1.26it/s]                                                 {'loss': 0.3624, 'grad_norm': 5.012252161207785, 'learning_rate': 1.1480804387568555e-06, 'epoch': 1.34}
 45%|████▍     | 251/564 [03:18<04:08,  1.26it/s] 45%|████▍     | 252/564 [03:19<04:09,  1.25it/s]                                                 {'loss': 0.3613, 'grad_norm': 5.865814159393271, 'learning_rate': 1.1444241316270566e-06, 'epoch': 1.34}
 45%|████▍     | 252/564 [03:19<04:09,  1.25it/s] 45%|████▍     | 253/564 [03:20<03:50,  1.35it/s]                                                 {'loss': 0.4321, 'grad_norm': 5.176316573491845, 'learning_rate': 1.1407678244972576e-06, 'epoch': 1.35}
 45%|████▍     | 253/564 [03:20<03:50,  1.35it/s] 45%|████▌     | 254/564 [03:20<03:36,  1.43it/s]                                                 {'loss': 0.5502, 'grad_norm': 5.745870153000209, 'learning_rate': 1.1371115173674588e-06, 'epoch': 1.35}
 45%|████▌     | 254/564 [03:20<03:36,  1.43it/s] 45%|████▌     | 255/564 [03:21<03:52,  1.33it/s]                                                 {'loss': 0.3444, 'grad_norm': 4.856174624427184, 'learning_rate': 1.13345521023766e-06, 'epoch': 1.36}
 45%|████▌     | 255/564 [03:21<03:52,  1.33it/s] 45%|████▌     | 256/564 [03:22<03:57,  1.29it/s]                                                 {'loss': 0.3557, 'grad_norm': 5.50984278088911, 'learning_rate': 1.1297989031078609e-06, 'epoch': 1.36}
 45%|████▌     | 256/564 [03:22<03:57,  1.29it/s] 46%|████▌     | 257/564 [03:23<03:41,  1.39it/s]                                                 {'loss': 0.3711, 'grad_norm': 4.823215005253623, 'learning_rate': 1.126142595978062e-06, 'epoch': 1.37}
 46%|████▌     | 257/564 [03:23<03:41,  1.39it/s] 46%|████▌     | 258/564 [03:23<03:31,  1.45it/s]                                                 {'loss': 0.4611, 'grad_norm': 5.358882880393085, 'learning_rate': 1.1224862888482631e-06, 'epoch': 1.37}
 46%|████▌     | 258/564 [03:23<03:31,  1.45it/s] 46%|████▌     | 259/564 [03:24<03:22,  1.50it/s]                                                 {'loss': 0.4569, 'grad_norm': 4.631179643911215, 'learning_rate': 1.1188299817184644e-06, 'epoch': 1.38}
 46%|████▌     | 259/564 [03:24<03:22,  1.50it/s] 46%|████▌     | 260/564 [03:25<03:35,  1.41it/s]                                                 {'loss': 0.4359, 'grad_norm': 5.405805327866641, 'learning_rate': 1.1151736745886654e-06, 'epoch': 1.38}
 46%|████▌     | 260/564 [03:25<03:35,  1.41it/s] 46%|████▋     | 261/564 [03:26<03:46,  1.34it/s]                                                 {'loss': 0.366, 'grad_norm': 5.351807819470124, 'learning_rate': 1.1115173674588664e-06, 'epoch': 1.39}
 46%|████▋     | 261/564 [03:26<03:46,  1.34it/s] 46%|████▋     | 262/564 [03:26<03:32,  1.42it/s]                                                 {'loss': 0.3844, 'grad_norm': 4.761072179266729, 'learning_rate': 1.1078610603290676e-06, 'epoch': 1.39}
 46%|████▋     | 262/564 [03:26<03:32,  1.42it/s] 47%|████▋     | 263/564 [03:27<03:41,  1.36it/s]                                                 {'loss': 0.4617, 'grad_norm': 4.903271404792976, 'learning_rate': 1.1042047531992687e-06, 'epoch': 1.4}
 47%|████▋     | 263/564 [03:27<03:41,  1.36it/s] 47%|████▋     | 264/564 [03:28<03:50,  1.30it/s]                                                 {'loss': 0.399, 'grad_norm': 5.410006598143477, 'learning_rate': 1.10054844606947e-06, 'epoch': 1.4}
 47%|████▋     | 264/564 [03:28<03:50,  1.30it/s] 47%|████▋     | 265/564 [03:29<03:53,  1.28it/s]                                                 {'loss': 0.4167, 'grad_norm': 5.14073172573366, 'learning_rate': 1.0968921389396707e-06, 'epoch': 1.41}
 47%|████▋     | 265/564 [03:29<03:53,  1.28it/s] 47%|████▋     | 266/564 [03:29<03:36,  1.38it/s]                                                 {'loss': 0.3607, 'grad_norm': 4.47833197103452, 'learning_rate': 1.093235831809872e-06, 'epoch': 1.41}
 47%|████▋     | 266/564 [03:29<03:36,  1.38it/s] 47%|████▋     | 267/564 [03:30<03:34,  1.38it/s]                                                 {'loss': 0.4393, 'grad_norm': 4.515819189469687, 'learning_rate': 1.089579524680073e-06, 'epoch': 1.42}
 47%|████▋     | 267/564 [03:30<03:34,  1.38it/s] 48%|████▊     | 268/564 [03:31<03:49,  1.29it/s]                                                 {'loss': 0.4692, 'grad_norm': 5.625372563978679, 'learning_rate': 1.0859232175502742e-06, 'epoch': 1.43}
 48%|████▊     | 268/564 [03:31<03:49,  1.29it/s] 48%|████▊     | 269/564 [03:32<03:53,  1.26it/s]                                                 {'loss': 0.4162, 'grad_norm': 5.375091513163949, 'learning_rate': 1.0822669104204754e-06, 'epoch': 1.43}
 48%|████▊     | 269/564 [03:32<03:53,  1.26it/s] 48%|████▊     | 270/564 [03:32<04:00,  1.22it/s]                                                 {'loss': 0.4756, 'grad_norm': 5.877390718545233, 'learning_rate': 1.0786106032906763e-06, 'epoch': 1.44}
 48%|████▊     | 270/564 [03:32<04:00,  1.22it/s] 48%|████▊     | 271/564 [03:33<03:54,  1.25it/s]                                                 {'loss': 0.3552, 'grad_norm': 4.7230147767113735, 'learning_rate': 1.0749542961608775e-06, 'epoch': 1.44}
 48%|████▊     | 271/564 [03:33<03:54,  1.25it/s] 48%|████▊     | 272/564 [03:35<04:45,  1.02it/s]                                                 {'loss': 0.4137, 'grad_norm': 4.2679083900742025, 'learning_rate': 1.0712979890310785e-06, 'epoch': 1.45}
 48%|████▊     | 272/564 [03:35<04:45,  1.02it/s] 48%|████▊     | 273/564 [03:35<04:23,  1.10it/s]                                                 {'loss': 0.4372, 'grad_norm': 4.851627147747617, 'learning_rate': 1.0676416819012797e-06, 'epoch': 1.45}
 48%|████▊     | 273/564 [03:35<04:23,  1.10it/s] 49%|████▊     | 274/564 [03:36<04:22,  1.10it/s]                                                 {'loss': 0.39, 'grad_norm': 4.6164239161735825, 'learning_rate': 1.0639853747714806e-06, 'epoch': 1.46}
 49%|████▊     | 274/564 [03:36<04:22,  1.10it/s] 49%|████▉     | 275/564 [03:37<04:15,  1.13it/s]                                                 {'loss': 0.5233, 'grad_norm': 5.752569676122421, 'learning_rate': 1.0603290676416818e-06, 'epoch': 1.46}
 49%|████▉     | 275/564 [03:37<04:15,  1.13it/s] 49%|████▉     | 276/564 [03:38<04:15,  1.13it/s]                                                 {'loss': 0.4244, 'grad_norm': 5.522210651688828, 'learning_rate': 1.056672760511883e-06, 'epoch': 1.47}
 49%|████▉     | 276/564 [03:38<04:15,  1.13it/s] 49%|████▉     | 277/564 [03:39<04:05,  1.17it/s]                                                 {'loss': 0.3864, 'grad_norm': 5.835823357534402, 'learning_rate': 1.053016453382084e-06, 'epoch': 1.47}
 49%|████▉     | 277/564 [03:39<04:05,  1.17it/s] 49%|████▉     | 278/564 [03:40<03:56,  1.21it/s]                                                 {'loss': 0.3783, 'grad_norm': 4.639443859004636, 'learning_rate': 1.0493601462522853e-06, 'epoch': 1.48}
 49%|████▉     | 278/564 [03:40<03:56,  1.21it/s] 49%|████▉     | 279/564 [03:40<03:51,  1.23it/s]                                                 {'loss': 0.3382, 'grad_norm': 4.977129072281447, 'learning_rate': 1.045703839122486e-06, 'epoch': 1.48}
 49%|████▉     | 279/564 [03:40<03:51,  1.23it/s] 50%|████▉     | 280/564 [03:41<03:52,  1.22it/s]                                                 {'loss': 0.3208, 'grad_norm': 6.037004632567618, 'learning_rate': 1.0420475319926873e-06, 'epoch': 1.49}
 50%|████▉     | 280/564 [03:41<03:52,  1.22it/s] 50%|████▉     | 281/564 [03:42<03:50,  1.23it/s]                                                 {'loss': 0.3214, 'grad_norm': 5.025039344987642, 'learning_rate': 1.0383912248628884e-06, 'epoch': 1.49}
 50%|████▉     | 281/564 [03:42<03:50,  1.23it/s] 50%|█████     | 282/564 [03:44<05:10,  1.10s/it]                                                 {'loss': 0.4768, 'grad_norm': 5.753947373466774, 'learning_rate': 1.0347349177330896e-06, 'epoch': 1.5}
 50%|█████     | 282/564 [03:44<05:10,  1.10s/it] 50%|█████     | 283/564 [03:45<04:52,  1.04s/it]                                                 {'loss': 0.4605, 'grad_norm': 5.020796041130052, 'learning_rate': 1.0310786106032906e-06, 'epoch': 1.51}
 50%|█████     | 283/564 [03:45<04:52,  1.04s/it] 50%|█████     | 284/564 [03:45<04:29,  1.04it/s]                                                 {'loss': 0.3797, 'grad_norm': 4.504776978012532, 'learning_rate': 1.0274223034734916e-06, 'epoch': 1.51}
 50%|█████     | 284/564 [03:45<04:29,  1.04it/s] 51%|█████     | 285/564 [03:47<04:52,  1.05s/it]                                                 {'loss': 0.3988, 'grad_norm': 5.301790238772878, 'learning_rate': 1.0237659963436929e-06, 'epoch': 1.52}
 51%|█████     | 285/564 [03:47<04:52,  1.05s/it] 51%|█████     | 286/564 [03:47<04:32,  1.02it/s]                                                 {'loss': 0.3846, 'grad_norm': 5.841080102049812, 'learning_rate': 1.020109689213894e-06, 'epoch': 1.52}
 51%|█████     | 286/564 [03:47<04:32,  1.02it/s] 51%|█████     | 287/564 [03:48<04:18,  1.07it/s]                                                 {'loss': 0.4459, 'grad_norm': 5.0987644434472115, 'learning_rate': 1.0164533820840951e-06, 'epoch': 1.53}
 51%|█████     | 287/564 [03:48<04:18,  1.07it/s] 51%|█████     | 288/564 [03:49<03:59,  1.15it/s]                                                 {'loss': 0.3034, 'grad_norm': 4.171826531188455, 'learning_rate': 1.012797074954296e-06, 'epoch': 1.53}
 51%|█████     | 288/564 [03:49<03:59,  1.15it/s] 51%|█████     | 289/564 [03:50<03:55,  1.17it/s]                                                 {'loss': 0.466, 'grad_norm': 5.173556733518716, 'learning_rate': 1.0091407678244972e-06, 'epoch': 1.54}
 51%|█████     | 289/564 [03:50<03:55,  1.17it/s] 51%|█████▏    | 290/564 [03:50<03:33,  1.28it/s]                                                 {'loss': 0.438, 'grad_norm': 4.78573525510912, 'learning_rate': 1.0054844606946984e-06, 'epoch': 1.54}
 51%|█████▏    | 290/564 [03:50<03:33,  1.28it/s] 52%|█████▏    | 291/564 [03:51<03:36,  1.26it/s]                                                 {'loss': 0.494, 'grad_norm': 5.796381254976795, 'learning_rate': 1.0018281535648994e-06, 'epoch': 1.55}
 52%|█████▏    | 291/564 [03:51<03:36,  1.26it/s] 52%|█████▏    | 292/564 [03:52<03:37,  1.25it/s]                                                 {'loss': 0.3843, 'grad_norm': 5.043431946762915, 'learning_rate': 9.981718464351005e-07, 'epoch': 1.55}
 52%|█████▏    | 292/564 [03:52<03:37,  1.25it/s] 52%|█████▏    | 293/564 [03:53<03:21,  1.35it/s]                                                 {'loss': 0.3769, 'grad_norm': 5.1227206500707325, 'learning_rate': 9.945155393053017e-07, 'epoch': 1.56}
 52%|█████▏    | 293/564 [03:53<03:21,  1.35it/s] 52%|█████▏    | 294/564 [03:54<03:26,  1.31it/s]                                                 {'loss': 0.4692, 'grad_norm': 5.549554028103187, 'learning_rate': 9.908592321755027e-07, 'epoch': 1.56}
 52%|█████▏    | 294/564 [03:54<03:26,  1.31it/s] 52%|█████▏    | 295/564 [03:54<03:29,  1.28it/s]                                                 {'loss': 0.4219, 'grad_norm': 5.220672031921991, 'learning_rate': 9.872029250457037e-07, 'epoch': 1.57}
 52%|█████▏    | 295/564 [03:54<03:29,  1.28it/s] 52%|█████▏    | 296/564 [03:55<03:36,  1.24it/s]                                                 {'loss': 0.6007, 'grad_norm': 6.356642576287987, 'learning_rate': 9.835466179159048e-07, 'epoch': 1.57}
 52%|█████▏    | 296/564 [03:55<03:36,  1.24it/s] 53%|█████▎    | 297/564 [03:56<03:36,  1.23it/s]                                                 {'loss': 0.4115, 'grad_norm': 5.644164726613986, 'learning_rate': 9.79890310786106e-07, 'epoch': 1.58}
 53%|█████▎    | 297/564 [03:56<03:36,  1.23it/s] 53%|█████▎    | 298/564 [03:58<04:49,  1.09s/it]                                                 {'loss': 0.436, 'grad_norm': 5.049793647132612, 'learning_rate': 9.76234003656307e-07, 'epoch': 1.59}
 53%|█████▎    | 298/564 [03:58<04:49,  1.09s/it] 53%|█████▎    | 299/564 [04:00<05:39,  1.28s/it]                                                 {'loss': 0.3518, 'grad_norm': 4.583861016699701, 'learning_rate': 9.725776965265083e-07, 'epoch': 1.59}
 53%|█████▎    | 299/564 [04:00<05:39,  1.28s/it] 53%|█████▎    | 300/564 [04:00<05:15,  1.20s/it]                                                 {'loss': 0.4026, 'grad_norm': 5.075182959714638, 'learning_rate': 9.689213893967093e-07, 'epoch': 1.6}
 53%|█████▎    | 300/564 [04:01<05:15,  1.20s/it] 53%|█████▎    | 301/564 [04:01<04:44,  1.08s/it]                                                 {'loss': 0.4217, 'grad_norm': 5.156424989268685, 'learning_rate': 9.652650822669103e-07, 'epoch': 1.6}
 53%|█████▎    | 301/564 [04:01<04:44,  1.08s/it] 54%|█████▎    | 302/564 [04:02<04:28,  1.03s/it]                                                 {'loss': 0.3526, 'grad_norm': 5.1446369270770695, 'learning_rate': 9.616087751371115e-07, 'epoch': 1.61}
 54%|█████▎    | 302/564 [04:02<04:28,  1.03s/it] 54%|█████▎    | 303/564 [04:03<04:19,  1.01it/s]                                                 {'loss': 0.3498, 'grad_norm': 4.919409374956121, 'learning_rate': 9.579524680073126e-07, 'epoch': 1.61}
 54%|█████▎    | 303/564 [04:03<04:19,  1.01it/s] 54%|█████▍    | 304/564 [04:04<03:57,  1.10it/s]                                                 {'loss': 0.4635, 'grad_norm': 6.262727160186305, 'learning_rate': 9.542961608775136e-07, 'epoch': 1.62}
 54%|█████▍    | 304/564 [04:04<03:57,  1.10it/s] 54%|█████▍    | 305/564 [04:05<03:44,  1.16it/s]                                                 {'loss': 0.4446, 'grad_norm': 5.447317523820001, 'learning_rate': 9.506398537477147e-07, 'epoch': 1.62}
 54%|█████▍    | 305/564 [04:05<03:44,  1.16it/s] 54%|█████▍    | 306/564 [04:06<04:53,  1.14s/it]                                                 {'loss': 0.3786, 'grad_norm': 5.428082994302371, 'learning_rate': 9.469835466179159e-07, 'epoch': 1.63}
 54%|█████▍    | 306/564 [04:06<04:53,  1.14s/it] 54%|█████▍    | 307/564 [04:07<04:38,  1.08s/it]                                                 {'loss': 0.432, 'grad_norm': 4.633192152830061, 'learning_rate': 9.43327239488117e-07, 'epoch': 1.63}
 54%|█████▍    | 307/564 [04:07<04:38,  1.08s/it] 55%|█████▍    | 308/564 [04:08<04:22,  1.03s/it]                                                 {'loss': 0.3595, 'grad_norm': 5.541716144862602, 'learning_rate': 9.396709323583181e-07, 'epoch': 1.64}
 55%|█████▍    | 308/564 [04:08<04:22,  1.03s/it] 55%|█████▍    | 309/564 [04:09<03:58,  1.07it/s]                                                 {'loss': 0.4301, 'grad_norm': 4.915108091762247, 'learning_rate': 9.360146252285191e-07, 'epoch': 1.64}
 55%|█████▍    | 309/564 [04:09<03:58,  1.07it/s] 55%|█████▍    | 310/564 [04:10<03:42,  1.14it/s]                                                 {'loss': 0.4534, 'grad_norm': 4.999001782571548, 'learning_rate': 9.323583180987203e-07, 'epoch': 1.65}
 55%|█████▍    | 310/564 [04:10<03:42,  1.14it/s] 55%|█████▌    | 311/564 [04:11<03:43,  1.13it/s]                                                 {'loss': 0.4928, 'grad_norm': 6.388012751667308, 'learning_rate': 9.287020109689213e-07, 'epoch': 1.65}
 55%|█████▌    | 311/564 [04:11<03:43,  1.13it/s] 55%|█████▌    | 312/564 [04:12<03:52,  1.08it/s]                                                 {'loss': 0.4635, 'grad_norm': 5.668589483783656, 'learning_rate': 9.250457038391224e-07, 'epoch': 1.66}
 55%|█████▌    | 312/564 [04:12<03:52,  1.08it/s] 55%|█████▌    | 313/564 [04:12<03:47,  1.10it/s]                                                 {'loss': 0.4717, 'grad_norm': 5.084685580375978, 'learning_rate': 9.213893967093235e-07, 'epoch': 1.66}
 55%|█████▌    | 313/564 [04:12<03:47,  1.10it/s] 56%|█████▌    | 314/564 [04:13<03:33,  1.17it/s]                                                 {'loss': 0.4365, 'grad_norm': 6.61120683891339, 'learning_rate': 9.177330895795247e-07, 'epoch': 1.67}
 56%|█████▌    | 314/564 [04:13<03:33,  1.17it/s] 56%|█████▌    | 315/564 [04:14<03:34,  1.16it/s]                                                 {'loss': 0.4025, 'grad_norm': 5.206968040870527, 'learning_rate': 9.140767824497257e-07, 'epoch': 1.68}
 56%|█████▌    | 315/564 [04:14<03:34,  1.16it/s] 56%|█████▌    | 316/564 [04:15<03:31,  1.17it/s]                                                 {'loss': 0.4349, 'grad_norm': 5.439186021317781, 'learning_rate': 9.104204753199268e-07, 'epoch': 1.68}
 56%|█████▌    | 316/564 [04:15<03:31,  1.17it/s] 56%|█████▌    | 317/564 [04:16<03:26,  1.19it/s]                                                 {'loss': 0.3243, 'grad_norm': 4.6426616777804695, 'learning_rate': 9.06764168190128e-07, 'epoch': 1.69}
 56%|█████▌    | 317/564 [04:16<03:26,  1.19it/s] 56%|█████▋    | 318/564 [04:17<03:32,  1.16it/s]                                                 {'loss': 0.4143, 'grad_norm': 5.6731129254568, 'learning_rate': 9.03107861060329e-07, 'epoch': 1.69}
 56%|█████▋    | 318/564 [04:17<03:32,  1.16it/s] 57%|█████▋    | 319/564 [04:17<03:22,  1.21it/s]                                                 {'loss': 0.5484, 'grad_norm': 4.879608151485391, 'learning_rate': 8.994515539305301e-07, 'epoch': 1.7}
 57%|█████▋    | 319/564 [04:17<03:22,  1.21it/s] 57%|█████▋    | 320/564 [04:18<03:15,  1.25it/s]                                                 {'loss': 0.4229, 'grad_norm': 5.6998226105909, 'learning_rate': 8.957952468007312e-07, 'epoch': 1.7}
 57%|█████▋    | 320/564 [04:18<03:15,  1.25it/s] 57%|█████▋    | 321/564 [04:19<03:22,  1.20it/s]                                                 {'loss': 0.4307, 'grad_norm': 4.9872208844708785, 'learning_rate': 8.921389396709324e-07, 'epoch': 1.71}
 57%|█████▋    | 321/564 [04:19<03:22,  1.20it/s] 57%|█████▋    | 322/564 [04:20<03:21,  1.20it/s]                                                 {'loss': 0.3613, 'grad_norm': 5.236676387174031, 'learning_rate': 8.884826325411334e-07, 'epoch': 1.71}
 57%|█████▋    | 322/564 [04:20<03:21,  1.20it/s] 57%|█████▋    | 323/564 [04:21<03:28,  1.16it/s]                                                 {'loss': 0.3372, 'grad_norm': 4.927925260384917, 'learning_rate': 8.848263254113345e-07, 'epoch': 1.72}
 57%|█████▋    | 323/564 [04:21<03:28,  1.16it/s] 57%|█████▋    | 324/564 [04:22<03:26,  1.16it/s]                                                 {'loss': 0.3987, 'grad_norm': 5.484496793421991, 'learning_rate': 8.811700182815355e-07, 'epoch': 1.72}
 57%|█████▋    | 324/564 [04:22<03:26,  1.16it/s] 58%|█████▊    | 325/564 [04:23<03:32,  1.13it/s]                                                 {'loss': 0.3652, 'grad_norm': 4.726441488660736, 'learning_rate': 8.775137111517367e-07, 'epoch': 1.73}
 58%|█████▊    | 325/564 [04:23<03:32,  1.13it/s] 58%|█████▊    | 326/564 [04:24<03:35,  1.11it/s]                                                 {'loss': 0.327, 'grad_norm': 5.0057545638955245, 'learning_rate': 8.738574040219377e-07, 'epoch': 1.73}
 58%|█████▊    | 326/564 [04:24<03:35,  1.11it/s] 58%|█████▊    | 327/564 [04:24<03:23,  1.16it/s]                                                 {'loss': 0.4928, 'grad_norm': 4.450999146893047, 'learning_rate': 8.702010968921389e-07, 'epoch': 1.74}
 58%|█████▊    | 327/564 [04:24<03:23,  1.16it/s] 58%|█████▊    | 328/564 [04:25<03:23,  1.16it/s]                                                 {'loss': 0.4603, 'grad_norm': 5.349325520601931, 'learning_rate': 8.665447897623401e-07, 'epoch': 1.74}
 58%|█████▊    | 328/564 [04:25<03:23,  1.16it/s] 58%|█████▊    | 329/564 [04:26<03:18,  1.18it/s]                                                 {'loss': 0.3575, 'grad_norm': 5.088477766122675, 'learning_rate': 8.628884826325411e-07, 'epoch': 1.75}
 58%|█████▊    | 329/564 [04:26<03:18,  1.18it/s] 59%|█████▊    | 330/564 [04:27<03:22,  1.15it/s]                                                 {'loss': 0.4672, 'grad_norm': 4.920276851316782, 'learning_rate': 8.592321755027422e-07, 'epoch': 1.76}
 59%|█████▊    | 330/564 [04:27<03:22,  1.15it/s] 59%|█████▊    | 331/564 [04:28<03:28,  1.12it/s]                                                 {'loss': 0.3813, 'grad_norm': 5.579899925863064, 'learning_rate': 8.555758683729432e-07, 'epoch': 1.76}
 59%|█████▊    | 331/564 [04:28<03:28,  1.12it/s] 59%|█████▉    | 332/564 [04:29<03:23,  1.14it/s]                                                 {'loss': 0.4429, 'grad_norm': 5.652369335231042, 'learning_rate': 8.519195612431444e-07, 'epoch': 1.77}
 59%|█████▉    | 332/564 [04:29<03:23,  1.14it/s] 59%|█████▉    | 333/564 [04:29<03:06,  1.24it/s]                                                 {'loss': 0.4589, 'grad_norm': 5.603821495016237, 'learning_rate': 8.482632541133454e-07, 'epoch': 1.77}
 59%|█████▉    | 333/564 [04:29<03:06,  1.24it/s] 59%|█████▉    | 334/564 [04:30<03:07,  1.23it/s]                                                 {'loss': 0.3993, 'grad_norm': 5.528583427521926, 'learning_rate': 8.446069469835466e-07, 'epoch': 1.78}
 59%|█████▉    | 334/564 [04:30<03:07,  1.23it/s] 59%|█████▉    | 335/564 [04:31<03:03,  1.25it/s]                                                 {'loss': 0.3056, 'grad_norm': 5.20571772893183, 'learning_rate': 8.409506398537477e-07, 'epoch': 1.78}
 59%|█████▉    | 335/564 [04:31<03:03,  1.25it/s] 60%|█████▉    | 336/564 [04:32<03:06,  1.22it/s]                                                 {'loss': 0.4225, 'grad_norm': 4.910450810934915, 'learning_rate': 8.372943327239488e-07, 'epoch': 1.79}
 60%|█████▉    | 336/564 [04:32<03:06,  1.22it/s] 60%|█████▉    | 337/564 [04:33<03:13,  1.17it/s]                                                 {'loss': 0.3952, 'grad_norm': 5.5961177559308215, 'learning_rate': 8.336380255941499e-07, 'epoch': 1.79}
 60%|█████▉    | 337/564 [04:33<03:13,  1.17it/s] 60%|█████▉    | 338/564 [04:34<03:19,  1.13it/s]                                                 {'loss': 0.4045, 'grad_norm': 5.609214736653091, 'learning_rate': 8.299817184643509e-07, 'epoch': 1.8}
 60%|█████▉    | 338/564 [04:34<03:19,  1.13it/s] 60%|██████    | 339/564 [04:35<03:21,  1.11it/s]                                                 {'loss': 0.3124, 'grad_norm': 4.321678294302946, 'learning_rate': 8.263254113345521e-07, 'epoch': 1.8}
 60%|██████    | 339/564 [04:35<03:21,  1.11it/s] 60%|██████    | 340/564 [04:36<03:24,  1.10it/s]                                                 {'loss': 0.4857, 'grad_norm': 5.129467123249161, 'learning_rate': 8.226691042047532e-07, 'epoch': 1.81}
 60%|██████    | 340/564 [04:36<03:24,  1.10it/s] 60%|██████    | 341/564 [04:36<03:18,  1.12it/s]                                                 {'loss': 0.4027, 'grad_norm': 5.2846263784743, 'learning_rate': 8.190127970749543e-07, 'epoch': 1.81}
 60%|██████    | 341/564 [04:36<03:18,  1.12it/s] 61%|██████    | 342/564 [04:37<03:22,  1.10it/s]                                                 {'loss': 0.3948, 'grad_norm': 5.706586859553711, 'learning_rate': 8.153564899451553e-07, 'epoch': 1.82}
 61%|██████    | 342/564 [04:37<03:22,  1.10it/s] 61%|██████    | 343/564 [04:38<03:17,  1.12it/s]                                                 {'loss': 0.4421, 'grad_norm': 6.192210037948363, 'learning_rate': 8.117001828153565e-07, 'epoch': 1.82}
 61%|██████    | 343/564 [04:38<03:17,  1.12it/s] 61%|██████    | 344/564 [04:39<03:14,  1.13it/s]                                                 {'loss': 0.3818, 'grad_norm': 5.437820265710325, 'learning_rate': 8.080438756855575e-07, 'epoch': 1.83}
 61%|██████    | 344/564 [04:39<03:14,  1.13it/s] 61%|██████    | 345/564 [04:40<03:17,  1.11it/s]                                                 {'loss': 0.3434, 'grad_norm': 5.141615837154333, 'learning_rate': 8.043875685557586e-07, 'epoch': 1.84}
 61%|██████    | 345/564 [04:40<03:17,  1.11it/s] 61%|██████▏   | 346/564 [04:41<03:08,  1.16it/s]                                                 {'loss': 0.4163, 'grad_norm': 6.020854307140908, 'learning_rate': 8.007312614259597e-07, 'epoch': 1.84}
 61%|██████▏   | 346/564 [04:41<03:08,  1.16it/s] 62%|██████▏   | 347/564 [04:42<03:09,  1.14it/s]                                                 {'loss': 0.4484, 'grad_norm': 5.456756333371313, 'learning_rate': 7.970749542961609e-07, 'epoch': 1.85}
 62%|██████▏   | 347/564 [04:42<03:09,  1.14it/s] 62%|██████▏   | 348/564 [04:42<02:57,  1.21it/s]                                                 {'loss': 0.5182, 'grad_norm': 5.207678119092756, 'learning_rate': 7.93418647166362e-07, 'epoch': 1.85}
 62%|██████▏   | 348/564 [04:42<02:57,  1.21it/s] 62%|██████▏   | 349/564 [04:43<03:14,  1.10it/s]                                                 {'loss': 0.3705, 'grad_norm': 4.5620219257115044, 'learning_rate': 7.89762340036563e-07, 'epoch': 1.86}
 62%|██████▏   | 349/564 [04:43<03:14,  1.10it/s] 62%|██████▏   | 350/564 [04:44<03:16,  1.09it/s]                                                 {'loss': 0.3517, 'grad_norm': 5.426154112795405, 'learning_rate': 7.861060329067642e-07, 'epoch': 1.86}
 62%|██████▏   | 350/564 [04:44<03:16,  1.09it/s] 62%|██████▏   | 351/564 [04:45<03:16,  1.08it/s]                                                 {'loss': 0.3619, 'grad_norm': 5.110471170103591, 'learning_rate': 7.824497257769652e-07, 'epoch': 1.87}
 62%|██████▏   | 351/564 [04:45<03:16,  1.08it/s] 62%|██████▏   | 352/564 [04:46<03:12,  1.10it/s]                                                 {'loss': 0.2778, 'grad_norm': 4.689887407307823, 'learning_rate': 7.787934186471663e-07, 'epoch': 1.87}
 62%|██████▏   | 352/564 [04:46<03:12,  1.10it/s] 63%|██████▎   | 353/564 [04:47<03:10,  1.11it/s]                                                 {'loss': 0.3923, 'grad_norm': 4.790414172115763, 'learning_rate': 7.751371115173673e-07, 'epoch': 1.88}
 63%|██████▎   | 353/564 [04:47<03:10,  1.11it/s] 63%|██████▎   | 354/564 [04:48<03:17,  1.06it/s]                                                 {'loss': 0.4102, 'grad_norm': 5.549639795702563, 'learning_rate': 7.714808043875686e-07, 'epoch': 1.88}
 63%|██████▎   | 354/564 [04:48<03:17,  1.06it/s] 63%|██████▎   | 355/564 [04:49<03:13,  1.08it/s]                                                 {'loss': 0.4026, 'grad_norm': 4.886995128005326, 'learning_rate': 7.678244972577696e-07, 'epoch': 1.89}
 63%|██████▎   | 355/564 [04:49<03:13,  1.08it/s] 63%|██████▎   | 356/564 [04:50<03:12,  1.08it/s]                                                 {'loss': 0.4284, 'grad_norm': 6.0669916760903595, 'learning_rate': 7.641681901279707e-07, 'epoch': 1.89}
 63%|██████▎   | 356/564 [04:50<03:12,  1.08it/s] 63%|██████▎   | 357/564 [04:51<03:16,  1.05it/s]                                                 {'loss': 0.5373, 'grad_norm': 4.833486067512151, 'learning_rate': 7.605118829981719e-07, 'epoch': 1.9}
 63%|██████▎   | 357/564 [04:51<03:16,  1.05it/s] 63%|██████▎   | 358/564 [04:52<03:15,  1.05it/s]                                                 {'loss': 0.4668, 'grad_norm': 5.649944759217724, 'learning_rate': 7.568555758683729e-07, 'epoch': 1.9}
 63%|██████▎   | 358/564 [04:52<03:15,  1.05it/s] 64%|██████▎   | 359/564 [04:53<03:16,  1.04it/s]                                                 {'loss': 0.4192, 'grad_norm': 5.8135797552005295, 'learning_rate': 7.53199268738574e-07, 'epoch': 1.91}
 64%|██████▎   | 359/564 [04:53<03:16,  1.04it/s] 64%|██████▍   | 360/564 [04:54<03:05,  1.10it/s]                                                 {'loss': 0.3547, 'grad_norm': 4.856097220221036, 'learning_rate': 7.49542961608775e-07, 'epoch': 1.91}
 64%|██████▍   | 360/564 [04:54<03:05,  1.10it/s] 64%|██████▍   | 361/564 [04:55<02:58,  1.14it/s]                                                 {'loss': 0.3791, 'grad_norm': 4.8993903224357975, 'learning_rate': 7.458866544789763e-07, 'epoch': 1.92}
 64%|██████▍   | 361/564 [04:55<02:58,  1.14it/s] 64%|██████▍   | 362/564 [04:55<02:52,  1.17it/s]                                                 {'loss': 0.3026, 'grad_norm': 3.8069730063483225, 'learning_rate': 7.422303473491773e-07, 'epoch': 1.93}
 64%|██████▍   | 362/564 [04:55<02:52,  1.17it/s] 64%|██████▍   | 363/564 [04:56<02:53,  1.16it/s]                                                 {'loss': 0.3781, 'grad_norm': 4.51102741764574, 'learning_rate': 7.385740402193784e-07, 'epoch': 1.93}
 64%|██████▍   | 363/564 [04:56<02:53,  1.16it/s] 65%|██████▍   | 364/564 [04:57<03:01,  1.10it/s]                                                 {'loss': 0.3808, 'grad_norm': 5.142283563960517, 'learning_rate': 7.349177330895795e-07, 'epoch': 1.94}
 65%|██████▍   | 364/564 [04:57<03:01,  1.10it/s] 65%|██████▍   | 365/564 [04:58<02:59,  1.11it/s]                                                 {'loss': 0.4428, 'grad_norm': 4.780595207568666, 'learning_rate': 7.312614259597806e-07, 'epoch': 1.94}
 65%|██████▍   | 365/564 [04:58<02:59,  1.11it/s] 65%|██████▍   | 366/564 [04:59<02:50,  1.16it/s]                                                 {'loss': 0.389, 'grad_norm': 4.976403445674736, 'learning_rate': 7.276051188299816e-07, 'epoch': 1.95}
 65%|██████▍   | 366/564 [04:59<02:50,  1.16it/s] 65%|██████▌   | 367/564 [05:00<02:48,  1.17it/s]                                                 {'loss': 0.348, 'grad_norm': 4.982537996720069, 'learning_rate': 7.239488117001827e-07, 'epoch': 1.95}
 65%|██████▌   | 367/564 [05:00<02:48,  1.17it/s] 65%|██████▌   | 368/564 [05:01<02:45,  1.18it/s]                                                 {'loss': 0.4256, 'grad_norm': 5.625805117012033, 'learning_rate': 7.20292504570384e-07, 'epoch': 1.96}
 65%|██████▌   | 368/564 [05:01<02:45,  1.18it/s] 65%|██████▌   | 369/564 [05:01<02:42,  1.20it/s]                                                 {'loss': 0.4402, 'grad_norm': 4.707305692135148, 'learning_rate': 7.16636197440585e-07, 'epoch': 1.96}
 65%|██████▌   | 369/564 [05:01<02:42,  1.20it/s] 66%|██████▌   | 370/564 [05:02<02:41,  1.20it/s]                                                 {'loss': 0.431, 'grad_norm': 5.065005109844672, 'learning_rate': 7.129798903107861e-07, 'epoch': 1.97}
 66%|██████▌   | 370/564 [05:02<02:41,  1.20it/s] 66%|██████▌   | 371/564 [05:03<02:44,  1.17it/s]                                                 {'loss': 0.3848, 'grad_norm': 4.71290737742275, 'learning_rate': 7.093235831809871e-07, 'epoch': 1.97}
 66%|██████▌   | 371/564 [05:03<02:44,  1.17it/s] 66%|██████▌   | 372/564 [05:05<03:33,  1.11s/it]                                                 {'loss': 0.4061, 'grad_norm': 6.089522688976169, 'learning_rate': 7.056672760511883e-07, 'epoch': 1.98}
 66%|██████▌   | 372/564 [05:05<03:33,  1.11s/it] 66%|██████▌   | 373/564 [05:06<03:20,  1.05s/it]                                                 {'loss': 0.4837, 'grad_norm': 5.278445260548871, 'learning_rate': 7.020109689213893e-07, 'epoch': 1.98}
 66%|██████▌   | 373/564 [05:06<03:20,  1.05s/it] 66%|██████▋   | 374/564 [05:07<03:07,  1.01it/s]                                                 {'loss': 0.392, 'grad_norm': 5.264931306629396, 'learning_rate': 6.983546617915904e-07, 'epoch': 1.99}
 66%|██████▋   | 374/564 [05:07<03:07,  1.01it/s] 66%|██████▋   | 375/564 [05:07<02:50,  1.11it/s]                                                 {'loss': 0.3726, 'grad_norm': 5.58990570960354, 'learning_rate': 6.946983546617916e-07, 'epoch': 1.99}
 66%|██████▋   | 375/564 [05:07<02:50,  1.11it/s] 67%|██████▋   | 376/564 [05:08<02:49,  1.11it/s]                                                 {'loss': 0.5399, 'grad_norm': 5.5294469518592075, 'learning_rate': 6.910420475319927e-07, 'epoch': 2.0}
 67%|██████▋   | 376/564 [05:08<02:49,  1.11it/s] 67%|██████▋   | 377/564 [05:09<02:44,  1.14it/s]                                                 {'loss': 0.3204, 'grad_norm': 4.172004912092194, 'learning_rate': 6.873857404021938e-07, 'epoch': 2.01}
 67%|██████▋   | 377/564 [05:09<02:44,  1.14it/s] 67%|██████▋   | 378/564 [05:10<02:38,  1.18it/s]                                                 {'loss': 0.2814, 'grad_norm': 4.215078178100873, 'learning_rate': 6.837294332723948e-07, 'epoch': 2.01}
 67%|██████▋   | 378/564 [05:10<02:38,  1.18it/s] 67%|██████▋   | 379/564 [05:11<02:39,  1.16it/s]                                                 {'loss': 0.251, 'grad_norm': 4.166751339586927, 'learning_rate': 6.80073126142596e-07, 'epoch': 2.02}
 67%|██████▋   | 379/564 [05:11<02:39,  1.16it/s] 67%|██████▋   | 380/564 [05:11<02:38,  1.16it/s]                                                 {'loss': 0.2729, 'grad_norm': 4.883403798141544, 'learning_rate': 6.76416819012797e-07, 'epoch': 2.02}
 67%|██████▋   | 380/564 [05:11<02:38,  1.16it/s] 68%|██████▊   | 381/564 [05:12<02:39,  1.15it/s]                                                 {'loss': 0.267, 'grad_norm': 3.8064115133814775, 'learning_rate': 6.727605118829981e-07, 'epoch': 2.03}
 68%|██████▊   | 381/564 [05:12<02:39,  1.15it/s] 68%|██████▊   | 382/564 [05:13<02:39,  1.14it/s]                                                 {'loss': 0.3349, 'grad_norm': 3.818603496357707, 'learning_rate': 6.691042047531993e-07, 'epoch': 2.03}
 68%|██████▊   | 382/564 [05:13<02:39,  1.14it/s] 68%|██████▊   | 383/564 [05:14<02:39,  1.13it/s]                                                 {'loss': 0.2763, 'grad_norm': 4.224736163908536, 'learning_rate': 6.654478976234004e-07, 'epoch': 2.04}
 68%|██████▊   | 383/564 [05:14<02:39,  1.13it/s] 68%|██████▊   | 384/564 [05:15<02:32,  1.18it/s]                                                 {'loss': 0.3046, 'grad_norm': 4.353933334217627, 'learning_rate': 6.617915904936014e-07, 'epoch': 2.04}
 68%|██████▊   | 384/564 [05:15<02:32,  1.18it/s] 68%|██████▊   | 385/564 [05:16<02:28,  1.21it/s]                                                 {'loss': 0.3101, 'grad_norm': 4.2253023340906095, 'learning_rate': 6.581352833638025e-07, 'epoch': 2.05}
 68%|██████▊   | 385/564 [05:16<02:28,  1.21it/s] 68%|██████▊   | 386/564 [05:17<03:15,  1.10s/it]                                                 {'loss': 0.2502, 'grad_norm': 3.9872595414371363, 'learning_rate': 6.544789762340036e-07, 'epoch': 2.05}
 68%|██████▊   | 386/564 [05:17<03:15,  1.10s/it] 69%|██████▊   | 387/564 [05:18<02:57,  1.00s/it]                                                 {'loss': 0.2859, 'grad_norm': 4.869588310125799, 'learning_rate': 6.508226691042047e-07, 'epoch': 2.06}
 69%|██████▊   | 387/564 [05:18<02:57,  1.00s/it] 69%|██████▉   | 388/564 [05:19<02:35,  1.13it/s]                                                 {'loss': 0.2165, 'grad_norm': 3.790508315539856, 'learning_rate': 6.471663619744058e-07, 'epoch': 2.06}
 69%|██████▉   | 388/564 [05:19<02:35,  1.13it/s] 69%|██████▉   | 389/564 [05:20<02:34,  1.13it/s]                                                 {'loss': 0.2881, 'grad_norm': 4.295622116698929, 'learning_rate': 6.435100548446069e-07, 'epoch': 2.07}
 69%|██████▉   | 389/564 [05:20<02:34,  1.13it/s] 69%|██████▉   | 390/564 [05:20<02:23,  1.21it/s]                                                 {'loss': 0.1919, 'grad_norm': 4.1302228659285225, 'learning_rate': 6.398537477148081e-07, 'epoch': 2.07}
 69%|██████▉   | 390/564 [05:20<02:23,  1.21it/s] 69%|██████▉   | 391/564 [05:21<02:19,  1.24it/s]                                                 {'loss': 0.4133, 'grad_norm': 4.7853940554932475, 'learning_rate': 6.361974405850091e-07, 'epoch': 2.08}
 69%|██████▉   | 391/564 [05:21<02:19,  1.24it/s] 70%|██████▉   | 392/564 [05:22<02:13,  1.29it/s]                                                 {'loss': 0.3377, 'grad_norm': 4.98849629768282, 'learning_rate': 6.325411334552102e-07, 'epoch': 2.09}
 70%|██████▉   | 392/564 [05:22<02:13,  1.29it/s] 70%|██████▉   | 393/564 [05:23<02:08,  1.34it/s]                                                 {'loss': 0.2542, 'grad_norm': 4.223293862112854, 'learning_rate': 6.288848263254113e-07, 'epoch': 2.09}
 70%|██████▉   | 393/564 [05:23<02:08,  1.34it/s] 70%|██████▉   | 394/564 [05:23<02:14,  1.26it/s]                                                 {'loss': 0.2927, 'grad_norm': 4.685914948326825, 'learning_rate': 6.252285191956124e-07, 'epoch': 2.1}
 70%|██████▉   | 394/564 [05:23<02:14,  1.26it/s] 70%|███████   | 395/564 [05:24<02:12,  1.28it/s]                                                 {'loss': 0.2833, 'grad_norm': 4.911171432464794, 'learning_rate': 6.215722120658134e-07, 'epoch': 2.1}
 70%|███████   | 395/564 [05:24<02:12,  1.28it/s] 70%|███████   | 396/564 [05:25<02:08,  1.30it/s]                                                 {'loss': 0.2529, 'grad_norm': 4.335729281166668, 'learning_rate': 6.179159049360146e-07, 'epoch': 2.11}
 70%|███████   | 396/564 [05:25<02:08,  1.30it/s] 70%|███████   | 397/564 [05:26<02:04,  1.34it/s]                                                 {'loss': 0.3218, 'grad_norm': 4.963368204034849, 'learning_rate': 6.142595978062158e-07, 'epoch': 2.11}
 70%|███████   | 397/564 [05:26<02:04,  1.34it/s] 71%|███████   | 398/564 [05:27<02:10,  1.28it/s]                                                 {'loss': 0.3459, 'grad_norm': 6.300964015168823, 'learning_rate': 6.106032906764168e-07, 'epoch': 2.12}
 71%|███████   | 398/564 [05:27<02:10,  1.28it/s] 71%|███████   | 399/564 [05:27<02:11,  1.26it/s]                                                 {'loss': 0.3116, 'grad_norm': 5.652105654617779, 'learning_rate': 6.069469835466179e-07, 'epoch': 2.12}
 71%|███████   | 399/564 [05:27<02:11,  1.26it/s] 71%|███████   | 400/564 [05:28<02:12,  1.24it/s]                                                 {'loss': 0.2653, 'grad_norm': 4.88487169175452, 'learning_rate': 6.032906764168189e-07, 'epoch': 2.13}
 71%|███████   | 400/564 [05:28<02:12,  1.24it/s] 71%|███████   | 401/564 [05:29<02:15,  1.20it/s]                                                 {'loss': 0.2691, 'grad_norm': 5.6144090832385105, 'learning_rate': 5.996343692870201e-07, 'epoch': 2.13}
 71%|███████   | 401/564 [05:29<02:15,  1.20it/s] 71%|███████▏  | 402/564 [05:30<02:18,  1.17it/s]                                                 {'loss': 0.3113, 'grad_norm': 4.593895730997207, 'learning_rate': 5.959780621572211e-07, 'epoch': 2.14}
 71%|███████▏  | 402/564 [05:30<02:18,  1.17it/s] 71%|███████▏  | 403/564 [05:31<02:17,  1.17it/s]                                                 {'loss': 0.355, 'grad_norm': 5.190951466388789, 'learning_rate': 5.923217550274223e-07, 'epoch': 2.14}
 71%|███████▏  | 403/564 [05:31<02:17,  1.17it/s] 72%|███████▏  | 404/564 [05:33<03:00,  1.13s/it]                                                 {'loss': 0.3119, 'grad_norm': 4.700313780291359, 'learning_rate': 5.886654478976234e-07, 'epoch': 2.15}
 72%|███████▏  | 404/564 [05:33<03:00,  1.13s/it] 72%|███████▏  | 405/564 [05:33<02:48,  1.06s/it]                                                 {'loss': 0.3199, 'grad_norm': 5.473540732717523, 'learning_rate': 5.850091407678245e-07, 'epoch': 2.15}
 72%|███████▏  | 405/564 [05:33<02:48,  1.06s/it] 72%|███████▏  | 406/564 [05:34<02:39,  1.01s/it]                                                 {'loss': 0.2919, 'grad_norm': 4.730923371405481, 'learning_rate': 5.813528336380256e-07, 'epoch': 2.16}
 72%|███████▏  | 406/564 [05:34<02:39,  1.01s/it] 72%|███████▏  | 407/564 [05:35<02:28,  1.06it/s]                                                 {'loss': 0.3294, 'grad_norm': 4.809813286895919, 'learning_rate': 5.776965265082266e-07, 'epoch': 2.16}
 72%|███████▏  | 407/564 [05:35<02:28,  1.06it/s] 72%|███████▏  | 408/564 [05:36<02:24,  1.08it/s]                                                 {'loss': 0.3317, 'grad_norm': 4.702920090549222, 'learning_rate': 5.740402193784278e-07, 'epoch': 2.17}
 72%|███████▏  | 408/564 [05:36<02:24,  1.08it/s] 73%|███████▎  | 409/564 [05:37<02:22,  1.09it/s]                                                 {'loss': 0.2392, 'grad_norm': 5.0236702066193475, 'learning_rate': 5.703839122486288e-07, 'epoch': 2.18}
 73%|███████▎  | 409/564 [05:37<02:22,  1.09it/s] 73%|███████▎  | 410/564 [05:38<02:23,  1.07it/s]                                                 {'loss': 0.3732, 'grad_norm': 3.9693095602511144, 'learning_rate': 5.6672760511883e-07, 'epoch': 2.18}
 73%|███████▎  | 410/564 [05:38<02:23,  1.07it/s] 73%|███████▎  | 411/564 [05:39<02:15,  1.13it/s]                                                 {'loss': 0.2835, 'grad_norm': 6.502840388489123, 'learning_rate': 5.63071297989031e-07, 'epoch': 2.19}
 73%|███████▎  | 411/564 [05:39<02:15,  1.13it/s] 73%|███████▎  | 412/564 [05:40<02:13,  1.14it/s]                                                 {'loss': 0.3293, 'grad_norm': 4.79378752061249, 'learning_rate': 5.594149908592322e-07, 'epoch': 2.19}
 73%|███████▎  | 412/564 [05:40<02:13,  1.14it/s] 73%|███████▎  | 413/564 [05:40<02:13,  1.13it/s]                                                 {'loss': 0.2976, 'grad_norm': 5.279810912672946, 'learning_rate': 5.557586837294332e-07, 'epoch': 2.2}
 73%|███████▎  | 413/564 [05:40<02:13,  1.13it/s] 73%|███████▎  | 414/564 [05:41<02:11,  1.14it/s]                                                 {'loss': 0.2623, 'grad_norm': 5.395928439796214, 'learning_rate': 5.521023765996343e-07, 'epoch': 2.2}
 73%|███████▎  | 414/564 [05:41<02:11,  1.14it/s] 74%|███████▎  | 415/564 [05:42<02:03,  1.20it/s]                                                 {'loss': 0.3584, 'grad_norm': 5.086161170445809, 'learning_rate': 5.484460694698354e-07, 'epoch': 2.21}
 74%|███████▎  | 415/564 [05:42<02:03,  1.20it/s] 74%|███████▍  | 416/564 [05:43<02:06,  1.17it/s]                                                 {'loss': 0.3481, 'grad_norm': 6.626775147310134, 'learning_rate': 5.447897623400365e-07, 'epoch': 2.21}
 74%|███████▍  | 416/564 [05:43<02:06,  1.17it/s] 74%|███████▍  | 417/564 [05:44<02:08,  1.15it/s]                                                 {'loss': 0.3092, 'grad_norm': 4.578506306996151, 'learning_rate': 5.411334552102377e-07, 'epoch': 2.22}
 74%|███████▍  | 417/564 [05:44<02:08,  1.15it/s] 74%|███████▍  | 418/564 [05:45<02:01,  1.20it/s]                                                 {'loss': 0.3189, 'grad_norm': 4.778521036181945, 'learning_rate': 5.374771480804387e-07, 'epoch': 2.22}
 74%|███████▍  | 418/564 [05:45<02:01,  1.20it/s] 74%|███████▍  | 419/564 [05:45<01:52,  1.29it/s]                                                 {'loss': 0.3608, 'grad_norm': 4.606606885775128, 'learning_rate': 5.338208409506399e-07, 'epoch': 2.23}
 74%|███████▍  | 419/564 [05:45<01:52,  1.29it/s] 74%|███████▍  | 420/564 [05:46<01:53,  1.27it/s]                                                 {'loss': 0.2702, 'grad_norm': 4.856986440766681, 'learning_rate': 5.301645338208409e-07, 'epoch': 2.23}
 74%|███████▍  | 420/564 [05:46<01:53,  1.27it/s] 75%|███████▍  | 421/564 [05:47<01:53,  1.26it/s]                                                 {'loss': 0.4519, 'grad_norm': 4.85776445563317, 'learning_rate': 5.26508226691042e-07, 'epoch': 2.24}
 75%|███████▍  | 421/564 [05:47<01:53,  1.26it/s] 75%|███████▍  | 422/564 [05:48<01:54,  1.24it/s]                                                 {'loss': 0.3329, 'grad_norm': 5.134580014465281, 'learning_rate': 5.22851919561243e-07, 'epoch': 2.24}
 75%|███████▍  | 422/564 [05:48<01:54,  1.24it/s] 75%|███████▌  | 423/564 [05:49<01:55,  1.23it/s]                                                 {'loss': 0.2582, 'grad_norm': 5.245968634048148, 'learning_rate': 5.191956124314442e-07, 'epoch': 2.25}
 75%|███████▌  | 423/564 [05:49<01:55,  1.23it/s] 75%|███████▌  | 424/564 [05:49<01:49,  1.28it/s]                                                 {'loss': 0.2984, 'grad_norm': 5.062677570696575, 'learning_rate': 5.155393053016453e-07, 'epoch': 2.26}
 75%|███████▌  | 424/564 [05:49<01:49,  1.28it/s] 75%|███████▌  | 425/564 [05:50<01:54,  1.21it/s]                                                 {'loss': 0.3103, 'grad_norm': 4.888347162117905, 'learning_rate': 5.118829981718464e-07, 'epoch': 2.26}
 75%|███████▌  | 425/564 [05:50<01:54,  1.21it/s] 76%|███████▌  | 426/564 [05:51<01:58,  1.17it/s]                                                 {'loss': 0.3214, 'grad_norm': 5.1454932466827, 'learning_rate': 5.082266910420476e-07, 'epoch': 2.27}
 76%|███████▌  | 426/564 [05:51<01:58,  1.17it/s] 76%|███████▌  | 427/564 [05:52<01:55,  1.18it/s]                                                 {'loss': 0.3193, 'grad_norm': 5.009894809786017, 'learning_rate': 5.045703839122486e-07, 'epoch': 2.27}
 76%|███████▌  | 427/564 [05:52<01:55,  1.18it/s] 76%|███████▌  | 428/564 [05:53<01:52,  1.21it/s]                                                 {'loss': 0.3202, 'grad_norm': 5.528760154218517, 'learning_rate': 5.009140767824497e-07, 'epoch': 2.28}
 76%|███████▌  | 428/564 [05:53<01:52,  1.21it/s] 76%|███████▌  | 429/564 [05:53<01:42,  1.31it/s]                                                 {'loss': 0.2884, 'grad_norm': 5.0635801259525275, 'learning_rate': 4.972577696526509e-07, 'epoch': 2.28}
 76%|███████▌  | 429/564 [05:53<01:42,  1.31it/s] 76%|███████▌  | 430/564 [05:54<01:43,  1.30it/s]                                                 {'loss': 0.2862, 'grad_norm': 5.430760076343597, 'learning_rate': 4.936014625228519e-07, 'epoch': 2.29}
 76%|███████▌  | 430/564 [05:54<01:43,  1.30it/s] 76%|███████▋  | 431/564 [05:55<01:46,  1.25it/s]                                                 {'loss': 0.3046, 'grad_norm': 4.693782730611927, 'learning_rate': 4.89945155393053e-07, 'epoch': 2.29}
 76%|███████▋  | 431/564 [05:55<01:46,  1.25it/s] 77%|███████▋  | 432/564 [05:56<01:45,  1.25it/s]                                                 {'loss': 0.2827, 'grad_norm': 5.221341831418521, 'learning_rate': 4.862888482632541e-07, 'epoch': 2.3}
 77%|███████▋  | 432/564 [05:56<01:45,  1.25it/s] 77%|███████▋  | 433/564 [05:57<01:48,  1.21it/s]                                                 {'loss': 0.2845, 'grad_norm': 4.770066857618028, 'learning_rate': 4.826325411334552e-07, 'epoch': 2.3}
 77%|███████▋  | 433/564 [05:57<01:48,  1.21it/s] 77%|███████▋  | 434/564 [05:58<01:49,  1.18it/s]                                                 {'loss': 0.3034, 'grad_norm': 5.4268101877716095, 'learning_rate': 4.789762340036563e-07, 'epoch': 2.31}
 77%|███████▋  | 434/564 [05:58<01:49,  1.18it/s] 77%|███████▋  | 435/564 [05:58<01:48,  1.18it/s]                                                 {'loss': 0.2457, 'grad_norm': 4.882079246749873, 'learning_rate': 4.7531992687385736e-07, 'epoch': 2.31}
 77%|███████▋  | 435/564 [05:58<01:48,  1.18it/s] 77%|███████▋  | 436/564 [05:59<01:50,  1.16it/s]                                                 {'loss': 0.3027, 'grad_norm': 6.098174674765996, 'learning_rate': 4.716636197440585e-07, 'epoch': 2.32}
 77%|███████▋  | 436/564 [05:59<01:50,  1.16it/s] 77%|███████▋  | 437/564 [06:00<01:50,  1.14it/s]                                                 {'loss': 0.2906, 'grad_norm': 4.854038646767941, 'learning_rate': 4.6800731261425957e-07, 'epoch': 2.32}
 77%|███████▋  | 437/564 [06:00<01:50,  1.14it/s] 78%|███████▊  | 438/564 [06:01<01:50,  1.14it/s]                                                 {'loss': 0.3135, 'grad_norm': 4.906345684827162, 'learning_rate': 4.6435100548446064e-07, 'epoch': 2.33}
 78%|███████▊  | 438/564 [06:01<01:50,  1.14it/s] 78%|███████▊  | 439/564 [06:02<01:50,  1.13it/s]                                                 {'loss': 0.2691, 'grad_norm': 6.3091176126202155, 'learning_rate': 4.606946983546618e-07, 'epoch': 2.34}
 78%|███████▊  | 439/564 [06:02<01:50,  1.13it/s] 78%|███████▊  | 440/564 [06:03<01:48,  1.14it/s]                                                 {'loss': 0.2836, 'grad_norm': 5.0063750592570555, 'learning_rate': 4.5703839122486285e-07, 'epoch': 2.34}
 78%|███████▊  | 440/564 [06:03<01:48,  1.14it/s] 78%|███████▊  | 441/564 [06:04<01:44,  1.17it/s]                                                 {'loss': 0.2681, 'grad_norm': 4.80459200210487, 'learning_rate': 4.53382084095064e-07, 'epoch': 2.35}
 78%|███████▊  | 441/564 [06:04<01:44,  1.17it/s] 78%|███████▊  | 442/564 [06:05<02:19,  1.14s/it]                                                 {'loss': 0.3104, 'grad_norm': 5.411248120321792, 'learning_rate': 4.4972577696526506e-07, 'epoch': 2.35}
 78%|███████▊  | 442/564 [06:05<02:19,  1.14s/it] 79%|███████▊  | 443/564 [06:06<02:09,  1.07s/it]                                                 {'loss': 0.3662, 'grad_norm': 6.312687814741591, 'learning_rate': 4.460694698354662e-07, 'epoch': 2.36}
 79%|███████▊  | 443/564 [06:06<02:09,  1.07s/it] 79%|███████▊  | 444/564 [06:07<02:02,  1.02s/it]                                                 {'loss': 0.2437, 'grad_norm': 5.2737722423697395, 'learning_rate': 4.4241316270566726e-07, 'epoch': 2.36}
 79%|███████▊  | 444/564 [06:07<02:02,  1.02s/it] 79%|███████▉  | 445/564 [06:08<01:52,  1.06it/s]                                                 {'loss': 0.2932, 'grad_norm': 5.163793304696043, 'learning_rate': 4.3875685557586834e-07, 'epoch': 2.37}
 79%|███████▉  | 445/564 [06:08<01:52,  1.06it/s] 79%|███████▉  | 446/564 [06:10<02:17,  1.17s/it]                                                 {'loss': 0.1684, 'grad_norm': 4.58938694867143, 'learning_rate': 4.3510054844606947e-07, 'epoch': 2.37}
 79%|███████▉  | 446/564 [06:10<02:17,  1.17s/it] 79%|███████▉  | 447/564 [06:11<02:05,  1.07s/it]                                                 {'loss': 0.3376, 'grad_norm': 6.346489062411506, 'learning_rate': 4.3144424131627054e-07, 'epoch': 2.38}
 79%|███████▉  | 447/564 [06:11<02:05,  1.07s/it] 79%|███████▉  | 448/564 [06:11<01:56,  1.00s/it]                                                 {'loss': 0.281, 'grad_norm': 4.976557651625635, 'learning_rate': 4.277879341864716e-07, 'epoch': 2.38}
 79%|███████▉  | 448/564 [06:11<01:56,  1.00s/it] 80%|███████▉  | 449/564 [06:12<01:45,  1.09it/s]                                                 {'loss': 0.254, 'grad_norm': 4.677927125657584, 'learning_rate': 4.241316270566727e-07, 'epoch': 2.39}
 80%|███████▉  | 449/564 [06:12<01:45,  1.09it/s] 80%|███████▉  | 450/564 [06:13<01:36,  1.18it/s]                                                 {'loss': 0.2646, 'grad_norm': 8.05458018745593, 'learning_rate': 4.2047531992687383e-07, 'epoch': 2.39}
 80%|███████▉  | 450/564 [06:13<01:36,  1.18it/s] 80%|███████▉  | 451/564 [06:14<01:33,  1.21it/s]                                                 {'loss': 0.2942, 'grad_norm': 5.552513417979056, 'learning_rate': 4.1681901279707496e-07, 'epoch': 2.4}
 80%|███████▉  | 451/564 [06:14<01:33,  1.21it/s] 80%|████████  | 452/564 [06:14<01:33,  1.20it/s]                                                 {'loss': 0.292, 'grad_norm': 5.934537392859631, 'learning_rate': 4.1316270566727603e-07, 'epoch': 2.4}
 80%|████████  | 452/564 [06:14<01:33,  1.20it/s] 80%|████████  | 453/564 [06:15<01:34,  1.17it/s]                                                 {'loss': 0.3513, 'grad_norm': 5.52418510818391, 'learning_rate': 4.0950639853747716e-07, 'epoch': 2.41}
 80%|████████  | 453/564 [06:15<01:34,  1.17it/s] 80%|████████  | 454/564 [06:16<01:34,  1.17it/s]                                                 {'loss': 0.3246, 'grad_norm': 9.286392482428942, 'learning_rate': 4.0585009140767824e-07, 'epoch': 2.41}
 80%|████████  | 454/564 [06:16<01:34,  1.17it/s] 81%|████████  | 455/564 [06:17<01:33,  1.17it/s]                                                 {'loss': 0.2946, 'grad_norm': 6.19201357928511, 'learning_rate': 4.021937842778793e-07, 'epoch': 2.42}
 81%|████████  | 455/564 [06:17<01:33,  1.17it/s] 81%|████████  | 456/564 [06:18<01:31,  1.18it/s]                                                 {'loss': 0.2172, 'grad_norm': 5.07318300647062, 'learning_rate': 3.9853747714808044e-07, 'epoch': 2.43}
 81%|████████  | 456/564 [06:18<01:31,  1.18it/s] 81%|████████  | 457/564 [06:19<01:31,  1.17it/s]                                                 {'loss': 0.301, 'grad_norm': 6.102141413264417, 'learning_rate': 3.948811700182815e-07, 'epoch': 2.43}
 81%|████████  | 457/564 [06:19<01:31,  1.17it/s] 81%|████████  | 458/564 [06:20<01:31,  1.16it/s]                                                 {'loss': 0.24, 'grad_norm': 4.894211807609186, 'learning_rate': 3.912248628884826e-07, 'epoch': 2.44}
 81%|████████  | 458/564 [06:20<01:31,  1.16it/s] 81%|████████▏ | 459/564 [06:20<01:29,  1.17it/s]                                                 {'loss': 0.2844, 'grad_norm': 5.235082083471842, 'learning_rate': 3.875685557586837e-07, 'epoch': 2.44}
 81%|████████▏ | 459/564 [06:20<01:29,  1.17it/s] 82%|████████▏ | 460/564 [06:21<01:25,  1.21it/s]                                                 {'loss': 0.213, 'grad_norm': 4.39939719494729, 'learning_rate': 3.839122486288848e-07, 'epoch': 2.45}
 82%|████████▏ | 460/564 [06:21<01:25,  1.21it/s] 82%|████████▏ | 461/564 [06:22<01:23,  1.24it/s]                                                 {'loss': 0.2776, 'grad_norm': 5.433940354341434, 'learning_rate': 3.8025594149908593e-07, 'epoch': 2.45}
 82%|████████▏ | 461/564 [06:22<01:23,  1.24it/s] 82%|████████▏ | 462/564 [06:24<01:51,  1.09s/it]                                                 {'loss': 0.2654, 'grad_norm': 4.998960990918827, 'learning_rate': 3.76599634369287e-07, 'epoch': 2.46}
 82%|████████▏ | 462/564 [06:24<01:51,  1.09s/it] 82%|████████▏ | 463/564 [06:25<01:42,  1.02s/it]                                                 {'loss': 0.2386, 'grad_norm': 5.697847329218594, 'learning_rate': 3.7294332723948814e-07, 'epoch': 2.46}
 82%|████████▏ | 463/564 [06:25<01:42,  1.02s/it] 82%|████████▏ | 464/564 [06:25<01:35,  1.05it/s]                                                 {'loss': 0.3182, 'grad_norm': 4.839177393567881, 'learning_rate': 3.692870201096892e-07, 'epoch': 2.47}
 82%|████████▏ | 464/564 [06:25<01:35,  1.05it/s] 82%|████████▏ | 465/564 [06:26<01:29,  1.10it/s]                                                 {'loss': 0.2512, 'grad_norm': 4.843853315230785, 'learning_rate': 3.656307129798903e-07, 'epoch': 2.47}
 82%|████████▏ | 465/564 [06:26<01:29,  1.10it/s] 83%|████████▎ | 466/564 [06:27<01:28,  1.11it/s]                                                 {'loss': 0.2872, 'grad_norm': 5.04823239920516, 'learning_rate': 3.6197440585009137e-07, 'epoch': 2.48}
 83%|████████▎ | 466/564 [06:27<01:28,  1.11it/s] 83%|████████▎ | 467/564 [06:28<01:27,  1.11it/s]                                                 {'loss': 0.2183, 'grad_norm': 4.87520060591648, 'learning_rate': 3.583180987202925e-07, 'epoch': 2.48}
 83%|████████▎ | 467/564 [06:28<01:27,  1.11it/s] 83%|████████▎ | 468/564 [06:30<01:51,  1.16s/it]                                                 {'loss': 0.2643, 'grad_norm': 5.5038883019927285, 'learning_rate': 3.5466179159049357e-07, 'epoch': 2.49}
 83%|████████▎ | 468/564 [06:30<01:51,  1.16s/it] 83%|████████▎ | 469/564 [06:31<01:40,  1.06s/it]                                                 {'loss': 0.2638, 'grad_norm': 4.59382294010185, 'learning_rate': 3.5100548446069465e-07, 'epoch': 2.49}
 83%|████████▎ | 469/564 [06:31<01:40,  1.06s/it] 83%|████████▎ | 470/564 [06:31<01:35,  1.02s/it]                                                 {'loss': 0.3244, 'grad_norm': 5.84568786336627, 'learning_rate': 3.473491773308958e-07, 'epoch': 2.5}
 83%|████████▎ | 470/564 [06:31<01:35,  1.02s/it] 84%|████████▎ | 471/564 [06:32<01:28,  1.05it/s]                                                 {'loss': 0.3957, 'grad_norm': 7.964690772195313, 'learning_rate': 3.436928702010969e-07, 'epoch': 2.51}
 84%|████████▎ | 471/564 [06:32<01:28,  1.05it/s] 84%|████████▎ | 472/564 [06:34<01:50,  1.21s/it]                                                 {'loss': 0.3263, 'grad_norm': 5.5344978221003585, 'learning_rate': 3.40036563071298e-07, 'epoch': 2.51}
 84%|████████▎ | 472/564 [06:34<01:50,  1.21s/it] 84%|████████▍ | 473/564 [06:35<01:42,  1.13s/it]                                                 {'loss': 0.2942, 'grad_norm': 5.122918361060019, 'learning_rate': 3.3638025594149906e-07, 'epoch': 2.52}
 84%|████████▍ | 473/564 [06:35<01:42,  1.13s/it] 84%|████████▍ | 474/564 [06:36<01:29,  1.01it/s]                                                 {'loss': 0.2948, 'grad_norm': 5.986632997461709, 'learning_rate': 3.327239488117002e-07, 'epoch': 2.52}
 84%|████████▍ | 474/564 [06:36<01:29,  1.01it/s] 84%|████████▍ | 475/564 [06:36<01:19,  1.12it/s]                                                 {'loss': 0.2996, 'grad_norm': 5.503565139824644, 'learning_rate': 3.2906764168190127e-07, 'epoch': 2.53}
 84%|████████▍ | 475/564 [06:36<01:19,  1.12it/s] 84%|████████▍ | 476/564 [06:37<01:17,  1.14it/s]                                                 {'loss': 0.314, 'grad_norm': 5.394705046614268, 'learning_rate': 3.2541133455210234e-07, 'epoch': 2.53}
 84%|████████▍ | 476/564 [06:37<01:17,  1.14it/s] 85%|████████▍ | 477/564 [06:38<01:17,  1.13it/s]                                                 {'loss': 0.3005, 'grad_norm': 5.483905694426983, 'learning_rate': 3.2175502742230347e-07, 'epoch': 2.54}
 85%|████████▍ | 477/564 [06:38<01:17,  1.13it/s] 85%|████████▍ | 478/564 [06:39<01:15,  1.14it/s]                                                 {'loss': 0.3372, 'grad_norm': 6.378791323451438, 'learning_rate': 3.1809872029250455e-07, 'epoch': 2.54}
 85%|████████▍ | 478/564 [06:39<01:15,  1.14it/s] 85%|████████▍ | 479/564 [06:40<01:14,  1.14it/s]                                                 {'loss': 0.2835, 'grad_norm': 5.740751029120274, 'learning_rate': 3.144424131627056e-07, 'epoch': 2.55}
 85%|████████▍ | 479/564 [06:40<01:14,  1.14it/s] 85%|████████▌ | 480/564 [06:41<01:11,  1.17it/s]                                                 {'loss': 0.2955, 'grad_norm': 4.715488260062944, 'learning_rate': 3.107861060329067e-07, 'epoch': 2.55}
 85%|████████▌ | 480/564 [06:41<01:11,  1.17it/s] 85%|████████▌ | 481/564 [06:41<01:09,  1.20it/s]                                                 {'loss': 0.2494, 'grad_norm': 8.317611195199397, 'learning_rate': 3.071297989031079e-07, 'epoch': 2.56}
 85%|████████▌ | 481/564 [06:41<01:09,  1.20it/s] 85%|████████▌ | 482/564 [06:42<01:10,  1.16it/s]                                                 {'loss': 0.2818, 'grad_norm': 5.998925724855179, 'learning_rate': 3.0347349177330896e-07, 'epoch': 2.56}
 85%|████████▌ | 482/564 [06:42<01:10,  1.16it/s] 86%|████████▌ | 483/564 [06:43<01:09,  1.17it/s]                                                 {'loss': 0.3304, 'grad_norm': 5.336793007466454, 'learning_rate': 2.9981718464351004e-07, 'epoch': 2.57}
 86%|████████▌ | 483/564 [06:43<01:09,  1.17it/s] 86%|████████▌ | 484/564 [06:44<01:05,  1.23it/s]                                                 {'loss': 0.2638, 'grad_norm': 5.3406829470272426, 'learning_rate': 2.9616087751371117e-07, 'epoch': 2.57}
 86%|████████▌ | 484/564 [06:44<01:05,  1.23it/s] 86%|████████▌ | 485/564 [06:45<00:59,  1.33it/s]                                                 {'loss': 0.3239, 'grad_norm': 5.588025773787794, 'learning_rate': 2.9250457038391224e-07, 'epoch': 2.58}
 86%|████████▌ | 485/564 [06:45<00:59,  1.33it/s] 86%|████████▌ | 486/564 [06:45<00:59,  1.31it/s]                                                 {'loss': 0.2425, 'grad_norm': 4.701207265505373, 'learning_rate': 2.888482632541133e-07, 'epoch': 2.59}
 86%|████████▌ | 486/564 [06:45<00:59,  1.31it/s] 86%|████████▋ | 487/564 [06:46<00:59,  1.29it/s]                                                 {'loss': 0.3776, 'grad_norm': 5.402315767693348, 'learning_rate': 2.851919561243144e-07, 'epoch': 2.59}
 86%|████████▋ | 487/564 [06:46<00:59,  1.29it/s] 87%|████████▋ | 488/564 [06:47<00:58,  1.30it/s]                                                 {'loss': 0.301, 'grad_norm': 6.19195303851805, 'learning_rate': 2.815356489945155e-07, 'epoch': 2.6}
 87%|████████▋ | 488/564 [06:47<00:58,  1.30it/s] 87%|████████▋ | 489/564 [06:48<00:57,  1.29it/s]                                                 {'loss': 0.2995, 'grad_norm': 5.371528766708928, 'learning_rate': 2.778793418647166e-07, 'epoch': 2.6}
 87%|████████▋ | 489/564 [06:48<00:57,  1.29it/s] 87%|████████▋ | 490/564 [06:48<00:56,  1.31it/s]                                                 {'loss': 0.2521, 'grad_norm': 4.567858918000611, 'learning_rate': 2.742230347349177e-07, 'epoch': 2.61}
 87%|████████▋ | 490/564 [06:48<00:56,  1.31it/s] 87%|████████▋ | 491/564 [06:49<00:58,  1.24it/s]                                                 {'loss': 0.2144, 'grad_norm': 4.674771929477173, 'learning_rate': 2.7056672760511886e-07, 'epoch': 2.61}
 87%|████████▋ | 491/564 [06:49<00:58,  1.24it/s] 87%|████████▋ | 492/564 [06:51<01:06,  1.08it/s]                                                 {'loss': 0.3436, 'grad_norm': 5.890299494130269, 'learning_rate': 2.6691042047531994e-07, 'epoch': 2.62}
 87%|████████▋ | 492/564 [06:51<01:06,  1.08it/s] 87%|████████▋ | 493/564 [06:51<01:05,  1.08it/s]                                                 {'loss': 0.2357, 'grad_norm': 4.430348491086389, 'learning_rate': 2.63254113345521e-07, 'epoch': 2.62}
 87%|████████▋ | 493/564 [06:51<01:05,  1.08it/s] 88%|████████▊ | 494/564 [06:52<01:04,  1.09it/s]                                                 {'loss': 0.2655, 'grad_norm': 5.762291799792025, 'learning_rate': 2.595978062157221e-07, 'epoch': 2.63}
 88%|████████▊ | 494/564 [06:52<01:04,  1.09it/s] 88%|████████▊ | 495/564 [06:53<01:02,  1.11it/s]                                                 {'loss': 0.2887, 'grad_norm': 5.540576957503879, 'learning_rate': 2.559414990859232e-07, 'epoch': 2.63}
 88%|████████▊ | 495/564 [06:53<01:02,  1.11it/s] 88%|████████▊ | 496/564 [06:54<01:01,  1.11it/s]                                                 {'loss': 0.3216, 'grad_norm': 4.559385605331467, 'learning_rate': 2.522851919561243e-07, 'epoch': 2.64}
 88%|████████▊ | 496/564 [06:54<01:01,  1.11it/s] 88%|████████▊ | 497/564 [06:55<01:04,  1.05it/s]                                                 {'loss': 0.2954, 'grad_norm': 5.15606983575422, 'learning_rate': 2.486288848263254e-07, 'epoch': 2.64}
 88%|████████▊ | 497/564 [06:55<01:04,  1.05it/s] 88%|████████▊ | 498/564 [06:56<01:01,  1.07it/s]                                                 {'loss': 0.2821, 'grad_norm': 4.577904609532976, 'learning_rate': 2.449725776965265e-07, 'epoch': 2.65}
 88%|████████▊ | 498/564 [06:56<01:01,  1.07it/s] 88%|████████▊ | 499/564 [06:57<00:58,  1.10it/s]                                                 {'loss': 0.2816, 'grad_norm': 5.1234625741427875, 'learning_rate': 2.413162705667276e-07, 'epoch': 2.65}
 88%|████████▊ | 499/564 [06:57<00:58,  1.10it/s] 89%|████████▊ | 500/564 [06:58<00:55,  1.16it/s]                                                 {'loss': 0.2849, 'grad_norm': 5.4035301842146986, 'learning_rate': 2.3765996343692868e-07, 'epoch': 2.66}
 89%|████████▊ | 500/564 [06:58<00:55,  1.16it/s]/data/home/yunyun/miniconda3/envs/newtorch2/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 89%|████████▉ | 501/564 [07:59<20:00, 19.05s/it]                                                 {'loss': 0.2026, 'grad_norm': 5.6508547404676746, 'learning_rate': 2.3400365630712978e-07, 'epoch': 2.66}
 89%|████████▉ | 501/564 [07:59<20:00, 19.05s/it] 89%|████████▉ | 502/564 [08:00<13:58, 13.52s/it]                                                 {'loss': 0.2699, 'grad_norm': 5.483430435203499, 'learning_rate': 2.303473491773309e-07, 'epoch': 2.67}
 89%|████████▉ | 502/564 [08:00<13:58, 13.52s/it] 89%|████████▉ | 503/564 [08:00<09:48,  9.64s/it]                                                 {'loss': 0.3267, 'grad_norm': 5.744851727994486, 'learning_rate': 2.26691042047532e-07, 'epoch': 2.68}
 89%|████████▉ | 503/564 [08:00<09:48,  9.64s/it] 89%|████████▉ | 504/564 [08:01<06:55,  6.93s/it]                                                 {'loss': 0.2975, 'grad_norm': 5.958155647808984, 'learning_rate': 2.230347349177331e-07, 'epoch': 2.68}
 89%|████████▉ | 504/564 [08:01<06:55,  6.93s/it] 90%|████████▉ | 505/564 [08:02<05:00,  5.10s/it]                                                 {'loss': 0.2464, 'grad_norm': 4.9575385031512935, 'learning_rate': 2.1937842778793417e-07, 'epoch': 2.69}
 90%|████████▉ | 505/564 [08:02<05:00,  5.10s/it] 90%|████████▉ | 506/564 [08:03<03:41,  3.81s/it]                                                 {'loss': 0.2847, 'grad_norm': 4.882358312135808, 'learning_rate': 2.1572212065813527e-07, 'epoch': 2.69}
 90%|████████▉ | 506/564 [08:03<03:41,  3.81s/it] 90%|████████▉ | 507/564 [08:03<02:46,  2.92s/it]                                                 {'loss': 0.2729, 'grad_norm': 4.957882730760336, 'learning_rate': 2.1206581352833635e-07, 'epoch': 2.7}
 90%|████████▉ | 507/564 [08:03<02:46,  2.92s/it] 90%|█████████ | 508/564 [08:04<02:08,  2.29s/it]                                                 {'loss': 0.225, 'grad_norm': 5.08589278819681, 'learning_rate': 2.0840950639853748e-07, 'epoch': 2.7}
 90%|█████████ | 508/564 [08:04<02:08,  2.29s/it] 90%|█████████ | 509/564 [08:05<01:41,  1.85s/it]                                                 {'loss': 0.3853, 'grad_norm': 6.984004633106633, 'learning_rate': 2.0475319926873858e-07, 'epoch': 2.71}
 90%|█████████ | 509/564 [08:05<01:41,  1.85s/it] 90%|█████████ | 510/564 [08:06<01:23,  1.55s/it]                                                 {'loss': 0.2762, 'grad_norm': 4.925551485470871, 'learning_rate': 2.0109689213893966e-07, 'epoch': 2.71}
 90%|█████████ | 510/564 [08:06<01:23,  1.55s/it] 91%|█████████ | 511/564 [08:07<01:10,  1.33s/it]                                                 {'loss': 0.2972, 'grad_norm': 4.900742288195001, 'learning_rate': 1.9744058500914076e-07, 'epoch': 2.72}
 91%|█████████ | 511/564 [08:07<01:10,  1.33s/it] 91%|█████████ | 512/564 [08:08<01:01,  1.18s/it]                                                 {'loss': 0.3039, 'grad_norm': 6.100169997083476, 'learning_rate': 1.9378427787934184e-07, 'epoch': 2.72}
 91%|█████████ | 512/564 [08:08<01:01,  1.18s/it] 91%|█████████ | 513/564 [08:08<00:51,  1.01s/it]                                                 {'loss': 0.3651, 'grad_norm': 4.562462986539854, 'learning_rate': 1.9012797074954297e-07, 'epoch': 2.73}
 91%|█████████ | 513/564 [08:08<00:51,  1.01s/it] 91%|█████████ | 514/564 [08:09<00:44,  1.12it/s]                                                 {'loss': 0.3372, 'grad_norm': 4.8409567046930615, 'learning_rate': 1.8647166361974407e-07, 'epoch': 2.73}
 91%|█████████ | 514/564 [08:09<00:44,  1.12it/s] 91%|█████████▏| 515/564 [08:09<00:39,  1.24it/s]                                                 {'loss': 0.3057, 'grad_norm': 5.216206660743967, 'learning_rate': 1.8281535648994515e-07, 'epoch': 2.74}
 91%|█████████▏| 515/564 [08:09<00:39,  1.24it/s] 91%|█████████▏| 516/564 [08:10<00:38,  1.23it/s]                                                 {'loss': 0.3338, 'grad_norm': 5.2864321352004495, 'learning_rate': 1.7915904936014625e-07, 'epoch': 2.74}
 91%|█████████▏| 516/564 [08:10<00:38,  1.23it/s] 92%|█████████▏| 517/564 [08:11<00:35,  1.33it/s]                                                 {'loss': 0.3335, 'grad_norm': 5.408407225841896, 'learning_rate': 1.7550274223034732e-07, 'epoch': 2.75}
 92%|█████████▏| 517/564 [08:11<00:35,  1.33it/s] 92%|█████████▏| 518/564 [08:12<00:35,  1.29it/s]                                                 {'loss': 0.2794, 'grad_norm': 5.345824975631078, 'learning_rate': 1.7184643510054845e-07, 'epoch': 2.76}
 92%|█████████▏| 518/564 [08:12<00:35,  1.29it/s] 92%|█████████▏| 519/564 [08:12<00:35,  1.28it/s]                                                 {'loss': 0.3387, 'grad_norm': 5.94371825431607, 'learning_rate': 1.6819012797074953e-07, 'epoch': 2.76}
 92%|█████████▏| 519/564 [08:12<00:35,  1.28it/s] 92%|█████████▏| 520/564 [08:13<00:34,  1.26it/s]                                                 {'loss': 0.3461, 'grad_norm': 5.203978594032337, 'learning_rate': 1.6453382084095063e-07, 'epoch': 2.77}
 92%|█████████▏| 520/564 [08:13<00:34,  1.26it/s] 92%|█████████▏| 521/564 [08:14<00:31,  1.36it/s]                                                 {'loss': 0.2886, 'grad_norm': 6.14041865692972, 'learning_rate': 1.6087751371115174e-07, 'epoch': 2.77}
 92%|█████████▏| 521/564 [08:14<00:31,  1.36it/s] 93%|█████████▎| 522/564 [08:15<00:31,  1.32it/s]                                                 {'loss': 0.3736, 'grad_norm': 4.976952277770994, 'learning_rate': 1.572212065813528e-07, 'epoch': 2.78}
 93%|█████████▎| 522/564 [08:15<00:31,  1.32it/s] 93%|█████████▎| 523/564 [08:15<00:29,  1.41it/s]                                                 {'loss': 0.342, 'grad_norm': 4.769909689461312, 'learning_rate': 1.5356489945155394e-07, 'epoch': 2.78}
 93%|█████████▎| 523/564 [08:15<00:29,  1.41it/s] 93%|█████████▎| 524/564 [08:16<00:28,  1.41it/s]                                                 {'loss': 0.28, 'grad_norm': 5.432518122701937, 'learning_rate': 1.4990859232175502e-07, 'epoch': 2.79}
 93%|█████████▎| 524/564 [08:16<00:28,  1.41it/s] 93%|█████████▎| 525/564 [08:17<00:26,  1.48it/s]                                                 {'loss': 0.3138, 'grad_norm': 5.357545679753682, 'learning_rate': 1.4625228519195612e-07, 'epoch': 2.79}
 93%|█████████▎| 525/564 [08:17<00:26,  1.48it/s] 93%|█████████▎| 526/564 [08:17<00:27,  1.38it/s]                                                 {'loss': 0.2575, 'grad_norm': 5.410394784073392, 'learning_rate': 1.425959780621572e-07, 'epoch': 2.8}
 93%|█████████▎| 526/564 [08:17<00:27,  1.38it/s] 93%|█████████▎| 527/564 [08:18<00:27,  1.33it/s]                                                 {'loss': 0.3101, 'grad_norm': 6.133809507641586, 'learning_rate': 1.389396709323583e-07, 'epoch': 2.8}
 93%|█████████▎| 527/564 [08:18<00:27,  1.33it/s] 94%|█████████▎| 528/564 [08:19<00:27,  1.29it/s]                                                 {'loss': 0.3157, 'grad_norm': 5.504034519920139, 'learning_rate': 1.3528336380255943e-07, 'epoch': 2.81}
 94%|█████████▎| 528/564 [08:19<00:27,  1.29it/s] 94%|█████████▍| 529/564 [08:20<00:25,  1.39it/s]                                                 {'loss': 0.2537, 'grad_norm': 4.774354556126711, 'learning_rate': 1.316270566727605e-07, 'epoch': 2.81}
 94%|█████████▍| 529/564 [08:20<00:25,  1.39it/s] 94%|█████████▍| 530/564 [08:20<00:24,  1.40it/s]                                                 {'loss': 0.243, 'grad_norm': 5.142852760573516, 'learning_rate': 1.279707495429616e-07, 'epoch': 2.82}
 94%|█████████▍| 530/564 [08:20<00:24,  1.40it/s] 94%|█████████▍| 531/564 [08:21<00:22,  1.47it/s]                                                 {'loss': 0.2375, 'grad_norm': 4.565113795614114, 'learning_rate': 1.243144424131627e-07, 'epoch': 2.82}
 94%|█████████▍| 531/564 [08:21<00:22,  1.47it/s] 94%|█████████▍| 532/564 [08:22<00:23,  1.38it/s]                                                 {'loss': 0.2333, 'grad_norm': 4.719811836818177, 'learning_rate': 1.206581352833638e-07, 'epoch': 2.83}
 94%|█████████▍| 532/564 [08:22<00:23,  1.38it/s] 95%|█████████▍| 533/564 [08:23<00:22,  1.39it/s]                                                 {'loss': 0.2398, 'grad_norm': 5.233117680762945, 'learning_rate': 1.1700182815356489e-07, 'epoch': 2.84}
 95%|█████████▍| 533/564 [08:23<00:22,  1.39it/s] 95%|█████████▍| 534/564 [08:23<00:22,  1.34it/s]                                                 {'loss': 0.339, 'grad_norm': 6.590469123039943, 'learning_rate': 1.13345521023766e-07, 'epoch': 2.84}
 95%|█████████▍| 534/564 [08:23<00:22,  1.34it/s] 95%|█████████▍| 535/564 [08:24<00:20,  1.42it/s]                                                 {'loss': 0.2004, 'grad_norm': 4.305455030720111, 'learning_rate': 1.0968921389396708e-07, 'epoch': 2.85}
 95%|█████████▍| 535/564 [08:24<00:20,  1.42it/s] 95%|█████████▌| 536/564 [08:25<00:18,  1.50it/s]                                                 {'loss': 0.3043, 'grad_norm': 5.718859217575516, 'learning_rate': 1.0603290676416817e-07, 'epoch': 2.85}
 95%|█████████▌| 536/564 [08:25<00:18,  1.50it/s] 95%|█████████▌| 537/564 [08:25<00:17,  1.55it/s]                                                 {'loss': 0.2883, 'grad_norm': 5.692289740810407, 'learning_rate': 1.0237659963436929e-07, 'epoch': 2.86}
 95%|█████████▌| 537/564 [08:25<00:17,  1.55it/s] 95%|█████████▌| 538/564 [08:26<00:16,  1.59it/s]                                                 {'loss': 0.3202, 'grad_norm': 4.9517972856851, 'learning_rate': 9.872029250457038e-08, 'epoch': 2.86}
 95%|█████████▌| 538/564 [08:26<00:16,  1.59it/s] 96%|█████████▌| 539/564 [08:26<00:15,  1.61it/s]                                                 {'loss': 0.4331, 'grad_norm': 5.4572688400690454, 'learning_rate': 9.506398537477148e-08, 'epoch': 2.87}
 96%|█████████▌| 539/564 [08:26<00:15,  1.61it/s] 96%|█████████▌| 540/564 [08:27<00:15,  1.50it/s]                                                 {'loss': 0.2608, 'grad_norm': 5.189295558093043, 'learning_rate': 9.140767824497257e-08, 'epoch': 2.87}
 96%|█████████▌| 540/564 [08:27<00:15,  1.50it/s] 96%|█████████▌| 541/564 [08:28<00:16,  1.40it/s]                                                 {'loss': 0.2358, 'grad_norm': 4.364452825656716, 'learning_rate': 8.775137111517366e-08, 'epoch': 2.88}
 96%|█████████▌| 541/564 [08:28<00:16,  1.40it/s] 96%|█████████▌| 542/564 [08:29<00:16,  1.34it/s]                                                 {'loss': 0.2523, 'grad_norm': 5.010252109897575, 'learning_rate': 8.409506398537477e-08, 'epoch': 2.88}
 96%|█████████▌| 542/564 [08:29<00:16,  1.34it/s] 96%|█████████▋| 543/564 [08:29<00:14,  1.43it/s]                                                 {'loss': 0.2882, 'grad_norm': 6.0770168852638085, 'learning_rate': 8.043875685557587e-08, 'epoch': 2.89}
 96%|█████████▋| 543/564 [08:29<00:14,  1.43it/s] 96%|█████████▋| 544/564 [08:30<00:14,  1.36it/s]                                                 {'loss': 0.2781, 'grad_norm': 5.297380127654924, 'learning_rate': 7.678244972577697e-08, 'epoch': 2.89}
 96%|█████████▋| 544/564 [08:30<00:14,  1.36it/s] 97%|█████████▋| 545/564 [08:31<00:14,  1.31it/s]                                                 {'loss': 0.2384, 'grad_norm': 5.8099952140793105, 'learning_rate': 7.312614259597806e-08, 'epoch': 2.9}
 97%|█████████▋| 545/564 [08:31<00:14,  1.31it/s] 97%|█████████▋| 546/564 [08:32<00:12,  1.41it/s]                                                 {'loss': 0.2764, 'grad_norm': 5.137865760292534, 'learning_rate': 6.946983546617915e-08, 'epoch': 2.9}
 97%|█████████▋| 546/564 [08:32<00:12,  1.41it/s] 97%|█████████▋| 547/564 [08:32<00:12,  1.35it/s]                                                 {'loss': 0.3482, 'grad_norm': 5.665814098069199, 'learning_rate': 6.581352833638025e-08, 'epoch': 2.91}
 97%|█████████▋| 547/564 [08:32<00:12,  1.35it/s] 97%|█████████▋| 548/564 [08:33<00:11,  1.43it/s]                                                 {'loss': 0.2507, 'grad_norm': 5.289222965857453, 'learning_rate': 6.215722120658136e-08, 'epoch': 2.91}
 97%|█████████▋| 548/564 [08:33<00:11,  1.43it/s] 97%|█████████▋| 549/564 [08:34<00:09,  1.50it/s]                                                 {'loss': 0.2705, 'grad_norm': 5.082411353107645, 'learning_rate': 5.8500914076782446e-08, 'epoch': 2.92}
 97%|█████████▋| 549/564 [08:34<00:09,  1.50it/s] 98%|█████████▊| 550/564 [08:34<00:09,  1.56it/s]                                                 {'loss': 0.336, 'grad_norm': 4.986086416803954, 'learning_rate': 5.484460694698354e-08, 'epoch': 2.93}
 98%|█████████▊| 550/564 [08:34<00:09,  1.56it/s] 98%|█████████▊| 551/564 [08:35<00:09,  1.44it/s]                                                 {'loss': 0.3248, 'grad_norm': 5.303527210267014, 'learning_rate': 5.1188299817184645e-08, 'epoch': 2.93}
 98%|█████████▊| 551/564 [08:35<00:09,  1.44it/s] 98%|█████████▊| 552/564 [08:36<00:08,  1.36it/s]                                                 {'loss': 0.3297, 'grad_norm': 5.01148872239722, 'learning_rate': 4.753199268738574e-08, 'epoch': 2.94}
 98%|█████████▊| 552/564 [08:36<00:08,  1.36it/s] 98%|█████████▊| 553/564 [08:36<00:07,  1.44it/s]                                                 {'loss': 0.1982, 'grad_norm': 4.615260516563062, 'learning_rate': 4.387568555758683e-08, 'epoch': 2.94}
 98%|█████████▊| 553/564 [08:36<00:07,  1.44it/s] 98%|█████████▊| 554/564 [08:37<00:06,  1.48it/s]                                                 {'loss': 0.3402, 'grad_norm': 5.070895633780165, 'learning_rate': 4.0219378427787934e-08, 'epoch': 2.95}
 98%|█████████▊| 554/564 [08:37<00:06,  1.48it/s] 98%|█████████▊| 555/564 [08:38<00:06,  1.39it/s]                                                 {'loss': 0.3409, 'grad_norm': 6.856003588218196, 'learning_rate': 3.656307129798903e-08, 'epoch': 2.95}
 98%|█████████▊| 555/564 [08:38<00:06,  1.39it/s] 99%|█████████▊| 556/564 [08:39<00:05,  1.37it/s]                                                 {'loss': 0.2459, 'grad_norm': 4.819891905298909, 'learning_rate': 3.290676416819013e-08, 'epoch': 2.96}
 99%|█████████▊| 556/564 [08:39<00:05,  1.37it/s] 99%|█████████▉| 557/564 [08:39<00:05,  1.33it/s]                                                 {'loss': 0.3439, 'grad_norm': 5.798944378742469, 'learning_rate': 2.9250457038391223e-08, 'epoch': 2.96}
 99%|█████████▉| 557/564 [08:39<00:05,  1.33it/s] 99%|█████████▉| 558/564 [08:40<00:04,  1.29it/s]                                                 {'loss': 0.2304, 'grad_norm': 4.874797368568888, 'learning_rate': 2.5594149908592323e-08, 'epoch': 2.97}
 99%|█████████▉| 558/564 [08:40<00:04,  1.29it/s] 99%|█████████▉| 559/564 [08:41<00:03,  1.39it/s]                                                 {'loss': 0.3225, 'grad_norm': 6.840661003643192, 'learning_rate': 2.1937842778793416e-08, 'epoch': 2.97}
 99%|█████████▉| 559/564 [08:41<00:03,  1.39it/s] 99%|█████████▉| 560/564 [08:41<00:02,  1.47it/s]                                                 {'loss': 0.27, 'grad_norm': 5.1834114178707535, 'learning_rate': 1.8281535648994515e-08, 'epoch': 2.98}
 99%|█████████▉| 560/564 [08:41<00:02,  1.47it/s] 99%|█████████▉| 561/564 [08:42<00:01,  1.53it/s]                                                 {'loss': 0.3728, 'grad_norm': 5.697501426478928, 'learning_rate': 1.4625228519195612e-08, 'epoch': 2.98}
 99%|█████████▉| 561/564 [08:42<00:01,  1.53it/s]100%|█████████▉| 562/564 [08:43<00:01,  1.57it/s]                                                 {'loss': 0.3463, 'grad_norm': 6.52366446763362, 'learning_rate': 1.0968921389396708e-08, 'epoch': 2.99}
100%|█████████▉| 562/564 [08:43<00:01,  1.57it/s]100%|█████████▉| 563/564 [08:43<00:00,  1.61it/s]                                                 {'loss': 0.2619, 'grad_norm': 6.090979852438438, 'learning_rate': 7.312614259597806e-09, 'epoch': 2.99}
100%|█████████▉| 563/564 [08:43<00:00,  1.61it/s]100%|██████████| 564/564 [08:44<00:00,  1.63it/s]                                                 {'loss': 0.2886, 'grad_norm': 4.7734873887111675, 'learning_rate': 3.656307129798903e-09, 'epoch': 3.0}
100%|██████████| 564/564 [08:44<00:00,  1.63it/s]                                                 {'train_runtime': 583.9679, 'train_samples_per_second': 77.059, 'train_steps_per_second': 0.966, 'train_loss': 0.4801037331633534, 'epoch': 3.0}
100%|██████████| 564/564 [09:43<00:00,  1.63it/s]100%|██████████| 564/564 [09:43<00:00,  1.04s/it]
[2024-08-26 17:21:15,239] [INFO] [launch.py:351:main] Process 951610 exits successfully.
[2024-08-26 17:21:16,240] [INFO] [launch.py:351:main] Process 951608 exits successfully.
[2024-08-26 17:21:16,240] [INFO] [launch.py:351:main] Process 951611 exits successfully.
[2024-08-26 17:21:16,240] [INFO] [launch.py:351:main] Process 951607 exits successfully.
[2024-08-26 17:21:16,240] [INFO] [launch.py:351:main] Process 951609 exits successfully.
[2024-08-26 17:21:17,241] [INFO] [launch.py:351:main] Process 951605 exits successfully.
[2024-08-26 17:21:17,241] [INFO] [launch.py:351:main] Process 951606 exits successfully.
[2024-08-26 17:21:41,244] [INFO] [launch.py:351:main] Process 951604 exits successfully.
